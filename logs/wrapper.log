============================================
Spark Streaming Auto-Restart Wrapper
============================================
[Thu Feb 12 17:49:15 UTC 2026] Starting Spark Streaming attempt...
============================================
Starting Spark Streaming Enrichment Service
============================================
[Thu Feb 12 17:49:15 UTC 2026] Waiting for Kafka and topics to be ready...
  Waiting 40 seconds for Kafka initialization...
✓ Kafka should be ready now
[Thu Feb 12 17:49:55 UTC 2026] Submitting Spark Streaming job...
============================================================
Starting Spark Streaming Enrichment
============================================================
26/02/12 17:49:59 INFO SparkContext: Running Spark version 3.5.1
26/02/12 17:49:59 INFO SparkContext: OS info Linux, 6.17.0-14-generic, amd64
26/02/12 17:49:59 INFO SparkContext: Java version 17.0.17
26/02/12 17:49:59 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
26/02/12 17:49:59 INFO ResourceUtils: ==============================================================
26/02/12 17:49:59 INFO ResourceUtils: No custom resources configured for spark.driver.
26/02/12 17:49:59 INFO ResourceUtils: ==============================================================
26/02/12 17:49:59 INFO SparkContext: Submitted application: FlightStreamEnrichment
26/02/12 17:49:59 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
26/02/12 17:49:59 INFO ResourceProfile: Limiting resource is cpu
26/02/12 17:49:59 INFO ResourceProfileManager: Added ResourceProfile id: 0
26/02/12 17:50:00 INFO SecurityManager: Changing view acls to: root
26/02/12 17:50:00 INFO SecurityManager: Changing modify acls to: root
26/02/12 17:50:00 INFO SecurityManager: Changing view acls groups to: 
26/02/12 17:50:00 INFO SecurityManager: Changing modify acls groups to: 
26/02/12 17:50:00 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY
26/02/12 17:50:00 INFO Utils: Successfully started service 'sparkDriver' on port 43405.
26/02/12 17:50:00 INFO SparkEnv: Registering MapOutputTracker
26/02/12 17:50:00 INFO SparkEnv: Registering BlockManagerMaster
26/02/12 17:50:00 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
26/02/12 17:50:00 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
26/02/12 17:50:00 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
26/02/12 17:50:00 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-01de016d-15c3-4410-a1db-e352f690415d
26/02/12 17:50:00 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
26/02/12 17:50:00 INFO SparkEnv: Registering OutputCommitCoordinator
26/02/12 17:50:00 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
26/02/12 17:50:00 INFO Utils: Successfully started service 'SparkUI' on port 4040.
26/02/12 17:50:01 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...
26/02/12 17:50:01 INFO TransportClientFactory: Successfully created connection to spark-master/172.18.0.11:7077 after 36 ms (0 ms spent in bootstraps)
26/02/12 17:50:01 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20260212175001-0000
26/02/12 17:50:01 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 44075.
26/02/12 17:50:01 INFO NettyBlockTransferService: Server created on spark-master 0.0.0.0:44075
26/02/12 17:50:01 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
26/02/12 17:50:01 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, spark-master, 44075, None)
26/02/12 17:50:01 INFO BlockManagerMasterEndpoint: Registering block manager spark-master:44075 with 434.4 MiB RAM, BlockManagerId(driver, spark-master, 44075, None)
26/02/12 17:50:01 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, spark-master, 44075, None)
26/02/12 17:50:01 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, spark-master, 44075, None)
26/02/12 17:50:01 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20260212175001-0000/0 on worker-20260212174931-172.18.0.15-43145 (172.18.0.15:43145) with 2 core(s)
26/02/12 17:50:01 INFO StandaloneSchedulerBackend: Granted executor ID app-20260212175001-0000/0 on hostPort 172.18.0.15:43145 with 2 core(s), 1024.0 MiB RAM
26/02/12 17:50:01 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20260212175001-0000/1 on worker-20260212174931-172.18.0.14-40233 (172.18.0.14:40233) with 2 core(s)
26/02/12 17:50:01 INFO StandaloneSchedulerBackend: Granted executor ID app-20260212175001-0000/1 on hostPort 172.18.0.14:40233 with 2 core(s), 1024.0 MiB RAM
26/02/12 17:50:02 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20260212175001-0000/1 is now RUNNING
26/02/12 17:50:02 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20260212175001-0000/0 is now RUNNING
26/02/12 17:50:02 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
✓ Spark session created
✓ Reading from: aviation-india-states
✓ Writing to: aviation-enriched-states
[DEBUG] Creating route mapping table...
Loading routes from CSV: /opt/airflow/data/routes.csv
26/02/12 17:50:03 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
26/02/12 17:50:03 INFO SharedState: Warehouse path is 'file:/opt/airflow/spark-warehouse'.
26/02/12 17:50:05 INFO InMemoryFileIndex: It took 117 ms to list leaf files for 1 paths.
26/02/12 17:50:05 INFO InMemoryFileIndex: It took 8 ms to list leaf files for 1 paths.
26/02/12 17:50:08 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.14:50138) with ID 1,  ResourceProfileId 0
26/02/12 17:50:08 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.15:55094) with ID 0,  ResourceProfileId 0
26/02/12 17:50:09 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.14:44081 with 434.4 MiB RAM, BlockManagerId(1, 172.18.0.14, 44081, None)
26/02/12 17:50:09 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.15:39271 with 434.4 MiB RAM, BlockManagerId(0, 172.18.0.15, 39271, None)
26/02/12 17:50:10 INFO FileSourceStrategy: Pushed Filters: 
26/02/12 17:50:10 INFO FileSourceStrategy: Post-Scan Filters: (length(trim(value#0, None)) > 0)
26/02/12 17:50:11 INFO CodeGenerator: Code generated in 359.528182 ms
26/02/12 17:50:11 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 200.4 KiB, free 434.2 MiB)
26/02/12 17:50:11 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 434.2 MiB)
26/02/12 17:50:11 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on spark-master:44075 (size: 34.5 KiB, free: 434.4 MiB)
26/02/12 17:50:11 INFO SparkContext: Created broadcast 0 from csv at NativeMethodAccessorImpl.java:0
26/02/12 17:50:11 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
26/02/12 17:50:12 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0
26/02/12 17:50:12 INFO DAGScheduler: Got job 0 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/02/12 17:50:12 INFO DAGScheduler: Final stage: ResultStage 0 (csv at NativeMethodAccessorImpl.java:0)
26/02/12 17:50:12 INFO DAGScheduler: Parents of final stage: List()
26/02/12 17:50:12 INFO DAGScheduler: Missing parents: List()
26/02/12 17:50:12 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/12 17:50:12 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 13.5 KiB, free 434.2 MiB)
26/02/12 17:50:12 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.4 KiB, free 434.2 MiB)
26/02/12 17:50:12 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on spark-master:44075 (size: 6.4 KiB, free: 434.4 MiB)
26/02/12 17:50:12 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1585
26/02/12 17:50:12 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/02/12 17:50:12 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
26/02/12 17:50:12 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.18.0.15, executor 0, partition 0, PROCESS_LOCAL, 8217 bytes) 
26/02/12 17:50:12 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.18.0.15:39271 (size: 6.4 KiB, free: 434.4 MiB)
26/02/12 17:50:13 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.18.0.15:39271 (size: 34.5 KiB, free: 434.4 MiB)
26/02/12 17:50:13 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1659 ms on 172.18.0.15 (executor 0) (1/1)
26/02/12 17:50:13 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
26/02/12 17:50:14 INFO DAGScheduler: ResultStage 0 (csv at NativeMethodAccessorImpl.java:0) finished in 1.809 s
26/02/12 17:50:14 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/12 17:50:14 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
26/02/12 17:50:14 INFO DAGScheduler: Job 0 finished: csv at NativeMethodAccessorImpl.java:0, took 1.864152 s
26/02/12 17:50:14 INFO CodeGenerator: Code generated in 12.942931 ms
26/02/12 17:50:14 INFO BlockManagerInfo: Removed broadcast_1_piece0 on spark-master:44075 in memory (size: 6.4 KiB, free: 434.4 MiB)
26/02/12 17:50:14 INFO FileSourceStrategy: Pushed Filters: 
26/02/12 17:50:14 INFO FileSourceStrategy: Post-Scan Filters: 
26/02/12 17:50:14 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 172.18.0.15:39271 in memory (size: 6.4 KiB, free: 434.4 MiB)
26/02/12 17:50:14 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 200.4 KiB, free 434.0 MiB)
26/02/12 17:50:14 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.9 MiB)
26/02/12 17:50:14 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on spark-master:44075 (size: 34.5 KiB, free: 434.3 MiB)
26/02/12 17:50:14 INFO SparkContext: Created broadcast 2 from csv at NativeMethodAccessorImpl.java:0
26/02/12 17:50:14 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
26/02/12 17:50:14 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0
26/02/12 17:50:14 INFO DAGScheduler: Got job 1 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/02/12 17:50:14 INFO DAGScheduler: Final stage: ResultStage 1 (csv at NativeMethodAccessorImpl.java:0)
26/02/12 17:50:14 INFO DAGScheduler: Parents of final stage: List()
26/02/12 17:50:14 INFO DAGScheduler: Missing parents: List()
26/02/12 17:50:14 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[9] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/12 17:50:14 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 27.8 KiB, free 433.9 MiB)
26/02/12 17:50:14 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 12.8 KiB, free 433.9 MiB)
26/02/12 17:50:14 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on spark-master:44075 (size: 12.8 KiB, free: 434.3 MiB)
26/02/12 17:50:14 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1585
26/02/12 17:50:14 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[9] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/02/12 17:50:14 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
26/02/12 17:50:14 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (172.18.0.15, executor 0, partition 0, PROCESS_LOCAL, 8217 bytes) 
26/02/12 17:50:14 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 172.18.0.15:39271 (size: 12.8 KiB, free: 434.4 MiB)
26/02/12 17:50:15 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.18.0.15:39271 (size: 34.5 KiB, free: 434.3 MiB)
26/02/12 17:50:16 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 1951 ms on 172.18.0.15 (executor 0) (1/1)
26/02/12 17:50:16 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
26/02/12 17:50:16 INFO DAGScheduler: ResultStage 1 (csv at NativeMethodAccessorImpl.java:0) finished in 1.984 s
26/02/12 17:50:16 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/12 17:50:16 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
26/02/12 17:50:16 INFO DAGScheduler: Job 1 finished: csv at NativeMethodAccessorImpl.java:0, took 1.991498 s
26/02/12 17:50:16 INFO BlockManagerInfo: Removed broadcast_3_piece0 on spark-master:44075 in memory (size: 12.8 KiB, free: 434.3 MiB)
26/02/12 17:50:16 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 172.18.0.15:39271 in memory (size: 12.8 KiB, free: 434.3 MiB)
26/02/12 17:50:17 INFO FileSourceStrategy: Pushed Filters: IsNotNull(FlightNo),IsNotNull(Origin),IsNotNull(Destination)
26/02/12 17:50:17 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(FlightNo#17),isnotnull(Origin#18),isnotnull(Destination#19),NOT (Origin#18 = Destination#19)
26/02/12 17:50:17 INFO CodeGenerator: Code generated in 121.581157 ms
26/02/12 17:50:17 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 200.3 KiB, free 433.7 MiB)
26/02/12 17:50:17 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.7 MiB)
26/02/12 17:50:17 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on spark-master:44075 (size: 34.5 KiB, free: 434.3 MiB)
26/02/12 17:50:17 INFO SparkContext: Created broadcast 4 from count at NativeMethodAccessorImpl.java:0
26/02/12 17:50:17 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
26/02/12 17:50:17 INFO BlockManagerInfo: Removed broadcast_0_piece0 on spark-master:44075 in memory (size: 34.5 KiB, free: 434.3 MiB)
26/02/12 17:50:17 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 172.18.0.15:39271 in memory (size: 34.5 KiB, free: 434.4 MiB)
26/02/12 17:50:17 INFO DAGScheduler: Registering RDD 13 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 0
26/02/12 17:50:17 INFO DAGScheduler: Got map stage job 2 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/02/12 17:50:17 INFO DAGScheduler: Final stage: ShuffleMapStage 2 (count at NativeMethodAccessorImpl.java:0)
26/02/12 17:50:17 INFO DAGScheduler: Parents of final stage: List()
26/02/12 17:50:17 INFO DAGScheduler: Missing parents: List()
26/02/12 17:50:17 INFO DAGScheduler: Submitting ShuffleMapStage 2 (MapPartitionsRDD[13] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/12 17:50:17 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 51.6 KiB, free 433.9 MiB)
26/02/12 17:50:17 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 21.5 KiB, free 433.9 MiB)
26/02/12 17:50:17 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on spark-master:44075 (size: 21.5 KiB, free: 434.3 MiB)
26/02/12 17:50:17 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1585
26/02/12 17:50:17 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[13] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/02/12 17:50:17 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
26/02/12 17:50:17 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (172.18.0.15, executor 0, partition 0, PROCESS_LOCAL, 8206 bytes) 
26/02/12 17:50:17 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 172.18.0.15:39271 (size: 21.5 KiB, free: 434.3 MiB)
26/02/12 17:50:18 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 172.18.0.15:39271 (size: 34.5 KiB, free: 434.3 MiB)
26/02/12 17:50:18 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 863 ms on 172.18.0.15 (executor 0) (1/1)
26/02/12 17:50:18 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool 
26/02/12 17:50:18 INFO DAGScheduler: ShuffleMapStage 2 (count at NativeMethodAccessorImpl.java:0) finished in 0.902 s
26/02/12 17:50:18 INFO DAGScheduler: looking for newly runnable stages
26/02/12 17:50:18 INFO DAGScheduler: running: Set()
26/02/12 17:50:18 INFO DAGScheduler: waiting: Set()
26/02/12 17:50:18 INFO DAGScheduler: failed: Set()
26/02/12 17:50:18 INFO ShufflePartitionsUtil: For shuffle(0), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
26/02/12 17:50:18 INFO CodeGenerator: Code generated in 59.066871 ms
26/02/12 17:50:18 INFO DAGScheduler: Registering RDD 16 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 1
26/02/12 17:50:18 INFO DAGScheduler: Got map stage job 3 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/02/12 17:50:18 INFO DAGScheduler: Final stage: ShuffleMapStage 4 (count at NativeMethodAccessorImpl.java:0)
26/02/12 17:50:18 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 3)
26/02/12 17:50:18 INFO DAGScheduler: Missing parents: List()
26/02/12 17:50:18 INFO DAGScheduler: Submitting ShuffleMapStage 4 (MapPartitionsRDD[16] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/12 17:50:18 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 54.7 KiB, free 433.8 MiB)
26/02/12 17:50:18 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 24.2 KiB, free 433.8 MiB)
26/02/12 17:50:18 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on spark-master:44075 (size: 24.2 KiB, free: 434.3 MiB)
26/02/12 17:50:18 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1585
26/02/12 17:50:18 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 4 (MapPartitionsRDD[16] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/02/12 17:50:18 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0
26/02/12 17:50:18 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 3) (172.18.0.15, executor 0, partition 0, NODE_LOCAL, 7608 bytes) 
26/02/12 17:50:19 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 172.18.0.15:39271 (size: 24.2 KiB, free: 434.3 MiB)
26/02/12 17:50:19 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 172.18.0.15:55094
26/02/12 17:50:19 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 3) in 344 ms on 172.18.0.15 (executor 0) (1/1)
26/02/12 17:50:19 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool 
26/02/12 17:50:19 INFO DAGScheduler: ShuffleMapStage 4 (count at NativeMethodAccessorImpl.java:0) finished in 0.373 s
26/02/12 17:50:19 INFO DAGScheduler: looking for newly runnable stages
26/02/12 17:50:19 INFO DAGScheduler: running: Set()
26/02/12 17:50:19 INFO DAGScheduler: waiting: Set()
26/02/12 17:50:19 INFO DAGScheduler: failed: Set()
26/02/12 17:50:19 INFO CodeGenerator: Code generated in 18.30487 ms
26/02/12 17:50:19 INFO SparkContext: Starting job: count at NativeMethodAccessorImpl.java:0
26/02/12 17:50:19 INFO DAGScheduler: Got job 4 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/02/12 17:50:19 INFO DAGScheduler: Final stage: ResultStage 7 (count at NativeMethodAccessorImpl.java:0)
26/02/12 17:50:19 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 6)
26/02/12 17:50:19 INFO DAGScheduler: Missing parents: List()
26/02/12 17:50:19 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[19] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/12 17:50:19 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 12.5 KiB, free 433.8 MiB)
26/02/12 17:50:19 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 5.9 KiB, free 433.8 MiB)
26/02/12 17:50:19 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on spark-master:44075 (size: 5.9 KiB, free: 434.3 MiB)
26/02/12 17:50:19 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1585
26/02/12 17:50:19 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[19] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/02/12 17:50:19 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks resource profile 0
26/02/12 17:50:19 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 4) (172.18.0.15, executor 0, partition 0, NODE_LOCAL, 7619 bytes) 
26/02/12 17:50:19 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 172.18.0.15:39271 (size: 5.9 KiB, free: 434.3 MiB)
26/02/12 17:50:19 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 172.18.0.15:55094
26/02/12 17:50:19 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 4) in 78 ms on 172.18.0.15 (executor 0) (1/1)
26/02/12 17:50:19 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool 
26/02/12 17:50:19 INFO DAGScheduler: ResultStage 7 (count at NativeMethodAccessorImpl.java:0) finished in 0.092 s
26/02/12 17:50:19 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/12 17:50:19 INFO TaskSchedulerImpl: Killing all running tasks in stage 7: Stage finished
26/02/12 17:50:19 INFO DAGScheduler: Job 4 finished: count at NativeMethodAccessorImpl.java:0, took 0.102439 s
✓ Loaded 2773 routes from CSV: /opt/airflow/data/routes.csv
✓ Using flight number-based route matching
✓ Sample callsigns: IGO102, AIC176, etc.
26/02/12 17:50:19 INFO FileSourceStrategy: Pushed Filters: IsNotNull(FlightNo),IsNotNull(Origin),IsNotNull(Destination)
26/02/12 17:50:19 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(FlightNo#17),isnotnull(Origin#18),isnotnull(Destination#19),NOT (Origin#18 = Destination#19)
26/02/12 17:50:19 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
26/02/12 17:50:20 INFO BlockManagerInfo: Removed broadcast_6_piece0 on spark-master:44075 in memory (size: 24.2 KiB, free: 434.3 MiB)
26/02/12 17:50:20 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 172.18.0.15:39271 in memory (size: 24.2 KiB, free: 434.3 MiB)
26/02/12 17:50:20 INFO BlockManagerInfo: Removed broadcast_7_piece0 on spark-master:44075 in memory (size: 5.9 KiB, free: 434.3 MiB)
26/02/12 17:50:20 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 172.18.0.15:39271 in memory (size: 5.9 KiB, free: 434.3 MiB)
26/02/12 17:50:20 INFO BlockManagerInfo: Removed broadcast_5_piece0 on spark-master:44075 in memory (size: 21.5 KiB, free: 434.3 MiB)
26/02/12 17:50:20 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 172.18.0.15:39271 in memory (size: 21.5 KiB, free: 434.3 MiB)
26/02/12 17:50:20 INFO BlockManagerInfo: Removed broadcast_4_piece0 on spark-master:44075 in memory (size: 34.5 KiB, free: 434.4 MiB)
26/02/12 17:50:20 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 172.18.0.15:39271 in memory (size: 34.5 KiB, free: 434.4 MiB)
26/02/12 17:50:20 INFO BlockManagerInfo: Removed broadcast_2_piece0 on spark-master:44075 in memory (size: 34.5 KiB, free: 434.4 MiB)
26/02/12 17:50:20 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 172.18.0.15:39271 in memory (size: 34.5 KiB, free: 434.4 MiB)
26/02/12 17:50:20 INFO CodeGenerator: Code generated in 152.669109 ms
26/02/12 17:50:20 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 200.3 KiB, free 434.2 MiB)
26/02/12 17:50:20 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 434.2 MiB)
26/02/12 17:50:20 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on spark-master:44075 (size: 34.5 KiB, free: 434.4 MiB)
26/02/12 17:50:20 INFO SparkContext: Created broadcast 8 from count at NativeMethodAccessorImpl.java:0
26/02/12 17:50:20 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
26/02/12 17:50:20 INFO DAGScheduler: Registering RDD 24 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 2
26/02/12 17:50:20 INFO DAGScheduler: Got map stage job 5 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/02/12 17:50:20 INFO DAGScheduler: Final stage: ShuffleMapStage 8 (count at NativeMethodAccessorImpl.java:0)
26/02/12 17:50:20 INFO DAGScheduler: Parents of final stage: List()
26/02/12 17:50:20 INFO DAGScheduler: Missing parents: List()
26/02/12 17:50:20 INFO DAGScheduler: Submitting ShuffleMapStage 8 (MapPartitionsRDD[24] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/12 17:50:20 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 111.9 KiB, free 434.1 MiB)
26/02/12 17:50:20 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 434.0 MiB)
26/02/12 17:50:20 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on spark-master:44075 (size: 34.0 KiB, free: 434.3 MiB)
26/02/12 17:50:20 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1585
26/02/12 17:50:20 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 8 (MapPartitionsRDD[24] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/02/12 17:50:20 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks resource profile 0
26/02/12 17:50:20 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 5) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8206 bytes) 
26/02/12 17:50:20 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 172.18.0.14:44081 (size: 34.0 KiB, free: 434.4 MiB)
26/02/12 17:50:22 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 172.18.0.14:44081 (size: 34.5 KiB, free: 434.3 MiB)
26/02/12 17:50:23 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 5) in 3002 ms on 172.18.0.14 (executor 1) (1/1)
26/02/12 17:50:23 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool 
26/02/12 17:50:23 INFO DAGScheduler: ShuffleMapStage 8 (count at NativeMethodAccessorImpl.java:0) finished in 3.028 s
26/02/12 17:50:23 INFO DAGScheduler: looking for newly runnable stages
26/02/12 17:50:23 INFO DAGScheduler: running: Set()
26/02/12 17:50:23 INFO DAGScheduler: waiting: Set()
26/02/12 17:50:23 INFO DAGScheduler: failed: Set()
26/02/12 17:50:23 INFO CodeGenerator: Code generated in 42.12076 ms
26/02/12 17:50:23 INFO DAGScheduler: Got job 6 (count at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/12 17:50:23 INFO DAGScheduler: Final stage: ResultStage 10 (count at NativeMethodAccessorImpl.java:0)
26/02/12 17:50:23 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 9)
26/02/12 17:50:23 INFO DAGScheduler: Missing parents: List()
26/02/12 17:50:23 INFO DAGScheduler: Submitting ResultStage 10 (AdaptiveSparkPlan isFinalPlan=false
+- SortAggregate(key=[callsign#67], functions=[first(flight_number#25, false), first(origin_code#26, false), first(destination_code#27, false), first(airline_iata#32, false), first(airline_name#37, false), first(airline_prefix#43, false), first(airline_icao#50, false), first(flight_num_only#58, false), first(origin_city#77, false), first(origin_airport#88, false), first(origin_lat#100, false), first(origin_lon#113, false), first(destination_city#127, false), first(destination_airport#142, false), first(destination_lat#158, false), first(destination_lon#175, false)], output=[flight_number#251, origin_code#253, destination_code#255, airline_iata#257, airline_name#259, airline_prefix#261, airline_icao#263, flight_num_only#265, callsign#67, origin_city#267, origin_airport#269, origin_lat#271, origin_lon#273, destination_city#275, destination_airport#277, destination_lat#279, destination_lon#281])
   +- Sort [callsign#67 ASC NULLS FIRST], false, 0
      +- Exchange hashpartit... MapPartitionsRDD[29] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/12 17:50:23 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 89.6 KiB, free 433.9 MiB)
26/02/12 17:50:23 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 29.7 KiB, free 433.9 MiB)
26/02/12 17:50:23 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on spark-master:44075 (size: 29.7 KiB, free: 434.3 MiB)
26/02/12 17:50:23 INFO SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:1585
26/02/12 17:50:23 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 10 (AdaptiveSparkPlan isFinalPlan=false
+- SortAggregate(key=[callsign#67], functions=[first(flight_number#25, false), first(origin_code#26, false), first(destination_code#27, false), first(airline_iata#32, false), first(airline_name#37, false), first(airline_prefix#43, false), first(airline_icao#50, false), first(flight_num_only#58, false), first(origin_city#77, false), first(origin_airport#88, false), first(origin_lat#100, false), first(origin_lon#113, false), first(destination_city#127, false), first(destination_airport#142, false), first(destination_lat#158, false), first(destination_lon#175, false)], output=[flight_number#251, origin_code#253, destination_code#255, airline_iata#257, airline_name#259, airline_prefix#261, airline_icao#263, flight_num_only#265, callsign#67, origin_city#267, origin_airport#269, origin_lat#271, origin_lon#273, destination_city#275, destination_airport#277, destination_lat#279, destination_lon#281])
   +- Sort [callsign#67 ASC NULLS FIRST], false, 0
      +- Exchange hashpartit... MapPartitionsRDD[29] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/12 17:50:23 INFO TaskSchedulerImpl: Adding task set 10.0 with 4 tasks resource profile 0
26/02/12 17:50:23 INFO TaskSetManager: Starting task 0.0 in stage 10.0 (TID 6) (172.18.0.14, executor 1, partition 0, NODE_LOCAL, 7619 bytes) 
26/02/12 17:50:23 INFO TaskSetManager: Starting task 1.0 in stage 10.0 (TID 7) (172.18.0.14, executor 1, partition 1, NODE_LOCAL, 7619 bytes) 
26/02/12 17:50:23 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 172.18.0.14:44081 (size: 29.7 KiB, free: 434.3 MiB)
26/02/12 17:50:23 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 2 to 172.18.0.14:50138
26/02/12 17:50:24 INFO BlockManagerInfo: Added rdd_29_0 in memory on 172.18.0.14:44081 (size: 51.7 KiB, free: 434.3 MiB)
26/02/12 17:50:24 INFO BlockManagerInfo: Added rdd_29_1 in memory on 172.18.0.14:44081 (size: 52.1 KiB, free: 434.2 MiB)
26/02/12 17:50:24 INFO TaskSetManager: Starting task 2.0 in stage 10.0 (TID 8) (172.18.0.14, executor 1, partition 2, NODE_LOCAL, 7619 bytes) 
26/02/12 17:50:24 INFO TaskSetManager: Starting task 3.0 in stage 10.0 (TID 9) (172.18.0.14, executor 1, partition 3, NODE_LOCAL, 7619 bytes) 
26/02/12 17:50:24 INFO TaskSetManager: Finished task 1.0 in stage 10.0 (TID 7) in 1356 ms on 172.18.0.14 (executor 1) (1/4)
26/02/12 17:50:24 INFO TaskSetManager: Finished task 0.0 in stage 10.0 (TID 6) in 1359 ms on 172.18.0.14 (executor 1) (2/4)
26/02/12 17:50:25 INFO BlockManagerInfo: Added rdd_29_3 in memory on 172.18.0.14:44081 (size: 51.3 KiB, free: 434.2 MiB)
26/02/12 17:50:25 INFO BlockManagerInfo: Added rdd_29_2 in memory on 172.18.0.14:44081 (size: 49.7 KiB, free: 434.1 MiB)
26/02/12 17:50:25 INFO TaskSetManager: Finished task 3.0 in stage 10.0 (TID 9) in 165 ms on 172.18.0.14 (executor 1) (3/4)
26/02/12 17:50:25 INFO TaskSetManager: Finished task 2.0 in stage 10.0 (TID 8) in 173 ms on 172.18.0.14 (executor 1) (4/4)
26/02/12 17:50:25 INFO TaskSchedulerImpl: Removed TaskSet 10.0, whose tasks have all completed, from pool 
26/02/12 17:50:25 INFO DAGScheduler: ResultStage 10 (count at NativeMethodAccessorImpl.java:0) finished in 1.578 s
26/02/12 17:50:25 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/12 17:50:25 INFO TaskSchedulerImpl: Killing all running tasks in stage 10: Stage finished
26/02/12 17:50:25 INFO CodeGenerator: Code generated in 8.359311 ms
26/02/12 17:50:25 INFO DAGScheduler: Registering RDD 34 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 3
26/02/12 17:50:25 INFO DAGScheduler: Got map stage job 7 (count at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/12 17:50:25 INFO DAGScheduler: Final stage: ShuffleMapStage 12 (count at NativeMethodAccessorImpl.java:0)
26/02/12 17:50:25 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 11)
26/02/12 17:50:25 INFO DAGScheduler: Missing parents: List()
26/02/12 17:50:25 INFO DAGScheduler: Submitting ShuffleMapStage 12 (MapPartitionsRDD[34] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/12 17:50:25 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 95.1 KiB, free 433.8 MiB)
26/02/12 17:50:25 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 32.0 KiB, free 433.8 MiB)
26/02/12 17:50:25 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on spark-master:44075 (size: 32.0 KiB, free: 434.3 MiB)
26/02/12 17:50:25 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1585
26/02/12 17:50:25 INFO DAGScheduler: Submitting 4 missing tasks from ShuffleMapStage 12 (MapPartitionsRDD[34] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/12 17:50:25 INFO TaskSchedulerImpl: Adding task set 12.0 with 4 tasks resource profile 0
26/02/12 17:50:25 INFO TaskSetManager: Starting task 0.0 in stage 12.0 (TID 10) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 7608 bytes) 
26/02/12 17:50:25 INFO TaskSetManager: Starting task 1.0 in stage 12.0 (TID 11) (172.18.0.14, executor 1, partition 1, PROCESS_LOCAL, 7608 bytes) 
26/02/12 17:50:25 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 172.18.0.14:44081 (size: 32.0 KiB, free: 434.1 MiB)
26/02/12 17:50:25 INFO TaskSetManager: Starting task 2.0 in stage 12.0 (TID 12) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 7608 bytes) 
26/02/12 17:50:25 INFO TaskSetManager: Starting task 3.0 in stage 12.0 (TID 13) (172.18.0.14, executor 1, partition 3, PROCESS_LOCAL, 7608 bytes) 
26/02/12 17:50:25 INFO TaskSetManager: Finished task 1.0 in stage 12.0 (TID 11) in 124 ms on 172.18.0.14 (executor 1) (1/4)
26/02/12 17:50:25 INFO TaskSetManager: Finished task 0.0 in stage 12.0 (TID 10) in 126 ms on 172.18.0.14 (executor 1) (2/4)
26/02/12 17:50:25 INFO TaskSetManager: Finished task 3.0 in stage 12.0 (TID 13) in 41 ms on 172.18.0.14 (executor 1) (3/4)
26/02/12 17:50:25 INFO TaskSetManager: Finished task 2.0 in stage 12.0 (TID 12) in 44 ms on 172.18.0.14 (executor 1) (4/4)
26/02/12 17:50:25 INFO TaskSchedulerImpl: Removed TaskSet 12.0, whose tasks have all completed, from pool 
26/02/12 17:50:25 INFO DAGScheduler: ShuffleMapStage 12 (count at NativeMethodAccessorImpl.java:0) finished in 0.191 s
26/02/12 17:50:25 INFO DAGScheduler: looking for newly runnable stages
26/02/12 17:50:25 INFO DAGScheduler: running: Set()
26/02/12 17:50:25 INFO DAGScheduler: waiting: Set()
26/02/12 17:50:25 INFO DAGScheduler: failed: Set()
26/02/12 17:50:25 INFO CodeGenerator: Code generated in 11.365621 ms
26/02/12 17:50:25 INFO SparkContext: Starting job: count at NativeMethodAccessorImpl.java:0
26/02/12 17:50:25 INFO DAGScheduler: Got job 8 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/02/12 17:50:25 INFO DAGScheduler: Final stage: ResultStage 15 (count at NativeMethodAccessorImpl.java:0)
26/02/12 17:50:25 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 14)
26/02/12 17:50:25 INFO DAGScheduler: Missing parents: List()
26/02/12 17:50:25 INFO DAGScheduler: Submitting ResultStage 15 (MapPartitionsRDD[37] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/12 17:50:25 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 12.5 KiB, free 433.8 MiB)
26/02/12 17:50:25 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 5.9 KiB, free 433.8 MiB)
26/02/12 17:50:25 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on spark-master:44075 (size: 5.9 KiB, free: 434.3 MiB)
26/02/12 17:50:25 INFO SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:1585
26/02/12 17:50:25 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 15 (MapPartitionsRDD[37] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/02/12 17:50:25 INFO TaskSchedulerImpl: Adding task set 15.0 with 1 tasks resource profile 0
26/02/12 17:50:25 INFO TaskSetManager: Starting task 0.0 in stage 15.0 (TID 14) (172.18.0.14, executor 1, partition 0, NODE_LOCAL, 7619 bytes) 
26/02/12 17:50:25 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 172.18.0.14:44081 (size: 5.9 KiB, free: 434.1 MiB)
26/02/12 17:50:25 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 3 to 172.18.0.14:50138
26/02/12 17:50:25 INFO TaskSetManager: Finished task 0.0 in stage 15.0 (TID 14) in 95 ms on 172.18.0.14 (executor 1) (1/1)
26/02/12 17:50:25 INFO TaskSchedulerImpl: Removed TaskSet 15.0, whose tasks have all completed, from pool 
26/02/12 17:50:25 INFO DAGScheduler: ResultStage 15 (count at NativeMethodAccessorImpl.java:0) finished in 0.108 s
26/02/12 17:50:25 INFO DAGScheduler: Job 8 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/12 17:50:25 INFO TaskSchedulerImpl: Killing all running tasks in stage 15: Stage finished
26/02/12 17:50:25 INFO DAGScheduler: Job 8 finished: count at NativeMethodAccessorImpl.java:0, took 0.115763 s
✓ Route mapping table created (2773 routes)
[DEBUG] Setting up Kafka input stream...
✓ Connected to Kafka input stream
[DEBUG] Parsing JSON and extracting fields...
26/02/12 17:50:25 INFO BlockManagerInfo: Removed broadcast_10_piece0 on spark-master:44075 in memory (size: 29.7 KiB, free: 434.3 MiB)
26/02/12 17:50:25 INFO BlockManagerInfo: Removed broadcast_10_piece0 on 172.18.0.14:44081 in memory (size: 29.7 KiB, free: 434.1 MiB)
26/02/12 17:50:25 INFO BlockManagerInfo: Removed broadcast_11_piece0 on spark-master:44075 in memory (size: 32.0 KiB, free: 434.3 MiB)
26/02/12 17:50:25 INFO BlockManagerInfo: Removed broadcast_11_piece0 on 172.18.0.14:44081 in memory (size: 32.0 KiB, free: 434.1 MiB)
26/02/12 17:50:25 INFO BlockManagerInfo: Removed broadcast_12_piece0 on spark-master:44075 in memory (size: 5.9 KiB, free: 434.3 MiB)
26/02/12 17:50:25 INFO BlockManagerInfo: Removed broadcast_12_piece0 on 172.18.0.14:44081 in memory (size: 5.9 KiB, free: 434.1 MiB)
26/02/12 17:50:25 INFO BlockManagerInfo: Removed broadcast_9_piece0 on spark-master:44075 in memory (size: 34.0 KiB, free: 434.4 MiB)
26/02/12 17:50:25 INFO BlockManagerInfo: Removed broadcast_9_piece0 on 172.18.0.14:44081 in memory (size: 34.0 KiB, free: 434.2 MiB)
[DEBUG] Joining with route mapping...
  Using CSV-based exact callsign matching
[DEBUG] Filtering records with valid routes...
✓ Filtering enabled: Only flights with routes will be sent to next topic
[DEBUG] Setting up output stream to Kafka...
26/02/12 17:50:26 INFO StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint
26/02/12 17:50:26 INFO ResolveWriteToStream: Checkpoint root /tmp/spark-checkpoint-enrichment resolved to file:/tmp/spark-checkpoint-enrichment.
26/02/12 17:50:26 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
26/02/12 17:50:26 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/metadata using temp file file:/tmp/spark-checkpoint-enrichment/.metadata.0b52ef8a-a9c0-4d13-841e-c09d44effd8a.tmp
26/02/12 17:50:26 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/.metadata.0b52ef8a-a9c0-4d13-841e-c09d44effd8a.tmp to file:/tmp/spark-checkpoint-enrichment/metadata
26/02/12 17:50:26 INFO MicroBatchExecution: Starting [id = 801223b3-8a58-496f-9f28-5a9fda8cbb73, runId = 005c83b3-ee40-4ce7-8dfd-e6942daa9533]. Use file:/tmp/spark-checkpoint-enrichment to store the query checkpoint.
============================================================
✓ Streaming query started successfully!
✓ Enriching flights with route information...
✓ Query ID: 801223b3-8a58-496f-9f28-5a9fda8cbb73
============================================================
[DEBUG] Waiting for stream termination...
26/02/12 17:50:26 INFO MicroBatchExecution: Reading table [org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@27af767] from DataSourceV2 named 'kafka' [org.apache.spark.sql.kafka010.KafkaSourceProvider@439e4957]
26/02/12 17:50:26 INFO OffsetSeqLog: BatchIds found from listing: 
26/02/12 17:50:26 INFO OffsetSeqLog: BatchIds found from listing: 
26/02/12 17:50:26 INFO MicroBatchExecution: Starting new streaming query.
26/02/12 17:50:26 INFO MicroBatchExecution: Stream started from {}
26/02/12 17:50:26 INFO AdminClientConfig: AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.servers = [kafka:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

26/02/12 17:50:26 INFO AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
26/02/12 17:50:26 INFO AppInfoParser: Kafka version: 3.5.1
26/02/12 17:50:26 INFO AppInfoParser: Kafka commitId: 2c6fb6c54472e90a
26/02/12 17:50:26 INFO AppInfoParser: Kafka startTimeMs: 1770918626731
26/02/12 17:50:27 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/sources/0/0 using temp file file:/tmp/spark-checkpoint-enrichment/sources/0/.0.5d800b65-a1d9-4ed2-917d-1d823da760b0.tmp
26/02/12 17:50:27 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/sources/0/.0.5d800b65-a1d9-4ed2-917d-1d823da760b0.tmp to file:/tmp/spark-checkpoint-enrichment/sources/0/0
26/02/12 17:50:27 INFO KafkaMicroBatchStream: Initial offsets: {"aviation-india-states":{"0":39045}}
26/02/12 17:50:27 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/0 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.0.9d1cda46-a9ea-456d-88ae-06442f6cd82e.tmp
26/02/12 17:50:27 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.0.9d1cda46-a9ea-456d-88ae-06442f6cd82e.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/0
26/02/12 17:50:27 INFO MicroBatchExecution: Committed offsets for batch 0. Metadata OffsetSeqMetadata(0,1770918627319,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/12 17:50:27 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/12 17:50:27 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/12 17:50:27 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/12 17:50:27 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/12 17:50:27 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/12 17:50:27 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/12 17:50:27 INFO CodeGenerator: Code generated in 10.4519 ms
26/02/12 17:50:27 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#1951 - origin_code.nullCount#1950) > 0)
26/02/12 17:50:27 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#1956 - destination_code.nullCount#1955) > 0)
26/02/12 17:50:27 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#1986 - callsign.nullCount#1985) > 0)
26/02/12 17:50:27 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/12 17:50:27 INFO DAGScheduler: Got job 9 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/12 17:50:27 INFO DAGScheduler: Final stage: ResultStage 17 (start at NativeMethodAccessorImpl.java:0)
26/02/12 17:50:27 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 16)
26/02/12 17:50:27 INFO DAGScheduler: Missing parents: List()
26/02/12 17:50:27 INFO DAGScheduler: Submitting ResultStage 17 (MapPartitionsRDD[42] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/12 17:50:27 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 99.3 KiB, free 434.1 MiB)
26/02/12 17:50:27 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 434.0 MiB)
26/02/12 17:50:27 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on spark-master:44075 (size: 33.3 KiB, free: 434.3 MiB)
26/02/12 17:50:27 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1585
26/02/12 17:50:27 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 17 (MapPartitionsRDD[42] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/12 17:50:27 INFO TaskSchedulerImpl: Adding task set 17.0 with 4 tasks resource profile 0
26/02/12 17:50:27 INFO TaskSetManager: Starting task 0.0 in stage 17.0 (TID 15) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/12 17:50:27 INFO TaskSetManager: Starting task 1.0 in stage 17.0 (TID 16) (172.18.0.14, executor 1, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/12 17:50:28 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on 172.18.0.14:44081 (size: 33.3 KiB, free: 434.1 MiB)
26/02/12 17:50:28 INFO TaskSetManager: Starting task 2.0 in stage 17.0 (TID 17) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/12 17:50:28 INFO TaskSetManager: Finished task 1.0 in stage 17.0 (TID 16) in 146 ms on 172.18.0.14 (executor 1) (1/4)
26/02/12 17:50:28 INFO TaskSetManager: Starting task 3.0 in stage 17.0 (TID 18) (172.18.0.14, executor 1, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/12 17:50:28 INFO TaskSetManager: Finished task 0.0 in stage 17.0 (TID 15) in 153 ms on 172.18.0.14 (executor 1) (2/4)
26/02/12 17:50:28 INFO TaskSetManager: Finished task 2.0 in stage 17.0 (TID 17) in 56 ms on 172.18.0.14 (executor 1) (3/4)
26/02/12 17:50:28 INFO TaskSetManager: Finished task 3.0 in stage 17.0 (TID 18) in 50 ms on 172.18.0.14 (executor 1) (4/4)
26/02/12 17:50:28 INFO TaskSchedulerImpl: Removed TaskSet 17.0, whose tasks have all completed, from pool 
26/02/12 17:50:28 INFO DAGScheduler: ResultStage 17 (start at NativeMethodAccessorImpl.java:0) finished in 0.217 s
26/02/12 17:50:28 INFO DAGScheduler: Job 9 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/12 17:50:28 INFO TaskSchedulerImpl: Killing all running tasks in stage 17: Stage finished
26/02/12 17:50:28 INFO DAGScheduler: Job 9 finished: start at NativeMethodAccessorImpl.java:0, took 0.225097 s
26/02/12 17:50:28 INFO CodeGenerator: Code generated in 10.592767 ms
26/02/12 17:50:28 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.9 MiB)
26/02/12 17:50:28 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on spark-master:44075 (size: 107.2 KiB, free: 434.2 MiB)
26/02/12 17:50:28 INFO SparkContext: Created broadcast 14 from start at NativeMethodAccessorImpl.java:0
26/02/12 17:50:28 INFO CodeGenerator: Code generated in 28.490185 ms
26/02/12 17:50:28 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 0, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@6d418eb9]. The input RDD has 1 partitions.
26/02/12 17:50:28 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/12 17:50:28 INFO DAGScheduler: Got job 10 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/02/12 17:50:28 INFO DAGScheduler: Final stage: ResultStage 18 (start at NativeMethodAccessorImpl.java:0)
26/02/12 17:50:28 INFO DAGScheduler: Parents of final stage: List()
26/02/12 17:50:28 INFO DAGScheduler: Missing parents: List()
26/02/12 17:50:28 INFO DAGScheduler: Submitting ResultStage 18 (ParallelCollectionRDD[49] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/12 17:50:28 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 4.4 KiB, free 433.9 MiB)
26/02/12 17:50:28 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 2.6 KiB, free 433.9 MiB)
26/02/12 17:50:28 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on spark-master:44075 (size: 2.6 KiB, free: 434.2 MiB)
26/02/12 17:50:28 INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1585
26/02/12 17:50:28 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 18 (ParallelCollectionRDD[49] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/02/12 17:50:28 INFO TaskSchedulerImpl: Adding task set 18.0 with 1 tasks resource profile 0
26/02/12 17:50:28 INFO TaskSetManager: Starting task 0.0 in stage 18.0 (TID 19) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 7640 bytes) 
26/02/12 17:50:28 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on 172.18.0.14:44081 (size: 2.6 KiB, free: 434.1 MiB)
26/02/12 17:50:28 INFO TaskSetManager: Finished task 0.0 in stage 18.0 (TID 19) in 143 ms on 172.18.0.14 (executor 1) (1/1)
26/02/12 17:50:28 INFO TaskSchedulerImpl: Removed TaskSet 18.0, whose tasks have all completed, from pool 
26/02/12 17:50:28 INFO DAGScheduler: ResultStage 18 (start at NativeMethodAccessorImpl.java:0) finished in 0.153 s
26/02/12 17:50:28 INFO DAGScheduler: Job 10 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/12 17:50:28 INFO TaskSchedulerImpl: Killing all running tasks in stage 18: Stage finished
26/02/12 17:50:28 INFO DAGScheduler: Job 10 finished: start at NativeMethodAccessorImpl.java:0, took 0.159230 s
26/02/12 17:50:28 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 0, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@6d418eb9] is committing.
26/02/12 17:50:28 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 0, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@6d418eb9] committed.
26/02/12 17:50:28 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/0 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.0.e26a7b8c-14c2-4fa9-ab86-9581da72ad50.tmp
26/02/12 17:50:28 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.0.e26a7b8c-14c2-4fa9-ab86-9581da72ad50.tmp to file:/tmp/spark-checkpoint-enrichment/commits/0
26/02/12 17:50:28 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "801223b3-8a58-496f-9f28-5a9fda8cbb73",
  "runId" : "005c83b3-ee40-4ce7-8dfd-e6942daa9533",
  "name" : null,
  "timestamp" : "2026-02-12T17:50:26.441Z",
  "batchId" : 0,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "addBatch" : 841,
    "commitOffsets" : 84,
    "getBatch" : 24,
    "latestOffset" : 866,
    "queryPlanning" : 231,
    "triggerExecution" : 2155,
    "walCommit" : 85
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : null,
    "endOffset" : {
      "aviation-india-states" : {
        "0" : 39045
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "0" : 39045
      }
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@444eb264",
    "numOutputRows" : 0
  }
}
26/02/12 17:50:33 INFO BlockManagerInfo: Removed broadcast_15_piece0 on spark-master:44075 in memory (size: 2.6 KiB, free: 434.2 MiB)
26/02/12 17:50:33 INFO BlockManagerInfo: Removed broadcast_15_piece0 on 172.18.0.14:44081 in memory (size: 2.6 KiB, free: 434.1 MiB)
26/02/12 17:50:33 INFO BlockManagerInfo: Removed broadcast_13_piece0 on spark-master:44075 in memory (size: 33.3 KiB, free: 434.3 MiB)
26/02/12 17:50:33 INFO BlockManagerInfo: Removed broadcast_13_piece0 on 172.18.0.14:44081 in memory (size: 33.3 KiB, free: 434.2 MiB)
26/02/12 17:50:38 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/12 17:50:39 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/1 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.1.c6a912d2-288b-43e6-9a3a-973ca8632708.tmp
26/02/12 17:50:39 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.1.c6a912d2-288b-43e6-9a3a-973ca8632708.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/1
26/02/12 17:50:39 INFO MicroBatchExecution: Committed offsets for batch 1. Metadata OffsetSeqMetadata(0,1770918639386,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/12 17:50:39 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/12 17:50:39 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/12 17:50:39 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/12 17:50:39 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/12 17:50:39 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/12 17:50:39 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/12 17:50:39 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#2805 - origin_code.nullCount#2804) > 0)
26/02/12 17:50:39 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#2810 - destination_code.nullCount#2809) > 0)
26/02/12 17:50:39 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#2840 - callsign.nullCount#2839) > 0)
26/02/12 17:50:39 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/12 17:50:39 INFO DAGScheduler: Got job 11 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/12 17:50:39 INFO DAGScheduler: Final stage: ResultStage 20 (start at NativeMethodAccessorImpl.java:0)
26/02/12 17:50:39 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 19)
26/02/12 17:50:39 INFO DAGScheduler: Missing parents: List()
26/02/12 17:50:39 INFO DAGScheduler: Submitting ResultStage 20 (MapPartitionsRDD[54] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/12 17:50:39 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 99.3 KiB, free 434.0 MiB)
26/02/12 17:50:39 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 433.9 MiB)
26/02/12 17:50:39 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on spark-master:44075 (size: 33.3 KiB, free: 434.2 MiB)
26/02/12 17:50:39 INFO SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1585
26/02/12 17:50:39 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 20 (MapPartitionsRDD[54] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/12 17:50:39 INFO TaskSchedulerImpl: Adding task set 20.0 with 4 tasks resource profile 0
26/02/12 17:50:39 INFO TaskSetManager: Starting task 0.0 in stage 20.0 (TID 20) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/12 17:50:39 INFO TaskSetManager: Starting task 1.0 in stage 20.0 (TID 21) (172.18.0.14, executor 1, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/12 17:50:39 INFO BlockManagerInfo: Removed broadcast_14_piece0 on spark-master:44075 in memory (size: 107.2 KiB, free: 434.3 MiB)
26/02/12 17:50:39 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on 172.18.0.14:44081 (size: 33.3 KiB, free: 434.1 MiB)
26/02/12 17:50:39 INFO TaskSetManager: Starting task 2.0 in stage 20.0 (TID 22) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/12 17:50:39 INFO TaskSetManager: Finished task 1.0 in stage 20.0 (TID 21) in 134 ms on 172.18.0.14 (executor 1) (1/4)
26/02/12 17:50:39 INFO TaskSetManager: Starting task 3.0 in stage 20.0 (TID 23) (172.18.0.14, executor 1, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/12 17:50:39 INFO TaskSetManager: Finished task 0.0 in stage 20.0 (TID 20) in 144 ms on 172.18.0.14 (executor 1) (2/4)
26/02/12 17:50:40 INFO TaskSetManager: Finished task 2.0 in stage 20.0 (TID 22) in 58 ms on 172.18.0.14 (executor 1) (3/4)
26/02/12 17:50:40 INFO TaskSetManager: Finished task 3.0 in stage 20.0 (TID 23) in 53 ms on 172.18.0.14 (executor 1) (4/4)
26/02/12 17:50:40 INFO TaskSchedulerImpl: Removed TaskSet 20.0, whose tasks have all completed, from pool 
26/02/12 17:50:40 INFO DAGScheduler: ResultStage 20 (start at NativeMethodAccessorImpl.java:0) finished in 0.210 s
26/02/12 17:50:40 INFO DAGScheduler: Job 11 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/12 17:50:40 INFO TaskSchedulerImpl: Killing all running tasks in stage 20: Stage finished
26/02/12 17:50:40 INFO DAGScheduler: Job 11 finished: start at NativeMethodAccessorImpl.java:0, took 0.216706 s
26/02/12 17:50:40 INFO MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.9 MiB)
26/02/12 17:50:40 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on spark-master:44075 (size: 107.2 KiB, free: 434.2 MiB)
26/02/12 17:50:40 INFO SparkContext: Created broadcast 17 from start at NativeMethodAccessorImpl.java:0
26/02/12 17:50:40 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 1, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@5b4c64e1]. The input RDD has 1 partitions.
26/02/12 17:50:40 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/12 17:50:40 INFO DAGScheduler: Got job 12 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/02/12 17:50:40 INFO DAGScheduler: Final stage: ResultStage 21 (start at NativeMethodAccessorImpl.java:0)
26/02/12 17:50:40 INFO DAGScheduler: Parents of final stage: List()
26/02/12 17:50:40 INFO DAGScheduler: Missing parents: List()
26/02/12 17:50:40 INFO DAGScheduler: Submitting ResultStage 21 (MapPartitionsRDD[60] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/12 17:50:40 INFO MemoryStore: Block broadcast_18 stored as values in memory (estimated size 49.9 KiB, free 433.9 MiB)
26/02/12 17:50:40 INFO MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.9 MiB)
26/02/12 17:50:40 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on spark-master:44075 (size: 19.5 KiB, free: 434.2 MiB)
26/02/12 17:50:40 INFO SparkContext: Created broadcast 18 from broadcast at DAGScheduler.scala:1585
26/02/12 17:50:40 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 21 (MapPartitionsRDD[60] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/02/12 17:50:40 INFO TaskSchedulerImpl: Adding task set 21.0 with 1 tasks resource profile 0
26/02/12 17:50:40 INFO TaskSetManager: Starting task 0.0 in stage 21.0 (TID 24) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/12 17:50:40 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on 172.18.0.14:44081 (size: 19.5 KiB, free: 434.1 MiB)
26/02/12 17:50:40 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on 172.18.0.14:44081 (size: 107.2 KiB, free: 434.0 MiB)
26/02/12 17:50:43 INFO TaskSetManager: Finished task 0.0 in stage 21.0 (TID 24) in 2925 ms on 172.18.0.14 (executor 1) (1/1)
26/02/12 17:50:43 INFO TaskSchedulerImpl: Removed TaskSet 21.0, whose tasks have all completed, from pool 
26/02/12 17:50:43 INFO DAGScheduler: ResultStage 21 (start at NativeMethodAccessorImpl.java:0) finished in 2.965 s
26/02/12 17:50:43 INFO DAGScheduler: Job 12 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/12 17:50:43 INFO TaskSchedulerImpl: Killing all running tasks in stage 21: Stage finished
26/02/12 17:50:43 INFO DAGScheduler: Job 12 finished: start at NativeMethodAccessorImpl.java:0, took 2.971543 s
26/02/12 17:50:43 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 1, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@5b4c64e1] is committing.
26/02/12 17:50:43 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 1, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@5b4c64e1] committed.
26/02/12 17:50:43 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/1 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.1.65d2c4b1-87e5-4822-a5d6-cadce0617314.tmp
26/02/12 17:50:43 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.1.65d2c4b1-87e5-4822-a5d6-cadce0617314.tmp to file:/tmp/spark-checkpoint-enrichment/commits/1
26/02/12 17:50:43 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "801223b3-8a58-496f-9f28-5a9fda8cbb73",
  "runId" : "005c83b3-ee40-4ce7-8dfd-e6942daa9533",
  "name" : null,
  "timestamp" : "2026-02-12T17:50:39.381Z",
  "batchId" : 1,
  "numInputRows" : 37,
  "inputRowsPerSecond" : 1947.3684210526317,
  "processedRowsPerSecond" : 9.829968119022316,
  "durationMs" : {
    "addBatch" : 3485,
    "commitOffsets" : 108,
    "getBatch" : 0,
    "latestOffset" : 4,
    "queryPlanning" : 87,
    "triggerExecution" : 3764,
    "walCommit" : 78
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "0" : 39045
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "0" : 39082
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "0" : 39082
      }
    },
    "numInputRows" : 37,
    "inputRowsPerSecond" : 1947.3684210526317,
    "processedRowsPerSecond" : 9.829968119022316,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@444eb264",
    "numOutputRows" : 14
  }
}
26/02/12 17:50:43 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/2 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.2.ec998083-f056-4a9a-8682-9b73cb596a88.tmp
26/02/12 17:50:43 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.2.ec998083-f056-4a9a-8682-9b73cb596a88.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/2
26/02/12 17:50:43 INFO MicroBatchExecution: Committed offsets for batch 2. Metadata OffsetSeqMetadata(0,1770918643153,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/12 17:50:43 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/12 17:50:43 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/12 17:50:43 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/12 17:50:43 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/12 17:50:43 INFO BlockManagerInfo: Removed broadcast_16_piece0 on spark-master:44075 in memory (size: 33.3 KiB, free: 434.2 MiB)
26/02/12 17:50:43 INFO BlockManagerInfo: Removed broadcast_16_piece0 on 172.18.0.14:44081 in memory (size: 33.3 KiB, free: 434.0 MiB)
26/02/12 17:50:43 INFO BlockManagerInfo: Removed broadcast_18_piece0 on spark-master:44075 in memory (size: 19.5 KiB, free: 434.3 MiB)
26/02/12 17:50:43 INFO BlockManagerInfo: Removed broadcast_18_piece0 on 172.18.0.14:44081 in memory (size: 19.5 KiB, free: 434.1 MiB)
26/02/12 17:50:43 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/12 17:50:43 INFO BlockManagerInfo: Removed broadcast_17_piece0 on spark-master:44075 in memory (size: 107.2 KiB, free: 434.4 MiB)
26/02/12 17:50:43 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/12 17:50:43 INFO BlockManagerInfo: Removed broadcast_17_piece0 on 172.18.0.14:44081 in memory (size: 107.2 KiB, free: 434.2 MiB)
26/02/12 17:50:43 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#3659 - origin_code.nullCount#3658) > 0)
26/02/12 17:50:43 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#3664 - destination_code.nullCount#3663) > 0)
26/02/12 17:50:43 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#3694 - callsign.nullCount#3693) > 0)
26/02/12 17:50:43 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/12 17:50:43 INFO DAGScheduler: Got job 13 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/12 17:50:43 INFO DAGScheduler: Final stage: ResultStage 23 (start at NativeMethodAccessorImpl.java:0)
26/02/12 17:50:43 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 22)
26/02/12 17:50:43 INFO DAGScheduler: Missing parents: List()
26/02/12 17:50:43 INFO DAGScheduler: Submitting ResultStage 23 (MapPartitionsRDD[65] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/12 17:50:43 INFO MemoryStore: Block broadcast_19 stored as values in memory (estimated size 99.3 KiB, free 434.1 MiB)
26/02/12 17:50:43 INFO MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 434.0 MiB)
26/02/12 17:50:43 INFO BlockManagerInfo: Added broadcast_19_piece0 in memory on spark-master:44075 (size: 33.3 KiB, free: 434.3 MiB)
26/02/12 17:50:43 INFO SparkContext: Created broadcast 19 from broadcast at DAGScheduler.scala:1585
26/02/12 17:50:43 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 23 (MapPartitionsRDD[65] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/12 17:50:43 INFO TaskSchedulerImpl: Adding task set 23.0 with 4 tasks resource profile 0
26/02/12 17:50:43 INFO TaskSetManager: Starting task 0.0 in stage 23.0 (TID 25) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/12 17:50:43 INFO TaskSetManager: Starting task 1.0 in stage 23.0 (TID 26) (172.18.0.14, executor 1, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/12 17:50:43 INFO BlockManagerInfo: Added broadcast_19_piece0 in memory on 172.18.0.14:44081 (size: 33.3 KiB, free: 434.1 MiB)
26/02/12 17:50:43 INFO TaskSetManager: Starting task 2.0 in stage 23.0 (TID 27) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/12 17:50:43 INFO TaskSetManager: Finished task 0.0 in stage 23.0 (TID 25) in 118 ms on 172.18.0.14 (executor 1) (1/4)
26/02/12 17:50:43 INFO TaskSetManager: Starting task 3.0 in stage 23.0 (TID 28) (172.18.0.14, executor 1, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/12 17:50:43 INFO TaskSetManager: Finished task 1.0 in stage 23.0 (TID 26) in 118 ms on 172.18.0.14 (executor 1) (2/4)
26/02/12 17:50:43 INFO TaskSetManager: Finished task 2.0 in stage 23.0 (TID 27) in 59 ms on 172.18.0.14 (executor 1) (3/4)
26/02/12 17:50:43 INFO TaskSetManager: Finished task 3.0 in stage 23.0 (TID 28) in 57 ms on 172.18.0.14 (executor 1) (4/4)
26/02/12 17:50:43 INFO TaskSchedulerImpl: Removed TaskSet 23.0, whose tasks have all completed, from pool 
26/02/12 17:50:43 INFO DAGScheduler: ResultStage 23 (start at NativeMethodAccessorImpl.java:0) finished in 0.206 s
26/02/12 17:50:43 INFO DAGScheduler: Job 13 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/12 17:50:43 INFO TaskSchedulerImpl: Killing all running tasks in stage 23: Stage finished
26/02/12 17:50:43 INFO DAGScheduler: Job 13 finished: start at NativeMethodAccessorImpl.java:0, took 0.213740 s
26/02/12 17:50:43 INFO MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.9 MiB)
26/02/12 17:50:43 INFO BlockManagerInfo: Added broadcast_20_piece0 in memory on spark-master:44075 (size: 107.2 KiB, free: 434.2 MiB)
26/02/12 17:50:43 INFO SparkContext: Created broadcast 20 from start at NativeMethodAccessorImpl.java:0
26/02/12 17:50:43 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 2, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@292b3253]. The input RDD has 1 partitions.
26/02/12 17:50:43 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/12 17:50:43 INFO DAGScheduler: Got job 14 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/02/12 17:50:43 INFO DAGScheduler: Final stage: ResultStage 24 (start at NativeMethodAccessorImpl.java:0)
26/02/12 17:50:43 INFO DAGScheduler: Parents of final stage: List()
26/02/12 17:50:43 INFO DAGScheduler: Missing parents: List()
26/02/12 17:50:43 INFO DAGScheduler: Submitting ResultStage 24 (MapPartitionsRDD[71] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/12 17:50:43 INFO MemoryStore: Block broadcast_21 stored as values in memory (estimated size 49.9 KiB, free 433.9 MiB)
26/02/12 17:50:43 INFO MemoryStore: Block broadcast_21_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.9 MiB)
26/02/12 17:50:43 INFO BlockManagerInfo: Added broadcast_21_piece0 in memory on spark-master:44075 (size: 19.5 KiB, free: 434.2 MiB)
26/02/12 17:50:43 INFO SparkContext: Created broadcast 21 from broadcast at DAGScheduler.scala:1585
26/02/12 17:50:43 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 24 (MapPartitionsRDD[71] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/02/12 17:50:43 INFO TaskSchedulerImpl: Adding task set 24.0 with 1 tasks resource profile 0
26/02/12 17:50:43 INFO TaskSetManager: Starting task 0.0 in stage 24.0 (TID 29) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/12 17:50:43 INFO BlockManagerInfo: Added broadcast_21_piece0 in memory on 172.18.0.14:44081 (size: 19.5 KiB, free: 434.1 MiB)
26/02/12 17:50:43 INFO BlockManagerInfo: Added broadcast_20_piece0 in memory on 172.18.0.14:44081 (size: 107.2 KiB, free: 434.0 MiB)
26/02/12 17:50:44 INFO TaskSetManager: Finished task 0.0 in stage 24.0 (TID 29) in 383 ms on 172.18.0.14 (executor 1) (1/1)
26/02/12 17:50:44 INFO TaskSchedulerImpl: Removed TaskSet 24.0, whose tasks have all completed, from pool 
26/02/12 17:50:44 INFO DAGScheduler: ResultStage 24 (start at NativeMethodAccessorImpl.java:0) finished in 0.396 s
26/02/12 17:50:44 INFO DAGScheduler: Job 14 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/12 17:50:44 INFO TaskSchedulerImpl: Killing all running tasks in stage 24: Stage finished
26/02/12 17:50:44 INFO DAGScheduler: Job 14 finished: start at NativeMethodAccessorImpl.java:0, took 0.402789 s
26/02/12 17:50:44 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 2, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@292b3253] is committing.
26/02/12 17:50:44 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 2, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@292b3253] committed.
26/02/12 17:50:44 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/2 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.2.fda4f196-ceb7-463d-8bc7-4369c2c795c6.tmp
26/02/12 17:50:44 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.2.fda4f196-ceb7-463d-8bc7-4369c2c795c6.tmp to file:/tmp/spark-checkpoint-enrichment/commits/2
26/02/12 17:50:44 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "801223b3-8a58-496f-9f28-5a9fda8cbb73",
  "runId" : "005c83b3-ee40-4ce7-8dfd-e6942daa9533",
  "name" : null,
  "timestamp" : "2026-02-12T17:50:43.147Z",
  "batchId" : 2,
  "numInputRows" : 192,
  "inputRowsPerSecond" : 50.982474774296335,
  "processedRowsPerSecond" : 158.41584158415841,
  "durationMs" : {
    "addBatch" : 961,
    "commitOffsets" : 86,
    "getBatch" : 1,
    "latestOffset" : 6,
    "queryPlanning" : 85,
    "triggerExecution" : 1212,
    "walCommit" : 73
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "0" : 39082
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "0" : 39274
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "0" : 39274
      }
    },
    "numInputRows" : 192,
    "inputRowsPerSecond" : 50.982474774296335,
    "processedRowsPerSecond" : 158.41584158415841,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@444eb264",
    "numOutputRows" : 34
  }
}
26/02/12 17:50:54 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/12 17:50:55 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/3 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.3.2bdcc924-e947-4fa9-9011-b6a53e15807a.tmp
26/02/12 17:50:55 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.3.2bdcc924-e947-4fa9-9011-b6a53e15807a.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/3
26/02/12 17:50:55 INFO MicroBatchExecution: Committed offsets for batch 3. Metadata OffsetSeqMetadata(0,1770918655595,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/12 17:50:55 INFO BlockManagerInfo: Removed broadcast_19_piece0 on spark-master:44075 in memory (size: 33.3 KiB, free: 434.2 MiB)
26/02/12 17:50:55 INFO BlockManagerInfo: Removed broadcast_19_piece0 on 172.18.0.14:44081 in memory (size: 33.3 KiB, free: 434.0 MiB)
26/02/12 17:50:55 INFO BlockManagerInfo: Removed broadcast_21_piece0 on spark-master:44075 in memory (size: 19.5 KiB, free: 434.3 MiB)
26/02/12 17:50:55 INFO BlockManagerInfo: Removed broadcast_21_piece0 on 172.18.0.14:44081 in memory (size: 19.5 KiB, free: 434.1 MiB)
26/02/12 17:50:55 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/12 17:50:55 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/12 17:50:55 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/12 17:50:55 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/12 17:50:55 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/12 17:50:55 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/12 17:50:55 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#4513 - origin_code.nullCount#4512) > 0)
26/02/12 17:50:55 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#4518 - destination_code.nullCount#4517) > 0)
26/02/12 17:50:55 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#4548 - callsign.nullCount#4547) > 0)
26/02/12 17:50:55 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/12 17:50:55 INFO DAGScheduler: Got job 15 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/12 17:50:55 INFO DAGScheduler: Final stage: ResultStage 26 (start at NativeMethodAccessorImpl.java:0)
26/02/12 17:50:55 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 25)
26/02/12 17:50:55 INFO DAGScheduler: Missing parents: List()
26/02/12 17:50:55 INFO DAGScheduler: Submitting ResultStage 26 (MapPartitionsRDD[76] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/12 17:50:55 INFO MemoryStore: Block broadcast_22 stored as values in memory (estimated size 99.3 KiB, free 434.0 MiB)
26/02/12 17:50:55 INFO MemoryStore: Block broadcast_22_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 433.9 MiB)
26/02/12 17:50:55 INFO BlockManagerInfo: Added broadcast_22_piece0 in memory on spark-master:44075 (size: 33.3 KiB, free: 434.2 MiB)
26/02/12 17:50:55 INFO SparkContext: Created broadcast 22 from broadcast at DAGScheduler.scala:1585
26/02/12 17:50:55 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 26 (MapPartitionsRDD[76] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/12 17:50:55 INFO TaskSchedulerImpl: Adding task set 26.0 with 4 tasks resource profile 0
26/02/12 17:50:55 INFO TaskSetManager: Starting task 0.0 in stage 26.0 (TID 30) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/12 17:50:55 INFO TaskSetManager: Starting task 1.0 in stage 26.0 (TID 31) (172.18.0.14, executor 1, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/12 17:50:55 INFO BlockManagerInfo: Added broadcast_22_piece0 in memory on 172.18.0.14:44081 (size: 33.3 KiB, free: 434.0 MiB)
26/02/12 17:50:55 INFO TaskSetManager: Starting task 2.0 in stage 26.0 (TID 32) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/12 17:50:55 INFO TaskSetManager: Starting task 3.0 in stage 26.0 (TID 33) (172.18.0.14, executor 1, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/12 17:50:55 INFO TaskSetManager: Finished task 0.0 in stage 26.0 (TID 30) in 58 ms on 172.18.0.14 (executor 1) (1/4)
26/02/12 17:50:55 INFO TaskSetManager: Finished task 1.0 in stage 26.0 (TID 31) in 60 ms on 172.18.0.14 (executor 1) (2/4)
26/02/12 17:50:55 INFO TaskSetManager: Finished task 3.0 in stage 26.0 (TID 33) in 37 ms on 172.18.0.14 (executor 1) (3/4)
26/02/12 17:50:55 INFO TaskSetManager: Finished task 2.0 in stage 26.0 (TID 32) in 40 ms on 172.18.0.14 (executor 1) (4/4)
26/02/12 17:50:55 INFO TaskSchedulerImpl: Removed TaskSet 26.0, whose tasks have all completed, from pool 
26/02/12 17:50:55 INFO DAGScheduler: ResultStage 26 (start at NativeMethodAccessorImpl.java:0) finished in 0.107 s
26/02/12 17:50:55 INFO DAGScheduler: Job 15 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/12 17:50:55 INFO TaskSchedulerImpl: Killing all running tasks in stage 26: Stage finished
26/02/12 17:50:55 INFO DAGScheduler: Job 15 finished: start at NativeMethodAccessorImpl.java:0, took 0.111705 s
26/02/12 17:50:56 INFO MemoryStore: Block broadcast_23_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.8 MiB)
26/02/12 17:50:56 INFO BlockManagerInfo: Added broadcast_23_piece0 in memory on spark-master:44075 (size: 107.2 KiB, free: 434.1 MiB)
26/02/12 17:50:56 INFO SparkContext: Created broadcast 23 from start at NativeMethodAccessorImpl.java:0
26/02/12 17:50:56 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 3, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@2f2e2e06]. The input RDD has 1 partitions.
26/02/12 17:50:56 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/12 17:50:56 INFO DAGScheduler: Got job 16 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/02/12 17:50:56 INFO DAGScheduler: Final stage: ResultStage 27 (start at NativeMethodAccessorImpl.java:0)
26/02/12 17:50:56 INFO DAGScheduler: Parents of final stage: List()
26/02/12 17:50:56 INFO DAGScheduler: Missing parents: List()
26/02/12 17:50:56 INFO DAGScheduler: Submitting ResultStage 27 (MapPartitionsRDD[82] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/12 17:50:56 INFO MemoryStore: Block broadcast_24 stored as values in memory (estimated size 49.9 KiB, free 433.8 MiB)
26/02/12 17:50:56 INFO MemoryStore: Block broadcast_24_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.8 MiB)
26/02/12 17:50:56 INFO BlockManagerInfo: Added broadcast_24_piece0 in memory on spark-master:44075 (size: 19.5 KiB, free: 434.1 MiB)
26/02/12 17:50:56 INFO SparkContext: Created broadcast 24 from broadcast at DAGScheduler.scala:1585
26/02/12 17:50:56 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 27 (MapPartitionsRDD[82] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/02/12 17:50:56 INFO TaskSchedulerImpl: Adding task set 27.0 with 1 tasks resource profile 0
26/02/12 17:50:56 INFO TaskSetManager: Starting task 0.0 in stage 27.0 (TID 34) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/12 17:50:56 INFO BlockManagerInfo: Added broadcast_24_piece0 in memory on 172.18.0.14:44081 (size: 19.5 KiB, free: 434.0 MiB)
26/02/12 17:50:56 INFO BlockManagerInfo: Added broadcast_23_piece0 in memory on 172.18.0.14:44081 (size: 107.2 KiB, free: 433.9 MiB)
26/02/12 17:50:56 INFO TaskSetManager: Finished task 0.0 in stage 27.0 (TID 34) in 599 ms on 172.18.0.14 (executor 1) (1/1)
26/02/12 17:50:56 INFO TaskSchedulerImpl: Removed TaskSet 27.0, whose tasks have all completed, from pool 
26/02/12 17:50:56 INFO DAGScheduler: ResultStage 27 (start at NativeMethodAccessorImpl.java:0) finished in 0.610 s
26/02/12 17:50:56 INFO DAGScheduler: Job 16 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/12 17:50:56 INFO TaskSchedulerImpl: Killing all running tasks in stage 27: Stage finished
26/02/12 17:50:56 INFO DAGScheduler: Job 16 finished: start at NativeMethodAccessorImpl.java:0, took 0.613815 s
26/02/12 17:50:56 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 3, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@2f2e2e06] is committing.
26/02/12 17:50:56 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 3, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@2f2e2e06] committed.
26/02/12 17:50:56 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/3 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.3.bb65bebc-bb6a-4303-807d-787a76059692.tmp
26/02/12 17:50:56 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.3.bb65bebc-bb6a-4303-807d-787a76059692.tmp to file:/tmp/spark-checkpoint-enrichment/commits/3
26/02/12 17:50:56 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "801223b3-8a58-496f-9f28-5a9fda8cbb73",
  "runId" : "005c83b3-ee40-4ce7-8dfd-e6942daa9533",
  "name" : null,
  "timestamp" : "2026-02-12T17:50:55.592Z",
  "batchId" : 3,
  "numInputRows" : 1,
  "inputRowsPerSecond" : 71.42857142857143,
  "processedRowsPerSecond" : 0.8976660682226211,
  "durationMs" : {
    "addBatch" : 910,
    "commitOffsets" : 66,
    "getBatch" : 0,
    "latestOffset" : 3,
    "queryPlanning" : 71,
    "triggerExecution" : 1114,
    "walCommit" : 64
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "0" : 39274
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "0" : 39275
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "0" : 39275
      }
    },
    "numInputRows" : 1,
    "inputRowsPerSecond" : 71.42857142857143,
    "processedRowsPerSecond" : 0.8976660682226211,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@444eb264",
    "numOutputRows" : 1
  }
}
26/02/12 17:50:56 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/4 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.4.293613a5-0297-4598-8186-f2908bd89fa1.tmp
26/02/12 17:50:56 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.4.293613a5-0297-4598-8186-f2908bd89fa1.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/4
26/02/12 17:50:56 INFO MicroBatchExecution: Committed offsets for batch 4. Metadata OffsetSeqMetadata(0,1770918656710,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/12 17:50:56 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/12 17:50:56 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/12 17:50:56 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/12 17:50:56 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/12 17:50:56 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/12 17:50:56 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/12 17:50:56 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#5367 - origin_code.nullCount#5366) > 0)
26/02/12 17:50:56 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#5372 - destination_code.nullCount#5371) > 0)
26/02/12 17:50:56 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#5402 - callsign.nullCount#5401) > 0)
26/02/12 17:50:56 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/12 17:50:56 INFO DAGScheduler: Got job 17 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/12 17:50:56 INFO DAGScheduler: Final stage: ResultStage 29 (start at NativeMethodAccessorImpl.java:0)
26/02/12 17:50:56 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 28)
26/02/12 17:50:56 INFO DAGScheduler: Missing parents: List()
26/02/12 17:50:56 INFO DAGScheduler: Submitting ResultStage 29 (MapPartitionsRDD[87] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/12 17:50:56 INFO MemoryStore: Block broadcast_25 stored as values in memory (estimated size 99.3 KiB, free 433.7 MiB)
26/02/12 17:50:56 INFO MemoryStore: Block broadcast_25_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 433.6 MiB)
26/02/12 17:50:56 INFO BlockManagerInfo: Added broadcast_25_piece0 in memory on spark-master:44075 (size: 33.3 KiB, free: 434.1 MiB)
26/02/12 17:50:56 INFO SparkContext: Created broadcast 25 from broadcast at DAGScheduler.scala:1585
26/02/12 17:50:56 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 29 (MapPartitionsRDD[87] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/12 17:50:56 INFO TaskSchedulerImpl: Adding task set 29.0 with 4 tasks resource profile 0
26/02/12 17:50:56 INFO TaskSetManager: Starting task 0.0 in stage 29.0 (TID 35) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/12 17:50:56 INFO TaskSetManager: Starting task 1.0 in stage 29.0 (TID 36) (172.18.0.14, executor 1, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/12 17:50:57 INFO BlockManagerInfo: Added broadcast_25_piece0 in memory on 172.18.0.14:44081 (size: 33.3 KiB, free: 433.9 MiB)
26/02/12 17:50:57 INFO TaskSetManager: Starting task 2.0 in stage 29.0 (TID 37) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/12 17:50:57 INFO TaskSetManager: Finished task 1.0 in stage 29.0 (TID 36) in 44 ms on 172.18.0.14 (executor 1) (1/4)
26/02/12 17:50:57 INFO TaskSetManager: Starting task 3.0 in stage 29.0 (TID 38) (172.18.0.14, executor 1, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/12 17:50:57 INFO TaskSetManager: Finished task 0.0 in stage 29.0 (TID 35) in 47 ms on 172.18.0.14 (executor 1) (2/4)
26/02/12 17:50:57 INFO TaskSetManager: Finished task 2.0 in stage 29.0 (TID 37) in 36 ms on 172.18.0.14 (executor 1) (3/4)
26/02/12 17:50:57 INFO TaskSetManager: Finished task 3.0 in stage 29.0 (TID 38) in 33 ms on 172.18.0.14 (executor 1) (4/4)
26/02/12 17:50:57 INFO TaskSchedulerImpl: Removed TaskSet 29.0, whose tasks have all completed, from pool 
26/02/12 17:50:57 INFO DAGScheduler: ResultStage 29 (start at NativeMethodAccessorImpl.java:0) finished in 0.087 s
26/02/12 17:50:57 INFO DAGScheduler: Job 17 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/12 17:50:57 INFO TaskSchedulerImpl: Killing all running tasks in stage 29: Stage finished
26/02/12 17:50:57 INFO DAGScheduler: Job 17 finished: start at NativeMethodAccessorImpl.java:0, took 0.090655 s
26/02/12 17:50:57 INFO MemoryStore: Block broadcast_26_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.5 MiB)
26/02/12 17:50:57 INFO BlockManagerInfo: Added broadcast_26_piece0 in memory on spark-master:44075 (size: 107.2 KiB, free: 434.0 MiB)
26/02/12 17:50:57 INFO SparkContext: Created broadcast 26 from start at NativeMethodAccessorImpl.java:0
26/02/12 17:50:57 INFO BlockManagerInfo: Removed broadcast_23_piece0 on spark-master:44075 in memory (size: 107.2 KiB, free: 434.1 MiB)
26/02/12 17:50:57 INFO BlockManagerInfo: Removed broadcast_23_piece0 on 172.18.0.14:44081 in memory (size: 107.2 KiB, free: 434.0 MiB)
26/02/12 17:50:57 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 4, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@46b06b70]. The input RDD has 1 partitions.
26/02/12 17:50:57 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/12 17:50:57 INFO DAGScheduler: Got job 18 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/02/12 17:50:57 INFO DAGScheduler: Final stage: ResultStage 30 (start at NativeMethodAccessorImpl.java:0)
26/02/12 17:50:57 INFO DAGScheduler: Parents of final stage: List()
26/02/12 17:50:57 INFO DAGScheduler: Missing parents: List()
26/02/12 17:50:57 INFO DAGScheduler: Submitting ResultStage 30 (MapPartitionsRDD[93] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/12 17:50:57 INFO BlockManagerInfo: Removed broadcast_25_piece0 on spark-master:44075 in memory (size: 33.3 KiB, free: 434.1 MiB)
26/02/12 17:50:57 INFO BlockManagerInfo: Removed broadcast_25_piece0 on 172.18.0.14:44081 in memory (size: 33.3 KiB, free: 434.0 MiB)
26/02/12 17:50:57 INFO MemoryStore: Block broadcast_27 stored as values in memory (estimated size 49.9 KiB, free 433.7 MiB)
26/02/12 17:50:57 INFO MemoryStore: Block broadcast_27_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.7 MiB)
26/02/12 17:50:57 INFO BlockManagerInfo: Added broadcast_27_piece0 in memory on spark-master:44075 (size: 19.5 KiB, free: 434.1 MiB)
26/02/12 17:50:57 INFO SparkContext: Created broadcast 27 from broadcast at DAGScheduler.scala:1585
26/02/12 17:50:57 INFO BlockManagerInfo: Removed broadcast_24_piece0 on spark-master:44075 in memory (size: 19.5 KiB, free: 434.1 MiB)
26/02/12 17:50:57 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 30 (MapPartitionsRDD[93] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/02/12 17:50:57 INFO TaskSchedulerImpl: Adding task set 30.0 with 1 tasks resource profile 0
26/02/12 17:50:57 INFO BlockManagerInfo: Removed broadcast_24_piece0 on 172.18.0.14:44081 in memory (size: 19.5 KiB, free: 434.0 MiB)
26/02/12 17:50:57 INFO TaskSetManager: Starting task 0.0 in stage 30.0 (TID 39) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/12 17:50:57 INFO BlockManagerInfo: Removed broadcast_22_piece0 on spark-master:44075 in memory (size: 33.3 KiB, free: 434.1 MiB)
26/02/12 17:50:57 INFO BlockManagerInfo: Removed broadcast_22_piece0 on 172.18.0.14:44081 in memory (size: 33.3 KiB, free: 434.1 MiB)
26/02/12 17:50:57 INFO BlockManagerInfo: Added broadcast_27_piece0 in memory on 172.18.0.14:44081 (size: 19.5 KiB, free: 434.0 MiB)
26/02/12 17:50:57 INFO BlockManagerInfo: Added broadcast_26_piece0 in memory on 172.18.0.14:44081 (size: 107.2 KiB, free: 433.9 MiB)
26/02/12 17:50:57 INFO TaskSetManager: Finished task 0.0 in stage 30.0 (TID 39) in 262 ms on 172.18.0.14 (executor 1) (1/1)
26/02/12 17:50:57 INFO TaskSchedulerImpl: Removed TaskSet 30.0, whose tasks have all completed, from pool 
26/02/12 17:50:57 INFO DAGScheduler: ResultStage 30 (start at NativeMethodAccessorImpl.java:0) finished in 0.284 s
26/02/12 17:50:57 INFO DAGScheduler: Job 18 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/12 17:50:57 INFO TaskSchedulerImpl: Killing all running tasks in stage 30: Stage finished
26/02/12 17:50:57 INFO DAGScheduler: Job 18 finished: start at NativeMethodAccessorImpl.java:0, took 0.293836 s
26/02/12 17:50:57 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 4, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@46b06b70] is committing.
26/02/12 17:50:57 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 4, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@46b06b70] committed.
26/02/12 17:50:57 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/4 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.4.0f1ce947-9769-4805-93b8-c885558d33ad.tmp
26/02/12 17:50:57 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.4.0f1ce947-9769-4805-93b8-c885558d33ad.tmp to file:/tmp/spark-checkpoint-enrichment/commits/4
26/02/12 17:50:57 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "801223b3-8a58-496f-9f28-5a9fda8cbb73",
  "runId" : "005c83b3-ee40-4ce7-8dfd-e6942daa9533",
  "name" : null,
  "timestamp" : "2026-02-12T17:50:56.708Z",
  "batchId" : 4,
  "numInputRows" : 225,
  "inputRowsPerSecond" : 201.61290322580643,
  "processedRowsPerSecond" : 289.9484536082474,
  "durationMs" : {
    "addBatch" : 572,
    "commitOffsets" : 78,
    "getBatch" : 0,
    "latestOffset" : 2,
    "queryPlanning" : 61,
    "triggerExecution" : 776,
    "walCommit" : 63
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "0" : 39275
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "0" : 39500
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "0" : 39500
      }
    },
    "numInputRows" : 225,
    "inputRowsPerSecond" : 201.61290322580643,
    "processedRowsPerSecond" : 289.9484536082474,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@444eb264",
    "numOutputRows" : 47
  }
}
26/02/12 17:50:57 INFO BlockManagerInfo: Removed broadcast_27_piece0 on spark-master:44075 in memory (size: 19.5 KiB, free: 434.2 MiB)
26/02/12 17:50:57 INFO BlockManagerInfo: Removed broadcast_27_piece0 on 172.18.0.14:44081 in memory (size: 19.5 KiB, free: 434.0 MiB)
26/02/12 17:51:07 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/12 17:51:11 INFO BlockManagerInfo: Removed broadcast_20_piece0 on spark-master:44075 in memory (size: 107.2 KiB, free: 434.3 MiB)
26/02/12 17:51:11 INFO BlockManagerInfo: Removed broadcast_20_piece0 on 172.18.0.14:44081 in memory (size: 107.2 KiB, free: 434.1 MiB)
26/02/12 17:51:11 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/5 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.5.23951ade-aada-4a06-866f-fcb5b276b002.tmp
26/02/12 17:51:11 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.5.23951ade-aada-4a06-866f-fcb5b276b002.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/5
26/02/12 17:51:11 INFO MicroBatchExecution: Committed offsets for batch 5. Metadata OffsetSeqMetadata(0,1770918671675,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/12 17:51:11 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/12 17:51:11 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/12 17:51:11 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/12 17:51:11 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/12 17:51:11 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/12 17:51:11 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/12 17:51:11 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#6221 - origin_code.nullCount#6220) > 0)
26/02/12 17:51:11 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#6226 - destination_code.nullCount#6225) > 0)
26/02/12 17:51:11 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#6256 - callsign.nullCount#6255) > 0)
26/02/12 17:51:11 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/12 17:51:11 INFO DAGScheduler: Got job 19 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/12 17:51:11 INFO DAGScheduler: Final stage: ResultStage 32 (start at NativeMethodAccessorImpl.java:0)
26/02/12 17:51:11 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 31)
26/02/12 17:51:11 INFO DAGScheduler: Missing parents: List()
26/02/12 17:51:11 INFO DAGScheduler: Submitting ResultStage 32 (MapPartitionsRDD[98] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/12 17:51:11 INFO MemoryStore: Block broadcast_28 stored as values in memory (estimated size 99.3 KiB, free 434.0 MiB)
26/02/12 17:51:11 INFO MemoryStore: Block broadcast_28_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 433.9 MiB)
26/02/12 17:51:11 INFO BlockManagerInfo: Added broadcast_28_piece0 in memory on spark-master:44075 (size: 33.3 KiB, free: 434.2 MiB)
26/02/12 17:51:11 INFO SparkContext: Created broadcast 28 from broadcast at DAGScheduler.scala:1585
26/02/12 17:51:11 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 32 (MapPartitionsRDD[98] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/12 17:51:11 INFO TaskSchedulerImpl: Adding task set 32.0 with 4 tasks resource profile 0
26/02/12 17:51:11 INFO TaskSetManager: Starting task 0.0 in stage 32.0 (TID 40) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/12 17:51:11 INFO TaskSetManager: Starting task 1.0 in stage 32.0 (TID 41) (172.18.0.14, executor 1, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/12 17:51:12 INFO BlockManagerInfo: Added broadcast_28_piece0 in memory on 172.18.0.14:44081 (size: 33.3 KiB, free: 434.0 MiB)
26/02/12 17:51:12 INFO TaskSetManager: Starting task 2.0 in stage 32.0 (TID 42) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/12 17:51:12 INFO TaskSetManager: Finished task 0.0 in stage 32.0 (TID 40) in 50 ms on 172.18.0.14 (executor 1) (1/4)
26/02/12 17:51:12 INFO TaskSetManager: Finished task 1.0 in stage 32.0 (TID 41) in 50 ms on 172.18.0.14 (executor 1) (2/4)
26/02/12 17:51:12 INFO TaskSetManager: Starting task 3.0 in stage 32.0 (TID 43) (172.18.0.14, executor 1, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/12 17:51:12 INFO TaskSetManager: Finished task 2.0 in stage 32.0 (TID 42) in 34 ms on 172.18.0.14 (executor 1) (3/4)
26/02/12 17:51:12 INFO TaskSetManager: Finished task 3.0 in stage 32.0 (TID 43) in 37 ms on 172.18.0.14 (executor 1) (4/4)
26/02/12 17:51:12 INFO TaskSchedulerImpl: Removed TaskSet 32.0, whose tasks have all completed, from pool 
26/02/12 17:51:12 INFO DAGScheduler: ResultStage 32 (start at NativeMethodAccessorImpl.java:0) finished in 0.106 s
26/02/12 17:51:12 INFO DAGScheduler: Job 19 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/12 17:51:12 INFO TaskSchedulerImpl: Killing all running tasks in stage 32: Stage finished
26/02/12 17:51:12 INFO DAGScheduler: Job 19 finished: start at NativeMethodAccessorImpl.java:0, took 0.112011 s
26/02/12 17:51:12 INFO MemoryStore: Block broadcast_29_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.8 MiB)
26/02/12 17:51:12 INFO BlockManagerInfo: Added broadcast_29_piece0 in memory on spark-master:44075 (size: 107.2 KiB, free: 434.1 MiB)
26/02/12 17:51:12 INFO SparkContext: Created broadcast 29 from start at NativeMethodAccessorImpl.java:0
26/02/12 17:51:12 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 5, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@1d26d310]. The input RDD has 1 partitions.
26/02/12 17:51:12 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/12 17:51:12 INFO DAGScheduler: Got job 20 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/02/12 17:51:12 INFO DAGScheduler: Final stage: ResultStage 33 (start at NativeMethodAccessorImpl.java:0)
26/02/12 17:51:12 INFO DAGScheduler: Parents of final stage: List()
26/02/12 17:51:12 INFO DAGScheduler: Missing parents: List()
26/02/12 17:51:12 INFO DAGScheduler: Submitting ResultStage 33 (MapPartitionsRDD[104] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/12 17:51:12 INFO MemoryStore: Block broadcast_30 stored as values in memory (estimated size 49.9 KiB, free 433.8 MiB)
26/02/12 17:51:12 INFO MemoryStore: Block broadcast_30_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.8 MiB)
26/02/12 17:51:12 INFO BlockManagerInfo: Added broadcast_30_piece0 in memory on spark-master:44075 (size: 19.5 KiB, free: 434.1 MiB)
26/02/12 17:51:12 INFO SparkContext: Created broadcast 30 from broadcast at DAGScheduler.scala:1585
26/02/12 17:51:12 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 33 (MapPartitionsRDD[104] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/02/12 17:51:12 INFO TaskSchedulerImpl: Adding task set 33.0 with 1 tasks resource profile 0
26/02/12 17:51:12 INFO TaskSetManager: Starting task 0.0 in stage 33.0 (TID 44) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/12 17:51:12 INFO BlockManagerInfo: Added broadcast_30_piece0 in memory on 172.18.0.14:44081 (size: 19.5 KiB, free: 434.0 MiB)
26/02/12 17:51:12 INFO BlockManagerInfo: Added broadcast_29_piece0 in memory on 172.18.0.14:44081 (size: 107.2 KiB, free: 433.9 MiB)
26/02/12 17:51:12 INFO TaskSetManager: Finished task 0.0 in stage 33.0 (TID 44) in 656 ms on 172.18.0.14 (executor 1) (1/1)
26/02/12 17:51:12 INFO TaskSchedulerImpl: Removed TaskSet 33.0, whose tasks have all completed, from pool 
26/02/12 17:51:12 INFO DAGScheduler: ResultStage 33 (start at NativeMethodAccessorImpl.java:0) finished in 0.666 s
26/02/12 17:51:12 INFO DAGScheduler: Job 20 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/12 17:51:12 INFO TaskSchedulerImpl: Killing all running tasks in stage 33: Stage finished
26/02/12 17:51:12 INFO DAGScheduler: Job 20 finished: start at NativeMethodAccessorImpl.java:0, took 0.671845 s
26/02/12 17:51:12 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 5, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@1d26d310] is committing.
26/02/12 17:51:12 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 5, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@1d26d310] committed.
26/02/12 17:51:12 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/5 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.5.55f91420-ea49-493b-958f-a99d7013bcb4.tmp
26/02/12 17:51:12 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.5.55f91420-ea49-493b-958f-a99d7013bcb4.tmp to file:/tmp/spark-checkpoint-enrichment/commits/5
26/02/12 17:51:12 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "801223b3-8a58-496f-9f28-5a9fda8cbb73",
  "runId" : "005c83b3-ee40-4ce7-8dfd-e6942daa9533",
  "name" : null,
  "timestamp" : "2026-02-12T17:51:11.672Z",
  "batchId" : 5,
  "numInputRows" : 74,
  "inputRowsPerSecond" : 6166.666666666667,
  "processedRowsPerSecond" : 62.76505513146734,
  "durationMs" : {
    "addBatch" : 963,
    "commitOffsets" : 70,
    "getBatch" : 0,
    "latestOffset" : 3,
    "queryPlanning" : 68,
    "triggerExecution" : 1179,
    "walCommit" : 74
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "0" : 39500
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "0" : 39574
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "0" : 39574
      }
    },
    "numInputRows" : 74,
    "inputRowsPerSecond" : 6166.666666666667,
    "processedRowsPerSecond" : 62.76505513146734,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@444eb264",
    "numOutputRows" : 27
  }
}
26/02/12 17:51:12 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/6 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.6.f4c20694-cac9-4e1b-ab78-c1c79efbc8bf.tmp
26/02/12 17:51:12 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.6.f4c20694-cac9-4e1b-ab78-c1c79efbc8bf.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/6
26/02/12 17:51:12 INFO MicroBatchExecution: Committed offsets for batch 6. Metadata OffsetSeqMetadata(0,1770918672857,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/12 17:51:12 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/12 17:51:12 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/12 17:51:12 INFO BlockManagerInfo: Removed broadcast_28_piece0 on spark-master:44075 in memory (size: 33.3 KiB, free: 434.1 MiB)
26/02/12 17:51:12 INFO BlockManagerInfo: Removed broadcast_28_piece0 on 172.18.0.14:44081 in memory (size: 33.3 KiB, free: 433.9 MiB)
26/02/12 17:51:13 INFO BlockManagerInfo: Removed broadcast_29_piece0 on spark-master:44075 in memory (size: 107.2 KiB, free: 434.2 MiB)
26/02/12 17:51:13 INFO BlockManagerInfo: Removed broadcast_29_piece0 on 172.18.0.14:44081 in memory (size: 107.2 KiB, free: 434.0 MiB)
26/02/12 17:51:13 INFO BlockManagerInfo: Removed broadcast_30_piece0 on spark-master:44075 in memory (size: 19.5 KiB, free: 434.3 MiB)
26/02/12 17:51:13 INFO BlockManagerInfo: Removed broadcast_30_piece0 on 172.18.0.14:44081 in memory (size: 19.5 KiB, free: 434.1 MiB)
26/02/12 17:51:13 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/12 17:51:13 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/12 17:51:13 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/12 17:51:13 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/12 17:51:13 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#7075 - origin_code.nullCount#7074) > 0)
26/02/12 17:51:13 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#7080 - destination_code.nullCount#7079) > 0)
26/02/12 17:51:13 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#7110 - callsign.nullCount#7109) > 0)
26/02/12 17:51:13 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/12 17:51:13 INFO DAGScheduler: Got job 21 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/12 17:51:13 INFO DAGScheduler: Final stage: ResultStage 35 (start at NativeMethodAccessorImpl.java:0)
26/02/12 17:51:13 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 34)
26/02/12 17:51:13 INFO DAGScheduler: Missing parents: List()
26/02/12 17:51:13 INFO DAGScheduler: Submitting ResultStage 35 (MapPartitionsRDD[109] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/12 17:51:13 INFO MemoryStore: Block broadcast_31 stored as values in memory (estimated size 99.3 KiB, free 434.0 MiB)
26/02/12 17:51:13 INFO MemoryStore: Block broadcast_31_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 433.9 MiB)
26/02/12 17:51:13 INFO BlockManagerInfo: Added broadcast_31_piece0 in memory on spark-master:44075 (size: 33.3 KiB, free: 434.2 MiB)
26/02/12 17:51:13 INFO SparkContext: Created broadcast 31 from broadcast at DAGScheduler.scala:1585
26/02/12 17:51:13 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 35 (MapPartitionsRDD[109] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/12 17:51:13 INFO TaskSchedulerImpl: Adding task set 35.0 with 4 tasks resource profile 0
26/02/12 17:51:13 INFO TaskSetManager: Starting task 0.0 in stage 35.0 (TID 45) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/12 17:51:13 INFO TaskSetManager: Starting task 1.0 in stage 35.0 (TID 46) (172.18.0.14, executor 1, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/12 17:51:13 INFO BlockManagerInfo: Added broadcast_31_piece0 in memory on 172.18.0.14:44081 (size: 33.3 KiB, free: 434.0 MiB)
26/02/12 17:51:13 INFO TaskSetManager: Starting task 2.0 in stage 35.0 (TID 47) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/12 17:51:13 INFO TaskSetManager: Finished task 0.0 in stage 35.0 (TID 45) in 37 ms on 172.18.0.14 (executor 1) (1/4)
26/02/12 17:51:13 INFO TaskSetManager: Starting task 3.0 in stage 35.0 (TID 48) (172.18.0.14, executor 1, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/12 17:51:13 INFO TaskSetManager: Finished task 1.0 in stage 35.0 (TID 46) in 43 ms on 172.18.0.14 (executor 1) (2/4)
26/02/12 17:51:13 INFO TaskSetManager: Finished task 2.0 in stage 35.0 (TID 47) in 35 ms on 172.18.0.14 (executor 1) (3/4)
26/02/12 17:51:13 INFO BlockManagerInfo: Removed broadcast_26_piece0 on spark-master:44075 in memory (size: 107.2 KiB, free: 434.3 MiB)
26/02/12 17:51:13 INFO TaskSetManager: Finished task 3.0 in stage 35.0 (TID 48) in 31 ms on 172.18.0.14 (executor 1) (4/4)
26/02/12 17:51:13 INFO TaskSchedulerImpl: Removed TaskSet 35.0, whose tasks have all completed, from pool 
26/02/12 17:51:13 INFO DAGScheduler: ResultStage 35 (start at NativeMethodAccessorImpl.java:0) finished in 0.084 s
26/02/12 17:51:13 INFO DAGScheduler: Job 21 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/12 17:51:13 INFO TaskSchedulerImpl: Killing all running tasks in stage 35: Stage finished
26/02/12 17:51:13 INFO DAGScheduler: Job 21 finished: start at NativeMethodAccessorImpl.java:0, took 0.088109 s
26/02/12 17:51:13 INFO BlockManagerInfo: Removed broadcast_26_piece0 on 172.18.0.14:44081 in memory (size: 107.2 KiB, free: 434.1 MiB)
26/02/12 17:51:13 INFO MemoryStore: Block broadcast_32_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.9 MiB)
26/02/12 17:51:13 INFO BlockManagerInfo: Added broadcast_32_piece0 in memory on spark-master:44075 (size: 107.2 KiB, free: 434.2 MiB)
26/02/12 17:51:13 INFO SparkContext: Created broadcast 32 from start at NativeMethodAccessorImpl.java:0
26/02/12 17:51:13 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 6, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@866cbc2]. The input RDD has 1 partitions.
26/02/12 17:51:13 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/12 17:51:13 INFO DAGScheduler: Got job 22 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/02/12 17:51:13 INFO DAGScheduler: Final stage: ResultStage 36 (start at NativeMethodAccessorImpl.java:0)
26/02/12 17:51:13 INFO DAGScheduler: Parents of final stage: List()
26/02/12 17:51:13 INFO DAGScheduler: Missing parents: List()
26/02/12 17:51:13 INFO DAGScheduler: Submitting ResultStage 36 (MapPartitionsRDD[115] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/12 17:51:13 INFO MemoryStore: Block broadcast_33 stored as values in memory (estimated size 49.9 KiB, free 433.9 MiB)
26/02/12 17:51:13 INFO MemoryStore: Block broadcast_33_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.9 MiB)
26/02/12 17:51:13 INFO BlockManagerInfo: Added broadcast_33_piece0 in memory on spark-master:44075 (size: 19.5 KiB, free: 434.2 MiB)
26/02/12 17:51:13 INFO SparkContext: Created broadcast 33 from broadcast at DAGScheduler.scala:1585
26/02/12 17:51:13 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 36 (MapPartitionsRDD[115] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/02/12 17:51:13 INFO TaskSchedulerImpl: Adding task set 36.0 with 1 tasks resource profile 0
26/02/12 17:51:13 INFO TaskSetManager: Starting task 0.0 in stage 36.0 (TID 49) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/12 17:51:13 INFO BlockManagerInfo: Added broadcast_33_piece0 in memory on 172.18.0.14:44081 (size: 19.5 KiB, free: 434.1 MiB)
26/02/12 17:51:13 INFO BlockManagerInfo: Added broadcast_32_piece0 in memory on 172.18.0.14:44081 (size: 107.2 KiB, free: 434.0 MiB)
26/02/12 17:51:13 INFO TaskSetManager: Finished task 0.0 in stage 36.0 (TID 49) in 114 ms on 172.18.0.14 (executor 1) (1/1)
26/02/12 17:51:13 INFO TaskSchedulerImpl: Removed TaskSet 36.0, whose tasks have all completed, from pool 
26/02/12 17:51:13 INFO DAGScheduler: ResultStage 36 (start at NativeMethodAccessorImpl.java:0) finished in 0.122 s
26/02/12 17:51:13 INFO DAGScheduler: Job 22 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/12 17:51:13 INFO TaskSchedulerImpl: Killing all running tasks in stage 36: Stage finished
26/02/12 17:51:13 INFO DAGScheduler: Job 22 finished: start at NativeMethodAccessorImpl.java:0, took 0.125059 s
26/02/12 17:51:13 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 6, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@866cbc2] is committing.
26/02/12 17:51:13 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 6, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@866cbc2] committed.
26/02/12 17:51:13 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/6 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.6.1bdf8cd1-7fa4-49d5-bcfb-fc8630bb3b56.tmp
26/02/12 17:51:13 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.6.1bdf8cd1-7fa4-49d5-bcfb-fc8630bb3b56.tmp to file:/tmp/spark-checkpoint-enrichment/commits/6
26/02/12 17:51:13 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "801223b3-8a58-496f-9f28-5a9fda8cbb73",
  "runId" : "005c83b3-ee40-4ce7-8dfd-e6942daa9533",
  "name" : null,
  "timestamp" : "2026-02-12T17:51:12.852Z",
  "batchId" : 6,
  "numInputRows" : 155,
  "inputRowsPerSecond" : 131.35593220338984,
  "processedRowsPerSecond" : 258.33333333333337,
  "durationMs" : {
    "addBatch" : 403,
    "commitOffsets" : 71,
    "getBatch" : 0,
    "latestOffset" : 5,
    "queryPlanning" : 53,
    "triggerExecution" : 600,
    "walCommit" : 68
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "0" : 39574
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "0" : 39729
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "0" : 39729
      }
    },
    "numInputRows" : 155,
    "inputRowsPerSecond" : 131.35593220338984,
    "processedRowsPerSecond" : 258.33333333333337,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@444eb264",
    "numOutputRows" : 21
  }
}
26/02/12 17:51:23 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/12 17:51:24 INFO BlockManagerInfo: Removed broadcast_31_piece0 on spark-master:44075 in memory (size: 33.3 KiB, free: 434.2 MiB)
26/02/12 17:51:24 INFO BlockManagerInfo: Removed broadcast_31_piece0 on 172.18.0.14:44081 in memory (size: 33.3 KiB, free: 434.0 MiB)
26/02/12 17:51:24 INFO BlockManagerInfo: Removed broadcast_33_piece0 on spark-master:44075 in memory (size: 19.5 KiB, free: 434.3 MiB)
26/02/12 17:51:24 INFO BlockManagerInfo: Removed broadcast_33_piece0 on 172.18.0.14:44081 in memory (size: 19.5 KiB, free: 434.1 MiB)
26/02/12 17:51:27 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/7 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.7.93c8155f-7daf-4e8f-b6a0-4bdfa70d45b3.tmp
26/02/12 17:51:27 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.7.93c8155f-7daf-4e8f-b6a0-4bdfa70d45b3.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/7
26/02/12 17:51:27 INFO MicroBatchExecution: Committed offsets for batch 7. Metadata OffsetSeqMetadata(0,1770918687768,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/12 17:51:27 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/12 17:51:27 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/12 17:51:27 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/12 17:51:27 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/12 17:51:27 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/12 17:51:27 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/12 17:51:27 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#7929 - origin_code.nullCount#7928) > 0)
26/02/12 17:51:27 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#7934 - destination_code.nullCount#7933) > 0)
26/02/12 17:51:27 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#7964 - callsign.nullCount#7963) > 0)
26/02/12 17:51:27 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/12 17:51:27 INFO DAGScheduler: Got job 23 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/12 17:51:27 INFO DAGScheduler: Final stage: ResultStage 38 (start at NativeMethodAccessorImpl.java:0)
26/02/12 17:51:27 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 37)
26/02/12 17:51:27 INFO DAGScheduler: Missing parents: List()
26/02/12 17:51:27 INFO DAGScheduler: Submitting ResultStage 38 (MapPartitionsRDD[120] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/12 17:51:28 INFO MemoryStore: Block broadcast_34 stored as values in memory (estimated size 99.3 KiB, free 434.0 MiB)
26/02/12 17:51:28 INFO MemoryStore: Block broadcast_34_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 433.9 MiB)
26/02/12 17:51:28 INFO BlockManagerInfo: Added broadcast_34_piece0 in memory on spark-master:44075 (size: 33.3 KiB, free: 434.2 MiB)
26/02/12 17:51:28 INFO SparkContext: Created broadcast 34 from broadcast at DAGScheduler.scala:1585
26/02/12 17:51:28 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 38 (MapPartitionsRDD[120] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/12 17:51:28 INFO TaskSchedulerImpl: Adding task set 38.0 with 4 tasks resource profile 0
26/02/12 17:51:28 INFO TaskSetManager: Starting task 0.0 in stage 38.0 (TID 50) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/12 17:51:28 INFO TaskSetManager: Starting task 1.0 in stage 38.0 (TID 51) (172.18.0.14, executor 1, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/12 17:51:28 INFO BlockManagerInfo: Added broadcast_34_piece0 in memory on 172.18.0.14:44081 (size: 33.3 KiB, free: 434.0 MiB)
26/02/12 17:51:28 INFO TaskSetManager: Starting task 2.0 in stage 38.0 (TID 52) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/12 17:51:28 INFO TaskSetManager: Starting task 3.0 in stage 38.0 (TID 53) (172.18.0.14, executor 1, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/12 17:51:28 INFO TaskSetManager: Finished task 0.0 in stage 38.0 (TID 50) in 44 ms on 172.18.0.14 (executor 1) (1/4)
26/02/12 17:51:28 INFO TaskSetManager: Finished task 1.0 in stage 38.0 (TID 51) in 48 ms on 172.18.0.14 (executor 1) (2/4)
26/02/12 17:51:28 INFO TaskSetManager: Finished task 2.0 in stage 38.0 (TID 52) in 29 ms on 172.18.0.14 (executor 1) (3/4)
26/02/12 17:51:28 INFO TaskSetManager: Finished task 3.0 in stage 38.0 (TID 53) in 26 ms on 172.18.0.14 (executor 1) (4/4)
26/02/12 17:51:28 INFO TaskSchedulerImpl: Removed TaskSet 38.0, whose tasks have all completed, from pool 
26/02/12 17:51:28 INFO DAGScheduler: ResultStage 38 (start at NativeMethodAccessorImpl.java:0) finished in 0.082 s
26/02/12 17:51:28 INFO DAGScheduler: Job 23 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/12 17:51:28 INFO TaskSchedulerImpl: Killing all running tasks in stage 38: Stage finished
26/02/12 17:51:28 INFO DAGScheduler: Job 23 finished: start at NativeMethodAccessorImpl.java:0, took 0.085495 s
26/02/12 17:51:28 INFO MemoryStore: Block broadcast_35_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.8 MiB)
26/02/12 17:51:28 INFO BlockManagerInfo: Added broadcast_35_piece0 in memory on spark-master:44075 (size: 107.2 KiB, free: 434.1 MiB)
26/02/12 17:51:28 INFO SparkContext: Created broadcast 35 from start at NativeMethodAccessorImpl.java:0
26/02/12 17:51:28 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 7, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@deae03d]. The input RDD has 1 partitions.
26/02/12 17:51:28 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/12 17:51:28 INFO DAGScheduler: Got job 24 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/02/12 17:51:28 INFO DAGScheduler: Final stage: ResultStage 39 (start at NativeMethodAccessorImpl.java:0)
26/02/12 17:51:28 INFO DAGScheduler: Parents of final stage: List()
26/02/12 17:51:28 INFO DAGScheduler: Missing parents: List()
26/02/12 17:51:28 INFO DAGScheduler: Submitting ResultStage 39 (MapPartitionsRDD[126] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/12 17:51:28 INFO MemoryStore: Block broadcast_36 stored as values in memory (estimated size 49.9 KiB, free 433.8 MiB)
26/02/12 17:51:28 INFO MemoryStore: Block broadcast_36_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.8 MiB)
26/02/12 17:51:28 INFO BlockManagerInfo: Added broadcast_36_piece0 in memory on spark-master:44075 (size: 19.5 KiB, free: 434.1 MiB)
26/02/12 17:51:28 INFO SparkContext: Created broadcast 36 from broadcast at DAGScheduler.scala:1585
26/02/12 17:51:28 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 39 (MapPartitionsRDD[126] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/02/12 17:51:28 INFO TaskSchedulerImpl: Adding task set 39.0 with 1 tasks resource profile 0
26/02/12 17:51:28 INFO TaskSetManager: Starting task 0.0 in stage 39.0 (TID 54) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/12 17:51:28 INFO BlockManagerInfo: Added broadcast_36_piece0 in memory on 172.18.0.14:44081 (size: 19.5 KiB, free: 434.0 MiB)
26/02/12 17:51:28 INFO BlockManagerInfo: Added broadcast_35_piece0 in memory on 172.18.0.14:44081 (size: 107.2 KiB, free: 433.9 MiB)
26/02/12 17:51:28 INFO TaskSetManager: Finished task 0.0 in stage 39.0 (TID 54) in 604 ms on 172.18.0.14 (executor 1) (1/1)
26/02/12 17:51:28 INFO TaskSchedulerImpl: Removed TaskSet 39.0, whose tasks have all completed, from pool 
26/02/12 17:51:28 INFO DAGScheduler: ResultStage 39 (start at NativeMethodAccessorImpl.java:0) finished in 0.611 s
26/02/12 17:51:28 INFO DAGScheduler: Job 24 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/12 17:51:28 INFO TaskSchedulerImpl: Killing all running tasks in stage 39: Stage finished
26/02/12 17:51:28 INFO DAGScheduler: Job 24 finished: start at NativeMethodAccessorImpl.java:0, took 0.613835 s
26/02/12 17:51:28 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 7, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@deae03d] is committing.
26/02/12 17:51:28 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 7, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@deae03d] committed.
26/02/12 17:51:28 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/7 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.7.5512997e-d633-44a5-b9cd-a6fc2cc72938.tmp
26/02/12 17:51:28 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.7.5512997e-d633-44a5-b9cd-a6fc2cc72938.tmp to file:/tmp/spark-checkpoint-enrichment/commits/7
26/02/12 17:51:28 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "801223b3-8a58-496f-9f28-5a9fda8cbb73",
  "runId" : "005c83b3-ee40-4ce7-8dfd-e6942daa9533",
  "name" : null,
  "timestamp" : "2026-02-12T17:51:27.766Z",
  "batchId" : 7,
  "numInputRows" : 37,
  "inputRowsPerSecond" : 2846.153846153846,
  "processedRowsPerSecond" : 36.489151873767256,
  "durationMs" : {
    "addBatch" : 821,
    "commitOffsets" : 61,
    "getBatch" : 0,
    "latestOffset" : 2,
    "queryPlanning" : 45,
    "triggerExecution" : 1014,
    "walCommit" : 85
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "0" : 39729
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "0" : 39766
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "0" : 39766
      }
    },
    "numInputRows" : 37,
    "inputRowsPerSecond" : 2846.153846153846,
    "processedRowsPerSecond" : 36.489151873767256,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@444eb264",
    "numOutputRows" : 15
  }
}
26/02/12 17:51:28 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/8 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.8.e0670792-b2b6-4df5-8c09-4cd0f9a64bf3.tmp
26/02/12 17:51:28 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.8.e0670792-b2b6-4df5-8c09-4cd0f9a64bf3.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/8
26/02/12 17:51:28 INFO MicroBatchExecution: Committed offsets for batch 8. Metadata OffsetSeqMetadata(0,1770918688782,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/12 17:51:28 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/12 17:51:28 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/12 17:51:28 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/12 17:51:28 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/12 17:51:28 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/12 17:51:28 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/12 17:51:28 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#8783 - origin_code.nullCount#8782) > 0)
26/02/12 17:51:28 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#8788 - destination_code.nullCount#8787) > 0)
26/02/12 17:51:28 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#8818 - callsign.nullCount#8817) > 0)
26/02/12 17:51:28 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/12 17:51:28 INFO DAGScheduler: Got job 25 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/12 17:51:28 INFO DAGScheduler: Final stage: ResultStage 41 (start at NativeMethodAccessorImpl.java:0)
26/02/12 17:51:28 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 40)
26/02/12 17:51:28 INFO DAGScheduler: Missing parents: List()
26/02/12 17:51:28 INFO DAGScheduler: Submitting ResultStage 41 (MapPartitionsRDD[131] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/12 17:51:28 INFO MemoryStore: Block broadcast_37 stored as values in memory (estimated size 99.3 KiB, free 433.7 MiB)
26/02/12 17:51:28 INFO MemoryStore: Block broadcast_37_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 433.6 MiB)
26/02/12 17:51:28 INFO BlockManagerInfo: Added broadcast_37_piece0 in memory on spark-master:44075 (size: 33.3 KiB, free: 434.1 MiB)
26/02/12 17:51:28 INFO SparkContext: Created broadcast 37 from broadcast at DAGScheduler.scala:1585
26/02/12 17:51:28 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 41 (MapPartitionsRDD[131] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/12 17:51:28 INFO TaskSchedulerImpl: Adding task set 41.0 with 4 tasks resource profile 0
26/02/12 17:51:28 INFO TaskSetManager: Starting task 0.0 in stage 41.0 (TID 55) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/12 17:51:28 INFO TaskSetManager: Starting task 1.0 in stage 41.0 (TID 56) (172.18.0.14, executor 1, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/12 17:51:29 INFO BlockManagerInfo: Added broadcast_37_piece0 in memory on 172.18.0.14:44081 (size: 33.3 KiB, free: 433.9 MiB)
26/02/12 17:51:29 INFO TaskSetManager: Starting task 2.0 in stage 41.0 (TID 57) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/12 17:51:29 INFO TaskSetManager: Finished task 0.0 in stage 41.0 (TID 55) in 33 ms on 172.18.0.14 (executor 1) (1/4)
26/02/12 17:51:29 INFO TaskSetManager: Starting task 3.0 in stage 41.0 (TID 58) (172.18.0.14, executor 1, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/12 17:51:29 INFO TaskSetManager: Finished task 1.0 in stage 41.0 (TID 56) in 44 ms on 172.18.0.14 (executor 1) (2/4)
26/02/12 17:51:29 INFO TaskSetManager: Finished task 2.0 in stage 41.0 (TID 57) in 31 ms on 172.18.0.14 (executor 1) (3/4)
26/02/12 17:51:29 INFO TaskSetManager: Finished task 3.0 in stage 41.0 (TID 58) in 29 ms on 172.18.0.14 (executor 1) (4/4)
26/02/12 17:51:29 INFO TaskSchedulerImpl: Removed TaskSet 41.0, whose tasks have all completed, from pool 
26/02/12 17:51:29 INFO DAGScheduler: ResultStage 41 (start at NativeMethodAccessorImpl.java:0) finished in 0.079 s
26/02/12 17:51:29 INFO DAGScheduler: Job 25 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/12 17:51:29 INFO TaskSchedulerImpl: Killing all running tasks in stage 41: Stage finished
26/02/12 17:51:29 INFO DAGScheduler: Job 25 finished: start at NativeMethodAccessorImpl.java:0, took 0.082117 s
26/02/12 17:51:29 INFO MemoryStore: Block broadcast_38_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.5 MiB)
26/02/12 17:51:29 INFO BlockManagerInfo: Added broadcast_38_piece0 in memory on spark-master:44075 (size: 107.2 KiB, free: 434.0 MiB)
26/02/12 17:51:29 INFO SparkContext: Created broadcast 38 from start at NativeMethodAccessorImpl.java:0
26/02/12 17:51:29 INFO BlockManagerInfo: Removed broadcast_37_piece0 on spark-master:44075 in memory (size: 33.3 KiB, free: 434.0 MiB)
26/02/12 17:51:29 INFO BlockManagerInfo: Removed broadcast_37_piece0 on 172.18.0.14:44081 in memory (size: 33.3 KiB, free: 433.9 MiB)
26/02/12 17:51:29 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 8, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@58b95bea]. The input RDD has 1 partitions.
26/02/12 17:51:29 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/12 17:51:29 INFO DAGScheduler: Got job 26 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/02/12 17:51:29 INFO DAGScheduler: Final stage: ResultStage 42 (start at NativeMethodAccessorImpl.java:0)
26/02/12 17:51:29 INFO DAGScheduler: Parents of final stage: List()
26/02/12 17:51:29 INFO DAGScheduler: Missing parents: List()
26/02/12 17:51:29 INFO DAGScheduler: Submitting ResultStage 42 (MapPartitionsRDD[137] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/12 17:51:29 INFO BlockManagerInfo: Removed broadcast_34_piece0 on spark-master:44075 in memory (size: 33.3 KiB, free: 434.0 MiB)
26/02/12 17:51:29 INFO BlockManagerInfo: Removed broadcast_34_piece0 on 172.18.0.14:44081 in memory (size: 33.3 KiB, free: 433.9 MiB)
26/02/12 17:51:29 INFO MemoryStore: Block broadcast_39 stored as values in memory (estimated size 49.9 KiB, free 433.7 MiB)
26/02/12 17:51:29 INFO MemoryStore: Block broadcast_39_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.7 MiB)
26/02/12 17:51:29 INFO BlockManagerInfo: Added broadcast_39_piece0 in memory on spark-master:44075 (size: 19.5 KiB, free: 434.0 MiB)
26/02/12 17:51:29 INFO SparkContext: Created broadcast 39 from broadcast at DAGScheduler.scala:1585
26/02/12 17:51:29 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 42 (MapPartitionsRDD[137] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/02/12 17:51:29 INFO TaskSchedulerImpl: Adding task set 42.0 with 1 tasks resource profile 0
26/02/12 17:51:29 INFO TaskSetManager: Starting task 0.0 in stage 42.0 (TID 59) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/12 17:51:29 INFO BlockManagerInfo: Removed broadcast_32_piece0 on spark-master:44075 in memory (size: 107.2 KiB, free: 434.1 MiB)
26/02/12 17:51:29 INFO BlockManagerInfo: Removed broadcast_32_piece0 on 172.18.0.14:44081 in memory (size: 107.2 KiB, free: 434.0 MiB)
26/02/12 17:51:29 INFO BlockManagerInfo: Removed broadcast_35_piece0 on spark-master:44075 in memory (size: 107.2 KiB, free: 434.2 MiB)
26/02/12 17:51:29 INFO BlockManagerInfo: Removed broadcast_35_piece0 on 172.18.0.14:44081 in memory (size: 107.2 KiB, free: 434.1 MiB)
26/02/12 17:51:29 INFO BlockManagerInfo: Added broadcast_39_piece0 in memory on 172.18.0.14:44081 (size: 19.5 KiB, free: 434.1 MiB)
26/02/12 17:51:29 INFO BlockManagerInfo: Removed broadcast_36_piece0 on spark-master:44075 in memory (size: 19.5 KiB, free: 434.2 MiB)
26/02/12 17:51:29 INFO BlockManagerInfo: Removed broadcast_36_piece0 on 172.18.0.14:44081 in memory (size: 19.5 KiB, free: 434.1 MiB)
26/02/12 17:51:29 INFO BlockManagerInfo: Added broadcast_38_piece0 in memory on 172.18.0.14:44081 (size: 107.2 KiB, free: 434.0 MiB)
26/02/12 17:51:29 INFO TaskSetManager: Finished task 0.0 in stage 42.0 (TID 59) in 134 ms on 172.18.0.14 (executor 1) (1/1)
26/02/12 17:51:29 INFO TaskSchedulerImpl: Removed TaskSet 42.0, whose tasks have all completed, from pool 
26/02/12 17:51:29 INFO DAGScheduler: ResultStage 42 (start at NativeMethodAccessorImpl.java:0) finished in 0.147 s
26/02/12 17:51:29 INFO DAGScheduler: Job 26 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/12 17:51:29 INFO TaskSchedulerImpl: Killing all running tasks in stage 42: Stage finished
26/02/12 17:51:29 INFO DAGScheduler: Job 26 finished: start at NativeMethodAccessorImpl.java:0, took 0.150162 s
26/02/12 17:51:29 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 8, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@58b95bea] is committing.
26/02/12 17:51:29 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 8, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@58b95bea] committed.
26/02/12 17:51:29 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/8 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.8.932a5c91-042c-436e-acff-929ab2278ab1.tmp
26/02/12 17:51:29 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.8.932a5c91-042c-436e-acff-929ab2278ab1.tmp to file:/tmp/spark-checkpoint-enrichment/commits/8
26/02/12 17:51:29 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "801223b3-8a58-496f-9f28-5a9fda8cbb73",
  "runId" : "005c83b3-ee40-4ce7-8dfd-e6942daa9533",
  "name" : null,
  "timestamp" : "2026-02-12T17:51:28.781Z",
  "batchId" : 8,
  "numInputRows" : 194,
  "inputRowsPerSecond" : 191.13300492610838,
  "processedRowsPerSecond" : 369.5238095238095,
  "durationMs" : {
    "addBatch" : 367,
    "commitOffsets" : 56,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 40,
    "triggerExecution" : 525,
    "walCommit" : 61
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "0" : 39766
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "0" : 39960
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "0" : 39960
      }
    },
    "numInputRows" : 194,
    "inputRowsPerSecond" : 191.13300492610838,
    "processedRowsPerSecond" : 369.5238095238095,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@444eb264",
    "numOutputRows" : 34
  }
}
26/02/12 17:51:29 INFO BlockManagerInfo: Removed broadcast_39_piece0 on spark-master:44075 in memory (size: 19.5 KiB, free: 434.3 MiB)
26/02/12 17:51:29 INFO BlockManagerInfo: Removed broadcast_39_piece0 on 172.18.0.14:44081 in memory (size: 19.5 KiB, free: 434.1 MiB)
26/02/12 17:51:39 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/12 17:51:44 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/9 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.9.6c2d5175-6c2f-4f58-b3cf-49919b1b7742.tmp
26/02/12 17:51:44 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.9.6c2d5175-6c2f-4f58-b3cf-49919b1b7742.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/9
26/02/12 17:51:44 INFO MicroBatchExecution: Committed offsets for batch 9. Metadata OffsetSeqMetadata(0,1770918704353,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/12 17:51:44 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/12 17:51:44 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/12 17:51:44 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/12 17:51:44 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/12 17:51:44 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/12 17:51:44 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/12 17:51:44 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#9637 - origin_code.nullCount#9636) > 0)
26/02/12 17:51:44 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#9642 - destination_code.nullCount#9641) > 0)
26/02/12 17:51:44 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#9672 - callsign.nullCount#9671) > 0)
26/02/12 17:51:44 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/12 17:51:44 INFO DAGScheduler: Got job 27 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/12 17:51:44 INFO DAGScheduler: Final stage: ResultStage 44 (start at NativeMethodAccessorImpl.java:0)
26/02/12 17:51:44 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 43)
26/02/12 17:51:44 INFO DAGScheduler: Missing parents: List()
26/02/12 17:51:44 INFO DAGScheduler: Submitting ResultStage 44 (MapPartitionsRDD[142] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/12 17:51:44 INFO MemoryStore: Block broadcast_40 stored as values in memory (estimated size 99.3 KiB, free 434.0 MiB)
26/02/12 17:51:44 INFO MemoryStore: Block broadcast_40_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 433.9 MiB)
26/02/12 17:51:44 INFO BlockManagerInfo: Added broadcast_40_piece0 in memory on spark-master:44075 (size: 33.3 KiB, free: 434.2 MiB)
26/02/12 17:51:44 INFO SparkContext: Created broadcast 40 from broadcast at DAGScheduler.scala:1585
26/02/12 17:51:44 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 44 (MapPartitionsRDD[142] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/12 17:51:44 INFO TaskSchedulerImpl: Adding task set 44.0 with 4 tasks resource profile 0
26/02/12 17:51:44 INFO TaskSetManager: Starting task 0.0 in stage 44.0 (TID 60) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/12 17:51:44 INFO TaskSetManager: Starting task 1.0 in stage 44.0 (TID 61) (172.18.0.14, executor 1, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/12 17:51:44 INFO BlockManagerInfo: Added broadcast_40_piece0 in memory on 172.18.0.14:44081 (size: 33.3 KiB, free: 434.0 MiB)
26/02/12 17:51:44 INFO TaskSetManager: Starting task 2.0 in stage 44.0 (TID 62) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/12 17:51:44 INFO TaskSetManager: Starting task 3.0 in stage 44.0 (TID 63) (172.18.0.14, executor 1, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/12 17:51:44 INFO TaskSetManager: Finished task 1.0 in stage 44.0 (TID 61) in 51 ms on 172.18.0.14 (executor 1) (1/4)
26/02/12 17:51:44 INFO TaskSetManager: Finished task 0.0 in stage 44.0 (TID 60) in 53 ms on 172.18.0.14 (executor 1) (2/4)
26/02/12 17:51:44 INFO TaskSetManager: Finished task 3.0 in stage 44.0 (TID 63) in 18 ms on 172.18.0.14 (executor 1) (3/4)
26/02/12 17:51:44 INFO TaskSetManager: Finished task 2.0 in stage 44.0 (TID 62) in 25 ms on 172.18.0.14 (executor 1) (4/4)
26/02/12 17:51:44 INFO TaskSchedulerImpl: Removed TaskSet 44.0, whose tasks have all completed, from pool 
26/02/12 17:51:44 INFO DAGScheduler: ResultStage 44 (start at NativeMethodAccessorImpl.java:0) finished in 0.085 s
26/02/12 17:51:44 INFO DAGScheduler: Job 27 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/12 17:51:44 INFO TaskSchedulerImpl: Killing all running tasks in stage 44: Stage finished
26/02/12 17:51:44 INFO DAGScheduler: Job 27 finished: start at NativeMethodAccessorImpl.java:0, took 0.088958 s
26/02/12 17:51:44 INFO MemoryStore: Block broadcast_41_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.8 MiB)
26/02/12 17:51:44 INFO BlockManagerInfo: Added broadcast_41_piece0 in memory on spark-master:44075 (size: 107.2 KiB, free: 434.1 MiB)
26/02/12 17:51:44 INFO SparkContext: Created broadcast 41 from start at NativeMethodAccessorImpl.java:0
26/02/12 17:51:44 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 9, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@50242bb8]. The input RDD has 1 partitions.
26/02/12 17:51:44 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/12 17:51:44 INFO DAGScheduler: Got job 28 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/02/12 17:51:44 INFO DAGScheduler: Final stage: ResultStage 45 (start at NativeMethodAccessorImpl.java:0)
26/02/12 17:51:44 INFO DAGScheduler: Parents of final stage: List()
26/02/12 17:51:44 INFO DAGScheduler: Missing parents: List()
26/02/12 17:51:44 INFO DAGScheduler: Submitting ResultStage 45 (MapPartitionsRDD[148] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/12 17:51:44 INFO MemoryStore: Block broadcast_42 stored as values in memory (estimated size 49.9 KiB, free 433.8 MiB)
26/02/12 17:51:44 INFO MemoryStore: Block broadcast_42_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.8 MiB)
26/02/12 17:51:44 INFO BlockManagerInfo: Added broadcast_42_piece0 in memory on spark-master:44075 (size: 19.5 KiB, free: 434.1 MiB)
26/02/12 17:51:44 INFO SparkContext: Created broadcast 42 from broadcast at DAGScheduler.scala:1585
26/02/12 17:51:44 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 45 (MapPartitionsRDD[148] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/02/12 17:51:44 INFO TaskSchedulerImpl: Adding task set 45.0 with 1 tasks resource profile 0
26/02/12 17:51:44 INFO TaskSetManager: Starting task 0.0 in stage 45.0 (TID 64) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/12 17:51:44 INFO BlockManagerInfo: Added broadcast_42_piece0 in memory on 172.18.0.14:44081 (size: 19.5 KiB, free: 434.0 MiB)
26/02/12 17:51:44 INFO BlockManagerInfo: Added broadcast_41_piece0 in memory on 172.18.0.14:44081 (size: 107.2 KiB, free: 433.9 MiB)
26/02/12 17:51:45 INFO TaskSetManager: Finished task 0.0 in stage 45.0 (TID 64) in 602 ms on 172.18.0.14 (executor 1) (1/1)
26/02/12 17:51:45 INFO TaskSchedulerImpl: Removed TaskSet 45.0, whose tasks have all completed, from pool 
26/02/12 17:51:45 INFO DAGScheduler: ResultStage 45 (start at NativeMethodAccessorImpl.java:0) finished in 0.610 s
26/02/12 17:51:45 INFO DAGScheduler: Job 28 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/12 17:51:45 INFO TaskSchedulerImpl: Killing all running tasks in stage 45: Stage finished
26/02/12 17:51:45 INFO DAGScheduler: Job 28 finished: start at NativeMethodAccessorImpl.java:0, took 0.611697 s
26/02/12 17:51:45 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 9, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@50242bb8] is committing.
26/02/12 17:51:45 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 9, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@50242bb8] committed.
26/02/12 17:51:45 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/9 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.9.019f94e4-a584-466f-8576-2e9a74dacb5c.tmp
26/02/12 17:51:45 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.9.019f94e4-a584-466f-8576-2e9a74dacb5c.tmp to file:/tmp/spark-checkpoint-enrichment/commits/9
26/02/12 17:51:45 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "801223b3-8a58-496f-9f28-5a9fda8cbb73",
  "runId" : "005c83b3-ee40-4ce7-8dfd-e6942daa9533",
  "name" : null,
  "timestamp" : "2026-02-12T17:51:44.348Z",
  "batchId" : 9,
  "numInputRows" : 37,
  "inputRowsPerSecond" : 2846.153846153846,
  "processedRowsPerSecond" : 34.67666354264293,
  "durationMs" : {
    "addBatch" : 829,
    "commitOffsets" : 70,
    "getBatch" : 0,
    "latestOffset" : 5,
    "queryPlanning" : 60,
    "triggerExecution" : 1067,
    "walCommit" : 102
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "0" : 39960
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "0" : 39997
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "0" : 39997
      }
    },
    "numInputRows" : 37,
    "inputRowsPerSecond" : 2846.153846153846,
    "processedRowsPerSecond" : 34.67666354264293,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@444eb264",
    "numOutputRows" : 15
  }
}
26/02/12 17:51:45 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/10 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.10.630d527e-931e-41d1-b42f-c386c29fdf68.tmp
26/02/12 17:51:45 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.10.630d527e-931e-41d1-b42f-c386c29fdf68.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/10
26/02/12 17:51:45 INFO MicroBatchExecution: Committed offsets for batch 10. Metadata OffsetSeqMetadata(0,1770918705418,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/12 17:51:45 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/12 17:51:45 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/12 17:51:45 INFO BlockManagerInfo: Removed broadcast_42_piece0 on spark-master:44075 in memory (size: 19.5 KiB, free: 434.1 MiB)
26/02/12 17:51:45 INFO BlockManagerInfo: Removed broadcast_42_piece0 on 172.18.0.14:44081 in memory (size: 19.5 KiB, free: 433.9 MiB)
26/02/12 17:51:45 INFO BlockManagerInfo: Removed broadcast_40_piece0 on spark-master:44075 in memory (size: 33.3 KiB, free: 434.2 MiB)
26/02/12 17:51:45 INFO BlockManagerInfo: Removed broadcast_40_piece0 on 172.18.0.14:44081 in memory (size: 33.3 KiB, free: 434.0 MiB)
26/02/12 17:51:45 INFO BlockManagerInfo: Removed broadcast_41_piece0 on spark-master:44075 in memory (size: 107.2 KiB, free: 434.3 MiB)
26/02/12 17:51:45 INFO BlockManagerInfo: Removed broadcast_41_piece0 on 172.18.0.14:44081 in memory (size: 107.2 KiB, free: 434.1 MiB)
26/02/12 17:51:45 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/12 17:51:45 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/12 17:51:45 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/12 17:51:45 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/12 17:51:45 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#10491 - origin_code.nullCount#10490) > 0)
26/02/12 17:51:45 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#10496 - destination_code.nullCount#10495) > 0)
26/02/12 17:51:45 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#10526 - callsign.nullCount#10525) > 0)
26/02/12 17:51:45 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/12 17:51:45 INFO DAGScheduler: Got job 29 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/12 17:51:45 INFO DAGScheduler: Final stage: ResultStage 47 (start at NativeMethodAccessorImpl.java:0)
26/02/12 17:51:45 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 46)
26/02/12 17:51:45 INFO DAGScheduler: Missing parents: List()
26/02/12 17:51:45 INFO DAGScheduler: Submitting ResultStage 47 (MapPartitionsRDD[153] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/12 17:51:45 INFO MemoryStore: Block broadcast_43 stored as values in memory (estimated size 99.3 KiB, free 434.0 MiB)
26/02/12 17:51:45 INFO MemoryStore: Block broadcast_43_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 433.9 MiB)
26/02/12 17:51:45 INFO BlockManagerInfo: Added broadcast_43_piece0 in memory on spark-master:44075 (size: 33.3 KiB, free: 434.2 MiB)
26/02/12 17:51:45 INFO SparkContext: Created broadcast 43 from broadcast at DAGScheduler.scala:1585
26/02/12 17:51:45 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 47 (MapPartitionsRDD[153] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/12 17:51:45 INFO TaskSchedulerImpl: Adding task set 47.0 with 4 tasks resource profile 0
26/02/12 17:51:45 INFO TaskSetManager: Starting task 0.0 in stage 47.0 (TID 65) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/12 17:51:45 INFO TaskSetManager: Starting task 1.0 in stage 47.0 (TID 66) (172.18.0.14, executor 1, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/12 17:51:45 INFO BlockManagerInfo: Added broadcast_43_piece0 in memory on 172.18.0.14:44081 (size: 33.3 KiB, free: 434.0 MiB)
26/02/12 17:51:45 INFO TaskSetManager: Starting task 2.0 in stage 47.0 (TID 67) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/12 17:51:45 INFO TaskSetManager: Finished task 0.0 in stage 47.0 (TID 65) in 40 ms on 172.18.0.14 (executor 1) (1/4)
26/02/12 17:51:45 INFO TaskSetManager: Starting task 3.0 in stage 47.0 (TID 68) (172.18.0.14, executor 1, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/12 17:51:45 INFO TaskSetManager: Finished task 1.0 in stage 47.0 (TID 66) in 54 ms on 172.18.0.14 (executor 1) (2/4)
26/02/12 17:51:45 INFO BlockManagerInfo: Removed broadcast_38_piece0 on spark-master:44075 in memory (size: 107.2 KiB, free: 434.3 MiB)
26/02/12 17:51:45 INFO TaskSetManager: Finished task 2.0 in stage 47.0 (TID 67) in 39 ms on 172.18.0.14 (executor 1) (3/4)
26/02/12 17:51:45 INFO BlockManagerInfo: Removed broadcast_38_piece0 on 172.18.0.14:44081 in memory (size: 107.2 KiB, free: 434.1 MiB)
26/02/12 17:51:45 INFO TaskSetManager: Finished task 3.0 in stage 47.0 (TID 68) in 34 ms on 172.18.0.14 (executor 1) (4/4)
26/02/12 17:51:45 INFO TaskSchedulerImpl: Removed TaskSet 47.0, whose tasks have all completed, from pool 
26/02/12 17:51:45 INFO DAGScheduler: ResultStage 47 (start at NativeMethodAccessorImpl.java:0) finished in 0.098 s
26/02/12 17:51:45 INFO DAGScheduler: Job 29 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/12 17:51:45 INFO TaskSchedulerImpl: Killing all running tasks in stage 47: Stage finished
26/02/12 17:51:45 INFO DAGScheduler: Job 29 finished: start at NativeMethodAccessorImpl.java:0, took 0.101990 s
26/02/12 17:51:45 INFO MemoryStore: Block broadcast_44_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.9 MiB)
26/02/12 17:51:45 INFO BlockManagerInfo: Added broadcast_44_piece0 in memory on spark-master:44075 (size: 107.2 KiB, free: 434.2 MiB)
26/02/12 17:51:45 INFO SparkContext: Created broadcast 44 from start at NativeMethodAccessorImpl.java:0
26/02/12 17:51:45 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 10, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@18f5eddc]. The input RDD has 1 partitions.
26/02/12 17:51:45 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/12 17:51:45 INFO DAGScheduler: Got job 30 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/02/12 17:51:45 INFO DAGScheduler: Final stage: ResultStage 48 (start at NativeMethodAccessorImpl.java:0)
26/02/12 17:51:45 INFO DAGScheduler: Parents of final stage: List()
26/02/12 17:51:45 INFO DAGScheduler: Missing parents: List()
26/02/12 17:51:45 INFO DAGScheduler: Submitting ResultStage 48 (MapPartitionsRDD[159] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/12 17:51:45 INFO MemoryStore: Block broadcast_45 stored as values in memory (estimated size 49.9 KiB, free 433.9 MiB)
26/02/12 17:51:45 INFO MemoryStore: Block broadcast_45_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.9 MiB)
26/02/12 17:51:45 INFO BlockManagerInfo: Added broadcast_45_piece0 in memory on spark-master:44075 (size: 19.5 KiB, free: 434.2 MiB)
26/02/12 17:51:45 INFO SparkContext: Created broadcast 45 from broadcast at DAGScheduler.scala:1585
26/02/12 17:51:45 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 48 (MapPartitionsRDD[159] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/02/12 17:51:45 INFO TaskSchedulerImpl: Adding task set 48.0 with 1 tasks resource profile 0
26/02/12 17:51:45 INFO TaskSetManager: Starting task 0.0 in stage 48.0 (TID 69) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/12 17:51:45 INFO BlockManagerInfo: Added broadcast_45_piece0 in memory on 172.18.0.14:44081 (size: 19.5 KiB, free: 434.1 MiB)
26/02/12 17:51:45 INFO BlockManagerInfo: Added broadcast_44_piece0 in memory on 172.18.0.14:44081 (size: 107.2 KiB, free: 434.0 MiB)
26/02/12 17:51:45 INFO TaskSetManager: Finished task 0.0 in stage 48.0 (TID 69) in 199 ms on 172.18.0.14 (executor 1) (1/1)
26/02/12 17:51:45 INFO TaskSchedulerImpl: Removed TaskSet 48.0, whose tasks have all completed, from pool 
26/02/12 17:51:45 INFO DAGScheduler: ResultStage 48 (start at NativeMethodAccessorImpl.java:0) finished in 0.210 s
26/02/12 17:51:45 INFO DAGScheduler: Job 30 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/12 17:51:45 INFO TaskSchedulerImpl: Killing all running tasks in stage 48: Stage finished
26/02/12 17:51:45 INFO DAGScheduler: Job 30 finished: start at NativeMethodAccessorImpl.java:0, took 0.214792 s
26/02/12 17:51:45 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 10, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@18f5eddc] is committing.
26/02/12 17:51:45 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 10, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@18f5eddc] committed.
26/02/12 17:51:46 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/10 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.10.de4298b9-d510-493d-912b-aae4e8b9e1a5.tmp
26/02/12 17:51:46 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.10.de4298b9-d510-493d-912b-aae4e8b9e1a5.tmp to file:/tmp/spark-checkpoint-enrichment/commits/10
26/02/12 17:51:46 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "801223b3-8a58-496f-9f28-5a9fda8cbb73",
  "runId" : "005c83b3-ee40-4ce7-8dfd-e6942daa9533",
  "name" : null,
  "timestamp" : "2026-02-12T17:51:45.417Z",
  "batchId" : 10,
  "numInputRows" : 194,
  "inputRowsPerSecond" : 181.47801683816652,
  "processedRowsPerSecond" : 294.83282674772033,
  "durationMs" : {
    "addBatch" : 451,
    "commitOffsets" : 92,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 50,
    "triggerExecution" : 658,
    "walCommit" : 63
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "0" : 39997
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "0" : 40191
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "0" : 40191
      }
    },
    "numInputRows" : 194,
    "inputRowsPerSecond" : 181.47801683816652,
    "processedRowsPerSecond" : 294.83282674772033,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@444eb264",
    "numOutputRows" : 32
  }
}
26/02/12 17:51:56 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/12 17:51:57 INFO BlockManagerInfo: Removed broadcast_43_piece0 on spark-master:44075 in memory (size: 33.3 KiB, free: 434.2 MiB)
26/02/12 17:51:57 INFO BlockManagerInfo: Removed broadcast_43_piece0 on 172.18.0.14:44081 in memory (size: 33.3 KiB, free: 434.0 MiB)
26/02/12 17:51:57 INFO BlockManagerInfo: Removed broadcast_45_piece0 on spark-master:44075 in memory (size: 19.5 KiB, free: 434.3 MiB)
26/02/12 17:51:57 INFO BlockManagerInfo: Removed broadcast_45_piece0 on 172.18.0.14:44081 in memory (size: 19.5 KiB, free: 434.1 MiB)
26/02/12 17:52:00 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/11 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.11.bb0989fa-d3a0-422e-8763-d4c236c1fc82.tmp
26/02/12 17:52:00 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.11.bb0989fa-d3a0-422e-8763-d4c236c1fc82.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/11
26/02/12 17:52:00 INFO MicroBatchExecution: Committed offsets for batch 11. Metadata OffsetSeqMetadata(0,1770918720848,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/12 17:52:01 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/12 17:52:01 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/12 17:52:01 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/12 17:52:01 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/12 17:52:01 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/12 17:52:01 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/12 17:52:01 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#11345 - origin_code.nullCount#11344) > 0)
26/02/12 17:52:01 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#11350 - destination_code.nullCount#11349) > 0)
26/02/12 17:52:01 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#11380 - callsign.nullCount#11379) > 0)
26/02/12 17:52:01 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/12 17:52:01 INFO DAGScheduler: Got job 31 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/12 17:52:01 INFO DAGScheduler: Final stage: ResultStage 50 (start at NativeMethodAccessorImpl.java:0)
26/02/12 17:52:01 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 49)
26/02/12 17:52:01 INFO DAGScheduler: Missing parents: List()
26/02/12 17:52:01 INFO DAGScheduler: Submitting ResultStage 50 (MapPartitionsRDD[164] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/12 17:52:01 INFO MemoryStore: Block broadcast_46 stored as values in memory (estimated size 99.3 KiB, free 434.0 MiB)
26/02/12 17:52:01 INFO MemoryStore: Block broadcast_46_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 433.9 MiB)
26/02/12 17:52:01 INFO BlockManagerInfo: Added broadcast_46_piece0 in memory on spark-master:44075 (size: 33.3 KiB, free: 434.2 MiB)
26/02/12 17:52:01 INFO SparkContext: Created broadcast 46 from broadcast at DAGScheduler.scala:1585
26/02/12 17:52:01 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 50 (MapPartitionsRDD[164] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/12 17:52:01 INFO TaskSchedulerImpl: Adding task set 50.0 with 4 tasks resource profile 0
26/02/12 17:52:01 INFO TaskSetManager: Starting task 0.0 in stage 50.0 (TID 70) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/12 17:52:01 INFO TaskSetManager: Starting task 1.0 in stage 50.0 (TID 71) (172.18.0.14, executor 1, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/12 17:52:01 INFO BlockManagerInfo: Added broadcast_46_piece0 in memory on 172.18.0.14:44081 (size: 33.3 KiB, free: 434.0 MiB)
26/02/12 17:52:01 INFO TaskSetManager: Starting task 2.0 in stage 50.0 (TID 72) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/12 17:52:01 INFO TaskSetManager: Finished task 1.0 in stage 50.0 (TID 71) in 29 ms on 172.18.0.14 (executor 1) (1/4)
26/02/12 17:52:01 INFO TaskSetManager: Starting task 3.0 in stage 50.0 (TID 73) (172.18.0.14, executor 1, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/12 17:52:01 INFO TaskSetManager: Finished task 0.0 in stage 50.0 (TID 70) in 33 ms on 172.18.0.14 (executor 1) (2/4)
26/02/12 17:52:01 INFO TaskSetManager: Finished task 2.0 in stage 50.0 (TID 72) in 32 ms on 172.18.0.14 (executor 1) (3/4)
26/02/12 17:52:01 INFO TaskSetManager: Finished task 3.0 in stage 50.0 (TID 73) in 31 ms on 172.18.0.14 (executor 1) (4/4)
26/02/12 17:52:01 INFO TaskSchedulerImpl: Removed TaskSet 50.0, whose tasks have all completed, from pool 
26/02/12 17:52:01 INFO DAGScheduler: ResultStage 50 (start at NativeMethodAccessorImpl.java:0) finished in 0.074 s
26/02/12 17:52:01 INFO DAGScheduler: Job 31 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/12 17:52:01 INFO TaskSchedulerImpl: Killing all running tasks in stage 50: Stage finished
26/02/12 17:52:01 INFO DAGScheduler: Job 31 finished: start at NativeMethodAccessorImpl.java:0, took 0.077138 s
26/02/12 17:52:01 INFO MemoryStore: Block broadcast_47_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.8 MiB)
26/02/12 17:52:01 INFO BlockManagerInfo: Added broadcast_47_piece0 in memory on spark-master:44075 (size: 107.2 KiB, free: 434.1 MiB)
26/02/12 17:52:01 INFO SparkContext: Created broadcast 47 from start at NativeMethodAccessorImpl.java:0
26/02/12 17:52:01 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 11, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@618f878b]. The input RDD has 1 partitions.
26/02/12 17:52:01 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/12 17:52:01 INFO DAGScheduler: Got job 32 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/02/12 17:52:01 INFO DAGScheduler: Final stage: ResultStage 51 (start at NativeMethodAccessorImpl.java:0)
26/02/12 17:52:01 INFO DAGScheduler: Parents of final stage: List()
26/02/12 17:52:01 INFO DAGScheduler: Missing parents: List()
26/02/12 17:52:01 INFO DAGScheduler: Submitting ResultStage 51 (MapPartitionsRDD[170] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/12 17:52:01 INFO MemoryStore: Block broadcast_48 stored as values in memory (estimated size 49.9 KiB, free 433.8 MiB)
26/02/12 17:52:01 INFO MemoryStore: Block broadcast_48_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.8 MiB)
26/02/12 17:52:01 INFO BlockManagerInfo: Added broadcast_48_piece0 in memory on spark-master:44075 (size: 19.5 KiB, free: 434.1 MiB)
26/02/12 17:52:01 INFO SparkContext: Created broadcast 48 from broadcast at DAGScheduler.scala:1585
26/02/12 17:52:01 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 51 (MapPartitionsRDD[170] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/02/12 17:52:01 INFO TaskSchedulerImpl: Adding task set 51.0 with 1 tasks resource profile 0
26/02/12 17:52:01 INFO TaskSetManager: Starting task 0.0 in stage 51.0 (TID 74) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/12 17:52:01 INFO BlockManagerInfo: Added broadcast_48_piece0 in memory on 172.18.0.14:44081 (size: 19.5 KiB, free: 434.0 MiB)
26/02/12 17:52:01 INFO BlockManagerInfo: Added broadcast_47_piece0 in memory on 172.18.0.14:44081 (size: 107.2 KiB, free: 433.9 MiB)
26/02/12 17:52:01 INFO TaskSetManager: Finished task 0.0 in stage 51.0 (TID 74) in 616 ms on 172.18.0.14 (executor 1) (1/1)
26/02/12 17:52:01 INFO TaskSchedulerImpl: Removed TaskSet 51.0, whose tasks have all completed, from pool 
26/02/12 17:52:01 INFO DAGScheduler: ResultStage 51 (start at NativeMethodAccessorImpl.java:0) finished in 0.623 s
26/02/12 17:52:01 INFO DAGScheduler: Job 32 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/12 17:52:01 INFO TaskSchedulerImpl: Killing all running tasks in stage 51: Stage finished
26/02/12 17:52:01 INFO DAGScheduler: Job 32 finished: start at NativeMethodAccessorImpl.java:0, took 0.627338 s
26/02/12 17:52:01 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 11, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@618f878b] is committing.
26/02/12 17:52:01 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 11, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@618f878b] committed.
26/02/12 17:52:01 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/11 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.11.b89ba205-267f-4f2c-a73f-5f8df877e590.tmp
26/02/12 17:52:01 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.11.b89ba205-267f-4f2c-a73f-5f8df877e590.tmp to file:/tmp/spark-checkpoint-enrichment/commits/11
26/02/12 17:52:01 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "801223b3-8a58-496f-9f28-5a9fda8cbb73",
  "runId" : "005c83b3-ee40-4ce7-8dfd-e6942daa9533",
  "name" : null,
  "timestamp" : "2026-02-12T17:52:00.844Z",
  "batchId" : 11,
  "numInputRows" : 37,
  "inputRowsPerSecond" : 2642.8571428571427,
  "processedRowsPerSecond" : 34.069981583793734,
  "durationMs" : {
    "addBatch" : 820,
    "commitOffsets" : 102,
    "getBatch" : 0,
    "latestOffset" : 4,
    "queryPlanning" : 49,
    "triggerExecution" : 1086,
    "walCommit" : 111
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "0" : 40191
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "0" : 40228
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "0" : 40228
      }
    },
    "numInputRows" : 37,
    "inputRowsPerSecond" : 2642.8571428571427,
    "processedRowsPerSecond" : 34.069981583793734,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@444eb264",
    "numOutputRows" : 14
  }
}
26/02/12 17:52:01 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/12 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.12.e16666bd-7368-4e25-9b7c-df2093d615ca.tmp
26/02/12 17:52:02 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.12.e16666bd-7368-4e25-9b7c-df2093d615ca.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/12
26/02/12 17:52:02 INFO MicroBatchExecution: Committed offsets for batch 12. Metadata OffsetSeqMetadata(0,1770918721940,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/12 17:52:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/12 17:52:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/12 17:52:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/12 17:52:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/12 17:52:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/12 17:52:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/12 17:52:02 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#12199 - origin_code.nullCount#12198) > 0)
26/02/12 17:52:02 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#12204 - destination_code.nullCount#12203) > 0)
26/02/12 17:52:02 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#12234 - callsign.nullCount#12233) > 0)
26/02/12 17:52:02 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/12 17:52:02 INFO DAGScheduler: Got job 33 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/12 17:52:02 INFO DAGScheduler: Final stage: ResultStage 53 (start at NativeMethodAccessorImpl.java:0)
26/02/12 17:52:02 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 52)
26/02/12 17:52:02 INFO DAGScheduler: Missing parents: List()
26/02/12 17:52:02 INFO DAGScheduler: Submitting ResultStage 53 (MapPartitionsRDD[175] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/12 17:52:02 INFO MemoryStore: Block broadcast_49 stored as values in memory (estimated size 99.3 KiB, free 433.7 MiB)
26/02/12 17:52:02 INFO MemoryStore: Block broadcast_49_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 433.6 MiB)
26/02/12 17:52:02 INFO BlockManagerInfo: Added broadcast_49_piece0 in memory on spark-master:44075 (size: 33.3 KiB, free: 434.1 MiB)
26/02/12 17:52:02 INFO SparkContext: Created broadcast 49 from broadcast at DAGScheduler.scala:1585
26/02/12 17:52:02 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 53 (MapPartitionsRDD[175] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/12 17:52:02 INFO TaskSchedulerImpl: Adding task set 53.0 with 4 tasks resource profile 0
26/02/12 17:52:02 INFO TaskSetManager: Starting task 0.0 in stage 53.0 (TID 75) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/12 17:52:02 INFO TaskSetManager: Starting task 1.0 in stage 53.0 (TID 76) (172.18.0.14, executor 1, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/12 17:52:02 INFO BlockManagerInfo: Added broadcast_49_piece0 in memory on 172.18.0.14:44081 (size: 33.3 KiB, free: 433.9 MiB)
26/02/12 17:52:02 INFO TaskSetManager: Starting task 2.0 in stage 53.0 (TID 77) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/12 17:52:02 INFO TaskSetManager: Finished task 0.0 in stage 53.0 (TID 75) in 34 ms on 172.18.0.14 (executor 1) (1/4)
26/02/12 17:52:02 INFO TaskSetManager: Starting task 3.0 in stage 53.0 (TID 78) (172.18.0.14, executor 1, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/12 17:52:02 INFO TaskSetManager: Finished task 1.0 in stage 53.0 (TID 76) in 35 ms on 172.18.0.14 (executor 1) (2/4)
26/02/12 17:52:02 INFO TaskSetManager: Finished task 3.0 in stage 53.0 (TID 78) in 19 ms on 172.18.0.14 (executor 1) (3/4)
26/02/12 17:52:02 INFO TaskSetManager: Finished task 2.0 in stage 53.0 (TID 77) in 23 ms on 172.18.0.14 (executor 1) (4/4)
26/02/12 17:52:02 INFO TaskSchedulerImpl: Removed TaskSet 53.0, whose tasks have all completed, from pool 
26/02/12 17:52:02 INFO DAGScheduler: ResultStage 53 (start at NativeMethodAccessorImpl.java:0) finished in 0.064 s
26/02/12 17:52:02 INFO DAGScheduler: Job 33 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/12 17:52:02 INFO TaskSchedulerImpl: Killing all running tasks in stage 53: Stage finished
26/02/12 17:52:02 INFO DAGScheduler: Job 33 finished: start at NativeMethodAccessorImpl.java:0, took 0.066488 s
26/02/12 17:52:02 INFO MemoryStore: Block broadcast_50_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.5 MiB)
26/02/12 17:52:02 INFO BlockManagerInfo: Added broadcast_50_piece0 in memory on spark-master:44075 (size: 107.2 KiB, free: 434.0 MiB)
26/02/12 17:52:02 INFO SparkContext: Created broadcast 50 from start at NativeMethodAccessorImpl.java:0
26/02/12 17:52:02 INFO BlockManagerInfo: Removed broadcast_44_piece0 on spark-master:44075 in memory (size: 107.2 KiB, free: 434.1 MiB)
26/02/12 17:52:02 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 12, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@383e6f4f]. The input RDD has 1 partitions.
26/02/12 17:52:02 INFO BlockManagerInfo: Removed broadcast_44_piece0 on 172.18.0.14:44081 in memory (size: 107.2 KiB, free: 434.0 MiB)
26/02/12 17:52:02 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/12 17:52:02 INFO DAGScheduler: Got job 34 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/02/12 17:52:02 INFO DAGScheduler: Final stage: ResultStage 54 (start at NativeMethodAccessorImpl.java:0)
26/02/12 17:52:02 INFO DAGScheduler: Parents of final stage: List()
26/02/12 17:52:02 INFO DAGScheduler: Missing parents: List()
26/02/12 17:52:02 INFO DAGScheduler: Submitting ResultStage 54 (MapPartitionsRDD[181] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/12 17:52:02 INFO BlockManagerInfo: Removed broadcast_47_piece0 on spark-master:44075 in memory (size: 107.2 KiB, free: 434.2 MiB)
26/02/12 17:52:02 INFO MemoryStore: Block broadcast_51 stored as values in memory (estimated size 49.9 KiB, free 433.7 MiB)
26/02/12 17:52:02 INFO MemoryStore: Block broadcast_51_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.7 MiB)
26/02/12 17:52:02 INFO BlockManagerInfo: Added broadcast_51_piece0 in memory on spark-master:44075 (size: 19.5 KiB, free: 434.2 MiB)
26/02/12 17:52:02 INFO SparkContext: Created broadcast 51 from broadcast at DAGScheduler.scala:1585
26/02/12 17:52:02 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 54 (MapPartitionsRDD[181] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/02/12 17:52:02 INFO TaskSchedulerImpl: Adding task set 54.0 with 1 tasks resource profile 0
26/02/12 17:52:02 INFO TaskSetManager: Starting task 0.0 in stage 54.0 (TID 79) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/12 17:52:02 INFO BlockManagerInfo: Removed broadcast_47_piece0 on 172.18.0.14:44081 in memory (size: 107.2 KiB, free: 434.1 MiB)
26/02/12 17:52:02 INFO BlockManagerInfo: Removed broadcast_46_piece0 on spark-master:44075 in memory (size: 33.3 KiB, free: 434.2 MiB)
26/02/12 17:52:02 INFO BlockManagerInfo: Removed broadcast_46_piece0 on 172.18.0.14:44081 in memory (size: 33.3 KiB, free: 434.1 MiB)
26/02/12 17:52:02 INFO BlockManagerInfo: Added broadcast_51_piece0 in memory on 172.18.0.14:44081 (size: 19.5 KiB, free: 434.1 MiB)
26/02/12 17:52:02 INFO BlockManagerInfo: Removed broadcast_48_piece0 on spark-master:44075 in memory (size: 19.5 KiB, free: 434.2 MiB)
26/02/12 17:52:02 INFO BlockManagerInfo: Removed broadcast_48_piece0 on 172.18.0.14:44081 in memory (size: 19.5 KiB, free: 434.1 MiB)
26/02/12 17:52:02 INFO BlockManagerInfo: Removed broadcast_49_piece0 on spark-master:44075 in memory (size: 33.3 KiB, free: 434.2 MiB)
26/02/12 17:52:02 INFO BlockManagerInfo: Removed broadcast_49_piece0 on 172.18.0.14:44081 in memory (size: 33.3 KiB, free: 434.1 MiB)
26/02/12 17:52:02 INFO BlockManagerInfo: Added broadcast_50_piece0 in memory on 172.18.0.14:44081 (size: 107.2 KiB, free: 434.0 MiB)
26/02/12 17:52:02 INFO TaskSetManager: Finished task 0.0 in stage 54.0 (TID 79) in 120 ms on 172.18.0.14 (executor 1) (1/1)
26/02/12 17:52:02 INFO TaskSchedulerImpl: Removed TaskSet 54.0, whose tasks have all completed, from pool 
26/02/12 17:52:02 INFO DAGScheduler: ResultStage 54 (start at NativeMethodAccessorImpl.java:0) finished in 0.134 s
26/02/12 17:52:02 INFO DAGScheduler: Job 34 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/12 17:52:02 INFO TaskSchedulerImpl: Killing all running tasks in stage 54: Stage finished
26/02/12 17:52:02 INFO DAGScheduler: Job 34 finished: start at NativeMethodAccessorImpl.java:0, took 0.139981 s
26/02/12 17:52:02 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 12, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@383e6f4f] is committing.
26/02/12 17:52:02 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 12, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@383e6f4f] committed.
26/02/12 17:52:02 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/12 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.12.0e6b7d6c-dfc4-4df7-a7a7-4d4e425bc3bb.tmp
26/02/12 17:52:02 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.12.0e6b7d6c-dfc4-4df7-a7a7-4d4e425bc3bb.tmp to file:/tmp/spark-checkpoint-enrichment/commits/12
26/02/12 17:52:02 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "801223b3-8a58-496f-9f28-5a9fda8cbb73",
  "runId" : "005c83b3-ee40-4ce7-8dfd-e6942daa9533",
  "name" : null,
  "timestamp" : "2026-02-12T17:52:01.934Z",
  "batchId" : 12,
  "numInputRows" : 192,
  "inputRowsPerSecond" : 176.14678899082568,
  "processedRowsPerSecond" : 377.21021611001964,
  "durationMs" : {
    "addBatch" : 323,
    "commitOffsets" : 83,
    "getBatch" : 0,
    "latestOffset" : 6,
    "queryPlanning" : 37,
    "triggerExecution" : 509,
    "walCommit" : 60
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "0" : 40228
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "0" : 40420
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "0" : 40420
      }
    },
    "numInputRows" : 192,
    "inputRowsPerSecond" : 176.14678899082568,
    "processedRowsPerSecond" : 377.21021611001964,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@444eb264",
    "numOutputRows" : 32
  }
}
26/02/12 17:52:02 INFO BlockManagerInfo: Removed broadcast_51_piece0 on spark-master:44075 in memory (size: 19.5 KiB, free: 434.3 MiB)
26/02/12 17:52:02 INFO BlockManagerInfo: Removed broadcast_51_piece0 on 172.18.0.14:44081 in memory (size: 19.5 KiB, free: 434.1 MiB)
26/02/12 17:52:12 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/12 17:52:17 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/13 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.13.f147b245-5f56-475b-a54b-056269ef1787.tmp
26/02/12 17:52:17 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.13.f147b245-5f56-475b-a54b-056269ef1787.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/13
26/02/12 17:52:17 INFO MicroBatchExecution: Committed offsets for batch 13. Metadata OffsetSeqMetadata(0,1770918737432,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/12 17:52:17 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/12 17:52:17 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/12 17:52:17 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/12 17:52:17 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/12 17:52:17 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/12 17:52:17 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/12 17:52:17 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#13053 - origin_code.nullCount#13052) > 0)
26/02/12 17:52:17 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#13058 - destination_code.nullCount#13057) > 0)
26/02/12 17:52:17 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#13088 - callsign.nullCount#13087) > 0)
26/02/12 17:52:17 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/12 17:52:17 INFO DAGScheduler: Got job 35 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/12 17:52:17 INFO DAGScheduler: Final stage: ResultStage 56 (start at NativeMethodAccessorImpl.java:0)
26/02/12 17:52:17 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 55)
26/02/12 17:52:17 INFO DAGScheduler: Missing parents: List()
26/02/12 17:52:17 INFO DAGScheduler: Submitting ResultStage 56 (MapPartitionsRDD[186] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/12 17:52:17 INFO MemoryStore: Block broadcast_52 stored as values in memory (estimated size 99.3 KiB, free 434.0 MiB)
26/02/12 17:52:17 INFO MemoryStore: Block broadcast_52_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 433.9 MiB)
26/02/12 17:52:17 INFO BlockManagerInfo: Added broadcast_52_piece0 in memory on spark-master:44075 (size: 33.3 KiB, free: 434.2 MiB)
26/02/12 17:52:17 INFO SparkContext: Created broadcast 52 from broadcast at DAGScheduler.scala:1585
26/02/12 17:52:17 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 56 (MapPartitionsRDD[186] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/12 17:52:17 INFO TaskSchedulerImpl: Adding task set 56.0 with 4 tasks resource profile 0
26/02/12 17:52:17 INFO TaskSetManager: Starting task 0.0 in stage 56.0 (TID 80) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/12 17:52:17 INFO TaskSetManager: Starting task 1.0 in stage 56.0 (TID 81) (172.18.0.14, executor 1, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/12 17:52:17 INFO BlockManagerInfo: Added broadcast_52_piece0 in memory on 172.18.0.14:44081 (size: 33.3 KiB, free: 434.0 MiB)
26/02/12 17:52:17 INFO TaskSetManager: Starting task 2.0 in stage 56.0 (TID 82) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/12 17:52:17 INFO TaskSetManager: Finished task 0.0 in stage 56.0 (TID 80) in 27 ms on 172.18.0.14 (executor 1) (1/4)
26/02/12 17:52:17 INFO TaskSetManager: Starting task 3.0 in stage 56.0 (TID 83) (172.18.0.14, executor 1, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/12 17:52:17 INFO TaskSetManager: Finished task 1.0 in stage 56.0 (TID 81) in 28 ms on 172.18.0.14 (executor 1) (2/4)
26/02/12 17:52:17 INFO TaskSetManager: Finished task 3.0 in stage 56.0 (TID 83) in 21 ms on 172.18.0.14 (executor 1) (3/4)
26/02/12 17:52:17 INFO TaskSetManager: Finished task 2.0 in stage 56.0 (TID 82) in 24 ms on 172.18.0.14 (executor 1) (4/4)
26/02/12 17:52:17 INFO TaskSchedulerImpl: Removed TaskSet 56.0, whose tasks have all completed, from pool 
26/02/12 17:52:17 INFO DAGScheduler: ResultStage 56 (start at NativeMethodAccessorImpl.java:0) finished in 0.062 s
26/02/12 17:52:17 INFO DAGScheduler: Job 35 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/12 17:52:17 INFO TaskSchedulerImpl: Killing all running tasks in stage 56: Stage finished
26/02/12 17:52:17 INFO DAGScheduler: Job 35 finished: start at NativeMethodAccessorImpl.java:0, took 0.066491 s
26/02/12 17:52:17 INFO MemoryStore: Block broadcast_53_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.8 MiB)
26/02/12 17:52:17 INFO BlockManagerInfo: Added broadcast_53_piece0 in memory on spark-master:44075 (size: 107.2 KiB, free: 434.1 MiB)
26/02/12 17:52:17 INFO SparkContext: Created broadcast 53 from start at NativeMethodAccessorImpl.java:0
26/02/12 17:52:17 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 13, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@14d355a8]. The input RDD has 1 partitions.
26/02/12 17:52:17 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/12 17:52:17 INFO DAGScheduler: Got job 36 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/02/12 17:52:17 INFO DAGScheduler: Final stage: ResultStage 57 (start at NativeMethodAccessorImpl.java:0)
26/02/12 17:52:17 INFO DAGScheduler: Parents of final stage: List()
26/02/12 17:52:17 INFO DAGScheduler: Missing parents: List()
26/02/12 17:52:17 INFO DAGScheduler: Submitting ResultStage 57 (MapPartitionsRDD[192] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/12 17:52:17 INFO MemoryStore: Block broadcast_54 stored as values in memory (estimated size 49.9 KiB, free 433.8 MiB)
26/02/12 17:52:17 INFO MemoryStore: Block broadcast_54_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.8 MiB)
26/02/12 17:52:17 INFO BlockManagerInfo: Added broadcast_54_piece0 in memory on spark-master:44075 (size: 19.5 KiB, free: 434.1 MiB)
26/02/12 17:52:17 INFO SparkContext: Created broadcast 54 from broadcast at DAGScheduler.scala:1585
26/02/12 17:52:17 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 57 (MapPartitionsRDD[192] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/02/12 17:52:17 INFO TaskSchedulerImpl: Adding task set 57.0 with 1 tasks resource profile 0
26/02/12 17:52:17 INFO TaskSetManager: Starting task 0.0 in stage 57.0 (TID 84) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/12 17:52:17 INFO BlockManagerInfo: Added broadcast_54_piece0 in memory on 172.18.0.14:44081 (size: 19.5 KiB, free: 434.0 MiB)
26/02/12 17:52:17 INFO BlockManagerInfo: Added broadcast_53_piece0 in memory on 172.18.0.14:44081 (size: 107.2 KiB, free: 433.9 MiB)
26/02/12 17:52:18 INFO TaskSetManager: Finished task 0.0 in stage 57.0 (TID 84) in 598 ms on 172.18.0.14 (executor 1) (1/1)
26/02/12 17:52:18 INFO TaskSchedulerImpl: Removed TaskSet 57.0, whose tasks have all completed, from pool 
26/02/12 17:52:18 INFO DAGScheduler: ResultStage 57 (start at NativeMethodAccessorImpl.java:0) finished in 0.604 s
26/02/12 17:52:18 INFO DAGScheduler: Job 36 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/12 17:52:18 INFO TaskSchedulerImpl: Killing all running tasks in stage 57: Stage finished
26/02/12 17:52:18 INFO DAGScheduler: Job 36 finished: start at NativeMethodAccessorImpl.java:0, took 0.606441 s
26/02/12 17:52:18 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 13, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@14d355a8] is committing.
26/02/12 17:52:18 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 13, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@14d355a8] committed.
26/02/12 17:52:18 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/13 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.13.29ea0df1-b4ce-488e-86a6-27e9468c813b.tmp
26/02/12 17:52:18 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.13.29ea0df1-b4ce-488e-86a6-27e9468c813b.tmp to file:/tmp/spark-checkpoint-enrichment/commits/13
26/02/12 17:52:18 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "801223b3-8a58-496f-9f28-5a9fda8cbb73",
  "runId" : "005c83b3-ee40-4ce7-8dfd-e6942daa9533",
  "name" : null,
  "timestamp" : "2026-02-12T17:52:17.427Z",
  "batchId" : 13,
  "numInputRows" : 25,
  "inputRowsPerSecond" : 1666.6666666666667,
  "processedRowsPerSecond" : 25.22704339051463,
  "durationMs" : {
    "addBatch" : 789,
    "commitOffsets" : 62,
    "getBatch" : 0,
    "latestOffset" : 5,
    "queryPlanning" : 48,
    "triggerExecution" : 991,
    "walCommit" : 86
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "0" : 40420
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "0" : 40445
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "0" : 40445
      }
    },
    "numInputRows" : 25,
    "inputRowsPerSecond" : 1666.6666666666667,
    "processedRowsPerSecond" : 25.22704339051463,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@444eb264",
    "numOutputRows" : 11
  }
}
26/02/12 17:52:18 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/14 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.14.658fd1a1-e5c0-49f8-ac1e-a75bb05ff1b2.tmp
26/02/12 17:52:18 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.14.658fd1a1-e5c0-49f8-ac1e-a75bb05ff1b2.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/14
26/02/12 17:52:18 INFO MicroBatchExecution: Committed offsets for batch 14. Metadata OffsetSeqMetadata(0,1770918738420,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/12 17:52:18 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/12 17:52:18 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/12 17:52:18 INFO BlockManagerInfo: Removed broadcast_54_piece0 on spark-master:44075 in memory (size: 19.5 KiB, free: 434.1 MiB)
26/02/12 17:52:18 INFO BlockManagerInfo: Removed broadcast_54_piece0 on 172.18.0.14:44081 in memory (size: 19.5 KiB, free: 433.9 MiB)
26/02/12 17:52:18 INFO BlockManagerInfo: Removed broadcast_52_piece0 on spark-master:44075 in memory (size: 33.3 KiB, free: 434.2 MiB)
26/02/12 17:52:18 INFO BlockManagerInfo: Removed broadcast_52_piece0 on 172.18.0.14:44081 in memory (size: 33.3 KiB, free: 434.0 MiB)
26/02/12 17:52:18 INFO BlockManagerInfo: Removed broadcast_53_piece0 on spark-master:44075 in memory (size: 107.2 KiB, free: 434.3 MiB)
26/02/12 17:52:18 INFO BlockManagerInfo: Removed broadcast_53_piece0 on 172.18.0.14:44081 in memory (size: 107.2 KiB, free: 434.1 MiB)
26/02/12 17:52:18 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/12 17:52:18 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/12 17:52:18 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/12 17:52:18 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/12 17:52:18 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#13907 - origin_code.nullCount#13906) > 0)
26/02/12 17:52:18 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#13912 - destination_code.nullCount#13911) > 0)
26/02/12 17:52:18 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#13942 - callsign.nullCount#13941) > 0)
26/02/12 17:52:18 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/12 17:52:18 INFO DAGScheduler: Got job 37 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/12 17:52:18 INFO DAGScheduler: Final stage: ResultStage 59 (start at NativeMethodAccessorImpl.java:0)
26/02/12 17:52:18 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 58)
26/02/12 17:52:18 INFO DAGScheduler: Missing parents: List()
26/02/12 17:52:18 INFO DAGScheduler: Submitting ResultStage 59 (MapPartitionsRDD[197] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/12 17:52:18 INFO MemoryStore: Block broadcast_55 stored as values in memory (estimated size 99.3 KiB, free 434.0 MiB)
26/02/12 17:52:18 INFO MemoryStore: Block broadcast_55_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 433.9 MiB)
26/02/12 17:52:18 INFO BlockManagerInfo: Added broadcast_55_piece0 in memory on spark-master:44075 (size: 33.3 KiB, free: 434.2 MiB)
26/02/12 17:52:18 INFO SparkContext: Created broadcast 55 from broadcast at DAGScheduler.scala:1585
26/02/12 17:52:18 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 59 (MapPartitionsRDD[197] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/12 17:52:18 INFO TaskSchedulerImpl: Adding task set 59.0 with 4 tasks resource profile 0
26/02/12 17:52:18 INFO TaskSetManager: Starting task 0.0 in stage 59.0 (TID 85) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/12 17:52:18 INFO TaskSetManager: Starting task 1.0 in stage 59.0 (TID 86) (172.18.0.14, executor 1, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/12 17:52:18 INFO BlockManagerInfo: Added broadcast_55_piece0 in memory on 172.18.0.14:44081 (size: 33.3 KiB, free: 434.0 MiB)
26/02/12 17:52:18 INFO TaskSetManager: Starting task 2.0 in stage 59.0 (TID 87) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/12 17:52:18 INFO TaskSetManager: Starting task 3.0 in stage 59.0 (TID 88) (172.18.0.14, executor 1, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/12 17:52:18 INFO TaskSetManager: Finished task 1.0 in stage 59.0 (TID 86) in 30 ms on 172.18.0.14 (executor 1) (1/4)
26/02/12 17:52:18 INFO TaskSetManager: Finished task 0.0 in stage 59.0 (TID 85) in 33 ms on 172.18.0.14 (executor 1) (2/4)
26/02/12 17:52:18 INFO TaskSetManager: Finished task 2.0 in stage 59.0 (TID 87) in 31 ms on 172.18.0.14 (executor 1) (3/4)
26/02/12 17:52:18 INFO TaskSetManager: Finished task 3.0 in stage 59.0 (TID 88) in 31 ms on 172.18.0.14 (executor 1) (4/4)
26/02/12 17:52:18 INFO TaskSchedulerImpl: Removed TaskSet 59.0, whose tasks have all completed, from pool 
26/02/12 17:52:18 INFO BlockManagerInfo: Removed broadcast_50_piece0 on spark-master:44075 in memory (size: 107.2 KiB, free: 434.3 MiB)
26/02/12 17:52:18 INFO DAGScheduler: ResultStage 59 (start at NativeMethodAccessorImpl.java:0) finished in 0.073 s
26/02/12 17:52:18 INFO DAGScheduler: Job 37 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/12 17:52:18 INFO TaskSchedulerImpl: Killing all running tasks in stage 59: Stage finished
26/02/12 17:52:18 INFO BlockManagerInfo: Removed broadcast_50_piece0 on 172.18.0.14:44081 in memory (size: 107.2 KiB, free: 434.1 MiB)
26/02/12 17:52:18 INFO DAGScheduler: Job 37 finished: start at NativeMethodAccessorImpl.java:0, took 0.076451 s
26/02/12 17:52:18 INFO MemoryStore: Block broadcast_56_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.9 MiB)
26/02/12 17:52:18 INFO BlockManagerInfo: Added broadcast_56_piece0 in memory on spark-master:44075 (size: 107.2 KiB, free: 434.2 MiB)
26/02/12 17:52:18 INFO SparkContext: Created broadcast 56 from start at NativeMethodAccessorImpl.java:0
26/02/12 17:52:18 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 14, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@6f3e005b]. The input RDD has 1 partitions.
26/02/12 17:52:18 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/12 17:52:18 INFO DAGScheduler: Got job 38 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/02/12 17:52:18 INFO DAGScheduler: Final stage: ResultStage 60 (start at NativeMethodAccessorImpl.java:0)
26/02/12 17:52:18 INFO DAGScheduler: Parents of final stage: List()
26/02/12 17:52:18 INFO DAGScheduler: Missing parents: List()
26/02/12 17:52:18 INFO DAGScheduler: Submitting ResultStage 60 (MapPartitionsRDD[203] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/12 17:52:18 INFO MemoryStore: Block broadcast_57 stored as values in memory (estimated size 49.9 KiB, free 433.9 MiB)
26/02/12 17:52:18 INFO MemoryStore: Block broadcast_57_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.9 MiB)
26/02/12 17:52:18 INFO BlockManagerInfo: Added broadcast_57_piece0 in memory on spark-master:44075 (size: 19.5 KiB, free: 434.2 MiB)
26/02/12 17:52:18 INFO SparkContext: Created broadcast 57 from broadcast at DAGScheduler.scala:1585
26/02/12 17:52:18 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 60 (MapPartitionsRDD[203] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/02/12 17:52:18 INFO TaskSchedulerImpl: Adding task set 60.0 with 1 tasks resource profile 0
26/02/12 17:52:18 INFO TaskSetManager: Starting task 0.0 in stage 60.0 (TID 89) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/12 17:52:18 INFO BlockManagerInfo: Added broadcast_57_piece0 in memory on 172.18.0.14:44081 (size: 19.5 KiB, free: 434.1 MiB)
26/02/12 17:52:18 INFO BlockManagerInfo: Added broadcast_56_piece0 in memory on 172.18.0.14:44081 (size: 107.2 KiB, free: 434.0 MiB)
26/02/12 17:52:18 INFO TaskSetManager: Finished task 0.0 in stage 60.0 (TID 89) in 89 ms on 172.18.0.14 (executor 1) (1/1)
26/02/12 17:52:18 INFO TaskSchedulerImpl: Removed TaskSet 60.0, whose tasks have all completed, from pool 
26/02/12 17:52:18 INFO DAGScheduler: ResultStage 60 (start at NativeMethodAccessorImpl.java:0) finished in 0.094 s
26/02/12 17:52:18 INFO DAGScheduler: Job 38 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/12 17:52:18 INFO TaskSchedulerImpl: Killing all running tasks in stage 60: Stage finished
26/02/12 17:52:18 INFO DAGScheduler: Job 38 finished: start at NativeMethodAccessorImpl.java:0, took 0.096914 s
26/02/12 17:52:18 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 14, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@6f3e005b] is committing.
26/02/12 17:52:18 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 14, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@6f3e005b] committed.
26/02/12 17:52:18 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/14 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.14.22e6d9c5-f5d4-47bf-a752-f7eeb322fbaa.tmp
26/02/12 17:52:18 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.14.22e6d9c5-f5d4-47bf-a752-f7eeb322fbaa.tmp to file:/tmp/spark-checkpoint-enrichment/commits/14
26/02/12 17:52:18 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "801223b3-8a58-496f-9f28-5a9fda8cbb73",
  "runId" : "005c83b3-ee40-4ce7-8dfd-e6942daa9533",
  "name" : null,
  "timestamp" : "2026-02-12T17:52:18.419Z",
  "batchId" : 14,
  "numInputRows" : 202,
  "inputRowsPerSecond" : 203.6290322580645,
  "processedRowsPerSecond" : 368.6131386861314,
  "durationMs" : {
    "addBatch" : 336,
    "commitOffsets" : 59,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 69,
    "triggerExecution" : 548,
    "walCommit" : 82
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "0" : 40445
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "0" : 40647
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "0" : 40647
      }
    },
    "numInputRows" : 202,
    "inputRowsPerSecond" : 203.6290322580645,
    "processedRowsPerSecond" : 368.6131386861314,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@444eb264",
    "numOutputRows" : 36
  }
}
26/02/12 17:52:28 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/12 17:52:30 INFO BlockManagerInfo: Removed broadcast_57_piece0 on spark-master:44075 in memory (size: 19.5 KiB, free: 434.2 MiB)
26/02/12 17:52:30 INFO BlockManagerInfo: Removed broadcast_57_piece0 on 172.18.0.14:44081 in memory (size: 19.5 KiB, free: 434.0 MiB)
26/02/12 17:52:30 INFO BlockManagerInfo: Removed broadcast_55_piece0 on spark-master:44075 in memory (size: 33.3 KiB, free: 434.3 MiB)
26/02/12 17:52:30 INFO BlockManagerInfo: Removed broadcast_55_piece0 on 172.18.0.14:44081 in memory (size: 33.3 KiB, free: 434.1 MiB)
26/02/12 17:52:37 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/15 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.15.ff627966-fe30-4d6d-94e3-1fe97f6d884c.tmp
26/02/12 17:52:37 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.15.ff627966-fe30-4d6d-94e3-1fe97f6d884c.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/15
26/02/12 17:52:37 INFO MicroBatchExecution: Committed offsets for batch 15. Metadata OffsetSeqMetadata(0,1770918757696,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/12 17:52:37 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/12 17:52:37 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/12 17:52:37 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/12 17:52:37 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/12 17:52:37 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/12 17:52:37 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/12 17:52:37 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#14761 - origin_code.nullCount#14760) > 0)
26/02/12 17:52:37 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#14766 - destination_code.nullCount#14765) > 0)
26/02/12 17:52:37 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#14796 - callsign.nullCount#14795) > 0)
26/02/12 17:52:37 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/12 17:52:37 INFO DAGScheduler: Got job 39 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/12 17:52:37 INFO DAGScheduler: Final stage: ResultStage 62 (start at NativeMethodAccessorImpl.java:0)
26/02/12 17:52:37 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 61)
26/02/12 17:52:37 INFO DAGScheduler: Missing parents: List()
26/02/12 17:52:37 INFO DAGScheduler: Submitting ResultStage 62 (MapPartitionsRDD[208] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/12 17:52:37 INFO MemoryStore: Block broadcast_58 stored as values in memory (estimated size 99.3 KiB, free 434.0 MiB)
26/02/12 17:52:37 INFO MemoryStore: Block broadcast_58_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 433.9 MiB)
26/02/12 17:52:37 INFO BlockManagerInfo: Added broadcast_58_piece0 in memory on spark-master:44075 (size: 33.3 KiB, free: 434.2 MiB)
26/02/12 17:52:37 INFO SparkContext: Created broadcast 58 from broadcast at DAGScheduler.scala:1585
26/02/12 17:52:37 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 62 (MapPartitionsRDD[208] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/12 17:52:37 INFO TaskSchedulerImpl: Adding task set 62.0 with 4 tasks resource profile 0
26/02/12 17:52:37 INFO TaskSetManager: Starting task 0.0 in stage 62.0 (TID 90) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/12 17:52:37 INFO TaskSetManager: Starting task 1.0 in stage 62.0 (TID 91) (172.18.0.14, executor 1, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/12 17:52:37 INFO BlockManagerInfo: Added broadcast_58_piece0 in memory on 172.18.0.14:44081 (size: 33.3 KiB, free: 434.0 MiB)
26/02/12 17:52:37 INFO TaskSetManager: Finished task 0.0 in stage 62.0 (TID 90) in 32 ms on 172.18.0.14 (executor 1) (1/4)
26/02/12 17:52:37 INFO TaskSetManager: Starting task 2.0 in stage 62.0 (TID 92) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/12 17:52:37 INFO TaskSetManager: Starting task 3.0 in stage 62.0 (TID 93) (172.18.0.14, executor 1, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/12 17:52:37 INFO TaskSetManager: Finished task 1.0 in stage 62.0 (TID 91) in 35 ms on 172.18.0.14 (executor 1) (2/4)
26/02/12 17:52:38 INFO TaskSetManager: Finished task 2.0 in stage 62.0 (TID 92) in 32 ms on 172.18.0.14 (executor 1) (3/4)
26/02/12 17:52:38 INFO TaskSetManager: Finished task 3.0 in stage 62.0 (TID 93) in 32 ms on 172.18.0.14 (executor 1) (4/4)
26/02/12 17:52:38 INFO TaskSchedulerImpl: Removed TaskSet 62.0, whose tasks have all completed, from pool 
26/02/12 17:52:38 INFO DAGScheduler: ResultStage 62 (start at NativeMethodAccessorImpl.java:0) finished in 0.082 s
26/02/12 17:52:38 INFO DAGScheduler: Job 39 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/12 17:52:38 INFO TaskSchedulerImpl: Killing all running tasks in stage 62: Stage finished
26/02/12 17:52:38 INFO DAGScheduler: Job 39 finished: start at NativeMethodAccessorImpl.java:0, took 0.087943 s
26/02/12 17:52:38 INFO MemoryStore: Block broadcast_59_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.8 MiB)
26/02/12 17:52:38 INFO BlockManagerInfo: Added broadcast_59_piece0 in memory on spark-master:44075 (size: 107.2 KiB, free: 434.1 MiB)
26/02/12 17:52:38 INFO SparkContext: Created broadcast 59 from start at NativeMethodAccessorImpl.java:0
26/02/12 17:52:38 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 15, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@1ae02d63]. The input RDD has 1 partitions.
26/02/12 17:52:38 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/12 17:52:38 INFO DAGScheduler: Got job 40 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/02/12 17:52:38 INFO DAGScheduler: Final stage: ResultStage 63 (start at NativeMethodAccessorImpl.java:0)
26/02/12 17:52:38 INFO DAGScheduler: Parents of final stage: List()
26/02/12 17:52:38 INFO DAGScheduler: Missing parents: List()
26/02/12 17:52:38 INFO DAGScheduler: Submitting ResultStage 63 (MapPartitionsRDD[214] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/12 17:52:38 INFO MemoryStore: Block broadcast_60 stored as values in memory (estimated size 49.9 KiB, free 433.8 MiB)
26/02/12 17:52:38 INFO MemoryStore: Block broadcast_60_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.8 MiB)
26/02/12 17:52:38 INFO BlockManagerInfo: Added broadcast_60_piece0 in memory on spark-master:44075 (size: 19.5 KiB, free: 434.1 MiB)
26/02/12 17:52:38 INFO SparkContext: Created broadcast 60 from broadcast at DAGScheduler.scala:1585
26/02/12 17:52:38 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 63 (MapPartitionsRDD[214] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/02/12 17:52:38 INFO TaskSchedulerImpl: Adding task set 63.0 with 1 tasks resource profile 0
26/02/12 17:52:38 INFO TaskSetManager: Starting task 0.0 in stage 63.0 (TID 94) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/12 17:52:38 INFO BlockManagerInfo: Added broadcast_60_piece0 in memory on 172.18.0.14:44081 (size: 19.5 KiB, free: 434.0 MiB)
26/02/12 17:52:38 INFO BlockManagerInfo: Added broadcast_59_piece0 in memory on 172.18.0.14:44081 (size: 107.2 KiB, free: 433.9 MiB)
26/02/12 17:52:38 INFO TaskSetManager: Finished task 0.0 in stage 63.0 (TID 94) in 583 ms on 172.18.0.14 (executor 1) (1/1)
26/02/12 17:52:38 INFO TaskSchedulerImpl: Removed TaskSet 63.0, whose tasks have all completed, from pool 
26/02/12 17:52:38 INFO DAGScheduler: ResultStage 63 (start at NativeMethodAccessorImpl.java:0) finished in 0.591 s
26/02/12 17:52:38 INFO DAGScheduler: Job 40 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/12 17:52:38 INFO TaskSchedulerImpl: Killing all running tasks in stage 63: Stage finished
26/02/12 17:52:38 INFO DAGScheduler: Job 40 finished: start at NativeMethodAccessorImpl.java:0, took 0.593611 s
26/02/12 17:52:38 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 15, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@1ae02d63] is committing.
26/02/12 17:52:38 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 15, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@1ae02d63] committed.
26/02/12 17:52:38 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/15 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.15.8931eb2d-525d-458f-bbab-b63102df17dd.tmp
26/02/12 17:52:38 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.15.8931eb2d-525d-458f-bbab-b63102df17dd.tmp to file:/tmp/spark-checkpoint-enrichment/commits/15
26/02/12 17:52:38 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "801223b3-8a58-496f-9f28-5a9fda8cbb73",
  "runId" : "005c83b3-ee40-4ce7-8dfd-e6942daa9533",
  "name" : null,
  "timestamp" : "2026-02-12T17:52:37.695Z",
  "batchId" : 15,
  "numInputRows" : 37,
  "inputRowsPerSecond" : 2846.153846153846,
  "processedRowsPerSecond" : 36.96303696303697,
  "durationMs" : {
    "addBatch" : 816,
    "commitOffsets" : 61,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 51,
    "triggerExecution" : 1001,
    "walCommit" : 71
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "0" : 40647
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "0" : 40684
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "0" : 40684
      }
    },
    "numInputRows" : 37,
    "inputRowsPerSecond" : 2846.153846153846,
    "processedRowsPerSecond" : 36.96303696303697,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@444eb264",
    "numOutputRows" : 15
  }
}
26/02/12 17:52:38 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/16 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.16.ddd7001a-11d3-4170-9673-58cd7d8013f3.tmp
26/02/12 17:52:38 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.16.ddd7001a-11d3-4170-9673-58cd7d8013f3.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/16
26/02/12 17:52:38 INFO MicroBatchExecution: Committed offsets for batch 16. Metadata OffsetSeqMetadata(0,1770918758700,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/12 17:52:38 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/12 17:52:38 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/12 17:52:38 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/12 17:52:38 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/12 17:52:38 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/12 17:52:38 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/12 17:52:38 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#15615 - origin_code.nullCount#15614) > 0)
26/02/12 17:52:38 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#15620 - destination_code.nullCount#15619) > 0)
26/02/12 17:52:38 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#15650 - callsign.nullCount#15649) > 0)
26/02/12 17:52:38 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/12 17:52:38 INFO DAGScheduler: Got job 41 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/12 17:52:38 INFO DAGScheduler: Final stage: ResultStage 65 (start at NativeMethodAccessorImpl.java:0)
26/02/12 17:52:38 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 64)
26/02/12 17:52:38 INFO DAGScheduler: Missing parents: List()
26/02/12 17:52:38 INFO DAGScheduler: Submitting ResultStage 65 (MapPartitionsRDD[219] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/12 17:52:38 INFO MemoryStore: Block broadcast_61 stored as values in memory (estimated size 99.3 KiB, free 433.7 MiB)
26/02/12 17:52:38 INFO MemoryStore: Block broadcast_61_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 433.6 MiB)
26/02/12 17:52:38 INFO BlockManagerInfo: Added broadcast_61_piece0 in memory on spark-master:44075 (size: 33.3 KiB, free: 434.1 MiB)
26/02/12 17:52:38 INFO SparkContext: Created broadcast 61 from broadcast at DAGScheduler.scala:1585
26/02/12 17:52:38 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 65 (MapPartitionsRDD[219] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/12 17:52:38 INFO TaskSchedulerImpl: Adding task set 65.0 with 4 tasks resource profile 0
26/02/12 17:52:38 INFO TaskSetManager: Starting task 0.0 in stage 65.0 (TID 95) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/12 17:52:38 INFO TaskSetManager: Starting task 1.0 in stage 65.0 (TID 96) (172.18.0.14, executor 1, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/12 17:52:38 INFO BlockManagerInfo: Added broadcast_61_piece0 in memory on 172.18.0.14:44081 (size: 33.3 KiB, free: 433.9 MiB)
26/02/12 17:52:38 INFO TaskSetManager: Starting task 2.0 in stage 65.0 (TID 97) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/12 17:52:38 INFO TaskSetManager: Finished task 1.0 in stage 65.0 (TID 96) in 23 ms on 172.18.0.14 (executor 1) (1/4)
26/02/12 17:52:38 INFO TaskSetManager: Starting task 3.0 in stage 65.0 (TID 98) (172.18.0.14, executor 1, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/12 17:52:38 INFO TaskSetManager: Finished task 0.0 in stage 65.0 (TID 95) in 31 ms on 172.18.0.14 (executor 1) (2/4)
26/02/12 17:52:38 INFO TaskSetManager: Finished task 2.0 in stage 65.0 (TID 97) in 19 ms on 172.18.0.14 (executor 1) (3/4)
26/02/12 17:52:38 INFO TaskSetManager: Finished task 3.0 in stage 65.0 (TID 98) in 22 ms on 172.18.0.14 (executor 1) (4/4)
26/02/12 17:52:38 INFO TaskSchedulerImpl: Removed TaskSet 65.0, whose tasks have all completed, from pool 
26/02/12 17:52:38 INFO DAGScheduler: ResultStage 65 (start at NativeMethodAccessorImpl.java:0) finished in 0.058 s
26/02/12 17:52:38 INFO DAGScheduler: Job 41 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/12 17:52:38 INFO TaskSchedulerImpl: Killing all running tasks in stage 65: Stage finished
26/02/12 17:52:38 INFO DAGScheduler: Job 41 finished: start at NativeMethodAccessorImpl.java:0, took 0.060383 s
26/02/12 17:52:38 INFO BlockManagerInfo: Removed broadcast_60_piece0 on spark-master:44075 in memory (size: 19.5 KiB, free: 434.1 MiB)
26/02/12 17:52:38 INFO BlockManagerInfo: Removed broadcast_60_piece0 on 172.18.0.14:44081 in memory (size: 19.5 KiB, free: 433.9 MiB)
26/02/12 17:52:38 INFO MemoryStore: Block broadcast_62_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.6 MiB)
26/02/12 17:52:38 INFO BlockManagerInfo: Added broadcast_62_piece0 in memory on spark-master:44075 (size: 107.2 KiB, free: 434.0 MiB)
26/02/12 17:52:38 INFO SparkContext: Created broadcast 62 from start at NativeMethodAccessorImpl.java:0
26/02/12 17:52:38 INFO BlockManagerInfo: Removed broadcast_59_piece0 on spark-master:44075 in memory (size: 107.2 KiB, free: 434.1 MiB)
26/02/12 17:52:38 INFO BlockManagerInfo: Removed broadcast_59_piece0 on 172.18.0.14:44081 in memory (size: 107.2 KiB, free: 434.0 MiB)
26/02/12 17:52:38 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 16, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@32428a53]. The input RDD has 1 partitions.
26/02/12 17:52:38 INFO BlockManagerInfo: Removed broadcast_58_piece0 on spark-master:44075 in memory (size: 33.3 KiB, free: 434.1 MiB)
26/02/12 17:52:38 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/12 17:52:38 INFO DAGScheduler: Got job 42 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/02/12 17:52:38 INFO DAGScheduler: Final stage: ResultStage 66 (start at NativeMethodAccessorImpl.java:0)
26/02/12 17:52:38 INFO DAGScheduler: Parents of final stage: List()
26/02/12 17:52:38 INFO DAGScheduler: Missing parents: List()
26/02/12 17:52:38 INFO DAGScheduler: Submitting ResultStage 66 (MapPartitionsRDD[225] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/12 17:52:38 INFO BlockManagerInfo: Removed broadcast_58_piece0 on 172.18.0.14:44081 in memory (size: 33.3 KiB, free: 434.0 MiB)
26/02/12 17:52:38 INFO MemoryStore: Block broadcast_63 stored as values in memory (estimated size 49.9 KiB, free 433.8 MiB)
26/02/12 17:52:38 INFO MemoryStore: Block broadcast_63_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.8 MiB)
26/02/12 17:52:38 INFO BlockManagerInfo: Added broadcast_63_piece0 in memory on spark-master:44075 (size: 19.5 KiB, free: 434.1 MiB)
26/02/12 17:52:38 INFO SparkContext: Created broadcast 63 from broadcast at DAGScheduler.scala:1585
26/02/12 17:52:38 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 66 (MapPartitionsRDD[225] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/02/12 17:52:38 INFO TaskSchedulerImpl: Adding task set 66.0 with 1 tasks resource profile 0
26/02/12 17:52:38 INFO BlockManagerInfo: Removed broadcast_61_piece0 on spark-master:44075 in memory (size: 33.3 KiB, free: 434.1 MiB)
26/02/12 17:52:38 INFO TaskSetManager: Starting task 0.0 in stage 66.0 (TID 99) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/12 17:52:38 INFO BlockManagerInfo: Removed broadcast_61_piece0 on 172.18.0.14:44081 in memory (size: 33.3 KiB, free: 434.1 MiB)
26/02/12 17:52:39 INFO BlockManagerInfo: Removed broadcast_56_piece0 on spark-master:44075 in memory (size: 107.2 KiB, free: 434.2 MiB)
26/02/12 17:52:39 INFO BlockManagerInfo: Removed broadcast_56_piece0 on 172.18.0.14:44081 in memory (size: 107.2 KiB, free: 434.2 MiB)
26/02/12 17:52:39 INFO BlockManagerInfo: Added broadcast_63_piece0 in memory on 172.18.0.14:44081 (size: 19.5 KiB, free: 434.1 MiB)
26/02/12 17:52:39 INFO BlockManagerInfo: Added broadcast_62_piece0 in memory on 172.18.0.14:44081 (size: 107.2 KiB, free: 434.0 MiB)
26/02/12 17:52:39 INFO TaskSetManager: Finished task 0.0 in stage 66.0 (TID 99) in 94 ms on 172.18.0.14 (executor 1) (1/1)
26/02/12 17:52:39 INFO TaskSchedulerImpl: Removed TaskSet 66.0, whose tasks have all completed, from pool 
26/02/12 17:52:39 INFO DAGScheduler: ResultStage 66 (start at NativeMethodAccessorImpl.java:0) finished in 0.104 s
26/02/12 17:52:39 INFO DAGScheduler: Job 42 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/12 17:52:39 INFO TaskSchedulerImpl: Killing all running tasks in stage 66: Stage finished
26/02/12 17:52:39 INFO DAGScheduler: Job 42 finished: start at NativeMethodAccessorImpl.java:0, took 0.106480 s
26/02/12 17:52:39 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 16, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@32428a53] is committing.
26/02/12 17:52:39 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 16, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@32428a53] committed.
26/02/12 17:52:39 INFO BlockManagerInfo: Removed broadcast_63_piece0 on spark-master:44075 in memory (size: 19.5 KiB, free: 434.3 MiB)
26/02/12 17:52:39 INFO BlockManagerInfo: Removed broadcast_63_piece0 on 172.18.0.14:44081 in memory (size: 19.5 KiB, free: 434.1 MiB)
26/02/12 17:52:39 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/16 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.16.0db5d79d-19a9-4d04-9185-55cfba55a010.tmp
26/02/12 17:52:39 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.16.0db5d79d-19a9-4d04-9185-55cfba55a010.tmp to file:/tmp/spark-checkpoint-enrichment/commits/16
26/02/12 17:52:39 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "801223b3-8a58-496f-9f28-5a9fda8cbb73",
  "runId" : "005c83b3-ee40-4ce7-8dfd-e6942daa9533",
  "name" : null,
  "timestamp" : "2026-02-12T17:52:38.697Z",
  "batchId" : 16,
  "numInputRows" : 191,
  "inputRowsPerSecond" : 190.6187624750499,
  "processedRowsPerSecond" : 416.12200435729847,
  "durationMs" : {
    "addBatch" : 290,
    "commitOffsets" : 69,
    "getBatch" : 0,
    "latestOffset" : 2,
    "queryPlanning" : 38,
    "triggerExecution" : 459,
    "walCommit" : 58
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "0" : 40684
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "0" : 40875
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "0" : 40875
      }
    },
    "numInputRows" : 191,
    "inputRowsPerSecond" : 190.6187624750499,
    "processedRowsPerSecond" : 416.12200435729847,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@444eb264",
    "numOutputRows" : 32
  }
}
26/02/12 17:52:49 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/12 17:52:55 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/17 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.17.94715936-fb93-4a45-a3f1-231f80463a36.tmp
26/02/12 17:52:55 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.17.94715936-fb93-4a45-a3f1-231f80463a36.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/17
26/02/12 17:52:55 INFO MicroBatchExecution: Committed offsets for batch 17. Metadata OffsetSeqMetadata(0,1770918775632,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/12 17:52:55 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/12 17:52:55 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/12 17:52:55 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/12 17:52:55 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/12 17:52:55 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/12 17:52:55 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/12 17:52:55 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#16469 - origin_code.nullCount#16468) > 0)
26/02/12 17:52:55 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#16474 - destination_code.nullCount#16473) > 0)
26/02/12 17:52:55 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#16504 - callsign.nullCount#16503) > 0)
26/02/12 17:52:55 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/12 17:52:55 INFO DAGScheduler: Got job 43 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/12 17:52:55 INFO DAGScheduler: Final stage: ResultStage 68 (start at NativeMethodAccessorImpl.java:0)
26/02/12 17:52:55 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 67)
26/02/12 17:52:55 INFO DAGScheduler: Missing parents: List()
26/02/12 17:52:55 INFO DAGScheduler: Submitting ResultStage 68 (MapPartitionsRDD[230] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/12 17:52:55 INFO MemoryStore: Block broadcast_64 stored as values in memory (estimated size 99.3 KiB, free 434.0 MiB)
26/02/12 17:52:55 INFO MemoryStore: Block broadcast_64_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 433.9 MiB)
26/02/12 17:52:55 INFO BlockManagerInfo: Added broadcast_64_piece0 in memory on spark-master:44075 (size: 33.3 KiB, free: 434.2 MiB)
26/02/12 17:52:55 INFO SparkContext: Created broadcast 64 from broadcast at DAGScheduler.scala:1585
26/02/12 17:52:55 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 68 (MapPartitionsRDD[230] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/12 17:52:55 INFO TaskSchedulerImpl: Adding task set 68.0 with 4 tasks resource profile 0
26/02/12 17:52:55 INFO TaskSetManager: Starting task 0.0 in stage 68.0 (TID 100) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/12 17:52:55 INFO TaskSetManager: Starting task 1.0 in stage 68.0 (TID 101) (172.18.0.14, executor 1, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/12 17:52:55 INFO BlockManagerInfo: Added broadcast_64_piece0 in memory on 172.18.0.14:44081 (size: 33.3 KiB, free: 434.0 MiB)
26/02/12 17:52:55 INFO TaskSetManager: Starting task 2.0 in stage 68.0 (TID 102) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/12 17:52:55 INFO TaskSetManager: Finished task 1.0 in stage 68.0 (TID 101) in 44 ms on 172.18.0.14 (executor 1) (1/4)
26/02/12 17:52:55 INFO TaskSetManager: Starting task 3.0 in stage 68.0 (TID 103) (172.18.0.14, executor 1, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/12 17:52:55 INFO TaskSetManager: Finished task 0.0 in stage 68.0 (TID 100) in 55 ms on 172.18.0.14 (executor 1) (2/4)
26/02/12 17:52:56 INFO TaskSetManager: Finished task 2.0 in stage 68.0 (TID 102) in 56 ms on 172.18.0.14 (executor 1) (3/4)
26/02/12 17:52:56 INFO TaskSetManager: Finished task 3.0 in stage 68.0 (TID 103) in 54 ms on 172.18.0.14 (executor 1) (4/4)
26/02/12 17:52:56 INFO DAGScheduler: ResultStage 68 (start at NativeMethodAccessorImpl.java:0) finished in 0.117 s
26/02/12 17:52:56 INFO TaskSchedulerImpl: Removed TaskSet 68.0, whose tasks have all completed, from pool 
26/02/12 17:52:56 INFO DAGScheduler: Job 43 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/12 17:52:56 INFO TaskSchedulerImpl: Killing all running tasks in stage 68: Stage finished
26/02/12 17:52:56 INFO DAGScheduler: Job 43 finished: start at NativeMethodAccessorImpl.java:0, took 0.142545 s
26/02/12 17:52:56 INFO MemoryStore: Block broadcast_65_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.8 MiB)
26/02/12 17:52:56 INFO BlockManagerInfo: Added broadcast_65_piece0 in memory on spark-master:44075 (size: 107.2 KiB, free: 434.1 MiB)
26/02/12 17:52:56 INFO SparkContext: Created broadcast 65 from start at NativeMethodAccessorImpl.java:0
26/02/12 17:52:56 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 17, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@75b93b99]. The input RDD has 1 partitions.
26/02/12 17:52:56 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/12 17:52:56 INFO DAGScheduler: Got job 44 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/02/12 17:52:56 INFO DAGScheduler: Final stage: ResultStage 69 (start at NativeMethodAccessorImpl.java:0)
26/02/12 17:52:56 INFO DAGScheduler: Parents of final stage: List()
26/02/12 17:52:56 INFO DAGScheduler: Missing parents: List()
26/02/12 17:52:56 INFO DAGScheduler: Submitting ResultStage 69 (MapPartitionsRDD[236] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/12 17:52:56 INFO MemoryStore: Block broadcast_66 stored as values in memory (estimated size 49.9 KiB, free 433.8 MiB)
26/02/12 17:52:56 INFO MemoryStore: Block broadcast_66_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.8 MiB)
26/02/12 17:52:56 INFO BlockManagerInfo: Added broadcast_66_piece0 in memory on spark-master:44075 (size: 19.5 KiB, free: 434.1 MiB)
26/02/12 17:52:56 INFO SparkContext: Created broadcast 66 from broadcast at DAGScheduler.scala:1585
26/02/12 17:52:56 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 69 (MapPartitionsRDD[236] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/02/12 17:52:56 INFO TaskSchedulerImpl: Adding task set 69.0 with 1 tasks resource profile 0
26/02/12 17:52:56 INFO TaskSetManager: Starting task 0.0 in stage 69.0 (TID 104) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/12 17:52:56 INFO BlockManagerInfo: Added broadcast_66_piece0 in memory on 172.18.0.14:44081 (size: 19.5 KiB, free: 434.0 MiB)
26/02/12 17:52:56 INFO BlockManagerInfo: Added broadcast_65_piece0 in memory on 172.18.0.14:44081 (size: 107.2 KiB, free: 433.9 MiB)
26/02/12 17:52:56 INFO BlockManagerInfo: Removed broadcast_64_piece0 on spark-master:44075 in memory (size: 33.3 KiB, free: 434.1 MiB)
26/02/12 17:52:56 INFO BlockManagerInfo: Removed broadcast_64_piece0 on 172.18.0.14:44081 in memory (size: 33.3 KiB, free: 433.9 MiB)
26/02/12 17:52:56 INFO BlockManagerInfo: Removed broadcast_62_piece0 on spark-master:44075 in memory (size: 107.2 KiB, free: 434.2 MiB)
26/02/12 17:52:56 INFO BlockManagerInfo: Removed broadcast_62_piece0 on 172.18.0.14:44081 in memory (size: 107.2 KiB, free: 434.0 MiB)
26/02/12 17:52:56 INFO TaskSetManager: Finished task 0.0 in stage 69.0 (TID 104) in 600 ms on 172.18.0.14 (executor 1) (1/1)
26/02/12 17:52:56 INFO TaskSchedulerImpl: Removed TaskSet 69.0, whose tasks have all completed, from pool 
26/02/12 17:52:56 INFO DAGScheduler: ResultStage 69 (start at NativeMethodAccessorImpl.java:0) finished in 0.621 s
26/02/12 17:52:56 INFO DAGScheduler: Job 44 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/12 17:52:56 INFO TaskSchedulerImpl: Killing all running tasks in stage 69: Stage finished
26/02/12 17:52:56 INFO DAGScheduler: Job 44 finished: start at NativeMethodAccessorImpl.java:0, took 0.623836 s
26/02/12 17:52:56 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 17, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@75b93b99] is committing.
26/02/12 17:52:56 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 17, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@75b93b99] committed.
26/02/12 17:52:56 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/17 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.17.11f058cb-36db-40ec-80c3-85a8869cd022.tmp
26/02/12 17:52:56 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.17.11f058cb-36db-40ec-80c3-85a8869cd022.tmp to file:/tmp/spark-checkpoint-enrichment/commits/17
26/02/12 17:52:56 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "801223b3-8a58-496f-9f28-5a9fda8cbb73",
  "runId" : "005c83b3-ee40-4ce7-8dfd-e6942daa9533",
  "name" : null,
  "timestamp" : "2026-02-12T17:52:55.630Z",
  "batchId" : 17,
  "numInputRows" : 37,
  "inputRowsPerSecond" : 3083.3333333333335,
  "processedRowsPerSecond" : 33.273381294964025,
  "durationMs" : {
    "addBatch" : 932,
    "commitOffsets" : 70,
    "getBatch" : 0,
    "latestOffset" : 2,
    "queryPlanning" : 37,
    "triggerExecution" : 1112,
    "walCommit" : 70
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "0" : 40875
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "0" : 40912
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "0" : 40912
      }
    },
    "numInputRows" : 37,
    "inputRowsPerSecond" : 3083.3333333333335,
    "processedRowsPerSecond" : 33.273381294964025,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@444eb264",
    "numOutputRows" : 15
  }
}
26/02/12 17:52:56 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/18 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.18.59b3d7a9-d481-45b2-bd78-b053fab20983.tmp
26/02/12 17:52:56 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.18.59b3d7a9-d481-45b2-bd78-b053fab20983.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/18
26/02/12 17:52:56 INFO MicroBatchExecution: Committed offsets for batch 18. Metadata OffsetSeqMetadata(0,1770918776744,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/12 17:52:56 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/12 17:52:56 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/12 17:52:56 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/12 17:52:56 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/12 17:52:56 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/12 17:52:56 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/12 17:52:56 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#17323 - origin_code.nullCount#17322) > 0)
26/02/12 17:52:56 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#17328 - destination_code.nullCount#17327) > 0)
26/02/12 17:52:56 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#17358 - callsign.nullCount#17357) > 0)
26/02/12 17:52:56 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/12 17:52:56 INFO DAGScheduler: Got job 45 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/12 17:52:56 INFO DAGScheduler: Final stage: ResultStage 71 (start at NativeMethodAccessorImpl.java:0)
26/02/12 17:52:56 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 70)
26/02/12 17:52:56 INFO DAGScheduler: Missing parents: List()
26/02/12 17:52:56 INFO DAGScheduler: Submitting ResultStage 71 (MapPartitionsRDD[241] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/12 17:52:56 INFO MemoryStore: Block broadcast_67 stored as values in memory (estimated size 99.3 KiB, free 433.9 MiB)
26/02/12 17:52:56 INFO MemoryStore: Block broadcast_67_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 433.9 MiB)
26/02/12 17:52:56 INFO BlockManagerInfo: Added broadcast_67_piece0 in memory on spark-master:44075 (size: 33.3 KiB, free: 434.2 MiB)
26/02/12 17:52:56 INFO SparkContext: Created broadcast 67 from broadcast at DAGScheduler.scala:1585
26/02/12 17:52:56 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 71 (MapPartitionsRDD[241] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/12 17:52:56 INFO TaskSchedulerImpl: Adding task set 71.0 with 4 tasks resource profile 0
26/02/12 17:52:56 INFO TaskSetManager: Starting task 0.0 in stage 71.0 (TID 105) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/12 17:52:56 INFO TaskSetManager: Starting task 1.0 in stage 71.0 (TID 106) (172.18.0.14, executor 1, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/12 17:52:56 INFO BlockManagerInfo: Added broadcast_67_piece0 in memory on 172.18.0.14:44081 (size: 33.3 KiB, free: 434.0 MiB)
26/02/12 17:52:57 INFO TaskSetManager: Starting task 2.0 in stage 71.0 (TID 107) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/12 17:52:57 INFO TaskSetManager: Finished task 1.0 in stage 71.0 (TID 106) in 44 ms on 172.18.0.14 (executor 1) (1/4)
26/02/12 17:52:57 INFO TaskSetManager: Starting task 3.0 in stage 71.0 (TID 108) (172.18.0.14, executor 1, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/12 17:52:57 INFO TaskSetManager: Finished task 0.0 in stage 71.0 (TID 105) in 49 ms on 172.18.0.14 (executor 1) (2/4)
26/02/12 17:52:57 INFO TaskSetManager: Finished task 2.0 in stage 71.0 (TID 107) in 32 ms on 172.18.0.14 (executor 1) (3/4)
26/02/12 17:52:57 INFO TaskSetManager: Finished task 3.0 in stage 71.0 (TID 108) in 36 ms on 172.18.0.14 (executor 1) (4/4)
26/02/12 17:52:57 INFO DAGScheduler: ResultStage 71 (start at NativeMethodAccessorImpl.java:0) finished in 0.102 s
26/02/12 17:52:57 INFO DAGScheduler: Job 45 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/12 17:52:57 INFO TaskSchedulerImpl: Removed TaskSet 71.0, whose tasks have all completed, from pool 
26/02/12 17:52:57 INFO TaskSchedulerImpl: Killing all running tasks in stage 71: Stage finished
26/02/12 17:52:57 INFO DAGScheduler: Job 45 finished: start at NativeMethodAccessorImpl.java:0, took 0.123152 s
26/02/12 17:52:57 INFO MemoryStore: Block broadcast_68_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.8 MiB)
26/02/12 17:52:57 INFO BlockManagerInfo: Added broadcast_68_piece0 in memory on spark-master:44075 (size: 107.2 KiB, free: 434.1 MiB)
26/02/12 17:52:57 INFO SparkContext: Created broadcast 68 from start at NativeMethodAccessorImpl.java:0
26/02/12 17:52:57 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 18, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@62ab09e7]. The input RDD has 1 partitions.
26/02/12 17:52:57 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/12 17:52:57 INFO DAGScheduler: Got job 46 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/02/12 17:52:57 INFO DAGScheduler: Final stage: ResultStage 72 (start at NativeMethodAccessorImpl.java:0)
26/02/12 17:52:57 INFO DAGScheduler: Parents of final stage: List()
26/02/12 17:52:57 INFO DAGScheduler: Missing parents: List()
26/02/12 17:52:57 INFO DAGScheduler: Submitting ResultStage 72 (MapPartitionsRDD[247] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/12 17:52:57 INFO MemoryStore: Block broadcast_69 stored as values in memory (estimated size 49.9 KiB, free 433.7 MiB)
26/02/12 17:52:57 INFO MemoryStore: Block broadcast_69_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.7 MiB)
26/02/12 17:52:57 INFO BlockManagerInfo: Added broadcast_69_piece0 in memory on spark-master:44075 (size: 19.5 KiB, free: 434.1 MiB)
26/02/12 17:52:57 INFO SparkContext: Created broadcast 69 from broadcast at DAGScheduler.scala:1585
26/02/12 17:52:57 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 72 (MapPartitionsRDD[247] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/02/12 17:52:57 INFO TaskSchedulerImpl: Adding task set 72.0 with 1 tasks resource profile 0
26/02/12 17:52:57 INFO TaskSetManager: Starting task 0.0 in stage 72.0 (TID 109) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/12 17:52:57 INFO BlockManagerInfo: Added broadcast_69_piece0 in memory on 172.18.0.14:44081 (size: 19.5 KiB, free: 434.0 MiB)
26/02/12 17:52:57 INFO BlockManagerInfo: Added broadcast_68_piece0 in memory on 172.18.0.14:44081 (size: 107.2 KiB, free: 433.9 MiB)
26/02/12 17:52:57 INFO TaskSetManager: Finished task 0.0 in stage 72.0 (TID 109) in 106 ms on 172.18.0.14 (executor 1) (1/1)
26/02/12 17:52:57 INFO TaskSchedulerImpl: Removed TaskSet 72.0, whose tasks have all completed, from pool 
26/02/12 17:52:57 INFO DAGScheduler: ResultStage 72 (start at NativeMethodAccessorImpl.java:0) finished in 0.121 s
26/02/12 17:52:57 INFO DAGScheduler: Job 46 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/12 17:52:57 INFO TaskSchedulerImpl: Killing all running tasks in stage 72: Stage finished
26/02/12 17:52:57 INFO DAGScheduler: Job 46 finished: start at NativeMethodAccessorImpl.java:0, took 0.129976 s
26/02/12 17:52:57 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 18, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@62ab09e7] is committing.
26/02/12 17:52:57 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 18, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@62ab09e7] committed.
26/02/12 17:52:57 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/18 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.18.31bb5a08-f710-49eb-bec4-621de5b073fe.tmp
26/02/12 17:52:57 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.18.31bb5a08-f710-49eb-bec4-621de5b073fe.tmp to file:/tmp/spark-checkpoint-enrichment/commits/18
26/02/12 17:52:57 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "801223b3-8a58-496f-9f28-5a9fda8cbb73",
  "runId" : "005c83b3-ee40-4ce7-8dfd-e6942daa9533",
  "name" : null,
  "timestamp" : "2026-02-12T17:52:56.743Z",
  "batchId" : 18,
  "numInputRows" : 193,
  "inputRowsPerSecond" : 173.4052111410602,
  "processedRowsPerSecond" : 317.9571663920923,
  "durationMs" : {
    "addBatch" : 392,
    "commitOffsets" : 95,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 47,
    "triggerExecution" : 607,
    "walCommit" : 72
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "0" : 40912
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "0" : 41105
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "0" : 41105
      }
    },
    "numInputRows" : 193,
    "inputRowsPerSecond" : 173.4052111410602,
    "processedRowsPerSecond" : 317.9571663920923,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@444eb264",
    "numOutputRows" : 32
  }
}
26/02/12 17:52:58 INFO BlockManagerInfo: Removed broadcast_67_piece0 on spark-master:44075 in memory (size: 33.3 KiB, free: 434.1 MiB)
26/02/12 17:52:58 INFO BlockManagerInfo: Removed broadcast_67_piece0 on 172.18.0.14:44081 in memory (size: 33.3 KiB, free: 433.9 MiB)
26/02/12 17:52:58 INFO BlockManagerInfo: Removed broadcast_66_piece0 on spark-master:44075 in memory (size: 19.5 KiB, free: 434.1 MiB)
26/02/12 17:52:58 INFO BlockManagerInfo: Removed broadcast_66_piece0 on 172.18.0.14:44081 in memory (size: 19.5 KiB, free: 433.9 MiB)
26/02/12 17:52:58 INFO BlockManagerInfo: Removed broadcast_69_piece0 on spark-master:44075 in memory (size: 19.5 KiB, free: 434.2 MiB)
26/02/12 17:52:58 INFO BlockManagerInfo: Removed broadcast_69_piece0 on 172.18.0.14:44081 in memory (size: 19.5 KiB, free: 434.0 MiB)
