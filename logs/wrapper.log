============================================
Spark Streaming Auto-Restart Wrapper
============================================
[Fri Feb 13 11:42:04 UTC 2026] Starting Spark Streaming attempt...
============================================
Starting Spark Streaming Enrichment Service
============================================
[Fri Feb 13 11:42:04 UTC 2026] Waiting for Kafka and topics to be ready...
  Waiting 40 seconds for Kafka initialization...
✓ Kafka should be ready now
[Fri Feb 13 11:42:44 UTC 2026] Submitting Spark Streaming job...
============================================================
Starting Spark Streaming Enrichment
============================================================
26/02/13 11:42:51 INFO SparkContext: Running Spark version 3.5.1
26/02/13 11:42:51 INFO SparkContext: OS info Linux, 6.17.0-14-generic, amd64
26/02/13 11:42:51 INFO SparkContext: Java version 17.0.17
26/02/13 11:42:51 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
26/02/13 11:42:51 INFO ResourceUtils: ==============================================================
26/02/13 11:42:51 INFO ResourceUtils: No custom resources configured for spark.driver.
26/02/13 11:42:51 INFO ResourceUtils: ==============================================================
26/02/13 11:42:51 INFO SparkContext: Submitted application: FlightStreamEnrichment
26/02/13 11:42:51 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
26/02/13 11:42:51 INFO ResourceProfile: Limiting resource is cpu
26/02/13 11:42:51 INFO ResourceProfileManager: Added ResourceProfile id: 0
26/02/13 11:42:51 INFO SecurityManager: Changing view acls to: root
26/02/13 11:42:51 INFO SecurityManager: Changing modify acls to: root
26/02/13 11:42:51 INFO SecurityManager: Changing view acls groups to: 
26/02/13 11:42:51 INFO SecurityManager: Changing modify acls groups to: 
26/02/13 11:42:51 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY
26/02/13 11:42:52 INFO Utils: Successfully started service 'sparkDriver' on port 39823.
26/02/13 11:42:52 INFO SparkEnv: Registering MapOutputTracker
26/02/13 11:42:52 INFO SparkEnv: Registering BlockManagerMaster
26/02/13 11:42:52 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
26/02/13 11:42:52 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
26/02/13 11:42:52 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
26/02/13 11:42:52 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-fba8fef4-9f90-4693-bb8b-c178267cd0b5
26/02/13 11:42:52 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
26/02/13 11:42:53 INFO SparkEnv: Registering OutputCommitCoordinator
26/02/13 11:42:53 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
26/02/13 11:42:53 INFO Utils: Successfully started service 'SparkUI' on port 4040.
26/02/13 11:42:53 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...
26/02/13 11:42:53 INFO TransportClientFactory: Successfully created connection to spark-master/172.18.0.10:7077 after 70 ms (0 ms spent in bootstraps)
26/02/13 11:42:54 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20260213114254-0000
26/02/13 11:42:54 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 40291.
26/02/13 11:42:54 INFO NettyBlockTransferService: Server created on spark-master 0.0.0.0:40291
26/02/13 11:42:54 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
26/02/13 11:42:54 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, spark-master, 40291, None)
26/02/13 11:42:54 INFO BlockManagerMasterEndpoint: Registering block manager spark-master:40291 with 434.4 MiB RAM, BlockManagerId(driver, spark-master, 40291, None)
26/02/13 11:42:54 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, spark-master, 40291, None)
26/02/13 11:42:54 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, spark-master, 40291, None)
26/02/13 11:42:54 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20260213114254-0000/0 on worker-20260213114143-172.18.0.15-35515 (172.18.0.15:35515) with 2 core(s)
26/02/13 11:42:54 INFO StandaloneSchedulerBackend: Granted executor ID app-20260213114254-0000/0 on hostPort 172.18.0.15:35515 with 2 core(s), 1024.0 MiB RAM
26/02/13 11:42:54 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20260213114254-0000/1 on worker-20260213114143-172.18.0.14-33335 (172.18.0.14:33335) with 2 core(s)
26/02/13 11:42:54 INFO StandaloneSchedulerBackend: Granted executor ID app-20260213114254-0000/1 on hostPort 172.18.0.14:33335 with 2 core(s), 1024.0 MiB RAM
26/02/13 11:42:55 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20260213114254-0000/1 is now RUNNING
26/02/13 11:42:55 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20260213114254-0000/0 is now RUNNING
26/02/13 11:42:55 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
✓ Spark session created
✓ Reading from: aviation-india-states
✓ Writing to: aviation-enriched-states
[DEBUG] Creating route mapping table...
Loading routes from CSV: /opt/airflow/data/routes.csv
26/02/13 11:42:56 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
26/02/13 11:42:56 INFO SharedState: Warehouse path is 'file:/opt/airflow/spark-warehouse'.
26/02/13 11:42:58 INFO InMemoryFileIndex: It took 119 ms to list leaf files for 1 paths.
26/02/13 11:42:58 INFO InMemoryFileIndex: It took 5 ms to list leaf files for 1 paths.
26/02/13 11:43:01 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.15:43180) with ID 0,  ResourceProfileId 0
26/02/13 11:43:01 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.14:54298) with ID 1,  ResourceProfileId 0
26/02/13 11:43:01 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.15:34959 with 434.4 MiB RAM, BlockManagerId(0, 172.18.0.15, 34959, None)
26/02/13 11:43:01 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.14:40939 with 434.4 MiB RAM, BlockManagerId(1, 172.18.0.14, 40939, None)
26/02/13 11:43:03 INFO FileSourceStrategy: Pushed Filters: 
26/02/13 11:43:03 INFO FileSourceStrategy: Post-Scan Filters: (length(trim(value#0, None)) > 0)
26/02/13 11:43:04 INFO CodeGenerator: Code generated in 456.461401 ms
26/02/13 11:43:04 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 200.4 KiB, free 434.2 MiB)
26/02/13 11:43:04 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 434.2 MiB)
26/02/13 11:43:04 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on spark-master:40291 (size: 34.5 KiB, free: 434.4 MiB)
26/02/13 11:43:04 INFO SparkContext: Created broadcast 0 from csv at NativeMethodAccessorImpl.java:0
26/02/13 11:43:04 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
26/02/13 11:43:05 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0
26/02/13 11:43:05 INFO DAGScheduler: Got job 0 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/02/13 11:43:05 INFO DAGScheduler: Final stage: ResultStage 0 (csv at NativeMethodAccessorImpl.java:0)
26/02/13 11:43:05 INFO DAGScheduler: Parents of final stage: List()
26/02/13 11:43:05 INFO DAGScheduler: Missing parents: List()
26/02/13 11:43:05 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:43:05 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 13.5 KiB, free 434.2 MiB)
26/02/13 11:43:05 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.4 KiB, free 434.2 MiB)
26/02/13 11:43:05 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on spark-master:40291 (size: 6.4 KiB, free: 434.4 MiB)
26/02/13 11:43:05 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1585
26/02/13 11:43:05 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/02/13 11:43:05 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
26/02/13 11:43:05 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8217 bytes) 
26/02/13 11:43:05 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.18.0.14:40939 (size: 6.4 KiB, free: 434.4 MiB)
26/02/13 11:43:07 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.18.0.14:40939 (size: 34.5 KiB, free: 434.4 MiB)
26/02/13 11:43:07 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 2460 ms on 172.18.0.14 (executor 1) (1/1)
26/02/13 11:43:07 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
26/02/13 11:43:07 INFO DAGScheduler: ResultStage 0 (csv at NativeMethodAccessorImpl.java:0) finished in 2.652 s
26/02/13 11:43:07 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:43:07 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
26/02/13 11:43:07 INFO DAGScheduler: Job 0 finished: csv at NativeMethodAccessorImpl.java:0, took 2.745555 s
26/02/13 11:43:07 INFO CodeGenerator: Code generated in 49.002091 ms
26/02/13 11:43:08 INFO FileSourceStrategy: Pushed Filters: 
26/02/13 11:43:08 INFO FileSourceStrategy: Post-Scan Filters: 
26/02/13 11:43:08 INFO BlockManagerInfo: Removed broadcast_1_piece0 on spark-master:40291 in memory (size: 6.4 KiB, free: 434.4 MiB)
26/02/13 11:43:08 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 172.18.0.14:40939 in memory (size: 6.4 KiB, free: 434.4 MiB)
26/02/13 11:43:08 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 200.4 KiB, free 434.0 MiB)
26/02/13 11:43:08 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.9 MiB)
26/02/13 11:43:08 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on spark-master:40291 (size: 34.5 KiB, free: 434.3 MiB)
26/02/13 11:43:08 INFO SparkContext: Created broadcast 2 from csv at NativeMethodAccessorImpl.java:0
26/02/13 11:43:08 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
26/02/13 11:43:08 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0
26/02/13 11:43:08 INFO DAGScheduler: Got job 1 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/02/13 11:43:08 INFO DAGScheduler: Final stage: ResultStage 1 (csv at NativeMethodAccessorImpl.java:0)
26/02/13 11:43:08 INFO DAGScheduler: Parents of final stage: List()
26/02/13 11:43:08 INFO DAGScheduler: Missing parents: List()
26/02/13 11:43:08 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[9] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:43:08 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 27.8 KiB, free 433.9 MiB)
26/02/13 11:43:08 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 12.8 KiB, free 433.9 MiB)
26/02/13 11:43:08 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on spark-master:40291 (size: 12.8 KiB, free: 434.3 MiB)
26/02/13 11:43:08 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1585
26/02/13 11:43:08 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[9] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/02/13 11:43:08 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
26/02/13 11:43:08 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8217 bytes) 
26/02/13 11:43:08 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 172.18.0.14:40939 (size: 12.8 KiB, free: 434.4 MiB)
26/02/13 11:43:10 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.18.0.14:40939 (size: 34.5 KiB, free: 434.3 MiB)
26/02/13 11:43:11 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 2705 ms on 172.18.0.14 (executor 1) (1/1)
26/02/13 11:43:11 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
26/02/13 11:43:11 INFO DAGScheduler: ResultStage 1 (csv at NativeMethodAccessorImpl.java:0) finished in 2.769 s
26/02/13 11:43:11 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:43:11 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
26/02/13 11:43:11 INFO DAGScheduler: Job 1 finished: csv at NativeMethodAccessorImpl.java:0, took 2.785273 s
26/02/13 11:43:12 INFO BlockManagerInfo: Removed broadcast_3_piece0 on spark-master:40291 in memory (size: 12.8 KiB, free: 434.3 MiB)
26/02/13 11:43:12 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 172.18.0.14:40939 in memory (size: 12.8 KiB, free: 434.3 MiB)
26/02/13 11:43:12 INFO FileSourceStrategy: Pushed Filters: IsNotNull(FlightNo),IsNotNull(Origin),IsNotNull(Destination)
26/02/13 11:43:12 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(FlightNo#17),isnotnull(Origin#18),isnotnull(Destination#19),NOT (Origin#18 = Destination#19)
26/02/13 11:43:13 INFO CodeGenerator: Code generated in 239.629005 ms
26/02/13 11:43:13 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 200.3 KiB, free 433.7 MiB)
26/02/13 11:43:13 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.7 MiB)
26/02/13 11:43:13 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on spark-master:40291 (size: 34.5 KiB, free: 434.3 MiB)
26/02/13 11:43:13 INFO SparkContext: Created broadcast 4 from count at NativeMethodAccessorImpl.java:0
26/02/13 11:43:13 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
26/02/13 11:43:13 INFO BlockManagerInfo: Removed broadcast_0_piece0 on spark-master:40291 in memory (size: 34.5 KiB, free: 434.3 MiB)
26/02/13 11:43:13 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 172.18.0.14:40939 in memory (size: 34.5 KiB, free: 434.4 MiB)
26/02/13 11:43:13 INFO DAGScheduler: Registering RDD 13 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 0
26/02/13 11:43:13 INFO DAGScheduler: Got map stage job 2 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/02/13 11:43:13 INFO DAGScheduler: Final stage: ShuffleMapStage 2 (count at NativeMethodAccessorImpl.java:0)
26/02/13 11:43:13 INFO DAGScheduler: Parents of final stage: List()
26/02/13 11:43:13 INFO DAGScheduler: Missing parents: List()
26/02/13 11:43:13 INFO DAGScheduler: Submitting ShuffleMapStage 2 (MapPartitionsRDD[13] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:43:13 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 51.6 KiB, free 433.9 MiB)
26/02/13 11:43:13 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 21.5 KiB, free 433.9 MiB)
26/02/13 11:43:13 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on spark-master:40291 (size: 21.5 KiB, free: 434.3 MiB)
26/02/13 11:43:13 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1585
26/02/13 11:43:13 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[13] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/02/13 11:43:13 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
26/02/13 11:43:13 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (172.18.0.15, executor 0, partition 0, PROCESS_LOCAL, 8206 bytes) 
26/02/13 11:43:13 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 172.18.0.15:34959 (size: 21.5 KiB, free: 434.4 MiB)
26/02/13 11:43:16 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 172.18.0.15:34959 (size: 34.5 KiB, free: 434.3 MiB)
26/02/13 11:43:17 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 4194 ms on 172.18.0.15 (executor 0) (1/1)
26/02/13 11:43:17 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool 
26/02/13 11:43:17 INFO DAGScheduler: ShuffleMapStage 2 (count at NativeMethodAccessorImpl.java:0) finished in 4.244 s
26/02/13 11:43:17 INFO DAGScheduler: looking for newly runnable stages
26/02/13 11:43:17 INFO DAGScheduler: running: Set()
26/02/13 11:43:17 INFO DAGScheduler: waiting: Set()
26/02/13 11:43:17 INFO DAGScheduler: failed: Set()
26/02/13 11:43:17 INFO ShufflePartitionsUtil: For shuffle(0), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
26/02/13 11:43:18 INFO CodeGenerator: Code generated in 117.298669 ms
26/02/13 11:43:18 INFO DAGScheduler: Registering RDD 16 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 1
26/02/13 11:43:18 INFO DAGScheduler: Got map stage job 3 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/02/13 11:43:18 INFO DAGScheduler: Final stage: ShuffleMapStage 4 (count at NativeMethodAccessorImpl.java:0)
26/02/13 11:43:18 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 3)
26/02/13 11:43:18 INFO DAGScheduler: Missing parents: List()
26/02/13 11:43:18 INFO DAGScheduler: Submitting ShuffleMapStage 4 (MapPartitionsRDD[16] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:43:18 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 54.7 KiB, free 433.8 MiB)
26/02/13 11:43:18 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 24.2 KiB, free 433.8 MiB)
26/02/13 11:43:18 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on spark-master:40291 (size: 24.2 KiB, free: 434.3 MiB)
26/02/13 11:43:18 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1585
26/02/13 11:43:18 INFO BlockManagerInfo: Removed broadcast_5_piece0 on spark-master:40291 in memory (size: 21.5 KiB, free: 434.3 MiB)
26/02/13 11:43:18 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 4 (MapPartitionsRDD[16] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/02/13 11:43:18 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0
26/02/13 11:43:18 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 172.18.0.15:34959 in memory (size: 21.5 KiB, free: 434.4 MiB)
26/02/13 11:43:18 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 3) (172.18.0.15, executor 0, partition 0, NODE_LOCAL, 7608 bytes) 
26/02/13 11:43:18 INFO BlockManagerInfo: Removed broadcast_2_piece0 on spark-master:40291 in memory (size: 34.5 KiB, free: 434.3 MiB)
26/02/13 11:43:18 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 172.18.0.15:34959 (size: 24.2 KiB, free: 434.3 MiB)
26/02/13 11:43:18 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 172.18.0.14:40939 in memory (size: 34.5 KiB, free: 434.4 MiB)
26/02/13 11:43:18 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 172.18.0.15:43180
26/02/13 11:43:19 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 3) in 905 ms on 172.18.0.15 (executor 0) (1/1)
26/02/13 11:43:19 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool 
26/02/13 11:43:19 INFO DAGScheduler: ShuffleMapStage 4 (count at NativeMethodAccessorImpl.java:0) finished in 0.981 s
26/02/13 11:43:19 INFO DAGScheduler: looking for newly runnable stages
26/02/13 11:43:19 INFO DAGScheduler: running: Set()
26/02/13 11:43:19 INFO DAGScheduler: waiting: Set()
26/02/13 11:43:19 INFO DAGScheduler: failed: Set()
26/02/13 11:43:19 INFO CodeGenerator: Code generated in 41.303829 ms
26/02/13 11:43:19 INFO SparkContext: Starting job: count at NativeMethodAccessorImpl.java:0
26/02/13 11:43:19 INFO DAGScheduler: Got job 4 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/02/13 11:43:19 INFO DAGScheduler: Final stage: ResultStage 7 (count at NativeMethodAccessorImpl.java:0)
26/02/13 11:43:19 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 6)
26/02/13 11:43:19 INFO DAGScheduler: Missing parents: List()
26/02/13 11:43:19 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[19] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:43:19 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 12.5 KiB, free 434.1 MiB)
26/02/13 11:43:19 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 5.9 KiB, free 434.1 MiB)
26/02/13 11:43:19 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on spark-master:40291 (size: 5.9 KiB, free: 434.3 MiB)
26/02/13 11:43:19 INFO BlockManagerInfo: Removed broadcast_6_piece0 on spark-master:40291 in memory (size: 24.2 KiB, free: 434.4 MiB)
26/02/13 11:43:19 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1585
26/02/13 11:43:19 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[19] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/02/13 11:43:19 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 172.18.0.15:34959 in memory (size: 24.2 KiB, free: 434.4 MiB)
26/02/13 11:43:19 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks resource profile 0
26/02/13 11:43:19 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 4) (172.18.0.15, executor 0, partition 0, NODE_LOCAL, 7619 bytes) 
26/02/13 11:43:19 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 172.18.0.15:34959 (size: 5.9 KiB, free: 434.4 MiB)
26/02/13 11:43:19 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 172.18.0.15:43180
26/02/13 11:43:19 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 4) in 473 ms on 172.18.0.15 (executor 0) (1/1)
26/02/13 11:43:19 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool 
26/02/13 11:43:19 INFO DAGScheduler: ResultStage 7 (count at NativeMethodAccessorImpl.java:0) finished in 0.523 s
26/02/13 11:43:19 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:43:19 INFO TaskSchedulerImpl: Killing all running tasks in stage 7: Stage finished
26/02/13 11:43:19 INFO DAGScheduler: Job 4 finished: count at NativeMethodAccessorImpl.java:0, took 0.553826 s
✓ Loaded 2773 routes from CSV: /opt/airflow/data/routes.csv
✓ Using flight number-based route matching
✓ Sample callsigns: IGO102, AIC176, etc.
26/02/13 11:43:20 INFO FileSourceStrategy: Pushed Filters: IsNotNull(FlightNo),IsNotNull(Origin),IsNotNull(Destination)
26/02/13 11:43:20 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(FlightNo#17),isnotnull(Origin#18),isnotnull(Destination#19),NOT (Origin#18 = Destination#19)
26/02/13 11:43:20 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
26/02/13 11:43:20 INFO BlockManagerInfo: Removed broadcast_7_piece0 on spark-master:40291 in memory (size: 5.9 KiB, free: 434.4 MiB)
26/02/13 11:43:20 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 172.18.0.15:34959 in memory (size: 5.9 KiB, free: 434.4 MiB)
26/02/13 11:43:21 INFO CodeGenerator: Code generated in 332.657278 ms
26/02/13 11:43:21 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 200.3 KiB, free 434.0 MiB)
26/02/13 11:43:21 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.9 MiB)
26/02/13 11:43:21 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on spark-master:40291 (size: 34.5 KiB, free: 434.3 MiB)
26/02/13 11:43:21 INFO SparkContext: Created broadcast 8 from count at NativeMethodAccessorImpl.java:0
26/02/13 11:43:21 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
26/02/13 11:43:21 INFO DAGScheduler: Registering RDD 24 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 2
26/02/13 11:43:21 INFO DAGScheduler: Got map stage job 5 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/02/13 11:43:21 INFO DAGScheduler: Final stage: ShuffleMapStage 8 (count at NativeMethodAccessorImpl.java:0)
26/02/13 11:43:21 INFO DAGScheduler: Parents of final stage: List()
26/02/13 11:43:21 INFO DAGScheduler: Missing parents: List()
26/02/13 11:43:21 INFO DAGScheduler: Submitting ShuffleMapStage 8 (MapPartitionsRDD[24] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:43:21 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 111.9 KiB, free 433.8 MiB)
26/02/13 11:43:21 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 33.9 KiB, free 433.8 MiB)
26/02/13 11:43:21 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on spark-master:40291 (size: 33.9 KiB, free: 434.3 MiB)
26/02/13 11:43:21 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1585
26/02/13 11:43:21 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 8 (MapPartitionsRDD[24] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/02/13 11:43:21 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks resource profile 0
26/02/13 11:43:21 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 5) (172.18.0.15, executor 0, partition 0, PROCESS_LOCAL, 8206 bytes) 
26/02/13 11:43:21 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 172.18.0.15:34959 (size: 33.9 KiB, free: 434.3 MiB)
26/02/13 11:43:21 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 172.18.0.15:34959 (size: 34.5 KiB, free: 434.3 MiB)
26/02/13 11:43:22 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 5) in 1481 ms on 172.18.0.15 (executor 0) (1/1)
26/02/13 11:43:22 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool 
26/02/13 11:43:22 INFO DAGScheduler: ShuffleMapStage 8 (count at NativeMethodAccessorImpl.java:0) finished in 1.523 s
26/02/13 11:43:22 INFO DAGScheduler: looking for newly runnable stages
26/02/13 11:43:22 INFO DAGScheduler: running: Set()
26/02/13 11:43:22 INFO DAGScheduler: waiting: Set()
26/02/13 11:43:22 INFO DAGScheduler: failed: Set()
26/02/13 11:43:22 INFO CodeGenerator: Code generated in 20.21438 ms
26/02/13 11:43:22 INFO DAGScheduler: Got job 6 (count at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 11:43:22 INFO DAGScheduler: Final stage: ResultStage 10 (count at NativeMethodAccessorImpl.java:0)
26/02/13 11:43:22 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 9)
26/02/13 11:43:22 INFO DAGScheduler: Missing parents: List()
26/02/13 11:43:22 INFO DAGScheduler: Submitting ResultStage 10 (AdaptiveSparkPlan isFinalPlan=false
+- SortAggregate(key=[callsign#67], functions=[first(flight_number#25, false), first(origin_code#26, false), first(destination_code#27, false), first(airline_iata#32, false), first(airline_name#37, false), first(airline_prefix#43, false), first(airline_icao#50, false), first(flight_num_only#58, false), first(origin_city#77, false), first(origin_airport#88, false), first(origin_lat#100, false), first(origin_lon#113, false), first(destination_city#127, false), first(destination_airport#142, false), first(destination_lat#158, false), first(destination_lon#175, false)], output=[flight_number#251, origin_code#253, destination_code#255, airline_iata#257, airline_name#259, airline_prefix#261, airline_icao#263, flight_num_only#265, callsign#67, origin_city#267, origin_airport#269, origin_lat#271, origin_lon#273, destination_city#275, destination_airport#277, destination_lat#279, destination_lon#281])
   +- Sort [callsign#67 ASC NULLS FIRST], false, 0
      +- Exchange hashpartit... MapPartitionsRDD[29] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:43:22 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 89.6 KiB, free 433.7 MiB)
26/02/13 11:43:22 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 29.7 KiB, free 433.7 MiB)
26/02/13 11:43:22 INFO BlockManagerInfo: Removed broadcast_9_piece0 on spark-master:40291 in memory (size: 33.9 KiB, free: 434.3 MiB)
26/02/13 11:43:22 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on spark-master:40291 (size: 29.7 KiB, free: 434.3 MiB)
26/02/13 11:43:22 INFO SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:1585
26/02/13 11:43:22 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 10 (AdaptiveSparkPlan isFinalPlan=false
+- SortAggregate(key=[callsign#67], functions=[first(flight_number#25, false), first(origin_code#26, false), first(destination_code#27, false), first(airline_iata#32, false), first(airline_name#37, false), first(airline_prefix#43, false), first(airline_icao#50, false), first(flight_num_only#58, false), first(origin_city#77, false), first(origin_airport#88, false), first(origin_lat#100, false), first(origin_lon#113, false), first(destination_city#127, false), first(destination_airport#142, false), first(destination_lat#158, false), first(destination_lon#175, false)], output=[flight_number#251, origin_code#253, destination_code#255, airline_iata#257, airline_name#259, airline_prefix#261, airline_icao#263, flight_num_only#265, callsign#67, origin_city#267, origin_airport#269, origin_lat#271, origin_lon#273, destination_city#275, destination_airport#277, destination_lat#279, destination_lon#281])
   +- Sort [callsign#67 ASC NULLS FIRST], false, 0
      +- Exchange hashpartit... MapPartitionsRDD[29] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 11:43:22 INFO BlockManagerInfo: Removed broadcast_9_piece0 on 172.18.0.15:34959 in memory (size: 33.9 KiB, free: 434.3 MiB)
26/02/13 11:43:22 INFO TaskSchedulerImpl: Adding task set 10.0 with 4 tasks resource profile 0
26/02/13 11:43:23 INFO TaskSetManager: Starting task 0.0 in stage 10.0 (TID 6) (172.18.0.15, executor 0, partition 0, NODE_LOCAL, 7619 bytes) 
26/02/13 11:43:23 INFO TaskSetManager: Starting task 1.0 in stage 10.0 (TID 7) (172.18.0.15, executor 0, partition 1, NODE_LOCAL, 7619 bytes) 
26/02/13 11:43:23 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 172.18.0.15:34959 (size: 29.7 KiB, free: 434.3 MiB)
26/02/13 11:43:23 INFO BlockManagerInfo: Removed broadcast_4_piece0 on spark-master:40291 in memory (size: 34.5 KiB, free: 434.3 MiB)
26/02/13 11:43:23 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 172.18.0.15:34959 in memory (size: 34.5 KiB, free: 434.3 MiB)
26/02/13 11:43:23 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 2 to 172.18.0.15:43180
26/02/13 11:43:23 INFO BlockManagerInfo: Added rdd_29_0 in memory on 172.18.0.15:34959 (size: 51.7 KiB, free: 434.3 MiB)
26/02/13 11:43:23 INFO BlockManagerInfo: Added rdd_29_1 in memory on 172.18.0.15:34959 (size: 52.1 KiB, free: 434.2 MiB)
26/02/13 11:43:24 INFO TaskSetManager: Starting task 2.0 in stage 10.0 (TID 8) (172.18.0.15, executor 0, partition 2, NODE_LOCAL, 7619 bytes) 
26/02/13 11:43:24 INFO TaskSetManager: Finished task 1.0 in stage 10.0 (TID 7) in 1015 ms on 172.18.0.15 (executor 0) (1/4)
26/02/13 11:43:24 INFO TaskSetManager: Starting task 3.0 in stage 10.0 (TID 9) (172.18.0.15, executor 0, partition 3, NODE_LOCAL, 7619 bytes) 
26/02/13 11:43:24 INFO TaskSetManager: Finished task 0.0 in stage 10.0 (TID 6) in 1035 ms on 172.18.0.15 (executor 0) (2/4)
26/02/13 11:43:24 INFO BlockManagerInfo: Added rdd_29_2 in memory on 172.18.0.15:34959 (size: 49.7 KiB, free: 434.2 MiB)
26/02/13 11:43:24 INFO TaskSetManager: Finished task 2.0 in stage 10.0 (TID 8) in 382 ms on 172.18.0.15 (executor 0) (3/4)
26/02/13 11:43:24 INFO BlockManagerInfo: Added rdd_29_3 in memory on 172.18.0.15:34959 (size: 51.3 KiB, free: 434.1 MiB)
26/02/13 11:43:24 INFO TaskSetManager: Finished task 3.0 in stage 10.0 (TID 9) in 491 ms on 172.18.0.15 (executor 0) (4/4)
26/02/13 11:43:24 INFO TaskSchedulerImpl: Removed TaskSet 10.0, whose tasks have all completed, from pool 
26/02/13 11:43:24 INFO DAGScheduler: ResultStage 10 (count at NativeMethodAccessorImpl.java:0) finished in 1.603 s
26/02/13 11:43:24 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:43:24 INFO TaskSchedulerImpl: Killing all running tasks in stage 10: Stage finished
26/02/13 11:43:24 INFO CodeGenerator: Code generated in 20.829463 ms
26/02/13 11:43:24 INFO DAGScheduler: Registering RDD 34 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 3
26/02/13 11:43:24 INFO DAGScheduler: Got map stage job 7 (count at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 11:43:24 INFO DAGScheduler: Final stage: ShuffleMapStage 12 (count at NativeMethodAccessorImpl.java:0)
26/02/13 11:43:24 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 11)
26/02/13 11:43:24 INFO DAGScheduler: Missing parents: List()
26/02/13 11:43:24 INFO DAGScheduler: Submitting ShuffleMapStage 12 (MapPartitionsRDD[34] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:43:24 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 95.1 KiB, free 434.0 MiB)
26/02/13 11:43:24 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 32.0 KiB, free 433.9 MiB)
26/02/13 11:43:24 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on spark-master:40291 (size: 32.0 KiB, free: 434.3 MiB)
26/02/13 11:43:24 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1585
26/02/13 11:43:24 INFO DAGScheduler: Submitting 4 missing tasks from ShuffleMapStage 12 (MapPartitionsRDD[34] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 11:43:24 INFO TaskSchedulerImpl: Adding task set 12.0 with 4 tasks resource profile 0
26/02/13 11:43:24 INFO TaskSetManager: Starting task 0.0 in stage 12.0 (TID 10) (172.18.0.15, executor 0, partition 0, PROCESS_LOCAL, 7608 bytes) 
26/02/13 11:43:24 INFO TaskSetManager: Starting task 1.0 in stage 12.0 (TID 11) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 7608 bytes) 
26/02/13 11:43:24 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 172.18.0.15:34959 (size: 32.0 KiB, free: 434.1 MiB)
26/02/13 11:43:24 INFO TaskSetManager: Starting task 2.0 in stage 12.0 (TID 12) (172.18.0.15, executor 0, partition 2, PROCESS_LOCAL, 7608 bytes) 
26/02/13 11:43:24 INFO TaskSetManager: Finished task 0.0 in stage 12.0 (TID 10) in 206 ms on 172.18.0.15 (executor 0) (1/4)
26/02/13 11:43:24 INFO TaskSetManager: Starting task 3.0 in stage 12.0 (TID 13) (172.18.0.15, executor 0, partition 3, PROCESS_LOCAL, 7608 bytes) 
26/02/13 11:43:24 INFO TaskSetManager: Finished task 1.0 in stage 12.0 (TID 11) in 216 ms on 172.18.0.15 (executor 0) (2/4)
26/02/13 11:43:25 INFO TaskSetManager: Finished task 2.0 in stage 12.0 (TID 12) in 92 ms on 172.18.0.15 (executor 0) (3/4)
26/02/13 11:43:25 INFO TaskSetManager: Finished task 3.0 in stage 12.0 (TID 13) in 82 ms on 172.18.0.15 (executor 0) (4/4)
26/02/13 11:43:25 INFO TaskSchedulerImpl: Removed TaskSet 12.0, whose tasks have all completed, from pool 
26/02/13 11:43:25 INFO DAGScheduler: ShuffleMapStage 12 (count at NativeMethodAccessorImpl.java:0) finished in 0.345 s
26/02/13 11:43:25 INFO DAGScheduler: looking for newly runnable stages
26/02/13 11:43:25 INFO DAGScheduler: running: Set()
26/02/13 11:43:25 INFO DAGScheduler: waiting: Set()
26/02/13 11:43:25 INFO DAGScheduler: failed: Set()
26/02/13 11:43:25 INFO CodeGenerator: Code generated in 21.436111 ms
26/02/13 11:43:25 INFO SparkContext: Starting job: count at NativeMethodAccessorImpl.java:0
26/02/13 11:43:25 INFO DAGScheduler: Got job 8 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/02/13 11:43:25 INFO DAGScheduler: Final stage: ResultStage 15 (count at NativeMethodAccessorImpl.java:0)
26/02/13 11:43:25 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 14)
26/02/13 11:43:25 INFO DAGScheduler: Missing parents: List()
26/02/13 11:43:25 INFO DAGScheduler: Submitting ResultStage 15 (MapPartitionsRDD[37] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:43:25 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 12.5 KiB, free 433.9 MiB)
26/02/13 11:43:25 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 5.9 KiB, free 433.9 MiB)
26/02/13 11:43:25 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on spark-master:40291 (size: 5.9 KiB, free: 434.3 MiB)
26/02/13 11:43:25 INFO SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:1585
26/02/13 11:43:25 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 15 (MapPartitionsRDD[37] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/02/13 11:43:25 INFO TaskSchedulerImpl: Adding task set 15.0 with 1 tasks resource profile 0
26/02/13 11:43:25 INFO TaskSetManager: Starting task 0.0 in stage 15.0 (TID 14) (172.18.0.15, executor 0, partition 0, NODE_LOCAL, 7619 bytes) 
26/02/13 11:43:25 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 172.18.0.15:34959 (size: 5.9 KiB, free: 434.1 MiB)
26/02/13 11:43:25 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 3 to 172.18.0.15:43180
26/02/13 11:43:25 INFO TaskSetManager: Finished task 0.0 in stage 15.0 (TID 14) in 116 ms on 172.18.0.15 (executor 0) (1/1)
26/02/13 11:43:25 INFO TaskSchedulerImpl: Removed TaskSet 15.0, whose tasks have all completed, from pool 
26/02/13 11:43:25 INFO DAGScheduler: ResultStage 15 (count at NativeMethodAccessorImpl.java:0) finished in 0.141 s
26/02/13 11:43:25 INFO DAGScheduler: Job 8 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:43:25 INFO TaskSchedulerImpl: Killing all running tasks in stage 15: Stage finished
26/02/13 11:43:25 INFO DAGScheduler: Job 8 finished: count at NativeMethodAccessorImpl.java:0, took 0.203477 s
✓ Route mapping table created (2773 routes)
[DEBUG] Setting up Kafka input stream...
✓ Connected to Kafka input stream
[DEBUG] Parsing JSON and extracting fields...
[DEBUG] Joining with route mapping...
  Using CSV-based exact callsign matching
26/02/13 11:43:26 INFO BlockManagerInfo: Removed broadcast_11_piece0 on 172.18.0.15:34959 in memory (size: 32.0 KiB, free: 434.1 MiB)
26/02/13 11:43:26 INFO BlockManagerInfo: Removed broadcast_11_piece0 on spark-master:40291 in memory (size: 32.0 KiB, free: 434.3 MiB)
[DEBUG] Filtering records with valid routes...
26/02/13 11:43:26 INFO BlockManagerInfo: Removed broadcast_12_piece0 on spark-master:40291 in memory (size: 5.9 KiB, free: 434.3 MiB)
26/02/13 11:43:26 INFO BlockManagerInfo: Removed broadcast_12_piece0 on 172.18.0.15:34959 in memory (size: 5.9 KiB, free: 434.1 MiB)
26/02/13 11:43:26 INFO BlockManagerInfo: Removed broadcast_10_piece0 on spark-master:40291 in memory (size: 29.7 KiB, free: 434.4 MiB)
26/02/13 11:43:26 INFO BlockManagerInfo: Removed broadcast_10_piece0 on 172.18.0.15:34959 in memory (size: 29.7 KiB, free: 434.2 MiB)
✓ Filtering enabled: Only flights with routes will be sent to next topic
[DEBUG] Setting up output stream to Kafka...
26/02/13 11:43:26 INFO StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint
26/02/13 11:43:26 INFO ResolveWriteToStream: Checkpoint root /tmp/spark-checkpoint-enrichment resolved to file:/tmp/spark-checkpoint-enrichment.
26/02/13 11:43:26 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
26/02/13 11:43:26 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/metadata using temp file file:/tmp/spark-checkpoint-enrichment/.metadata.01b2a3bd-2c05-471c-ad33-ade2adc968bd.tmp
26/02/13 11:43:26 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/.metadata.01b2a3bd-2c05-471c-ad33-ade2adc968bd.tmp to file:/tmp/spark-checkpoint-enrichment/metadata
26/02/13 11:43:26 INFO MicroBatchExecution: Starting [id = ebfab33f-0465-4e53-8f7a-961711cffb90, runId = fc5c6452-bd70-4c90-becf-772d3bebdb57]. Use file:/tmp/spark-checkpoint-enrichment to store the query checkpoint.
============================================================
✓ Streaming query started successfully!
✓ Enriching flights with route information...
✓ Query ID: ebfab33f-0465-4e53-8f7a-961711cffb90
============================================================
[DEBUG] Waiting for stream termination...
26/02/13 11:43:26 INFO MicroBatchExecution: Reading table [org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@17ff16b4] from DataSourceV2 named 'kafka' [org.apache.spark.sql.kafka010.KafkaSourceProvider@12bb6f44]
26/02/13 11:43:26 INFO OffsetSeqLog: BatchIds found from listing: 
26/02/13 11:43:26 INFO OffsetSeqLog: BatchIds found from listing: 
26/02/13 11:43:26 INFO MicroBatchExecution: Starting new streaming query.
26/02/13 11:43:26 INFO MicroBatchExecution: Stream started from {}
26/02/13 11:43:27 INFO AdminClientConfig: AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.servers = [kafka:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

26/02/13 11:43:27 INFO AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
26/02/13 11:43:27 INFO AppInfoParser: Kafka version: 3.5.1
26/02/13 11:43:27 INFO AppInfoParser: Kafka commitId: 2c6fb6c54472e90a
26/02/13 11:43:27 INFO AppInfoParser: Kafka startTimeMs: 1770983007311
26/02/13 11:43:28 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/sources/0/0 using temp file file:/tmp/spark-checkpoint-enrichment/sources/0/.0.54385cd0-b5c5-4f12-942a-cc0b5a77a106.tmp
26/02/13 11:43:28 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/sources/0/.0.54385cd0-b5c5-4f12-942a-cc0b5a77a106.tmp to file:/tmp/spark-checkpoint-enrichment/sources/0/0
26/02/13 11:43:28 INFO KafkaMicroBatchStream: Initial offsets: {"aviation-india-states":{"2":2803,"1":3106,"0":4970}}
26/02/13 11:43:28 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/0 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.0.c330ba55-a82f-435a-9830-c074d1be6624.tmp
26/02/13 11:43:28 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.0.c330ba55-a82f-435a-9830-c074d1be6624.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/0
26/02/13 11:43:28 INFO MicroBatchExecution: Committed offsets for batch 0. Metadata OffsetSeqMetadata(0,1770983008559,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 11:43:29 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:43:29 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:43:29 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:43:29 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:43:29 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:43:29 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:43:29 INFO CodeGenerator: Code generated in 17.094073 ms
26/02/13 11:43:29 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#1951 - origin_code.nullCount#1950) > 0)
26/02/13 11:43:29 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#1956 - destination_code.nullCount#1955) > 0)
26/02/13 11:43:29 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#1986 - callsign.nullCount#1985) > 0)
26/02/13 11:43:29 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:43:29 INFO DAGScheduler: Got job 9 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 11:43:29 INFO DAGScheduler: Final stage: ResultStage 17 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:43:29 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 16)
26/02/13 11:43:29 INFO DAGScheduler: Missing parents: List()
26/02/13 11:43:29 INFO DAGScheduler: Submitting ResultStage 17 (MapPartitionsRDD[42] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:43:29 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 99.3 KiB, free 434.1 MiB)
26/02/13 11:43:29 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 434.0 MiB)
26/02/13 11:43:29 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on spark-master:40291 (size: 33.3 KiB, free: 434.3 MiB)
26/02/13 11:43:29 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1585
26/02/13 11:43:29 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 17 (MapPartitionsRDD[42] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 11:43:29 INFO TaskSchedulerImpl: Adding task set 17.0 with 4 tasks resource profile 0
26/02/13 11:43:29 INFO TaskSetManager: Starting task 0.0 in stage 17.0 (TID 15) (172.18.0.15, executor 0, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:43:29 INFO TaskSetManager: Starting task 1.0 in stage 17.0 (TID 16) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:43:29 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on 172.18.0.15:34959 (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 11:43:29 INFO TaskSetManager: Starting task 2.0 in stage 17.0 (TID 17) (172.18.0.15, executor 0, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:43:29 INFO TaskSetManager: Starting task 3.0 in stage 17.0 (TID 18) (172.18.0.15, executor 0, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:43:29 INFO TaskSetManager: Finished task 0.0 in stage 17.0 (TID 15) in 201 ms on 172.18.0.15 (executor 0) (1/4)
26/02/13 11:43:29 INFO TaskSetManager: Finished task 1.0 in stage 17.0 (TID 16) in 202 ms on 172.18.0.15 (executor 0) (2/4)
26/02/13 11:43:29 INFO TaskSetManager: Finished task 3.0 in stage 17.0 (TID 18) in 63 ms on 172.18.0.15 (executor 0) (3/4)
26/02/13 11:43:29 INFO TaskSetManager: Finished task 2.0 in stage 17.0 (TID 17) in 67 ms on 172.18.0.15 (executor 0) (4/4)
26/02/13 11:43:29 INFO TaskSchedulerImpl: Removed TaskSet 17.0, whose tasks have all completed, from pool 
26/02/13 11:43:29 INFO DAGScheduler: ResultStage 17 (start at NativeMethodAccessorImpl.java:0) finished in 0.284 s
26/02/13 11:43:29 INFO DAGScheduler: Job 9 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:43:29 INFO TaskSchedulerImpl: Killing all running tasks in stage 17: Stage finished
26/02/13 11:43:29 INFO DAGScheduler: Job 9 finished: start at NativeMethodAccessorImpl.java:0, took 0.293412 s
26/02/13 11:43:29 INFO CodeGenerator: Code generated in 10.592796 ms
26/02/13 11:43:29 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.9 MiB)
26/02/13 11:43:29 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on spark-master:40291 (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 11:43:29 INFO SparkContext: Created broadcast 14 from start at NativeMethodAccessorImpl.java:0
26/02/13 11:43:29 INFO CodeGenerator: Code generated in 63.926169 ms
26/02/13 11:43:29 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 0, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@1e1a22b9]. The input RDD has 1 partitions.
26/02/13 11:43:29 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:43:29 INFO DAGScheduler: Got job 10 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/02/13 11:43:29 INFO DAGScheduler: Final stage: ResultStage 18 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:43:29 INFO DAGScheduler: Parents of final stage: List()
26/02/13 11:43:29 INFO DAGScheduler: Missing parents: List()
26/02/13 11:43:29 INFO DAGScheduler: Submitting ResultStage 18 (ParallelCollectionRDD[49] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:43:29 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 4.4 KiB, free 433.9 MiB)
26/02/13 11:43:29 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 2.6 KiB, free 433.9 MiB)
26/02/13 11:43:29 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on spark-master:40291 (size: 2.6 KiB, free: 434.2 MiB)
26/02/13 11:43:29 INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1585
26/02/13 11:43:29 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 18 (ParallelCollectionRDD[49] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/02/13 11:43:29 INFO TaskSchedulerImpl: Adding task set 18.0 with 1 tasks resource profile 0
26/02/13 11:43:29 INFO TaskSetManager: Starting task 0.0 in stage 18.0 (TID 19) (172.18.0.15, executor 0, partition 0, PROCESS_LOCAL, 7640 bytes) 
26/02/13 11:43:30 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on 172.18.0.15:34959 (size: 2.6 KiB, free: 434.1 MiB)
26/02/13 11:43:30 INFO TaskSetManager: Finished task 0.0 in stage 18.0 (TID 19) in 155 ms on 172.18.0.15 (executor 0) (1/1)
26/02/13 11:43:30 INFO TaskSchedulerImpl: Removed TaskSet 18.0, whose tasks have all completed, from pool 
26/02/13 11:43:30 INFO DAGScheduler: ResultStage 18 (start at NativeMethodAccessorImpl.java:0) finished in 0.169 s
26/02/13 11:43:30 INFO DAGScheduler: Job 10 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:43:30 INFO TaskSchedulerImpl: Killing all running tasks in stage 18: Stage finished
26/02/13 11:43:30 INFO DAGScheduler: Job 10 finished: start at NativeMethodAccessorImpl.java:0, took 0.174901 s
26/02/13 11:43:30 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 0, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@1e1a22b9] is committing.
26/02/13 11:43:30 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 0, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@1e1a22b9] committed.
26/02/13 11:43:30 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/0 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.0.973d8238-e1f2-4bfd-bf20-2e09eee446e3.tmp
26/02/13 11:43:30 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.0.973d8238-e1f2-4bfd-bf20-2e09eee446e3.tmp to file:/tmp/spark-checkpoint-enrichment/commits/0
26/02/13 11:43:30 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "fc5c6452-bd70-4c90-becf-772d3bebdb57",
  "name" : null,
  "timestamp" : "2026-02-13T11:43:26.881Z",
  "batchId" : 0,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "addBatch" : 1063,
    "commitOffsets" : 107,
    "getBatch" : 77,
    "latestOffset" : 1656,
    "queryPlanning" : 309,
    "triggerExecution" : 3378,
    "walCommit" : 125
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : null,
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 2803,
        "1" : 3106,
        "0" : 4970
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 2803,
        "1" : 3106,
        "0" : 4970
      }
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@7fe9b5de",
    "numOutputRows" : 0
  }
}
26/02/13 11:43:35 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/1 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.1.899e789e-15a9-49bc-bbf4-fdc9456a0ec9.tmp
26/02/13 11:43:35 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.1.899e789e-15a9-49bc-bbf4-fdc9456a0ec9.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/1
26/02/13 11:43:35 INFO MicroBatchExecution: Committed offsets for batch 1. Metadata OffsetSeqMetadata(0,1770983015522,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 11:43:35 INFO BlockManagerInfo: Removed broadcast_15_piece0 on spark-master:40291 in memory (size: 2.6 KiB, free: 434.2 MiB)
26/02/13 11:43:35 INFO BlockManagerInfo: Removed broadcast_15_piece0 on 172.18.0.15:34959 in memory (size: 2.6 KiB, free: 434.1 MiB)
26/02/13 11:43:35 INFO BlockManagerInfo: Removed broadcast_13_piece0 on spark-master:40291 in memory (size: 33.3 KiB, free: 434.3 MiB)
26/02/13 11:43:35 INFO BlockManagerInfo: Removed broadcast_13_piece0 on 172.18.0.15:34959 in memory (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 11:43:35 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:43:35 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:43:35 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:43:35 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:43:35 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:43:35 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:43:35 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#2805 - origin_code.nullCount#2804) > 0)
26/02/13 11:43:35 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#2810 - destination_code.nullCount#2809) > 0)
26/02/13 11:43:35 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#2840 - callsign.nullCount#2839) > 0)
26/02/13 11:43:36 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:43:36 INFO DAGScheduler: Got job 11 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 11:43:36 INFO DAGScheduler: Final stage: ResultStage 20 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:43:36 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 19)
26/02/13 11:43:36 INFO DAGScheduler: Missing parents: List()
26/02/13 11:43:36 INFO DAGScheduler: Submitting ResultStage 20 (MapPartitionsRDD[54] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:43:36 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 99.3 KiB, free 434.0 MiB)
26/02/13 11:43:36 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 433.9 MiB)
26/02/13 11:43:36 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on spark-master:40291 (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 11:43:36 INFO SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1585
26/02/13 11:43:36 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 20 (MapPartitionsRDD[54] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 11:43:36 INFO TaskSchedulerImpl: Adding task set 20.0 with 4 tasks resource profile 0
26/02/13 11:43:36 INFO TaskSetManager: Starting task 0.0 in stage 20.0 (TID 20) (172.18.0.15, executor 0, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:43:36 INFO TaskSetManager: Starting task 1.0 in stage 20.0 (TID 21) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:43:36 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on 172.18.0.15:34959 (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 11:43:36 INFO TaskSetManager: Starting task 2.0 in stage 20.0 (TID 22) (172.18.0.15, executor 0, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:43:36 INFO TaskSetManager: Finished task 0.0 in stage 20.0 (TID 20) in 57 ms on 172.18.0.15 (executor 0) (1/4)
26/02/13 11:43:36 INFO TaskSetManager: Starting task 3.0 in stage 20.0 (TID 23) (172.18.0.15, executor 0, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:43:36 INFO TaskSetManager: Finished task 1.0 in stage 20.0 (TID 21) in 63 ms on 172.18.0.15 (executor 0) (2/4)
26/02/13 11:43:36 INFO TaskSetManager: Finished task 2.0 in stage 20.0 (TID 22) in 41 ms on 172.18.0.15 (executor 0) (3/4)
26/02/13 11:43:36 INFO TaskSetManager: Finished task 3.0 in stage 20.0 (TID 23) in 48 ms on 172.18.0.15 (executor 0) (4/4)
26/02/13 11:43:36 INFO TaskSchedulerImpl: Removed TaskSet 20.0, whose tasks have all completed, from pool 
26/02/13 11:43:36 INFO DAGScheduler: ResultStage 20 (start at NativeMethodAccessorImpl.java:0) finished in 0.128 s
26/02/13 11:43:36 INFO DAGScheduler: Job 11 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:43:36 INFO TaskSchedulerImpl: Killing all running tasks in stage 20: Stage finished
26/02/13 11:43:36 INFO DAGScheduler: Job 11 finished: start at NativeMethodAccessorImpl.java:0, took 0.136501 s
26/02/13 11:43:36 INFO MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.8 MiB)
26/02/13 11:43:36 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on spark-master:40291 (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 11:43:36 INFO SparkContext: Created broadcast 17 from start at NativeMethodAccessorImpl.java:0
26/02/13 11:43:36 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 1, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@7fa1692c]. The input RDD has 3 partitions.
26/02/13 11:43:36 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:43:36 INFO DAGScheduler: Got job 12 (start at NativeMethodAccessorImpl.java:0) with 3 output partitions
26/02/13 11:43:36 INFO DAGScheduler: Final stage: ResultStage 21 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:43:36 INFO DAGScheduler: Parents of final stage: List()
26/02/13 11:43:36 INFO DAGScheduler: Missing parents: List()
26/02/13 11:43:36 INFO DAGScheduler: Submitting ResultStage 21 (MapPartitionsRDD[60] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:43:36 INFO MemoryStore: Block broadcast_18 stored as values in memory (estimated size 49.9 KiB, free 433.8 MiB)
26/02/13 11:43:36 INFO MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.8 MiB)
26/02/13 11:43:36 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on spark-master:40291 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 11:43:36 INFO SparkContext: Created broadcast 18 from broadcast at DAGScheduler.scala:1585
26/02/13 11:43:36 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 21 (MapPartitionsRDD[60] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
26/02/13 11:43:36 INFO TaskSchedulerImpl: Adding task set 21.0 with 3 tasks resource profile 0
26/02/13 11:43:36 INFO BlockManagerInfo: Removed broadcast_16_piece0 on spark-master:40291 in memory (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 11:43:36 INFO BlockManagerInfo: Removed broadcast_16_piece0 on 172.18.0.15:34959 in memory (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 11:43:36 INFO TaskSetManager: Starting task 1.0 in stage 21.0 (TID 24) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:43:36 INFO TaskSetManager: Starting task 0.0 in stage 21.0 (TID 25) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:43:36 INFO TaskSetManager: Starting task 2.0 in stage 21.0 (TID 26) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:43:36 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on 172.18.0.15:34959 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 11:43:36 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on 172.18.0.14:40939 (size: 19.5 KiB, free: 434.4 MiB)
26/02/13 11:43:36 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on 172.18.0.15:34959 (size: 107.2 KiB, free: 434.0 MiB)
26/02/13 11:43:37 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on 172.18.0.14:40939 (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 11:43:40 INFO TaskSetManager: Finished task 1.0 in stage 21.0 (TID 24) in 4617 ms on 172.18.0.15 (executor 0) (1/3)
26/02/13 11:43:41 INFO TaskSetManager: Finished task 2.0 in stage 21.0 (TID 26) in 4901 ms on 172.18.0.14 (executor 1) (2/3)
26/02/13 11:43:41 INFO TaskSetManager: Finished task 0.0 in stage 21.0 (TID 25) in 4908 ms on 172.18.0.14 (executor 1) (3/3)
26/02/13 11:43:41 INFO TaskSchedulerImpl: Removed TaskSet 21.0, whose tasks have all completed, from pool 
26/02/13 11:43:41 INFO DAGScheduler: ResultStage 21 (start at NativeMethodAccessorImpl.java:0) finished in 5.002 s
26/02/13 11:43:41 INFO DAGScheduler: Job 12 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:43:41 INFO TaskSchedulerImpl: Killing all running tasks in stage 21: Stage finished
26/02/13 11:43:41 INFO DAGScheduler: Job 12 finished: start at NativeMethodAccessorImpl.java:0, took 5.016553 s
26/02/13 11:43:41 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 1, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@7fa1692c] is committing.
26/02/13 11:43:41 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 1, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@7fa1692c] committed.
26/02/13 11:43:41 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/1 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.1.9d2762c4-5b0e-4e18-b857-77a48da776c0.tmp
26/02/13 11:43:41 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.1.9d2762c4-5b0e-4e18-b857-77a48da776c0.tmp to file:/tmp/spark-checkpoint-enrichment/commits/1
26/02/13 11:43:41 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "fc5c6452-bd70-4c90-becf-772d3bebdb57",
  "name" : null,
  "timestamp" : "2026-02-13T11:43:35.511Z",
  "batchId" : 1,
  "numInputRows" : 108,
  "inputRowsPerSecond" : 5142.857142857142,
  "processedRowsPerSecond" : 18.433179723502302,
  "durationMs" : {
    "addBatch" : 5390,
    "commitOffsets" : 155,
    "getBatch" : 0,
    "latestOffset" : 11,
    "queryPlanning" : 187,
    "triggerExecution" : 5859,
    "walCommit" : 114
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 2803,
        "1" : 3106,
        "0" : 4970
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 2837,
        "1" : 3143,
        "0" : 5007
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 2837,
        "1" : 3143,
        "0" : 5007
      }
    },
    "numInputRows" : 108,
    "inputRowsPerSecond" : 5142.857142857142,
    "processedRowsPerSecond" : 18.433179723502302,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@7fe9b5de",
    "numOutputRows" : 25
  }
}
26/02/13 11:43:41 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/2 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.2.66350239-0686-43fb-9aa2-71732e07dc7e.tmp
26/02/13 11:43:41 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.2.66350239-0686-43fb-9aa2-71732e07dc7e.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/2
26/02/13 11:43:41 INFO MicroBatchExecution: Committed offsets for batch 2. Metadata OffsetSeqMetadata(0,1770983021383,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 11:43:41 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:43:41 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:43:41 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:43:41 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:43:42 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:43:42 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:43:42 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#3659 - origin_code.nullCount#3658) > 0)
26/02/13 11:43:42 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#3664 - destination_code.nullCount#3663) > 0)
26/02/13 11:43:42 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#3694 - callsign.nullCount#3693) > 0)
26/02/13 11:43:42 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:43:42 INFO DAGScheduler: Got job 13 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 11:43:42 INFO DAGScheduler: Final stage: ResultStage 23 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:43:42 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 22)
26/02/13 11:43:42 INFO DAGScheduler: Missing parents: List()
26/02/13 11:43:42 INFO DAGScheduler: Submitting ResultStage 23 (MapPartitionsRDD[65] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:43:42 INFO MemoryStore: Block broadcast_19 stored as values in memory (estimated size 99.3 KiB, free 433.8 MiB)
26/02/13 11:43:42 INFO MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 433.8 MiB)
26/02/13 11:43:42 INFO BlockManagerInfo: Added broadcast_19_piece0 in memory on spark-master:40291 (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 11:43:42 INFO BlockManagerInfo: Removed broadcast_18_piece0 on spark-master:40291 in memory (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 11:43:42 INFO SparkContext: Created broadcast 19 from broadcast at DAGScheduler.scala:1585
26/02/13 11:43:42 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 23 (MapPartitionsRDD[65] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 11:43:42 INFO BlockManagerInfo: Removed broadcast_18_piece0 on 172.18.0.14:40939 in memory (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 11:43:42 INFO TaskSchedulerImpl: Adding task set 23.0 with 4 tasks resource profile 0
26/02/13 11:43:42 INFO BlockManagerInfo: Removed broadcast_18_piece0 on 172.18.0.15:34959 in memory (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 11:43:42 INFO TaskSetManager: Starting task 0.0 in stage 23.0 (TID 27) (172.18.0.15, executor 0, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:43:42 INFO TaskSetManager: Starting task 1.0 in stage 23.0 (TID 28) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:43:42 INFO BlockManagerInfo: Added broadcast_19_piece0 in memory on 172.18.0.15:34959 (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 11:43:42 INFO BlockManagerInfo: Removed broadcast_14_piece0 on spark-master:40291 in memory (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 11:43:42 INFO TaskSetManager: Starting task 2.0 in stage 23.0 (TID 29) (172.18.0.15, executor 0, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:43:42 INFO TaskSetManager: Finished task 1.0 in stage 23.0 (TID 28) in 155 ms on 172.18.0.15 (executor 0) (1/4)
26/02/13 11:43:42 INFO TaskSetManager: Starting task 3.0 in stage 23.0 (TID 30) (172.18.0.15, executor 0, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:43:42 INFO TaskSetManager: Finished task 0.0 in stage 23.0 (TID 27) in 197 ms on 172.18.0.15 (executor 0) (2/4)
26/02/13 11:43:42 INFO TaskSetManager: Finished task 2.0 in stage 23.0 (TID 29) in 115 ms on 172.18.0.15 (executor 0) (3/4)
26/02/13 11:43:42 INFO TaskSetManager: Finished task 3.0 in stage 23.0 (TID 30) in 114 ms on 172.18.0.15 (executor 0) (4/4)
26/02/13 11:43:42 INFO TaskSchedulerImpl: Removed TaskSet 23.0, whose tasks have all completed, from pool 
26/02/13 11:43:42 INFO DAGScheduler: ResultStage 23 (start at NativeMethodAccessorImpl.java:0) finished in 0.348 s
26/02/13 11:43:42 INFO DAGScheduler: Job 13 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:43:42 INFO TaskSchedulerImpl: Killing all running tasks in stage 23: Stage finished
26/02/13 11:43:42 INFO DAGScheduler: Job 13 finished: start at NativeMethodAccessorImpl.java:0, took 0.364224 s
26/02/13 11:43:42 INFO MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.8 MiB)
26/02/13 11:43:42 INFO BlockManagerInfo: Added broadcast_20_piece0 in memory on spark-master:40291 (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 11:43:42 INFO SparkContext: Created broadcast 20 from start at NativeMethodAccessorImpl.java:0
26/02/13 11:43:42 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 2, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@195047cd]. The input RDD has 3 partitions.
26/02/13 11:43:42 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:43:42 INFO DAGScheduler: Got job 14 (start at NativeMethodAccessorImpl.java:0) with 3 output partitions
26/02/13 11:43:42 INFO DAGScheduler: Final stage: ResultStage 24 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:43:42 INFO DAGScheduler: Parents of final stage: List()
26/02/13 11:43:42 INFO DAGScheduler: Missing parents: List()
26/02/13 11:43:42 INFO DAGScheduler: Submitting ResultStage 24 (MapPartitionsRDD[71] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:43:42 INFO MemoryStore: Block broadcast_21 stored as values in memory (estimated size 49.9 KiB, free 433.8 MiB)
26/02/13 11:43:42 INFO MemoryStore: Block broadcast_21_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.8 MiB)
26/02/13 11:43:42 INFO BlockManagerInfo: Added broadcast_21_piece0 in memory on spark-master:40291 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 11:43:42 INFO SparkContext: Created broadcast 21 from broadcast at DAGScheduler.scala:1585
26/02/13 11:43:42 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 24 (MapPartitionsRDD[71] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
26/02/13 11:43:42 INFO TaskSchedulerImpl: Adding task set 24.0 with 3 tasks resource profile 0
26/02/13 11:43:42 INFO TaskSetManager: Starting task 0.0 in stage 24.0 (TID 31) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:43:42 INFO TaskSetManager: Starting task 1.0 in stage 24.0 (TID 32) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:43:42 INFO TaskSetManager: Starting task 2.0 in stage 24.0 (TID 33) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:43:42 INFO BlockManagerInfo: Added broadcast_21_piece0 in memory on 172.18.0.14:40939 (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 11:43:42 INFO BlockManagerInfo: Added broadcast_21_piece0 in memory on 172.18.0.15:34959 (size: 19.5 KiB, free: 434.0 MiB)
26/02/13 11:43:42 INFO BlockManagerInfo: Added broadcast_20_piece0 in memory on 172.18.0.14:40939 (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 11:43:42 INFO BlockManagerInfo: Added broadcast_20_piece0 in memory on 172.18.0.15:34959 (size: 107.2 KiB, free: 433.9 MiB)
26/02/13 11:43:43 INFO TaskSetManager: Finished task 1.0 in stage 24.0 (TID 32) in 494 ms on 172.18.0.15 (executor 0) (1/3)
26/02/13 11:43:43 INFO TaskSetManager: Finished task 2.0 in stage 24.0 (TID 33) in 769 ms on 172.18.0.14 (executor 1) (2/3)
26/02/13 11:43:43 INFO TaskSetManager: Finished task 0.0 in stage 24.0 (TID 31) in 869 ms on 172.18.0.14 (executor 1) (3/3)
26/02/13 11:43:43 INFO TaskSchedulerImpl: Removed TaskSet 24.0, whose tasks have all completed, from pool 
26/02/13 11:43:43 INFO DAGScheduler: ResultStage 24 (start at NativeMethodAccessorImpl.java:0) finished in 0.902 s
26/02/13 11:43:43 INFO DAGScheduler: Job 14 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:43:43 INFO TaskSchedulerImpl: Killing all running tasks in stage 24: Stage finished
26/02/13 11:43:43 INFO DAGScheduler: Job 14 finished: start at NativeMethodAccessorImpl.java:0, took 0.921495 s
26/02/13 11:43:43 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 2, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@195047cd] is committing.
26/02/13 11:43:43 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 2, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@195047cd] committed.
26/02/13 11:43:43 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/2 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.2.1b1a66da-3829-42cd-88c7-34c971fa368c.tmp
26/02/13 11:43:43 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.2.1b1a66da-3829-42cd-88c7-34c971fa368c.tmp to file:/tmp/spark-checkpoint-enrichment/commits/2
26/02/13 11:43:43 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "fc5c6452-bd70-4c90-becf-772d3bebdb57",
  "name" : null,
  "timestamp" : "2026-02-13T11:43:41.374Z",
  "batchId" : 2,
  "numInputRows" : 132,
  "inputRowsPerSecond" : 22.5140712945591,
  "processedRowsPerSecond" : 57.94556628621598,
  "durationMs" : {
    "addBatch" : 1803,
    "commitOffsets" : 126,
    "getBatch" : 1,
    "latestOffset" : 9,
    "queryPlanning" : 223,
    "triggerExecution" : 2278,
    "walCommit" : 115
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 2837,
        "1" : 3143,
        "0" : 5007
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 2879,
        "1" : 3192,
        "0" : 5048
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 2879,
        "1" : 3192,
        "0" : 5048
      }
    },
    "numInputRows" : 132,
    "inputRowsPerSecond" : 22.5140712945591,
    "processedRowsPerSecond" : 57.94556628621598,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@7fe9b5de",
    "numOutputRows" : 27
  }
}
26/02/13 11:43:51 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/3 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.3.4b0f16b9-916a-40d8-94f9-9ad9ac2913ad.tmp
26/02/13 11:43:51 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.3.4b0f16b9-916a-40d8-94f9-9ad9ac2913ad.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/3
26/02/13 11:43:51 INFO MicroBatchExecution: Committed offsets for batch 3. Metadata OffsetSeqMetadata(0,1770983031688,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 11:43:52 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:43:52 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:43:52 INFO BlockManagerInfo: Removed broadcast_20_piece0 on spark-master:40291 in memory (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 11:43:52 INFO BlockManagerInfo: Removed broadcast_20_piece0 on 172.18.0.14:40939 in memory (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 11:43:52 INFO BlockManagerInfo: Removed broadcast_20_piece0 on 172.18.0.15:34959 in memory (size: 107.2 KiB, free: 434.0 MiB)
26/02/13 11:43:52 INFO BlockManagerInfo: Removed broadcast_21_piece0 on spark-master:40291 in memory (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 11:43:52 INFO BlockManagerInfo: Removed broadcast_21_piece0 on 172.18.0.14:40939 in memory (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 11:43:52 INFO BlockManagerInfo: Removed broadcast_21_piece0 on 172.18.0.15:34959 in memory (size: 19.5 KiB, free: 434.0 MiB)
26/02/13 11:43:52 INFO BlockManagerInfo: Removed broadcast_19_piece0 on spark-master:40291 in memory (size: 33.3 KiB, free: 434.3 MiB)
26/02/13 11:43:52 INFO BlockManagerInfo: Removed broadcast_19_piece0 on 172.18.0.15:34959 in memory (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 11:43:52 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:43:52 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:43:52 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:43:52 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:43:52 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#4513 - origin_code.nullCount#4512) > 0)
26/02/13 11:43:52 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#4518 - destination_code.nullCount#4517) > 0)
26/02/13 11:43:52 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#4548 - callsign.nullCount#4547) > 0)
26/02/13 11:43:52 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:43:52 INFO DAGScheduler: Got job 15 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 11:43:52 INFO DAGScheduler: Final stage: ResultStage 26 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:43:52 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 25)
26/02/13 11:43:52 INFO DAGScheduler: Missing parents: List()
26/02/13 11:43:52 INFO DAGScheduler: Submitting ResultStage 26 (MapPartitionsRDD[76] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:43:52 INFO MemoryStore: Block broadcast_22 stored as values in memory (estimated size 99.3 KiB, free 434.0 MiB)
26/02/13 11:43:52 INFO MemoryStore: Block broadcast_22_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 433.9 MiB)
26/02/13 11:43:52 INFO BlockManagerInfo: Added broadcast_22_piece0 in memory on spark-master:40291 (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 11:43:52 INFO SparkContext: Created broadcast 22 from broadcast at DAGScheduler.scala:1585
26/02/13 11:43:52 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 26 (MapPartitionsRDD[76] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 11:43:52 INFO TaskSchedulerImpl: Adding task set 26.0 with 4 tasks resource profile 0
26/02/13 11:43:52 INFO TaskSetManager: Starting task 0.0 in stage 26.0 (TID 34) (172.18.0.15, executor 0, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:43:52 INFO TaskSetManager: Starting task 1.0 in stage 26.0 (TID 35) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:43:52 INFO BlockManagerInfo: Added broadcast_22_piece0 in memory on 172.18.0.15:34959 (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 11:43:53 INFO TaskSetManager: Starting task 2.0 in stage 26.0 (TID 36) (172.18.0.15, executor 0, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:43:53 INFO TaskSetManager: Starting task 3.0 in stage 26.0 (TID 37) (172.18.0.15, executor 0, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:43:53 INFO TaskSetManager: Finished task 1.0 in stage 26.0 (TID 35) in 130 ms on 172.18.0.15 (executor 0) (1/4)
26/02/13 11:43:53 INFO TaskSetManager: Finished task 0.0 in stage 26.0 (TID 34) in 136 ms on 172.18.0.15 (executor 0) (2/4)
26/02/13 11:43:53 INFO TaskSetManager: Finished task 2.0 in stage 26.0 (TID 36) in 79 ms on 172.18.0.15 (executor 0) (3/4)
26/02/13 11:43:53 INFO TaskSetManager: Finished task 3.0 in stage 26.0 (TID 37) in 72 ms on 172.18.0.15 (executor 0) (4/4)
26/02/13 11:43:53 INFO TaskSchedulerImpl: Removed TaskSet 26.0, whose tasks have all completed, from pool 
26/02/13 11:43:53 INFO DAGScheduler: ResultStage 26 (start at NativeMethodAccessorImpl.java:0) finished in 0.231 s
26/02/13 11:43:53 INFO DAGScheduler: Job 15 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:43:53 INFO TaskSchedulerImpl: Killing all running tasks in stage 26: Stage finished
26/02/13 11:43:53 INFO DAGScheduler: Job 15 finished: start at NativeMethodAccessorImpl.java:0, took 0.254582 s
26/02/13 11:43:53 INFO MemoryStore: Block broadcast_23_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.8 MiB)
26/02/13 11:43:53 INFO BlockManagerInfo: Added broadcast_23_piece0 in memory on spark-master:40291 (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 11:43:53 INFO SparkContext: Created broadcast 23 from start at NativeMethodAccessorImpl.java:0
26/02/13 11:43:53 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 3, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@3f14e48c]. The input RDD has 3 partitions.
26/02/13 11:43:53 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:43:53 INFO DAGScheduler: Got job 16 (start at NativeMethodAccessorImpl.java:0) with 3 output partitions
26/02/13 11:43:53 INFO DAGScheduler: Final stage: ResultStage 27 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:43:53 INFO DAGScheduler: Parents of final stage: List()
26/02/13 11:43:53 INFO DAGScheduler: Missing parents: List()
26/02/13 11:43:53 INFO DAGScheduler: Submitting ResultStage 27 (MapPartitionsRDD[82] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:43:53 INFO MemoryStore: Block broadcast_24 stored as values in memory (estimated size 49.9 KiB, free 433.8 MiB)
26/02/13 11:43:53 INFO MemoryStore: Block broadcast_24_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.8 MiB)
26/02/13 11:43:53 INFO BlockManagerInfo: Added broadcast_24_piece0 in memory on spark-master:40291 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 11:43:53 INFO SparkContext: Created broadcast 24 from broadcast at DAGScheduler.scala:1585
26/02/13 11:43:53 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 27 (MapPartitionsRDD[82] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
26/02/13 11:43:53 INFO TaskSchedulerImpl: Adding task set 27.0 with 3 tasks resource profile 0
26/02/13 11:43:53 INFO TaskSetManager: Starting task 0.0 in stage 27.0 (TID 38) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:43:53 INFO TaskSetManager: Starting task 1.0 in stage 27.0 (TID 39) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:43:53 INFO TaskSetManager: Starting task 2.0 in stage 27.0 (TID 40) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:43:53 INFO BlockManagerInfo: Added broadcast_24_piece0 in memory on 172.18.0.14:40939 (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 11:43:53 INFO BlockManagerInfo: Added broadcast_24_piece0 in memory on 172.18.0.15:34959 (size: 19.5 KiB, free: 434.0 MiB)
26/02/13 11:43:53 INFO BlockManagerInfo: Added broadcast_23_piece0 in memory on 172.18.0.15:34959 (size: 107.2 KiB, free: 433.9 MiB)
26/02/13 11:43:53 INFO BlockManagerInfo: Added broadcast_23_piece0 in memory on 172.18.0.14:40939 (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 11:43:54 INFO TaskSetManager: Finished task 1.0 in stage 27.0 (TID 39) in 935 ms on 172.18.0.15 (executor 0) (1/3)
26/02/13 11:43:54 INFO TaskSetManager: Finished task 2.0 in stage 27.0 (TID 40) in 1008 ms on 172.18.0.14 (executor 1) (2/3)
26/02/13 11:43:54 INFO TaskSetManager: Finished task 0.0 in stage 27.0 (TID 38) in 1022 ms on 172.18.0.14 (executor 1) (3/3)
26/02/13 11:43:54 INFO TaskSchedulerImpl: Removed TaskSet 27.0, whose tasks have all completed, from pool 
26/02/13 11:43:54 INFO DAGScheduler: ResultStage 27 (start at NativeMethodAccessorImpl.java:0) finished in 1.036 s
26/02/13 11:43:54 INFO DAGScheduler: Job 16 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:43:54 INFO TaskSchedulerImpl: Killing all running tasks in stage 27: Stage finished
26/02/13 11:43:54 INFO DAGScheduler: Job 16 finished: start at NativeMethodAccessorImpl.java:0, took 1.051123 s
26/02/13 11:43:54 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 3, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@3f14e48c] is committing.
26/02/13 11:43:54 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 3, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@3f14e48c] committed.
26/02/13 11:43:54 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/3 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.3.f346ae75-2453-4136-aa1f-2ad7519704eb.tmp
26/02/13 11:43:54 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.3.f346ae75-2453-4136-aa1f-2ad7519704eb.tmp to file:/tmp/spark-checkpoint-enrichment/commits/3
26/02/13 11:43:54 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "fc5c6452-bd70-4c90-becf-772d3bebdb57",
  "name" : null,
  "timestamp" : "2026-02-13T11:43:51.671Z",
  "batchId" : 3,
  "numInputRows" : 66,
  "inputRowsPerSecond" : 2750.0,
  "processedRowsPerSecond" : 23.91304347826087,
  "durationMs" : {
    "addBatch" : 2170,
    "commitOffsets" : 128,
    "getBatch" : 0,
    "latestOffset" : 17,
    "queryPlanning" : 315,
    "triggerExecution" : 2760,
    "walCommit" : 126
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 2879,
        "1" : 3192,
        "0" : 5048
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 2896,
        "1" : 3218,
        "0" : 5071
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 2896,
        "1" : 3218,
        "0" : 5071
      }
    },
    "numInputRows" : 66,
    "inputRowsPerSecond" : 2750.0,
    "processedRowsPerSecond" : 23.91304347826087,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@7fe9b5de",
    "numOutputRows" : 20
  }
}
26/02/13 11:43:54 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/4 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.4.f178d9ae-1353-41c9-bf35-bdbc5d6cc59d.tmp
26/02/13 11:43:54 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.4.f178d9ae-1353-41c9-bf35-bdbc5d6cc59d.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/4
26/02/13 11:43:54 INFO MicroBatchExecution: Committed offsets for batch 4. Metadata OffsetSeqMetadata(0,1770983034450,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 11:43:54 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:43:54 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:43:54 INFO BlockManagerInfo: Removed broadcast_24_piece0 on spark-master:40291 in memory (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 11:43:54 INFO BlockManagerInfo: Removed broadcast_24_piece0 on 172.18.0.15:34959 in memory (size: 19.5 KiB, free: 433.9 MiB)
26/02/13 11:43:54 INFO BlockManagerInfo: Removed broadcast_24_piece0 on 172.18.0.14:40939 in memory (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 11:43:54 INFO BlockManagerInfo: Removed broadcast_22_piece0 on spark-master:40291 in memory (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 11:43:54 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:43:54 INFO BlockManagerInfo: Removed broadcast_22_piece0 on 172.18.0.15:34959 in memory (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 11:43:54 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:43:54 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:43:54 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:43:54 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#5367 - origin_code.nullCount#5366) > 0)
26/02/13 11:43:54 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#5372 - destination_code.nullCount#5371) > 0)
26/02/13 11:43:54 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#5402 - callsign.nullCount#5401) > 0)
26/02/13 11:43:55 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:43:55 INFO DAGScheduler: Got job 17 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 11:43:55 INFO DAGScheduler: Final stage: ResultStage 29 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:43:55 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 28)
26/02/13 11:43:55 INFO DAGScheduler: Missing parents: List()
26/02/13 11:43:55 INFO DAGScheduler: Submitting ResultStage 29 (MapPartitionsRDD[87] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:43:55 INFO MemoryStore: Block broadcast_25 stored as values in memory (estimated size 99.3 KiB, free 433.9 MiB)
26/02/13 11:43:55 INFO MemoryStore: Block broadcast_25_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 433.8 MiB)
26/02/13 11:43:55 INFO BlockManagerInfo: Added broadcast_25_piece0 in memory on spark-master:40291 (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 11:43:55 INFO SparkContext: Created broadcast 25 from broadcast at DAGScheduler.scala:1585
26/02/13 11:43:55 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 29 (MapPartitionsRDD[87] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 11:43:55 INFO TaskSchedulerImpl: Adding task set 29.0 with 4 tasks resource profile 0
26/02/13 11:43:55 INFO TaskSetManager: Starting task 0.0 in stage 29.0 (TID 41) (172.18.0.15, executor 0, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:43:55 INFO TaskSetManager: Starting task 1.0 in stage 29.0 (TID 42) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:43:55 INFO BlockManagerInfo: Added broadcast_25_piece0 in memory on 172.18.0.15:34959 (size: 33.3 KiB, free: 433.9 MiB)
26/02/13 11:43:55 INFO TaskSetManager: Starting task 2.0 in stage 29.0 (TID 43) (172.18.0.15, executor 0, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:43:55 INFO TaskSetManager: Finished task 0.0 in stage 29.0 (TID 41) in 82 ms on 172.18.0.15 (executor 0) (1/4)
26/02/13 11:43:55 INFO TaskSetManager: Starting task 3.0 in stage 29.0 (TID 44) (172.18.0.15, executor 0, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:43:55 INFO TaskSetManager: Finished task 1.0 in stage 29.0 (TID 42) in 88 ms on 172.18.0.15 (executor 0) (2/4)
26/02/13 11:43:55 INFO TaskSetManager: Finished task 3.0 in stage 29.0 (TID 44) in 55 ms on 172.18.0.15 (executor 0) (3/4)
26/02/13 11:43:55 INFO BlockManagerInfo: Removed broadcast_17_piece0 on spark-master:40291 in memory (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 11:43:55 INFO TaskSetManager: Finished task 2.0 in stage 29.0 (TID 43) in 64 ms on 172.18.0.15 (executor 0) (4/4)
26/02/13 11:43:55 INFO TaskSchedulerImpl: Removed TaskSet 29.0, whose tasks have all completed, from pool 
26/02/13 11:43:55 INFO DAGScheduler: ResultStage 29 (start at NativeMethodAccessorImpl.java:0) finished in 0.178 s
26/02/13 11:43:55 INFO DAGScheduler: Job 17 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:43:55 INFO TaskSchedulerImpl: Killing all running tasks in stage 29: Stage finished
26/02/13 11:43:55 INFO DAGScheduler: Job 17 finished: start at NativeMethodAccessorImpl.java:0, took 0.194174 s
26/02/13 11:43:55 INFO BlockManagerInfo: Removed broadcast_17_piece0 on 172.18.0.14:40939 in memory (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 11:43:55 INFO BlockManagerInfo: Removed broadcast_17_piece0 on 172.18.0.15:34959 in memory (size: 107.2 KiB, free: 434.0 MiB)
26/02/13 11:43:55 INFO MemoryStore: Block broadcast_26_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.8 MiB)
26/02/13 11:43:55 INFO BlockManagerInfo: Added broadcast_26_piece0 in memory on spark-master:40291 (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 11:43:55 INFO SparkContext: Created broadcast 26 from start at NativeMethodAccessorImpl.java:0
26/02/13 11:43:55 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 4, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@7096db47]. The input RDD has 3 partitions.
26/02/13 11:43:55 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:43:55 INFO DAGScheduler: Got job 18 (start at NativeMethodAccessorImpl.java:0) with 3 output partitions
26/02/13 11:43:55 INFO DAGScheduler: Final stage: ResultStage 30 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:43:55 INFO DAGScheduler: Parents of final stage: List()
26/02/13 11:43:55 INFO DAGScheduler: Missing parents: List()
26/02/13 11:43:55 INFO DAGScheduler: Submitting ResultStage 30 (MapPartitionsRDD[93] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:43:55 INFO MemoryStore: Block broadcast_27 stored as values in memory (estimated size 49.9 KiB, free 433.8 MiB)
26/02/13 11:43:55 INFO MemoryStore: Block broadcast_27_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.8 MiB)
26/02/13 11:43:55 INFO BlockManagerInfo: Added broadcast_27_piece0 in memory on spark-master:40291 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 11:43:55 INFO SparkContext: Created broadcast 27 from broadcast at DAGScheduler.scala:1585
26/02/13 11:43:55 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 30 (MapPartitionsRDD[93] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
26/02/13 11:43:55 INFO TaskSchedulerImpl: Adding task set 30.0 with 3 tasks resource profile 0
26/02/13 11:43:55 INFO TaskSetManager: Starting task 0.0 in stage 30.0 (TID 45) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:43:55 INFO TaskSetManager: Starting task 1.0 in stage 30.0 (TID 46) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:43:55 INFO TaskSetManager: Starting task 2.0 in stage 30.0 (TID 47) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:43:55 INFO BlockManagerInfo: Added broadcast_27_piece0 in memory on 172.18.0.15:34959 (size: 19.5 KiB, free: 434.0 MiB)
26/02/13 11:43:55 INFO BlockManagerInfo: Added broadcast_27_piece0 in memory on 172.18.0.14:40939 (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 11:43:55 INFO BlockManagerInfo: Added broadcast_26_piece0 in memory on 172.18.0.15:34959 (size: 107.2 KiB, free: 433.9 MiB)
26/02/13 11:43:55 INFO BlockManagerInfo: Added broadcast_26_piece0 in memory on 172.18.0.14:40939 (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 11:43:55 INFO TaskSetManager: Finished task 1.0 in stage 30.0 (TID 46) in 429 ms on 172.18.0.15 (executor 0) (1/3)
26/02/13 11:43:55 INFO TaskSetManager: Finished task 0.0 in stage 30.0 (TID 45) in 484 ms on 172.18.0.14 (executor 1) (2/3)
26/02/13 11:43:55 INFO TaskSetManager: Finished task 2.0 in stage 30.0 (TID 47) in 525 ms on 172.18.0.14 (executor 1) (3/3)
26/02/13 11:43:55 INFO TaskSchedulerImpl: Removed TaskSet 30.0, whose tasks have all completed, from pool 
26/02/13 11:43:55 INFO DAGScheduler: ResultStage 30 (start at NativeMethodAccessorImpl.java:0) finished in 0.544 s
26/02/13 11:43:55 INFO DAGScheduler: Job 18 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:43:55 INFO TaskSchedulerImpl: Killing all running tasks in stage 30: Stage finished
26/02/13 11:43:55 INFO DAGScheduler: Job 18 finished: start at NativeMethodAccessorImpl.java:0, took 0.560216 s
26/02/13 11:43:55 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 4, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@7096db47] is committing.
26/02/13 11:43:55 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 4, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@7096db47] committed.
26/02/13 11:43:55 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/4 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.4.1b5d8f30-72e5-44be-87d3-f2f1e818accf.tmp
26/02/13 11:43:56 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.4.1b5d8f30-72e5-44be-87d3-f2f1e818accf.tmp to file:/tmp/spark-checkpoint-enrichment/commits/4
26/02/13 11:43:56 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "fc5c6452-bd70-4c90-becf-772d3bebdb57",
  "name" : null,
  "timestamp" : "2026-02-13T11:43:54.439Z",
  "batchId" : 4,
  "numInputRows" : 173,
  "inputRowsPerSecond" : 62.50000000000001,
  "processedRowsPerSecond" : 108.125,
  "durationMs" : {
    "addBatch" : 1161,
    "commitOffsets" : 202,
    "getBatch" : 0,
    "latestOffset" : 11,
    "queryPlanning" : 105,
    "triggerExecution" : 1600,
    "walCommit" : 118
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 2896,
        "1" : 3218,
        "0" : 5071
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 2954,
        "1" : 3276,
        "0" : 5128
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 2954,
        "1" : 3276,
        "0" : 5128
      }
    },
    "numInputRows" : 173,
    "inputRowsPerSecond" : 62.50000000000001,
    "processedRowsPerSecond" : 108.125,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@7fe9b5de",
    "numOutputRows" : 32
  }
}
26/02/13 11:44:06 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 11:44:08 INFO BlockManagerInfo: Removed broadcast_23_piece0 on spark-master:40291 in memory (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 11:44:08 INFO BlockManagerInfo: Removed broadcast_23_piece0 on 172.18.0.14:40939 in memory (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 11:44:08 INFO BlockManagerInfo: Removed broadcast_23_piece0 on 172.18.0.15:34959 in memory (size: 107.2 KiB, free: 434.0 MiB)
26/02/13 11:44:08 INFO BlockManagerInfo: Removed broadcast_25_piece0 on spark-master:40291 in memory (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 11:44:08 INFO BlockManagerInfo: Removed broadcast_25_piece0 on 172.18.0.15:34959 in memory (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 11:44:08 INFO BlockManagerInfo: Removed broadcast_27_piece0 on spark-master:40291 in memory (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 11:44:08 INFO BlockManagerInfo: Removed broadcast_27_piece0 on 172.18.0.14:40939 in memory (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 11:44:08 INFO BlockManagerInfo: Removed broadcast_27_piece0 on 172.18.0.15:34959 in memory (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 11:44:12 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/5 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.5.3d7667fa-0ca6-4fcf-9514-1d409e1f5370.tmp
26/02/13 11:44:12 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.5.3d7667fa-0ca6-4fcf-9514-1d409e1f5370.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/5
26/02/13 11:44:12 INFO MicroBatchExecution: Committed offsets for batch 5. Metadata OffsetSeqMetadata(0,1770983052354,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 11:44:12 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:44:12 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:44:13 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:44:13 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:44:13 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:44:13 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:44:13 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#6221 - origin_code.nullCount#6220) > 0)
26/02/13 11:44:13 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#6226 - destination_code.nullCount#6225) > 0)
26/02/13 11:44:13 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#6256 - callsign.nullCount#6255) > 0)
26/02/13 11:44:13 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:44:13 INFO DAGScheduler: Got job 19 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 11:44:13 INFO DAGScheduler: Final stage: ResultStage 32 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:44:13 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 31)
26/02/13 11:44:13 INFO DAGScheduler: Missing parents: List()
26/02/13 11:44:13 INFO DAGScheduler: Submitting ResultStage 32 (MapPartitionsRDD[98] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:44:13 INFO MemoryStore: Block broadcast_28 stored as values in memory (estimated size 99.3 KiB, free 434.0 MiB)
26/02/13 11:44:13 INFO MemoryStore: Block broadcast_28_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 433.9 MiB)
26/02/13 11:44:13 INFO BlockManagerInfo: Added broadcast_28_piece0 in memory on spark-master:40291 (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 11:44:13 INFO SparkContext: Created broadcast 28 from broadcast at DAGScheduler.scala:1585
26/02/13 11:44:13 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 32 (MapPartitionsRDD[98] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 11:44:13 INFO TaskSchedulerImpl: Adding task set 32.0 with 4 tasks resource profile 0
26/02/13 11:44:13 INFO TaskSetManager: Starting task 0.0 in stage 32.0 (TID 48) (172.18.0.15, executor 0, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:44:13 INFO TaskSetManager: Starting task 1.0 in stage 32.0 (TID 49) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:44:13 INFO BlockManagerInfo: Added broadcast_28_piece0 in memory on 172.18.0.15:34959 (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 11:44:13 INFO TaskSetManager: Starting task 2.0 in stage 32.0 (TID 50) (172.18.0.15, executor 0, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:44:13 INFO TaskSetManager: Starting task 3.0 in stage 32.0 (TID 51) (172.18.0.15, executor 0, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:44:13 INFO TaskSetManager: Finished task 0.0 in stage 32.0 (TID 48) in 234 ms on 172.18.0.15 (executor 0) (1/4)
26/02/13 11:44:13 INFO TaskSetManager: Finished task 1.0 in stage 32.0 (TID 49) in 258 ms on 172.18.0.15 (executor 0) (2/4)
26/02/13 11:44:13 INFO TaskSetManager: Finished task 2.0 in stage 32.0 (TID 50) in 92 ms on 172.18.0.15 (executor 0) (3/4)
26/02/13 11:44:13 INFO TaskSetManager: Finished task 3.0 in stage 32.0 (TID 51) in 115 ms on 172.18.0.15 (executor 0) (4/4)
26/02/13 11:44:13 INFO TaskSchedulerImpl: Removed TaskSet 32.0, whose tasks have all completed, from pool 
26/02/13 11:44:13 INFO DAGScheduler: ResultStage 32 (start at NativeMethodAccessorImpl.java:0) finished in 0.423 s
26/02/13 11:44:13 INFO DAGScheduler: Job 19 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:44:13 INFO TaskSchedulerImpl: Killing all running tasks in stage 32: Stage finished
26/02/13 11:44:13 INFO DAGScheduler: Job 19 finished: start at NativeMethodAccessorImpl.java:0, took 0.454548 s
26/02/13 11:44:13 INFO MemoryStore: Block broadcast_29_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.8 MiB)
26/02/13 11:44:13 INFO BlockManagerInfo: Added broadcast_29_piece0 in memory on spark-master:40291 (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 11:44:13 INFO SparkContext: Created broadcast 29 from start at NativeMethodAccessorImpl.java:0
26/02/13 11:44:13 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 5, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@4e18a43]. The input RDD has 2 partitions.
26/02/13 11:44:13 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:44:13 INFO DAGScheduler: Got job 20 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/02/13 11:44:13 INFO DAGScheduler: Final stage: ResultStage 33 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:44:13 INFO DAGScheduler: Parents of final stage: List()
26/02/13 11:44:13 INFO DAGScheduler: Missing parents: List()
26/02/13 11:44:13 INFO DAGScheduler: Submitting ResultStage 33 (MapPartitionsRDD[104] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:44:14 INFO MemoryStore: Block broadcast_30 stored as values in memory (estimated size 49.9 KiB, free 433.8 MiB)
26/02/13 11:44:14 INFO MemoryStore: Block broadcast_30_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.8 MiB)
26/02/13 11:44:14 INFO BlockManagerInfo: Added broadcast_30_piece0 in memory on spark-master:40291 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 11:44:14 INFO SparkContext: Created broadcast 30 from broadcast at DAGScheduler.scala:1585
26/02/13 11:44:14 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 33 (MapPartitionsRDD[104] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/02/13 11:44:14 INFO TaskSchedulerImpl: Adding task set 33.0 with 2 tasks resource profile 0
26/02/13 11:44:14 INFO TaskSetManager: Starting task 0.0 in stage 33.0 (TID 52) (172.18.0.15, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:44:14 INFO TaskSetManager: Starting task 1.0 in stage 33.0 (TID 53) (172.18.0.14, executor 1, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:44:14 INFO BlockManagerInfo: Removed broadcast_28_piece0 on spark-master:40291 in memory (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 11:44:14 INFO BlockManagerInfo: Removed broadcast_28_piece0 on 172.18.0.15:34959 in memory (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 11:44:14 INFO BlockManagerInfo: Added broadcast_30_piece0 in memory on 172.18.0.15:34959 (size: 19.5 KiB, free: 434.0 MiB)
26/02/13 11:44:14 INFO BlockManagerInfo: Added broadcast_30_piece0 in memory on 172.18.0.14:40939 (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 11:44:14 INFO BlockManagerInfo: Added broadcast_29_piece0 in memory on 172.18.0.15:34959 (size: 107.2 KiB, free: 433.9 MiB)
26/02/13 11:44:14 INFO BlockManagerInfo: Added broadcast_29_piece0 in memory on 172.18.0.14:40939 (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 11:44:14 INFO TaskSetManager: Finished task 0.0 in stage 33.0 (TID 52) in 797 ms on 172.18.0.15 (executor 0) (1/2)
26/02/13 11:44:14 INFO TaskSetManager: Finished task 1.0 in stage 33.0 (TID 53) in 829 ms on 172.18.0.14 (executor 1) (2/2)
26/02/13 11:44:14 INFO TaskSchedulerImpl: Removed TaskSet 33.0, whose tasks have all completed, from pool 
26/02/13 11:44:14 INFO DAGScheduler: ResultStage 33 (start at NativeMethodAccessorImpl.java:0) finished in 0.887 s
26/02/13 11:44:14 INFO DAGScheduler: Job 20 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:44:14 INFO TaskSchedulerImpl: Killing all running tasks in stage 33: Stage finished
26/02/13 11:44:14 INFO DAGScheduler: Job 20 finished: start at NativeMethodAccessorImpl.java:0, took 0.912304 s
26/02/13 11:44:14 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 5, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@4e18a43] is committing.
26/02/13 11:44:14 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 5, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@4e18a43] committed.
26/02/13 11:44:14 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/5 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.5.cf869bfa-596d-45a3-8811-f11a6c5304bb.tmp
26/02/13 11:44:15 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.5.cf869bfa-596d-45a3-8811-f11a6c5304bb.tmp to file:/tmp/spark-checkpoint-enrichment/commits/5
26/02/13 11:44:15 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "fc5c6452-bd70-4c90-becf-772d3bebdb57",
  "name" : null,
  "timestamp" : "2026-02-13T11:44:12.348Z",
  "batchId" : 5,
  "numInputRows" : 4,
  "inputRowsPerSecond" : 222.22222222222223,
  "processedRowsPerSecond" : 1.4738393515106853,
  "durationMs" : {
    "addBatch" : 1890,
    "commitOffsets" : 176,
    "getBatch" : 0,
    "latestOffset" : 6,
    "queryPlanning" : 459,
    "triggerExecution" : 2714,
    "walCommit" : 168
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 2954,
        "1" : 3276,
        "0" : 5128
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 2954,
        "1" : 3279,
        "0" : 5129
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 2954,
        "1" : 3279,
        "0" : 5129
      }
    },
    "numInputRows" : 4,
    "inputRowsPerSecond" : 222.22222222222223,
    "processedRowsPerSecond" : 1.4738393515106853,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@7fe9b5de",
    "numOutputRows" : 1
  }
}
26/02/13 11:44:15 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/6 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.6.1b12d0ef-eceb-4cd4-95df-48df75ce7cca.tmp
26/02/13 11:44:15 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.6.1b12d0ef-eceb-4cd4-95df-48df75ce7cca.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/6
26/02/13 11:44:15 INFO MicroBatchExecution: Committed offsets for batch 6. Metadata OffsetSeqMetadata(0,1770983055089,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 11:44:15 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:44:15 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:44:15 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:44:15 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:44:15 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:44:15 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:44:15 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#7075 - origin_code.nullCount#7074) > 0)
26/02/13 11:44:15 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#7080 - destination_code.nullCount#7079) > 0)
26/02/13 11:44:15 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#7110 - callsign.nullCount#7109) > 0)
26/02/13 11:44:15 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:44:15 INFO DAGScheduler: Got job 21 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 11:44:15 INFO DAGScheduler: Final stage: ResultStage 35 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:44:15 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 34)
26/02/13 11:44:15 INFO DAGScheduler: Missing parents: List()
26/02/13 11:44:15 INFO DAGScheduler: Submitting ResultStage 35 (MapPartitionsRDD[109] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:44:15 INFO MemoryStore: Block broadcast_31 stored as values in memory (estimated size 99.3 KiB, free 433.8 MiB)
26/02/13 11:44:15 INFO MemoryStore: Block broadcast_31_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 433.9 MiB)
26/02/13 11:44:15 INFO BlockManagerInfo: Removed broadcast_29_piece0 on spark-master:40291 in memory (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 11:44:15 INFO BlockManagerInfo: Added broadcast_31_piece0 in memory on spark-master:40291 (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 11:44:15 INFO BlockManagerInfo: Removed broadcast_29_piece0 on 172.18.0.14:40939 in memory (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 11:44:15 INFO SparkContext: Created broadcast 31 from broadcast at DAGScheduler.scala:1585
26/02/13 11:44:15 INFO BlockManagerInfo: Removed broadcast_29_piece0 on 172.18.0.15:34959 in memory (size: 107.2 KiB, free: 434.0 MiB)
26/02/13 11:44:15 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 35 (MapPartitionsRDD[109] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 11:44:15 INFO TaskSchedulerImpl: Adding task set 35.0 with 4 tasks resource profile 0
26/02/13 11:44:15 INFO TaskSetManager: Starting task 0.0 in stage 35.0 (TID 54) (172.18.0.15, executor 0, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:44:15 INFO TaskSetManager: Starting task 1.0 in stage 35.0 (TID 55) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:44:15 INFO BlockManagerInfo: Removed broadcast_30_piece0 on spark-master:40291 in memory (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 11:44:15 INFO BlockManagerInfo: Removed broadcast_30_piece0 on 172.18.0.14:40939 in memory (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 11:44:15 INFO BlockManagerInfo: Removed broadcast_30_piece0 on 172.18.0.15:34959 in memory (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 11:44:15 INFO BlockManagerInfo: Added broadcast_31_piece0 in memory on 172.18.0.15:34959 (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 11:44:15 INFO TaskSetManager: Starting task 2.0 in stage 35.0 (TID 56) (172.18.0.15, executor 0, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:44:15 INFO TaskSetManager: Finished task 0.0 in stage 35.0 (TID 54) in 127 ms on 172.18.0.15 (executor 0) (1/4)
26/02/13 11:44:15 INFO TaskSetManager: Starting task 3.0 in stage 35.0 (TID 57) (172.18.0.15, executor 0, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:44:15 INFO TaskSetManager: Finished task 1.0 in stage 35.0 (TID 55) in 129 ms on 172.18.0.15 (executor 0) (2/4)
26/02/13 11:44:16 INFO TaskSetManager: Finished task 2.0 in stage 35.0 (TID 56) in 86 ms on 172.18.0.15 (executor 0) (3/4)
26/02/13 11:44:16 INFO TaskSetManager: Finished task 3.0 in stage 35.0 (TID 57) in 84 ms on 172.18.0.15 (executor 0) (4/4)
26/02/13 11:44:16 INFO TaskSchedulerImpl: Removed TaskSet 35.0, whose tasks have all completed, from pool 
26/02/13 11:44:16 INFO DAGScheduler: ResultStage 35 (start at NativeMethodAccessorImpl.java:0) finished in 0.269 s
26/02/13 11:44:16 INFO DAGScheduler: Job 21 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:44:16 INFO TaskSchedulerImpl: Killing all running tasks in stage 35: Stage finished
26/02/13 11:44:16 INFO DAGScheduler: Job 21 finished: start at NativeMethodAccessorImpl.java:0, took 0.289352 s
26/02/13 11:44:16 INFO MemoryStore: Block broadcast_32_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.8 MiB)
26/02/13 11:44:16 INFO BlockManagerInfo: Added broadcast_32_piece0 in memory on spark-master:40291 (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 11:44:16 INFO SparkContext: Created broadcast 32 from start at NativeMethodAccessorImpl.java:0
26/02/13 11:44:16 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 6, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@6f516c56]. The input RDD has 3 partitions.
26/02/13 11:44:16 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:44:16 INFO DAGScheduler: Got job 22 (start at NativeMethodAccessorImpl.java:0) with 3 output partitions
26/02/13 11:44:16 INFO DAGScheduler: Final stage: ResultStage 36 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:44:16 INFO DAGScheduler: Parents of final stage: List()
26/02/13 11:44:16 INFO DAGScheduler: Missing parents: List()
26/02/13 11:44:16 INFO DAGScheduler: Submitting ResultStage 36 (MapPartitionsRDD[115] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:44:16 INFO MemoryStore: Block broadcast_33 stored as values in memory (estimated size 49.9 KiB, free 433.8 MiB)
26/02/13 11:44:16 INFO MemoryStore: Block broadcast_33_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.8 MiB)
26/02/13 11:44:16 INFO BlockManagerInfo: Added broadcast_33_piece0 in memory on spark-master:40291 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 11:44:16 INFO SparkContext: Created broadcast 33 from broadcast at DAGScheduler.scala:1585
26/02/13 11:44:16 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 36 (MapPartitionsRDD[115] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
26/02/13 11:44:16 INFO TaskSchedulerImpl: Adding task set 36.0 with 3 tasks resource profile 0
26/02/13 11:44:16 INFO TaskSetManager: Starting task 1.0 in stage 36.0 (TID 58) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:44:16 INFO TaskSetManager: Starting task 0.0 in stage 36.0 (TID 59) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:44:16 INFO TaskSetManager: Starting task 2.0 in stage 36.0 (TID 60) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:44:16 INFO BlockManagerInfo: Added broadcast_33_piece0 in memory on 172.18.0.15:34959 (size: 19.5 KiB, free: 434.0 MiB)
26/02/13 11:44:16 INFO BlockManagerInfo: Added broadcast_33_piece0 in memory on 172.18.0.14:40939 (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 11:44:16 INFO BlockManagerInfo: Added broadcast_32_piece0 in memory on 172.18.0.15:34959 (size: 107.2 KiB, free: 433.9 MiB)
26/02/13 11:44:16 INFO BlockManagerInfo: Added broadcast_32_piece0 in memory on 172.18.0.14:40939 (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 11:44:16 INFO TaskSetManager: Finished task 1.0 in stage 36.0 (TID 58) in 298 ms on 172.18.0.15 (executor 0) (1/3)
26/02/13 11:44:16 INFO TaskSetManager: Finished task 2.0 in stage 36.0 (TID 60) in 478 ms on 172.18.0.14 (executor 1) (2/3)
26/02/13 11:44:17 INFO TaskSetManager: Finished task 0.0 in stage 36.0 (TID 59) in 909 ms on 172.18.0.14 (executor 1) (3/3)
26/02/13 11:44:17 INFO TaskSchedulerImpl: Removed TaskSet 36.0, whose tasks have all completed, from pool 
26/02/13 11:44:17 INFO DAGScheduler: ResultStage 36 (start at NativeMethodAccessorImpl.java:0) finished in 0.924 s
26/02/13 11:44:17 INFO DAGScheduler: Job 22 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:44:17 INFO TaskSchedulerImpl: Killing all running tasks in stage 36: Stage finished
26/02/13 11:44:17 INFO DAGScheduler: Job 22 finished: start at NativeMethodAccessorImpl.java:0, took 0.934948 s
26/02/13 11:44:17 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 6, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@6f516c56] is committing.
26/02/13 11:44:17 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 6, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@6f516c56] committed.
26/02/13 11:44:17 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/6 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.6.d855fdb4-7c67-45ce-94e0-8e29bb49b2ba.tmp
26/02/13 11:44:17 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.6.d855fdb4-7c67-45ce-94e0-8e29bb49b2ba.tmp to file:/tmp/spark-checkpoint-enrichment/commits/6
26/02/13 11:44:17 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "fc5c6452-bd70-4c90-becf-772d3bebdb57",
  "name" : null,
  "timestamp" : "2026-02-13T11:44:15.076Z",
  "batchId" : 6,
  "numInputRows" : 234,
  "inputRowsPerSecond" : 85.77712609970673,
  "processedRowsPerSecond" : 111.3755354593051,
  "durationMs" : {
    "addBatch" : 1616,
    "commitOffsets" : 165,
    "getBatch" : 0,
    "latestOffset" : 13,
    "queryPlanning" : 152,
    "triggerExecution" : 2101,
    "walCommit" : 153
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 2954,
        "1" : 3279,
        "0" : 5129
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 3030,
        "1" : 3360,
        "0" : 5206
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 3030,
        "1" : 3360,
        "0" : 5206
      }
    },
    "numInputRows" : 234,
    "inputRowsPerSecond" : 85.77712609970673,
    "processedRowsPerSecond" : 111.3755354593051,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@7fe9b5de",
    "numOutputRows" : 50
  }
}
26/02/13 11:44:27 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 11:44:28 INFO BlockManagerInfo: Removed broadcast_33_piece0 on spark-master:40291 in memory (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 11:44:28 INFO BlockManagerInfo: Removed broadcast_33_piece0 on 172.18.0.14:40939 in memory (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 11:44:28 INFO BlockManagerInfo: Removed broadcast_33_piece0 on 172.18.0.15:34959 in memory (size: 19.5 KiB, free: 433.9 MiB)
26/02/13 11:44:28 INFO BlockManagerInfo: Removed broadcast_31_piece0 on spark-master:40291 in memory (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 11:44:28 INFO BlockManagerInfo: Removed broadcast_31_piece0 on 172.18.0.15:34959 in memory (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 11:44:28 INFO BlockManagerInfo: Removed broadcast_26_piece0 on spark-master:40291 in memory (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 11:44:28 INFO BlockManagerInfo: Removed broadcast_26_piece0 on 172.18.0.14:40939 in memory (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 11:44:28 INFO BlockManagerInfo: Removed broadcast_26_piece0 on 172.18.0.15:34959 in memory (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 11:44:29 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/7 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.7.47c1b68a-8719-4a6e-a957-289b1eb21884.tmp
26/02/13 11:44:29 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.7.47c1b68a-8719-4a6e-a957-289b1eb21884.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/7
26/02/13 11:44:29 INFO MicroBatchExecution: Committed offsets for batch 7. Metadata OffsetSeqMetadata(0,1770983069167,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 11:44:29 INFO BlockManagerInfo: Removed broadcast_32_piece0 on spark-master:40291 in memory (size: 107.2 KiB, free: 434.4 MiB)
26/02/13 11:44:29 INFO BlockManagerInfo: Removed broadcast_32_piece0 on 172.18.0.14:40939 in memory (size: 107.2 KiB, free: 434.4 MiB)
26/02/13 11:44:29 INFO BlockManagerInfo: Removed broadcast_32_piece0 on 172.18.0.15:34959 in memory (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 11:44:29 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:44:29 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:44:29 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:44:29 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:44:30 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:44:30 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:44:30 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#7929 - origin_code.nullCount#7928) > 0)
26/02/13 11:44:30 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#7934 - destination_code.nullCount#7933) > 0)
26/02/13 11:44:30 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#7964 - callsign.nullCount#7963) > 0)
26/02/13 11:44:30 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:44:30 INFO DAGScheduler: Got job 23 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 11:44:30 INFO DAGScheduler: Final stage: ResultStage 38 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:44:30 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 37)
26/02/13 11:44:30 INFO DAGScheduler: Missing parents: List()
26/02/13 11:44:30 INFO DAGScheduler: Submitting ResultStage 38 (MapPartitionsRDD[120] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:44:30 INFO MemoryStore: Block broadcast_34 stored as values in memory (estimated size 99.3 KiB, free 434.1 MiB)
26/02/13 11:44:30 INFO MemoryStore: Block broadcast_34_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 434.0 MiB)
26/02/13 11:44:30 INFO BlockManagerInfo: Added broadcast_34_piece0 in memory on spark-master:40291 (size: 33.3 KiB, free: 434.3 MiB)
26/02/13 11:44:30 INFO SparkContext: Created broadcast 34 from broadcast at DAGScheduler.scala:1585
26/02/13 11:44:30 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 38 (MapPartitionsRDD[120] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 11:44:30 INFO TaskSchedulerImpl: Adding task set 38.0 with 4 tasks resource profile 0
26/02/13 11:44:30 INFO TaskSetManager: Starting task 0.0 in stage 38.0 (TID 61) (172.18.0.15, executor 0, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:44:30 INFO TaskSetManager: Starting task 1.0 in stage 38.0 (TID 62) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:44:30 INFO BlockManagerInfo: Added broadcast_34_piece0 in memory on 172.18.0.15:34959 (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 11:44:30 INFO TaskSetManager: Starting task 2.0 in stage 38.0 (TID 63) (172.18.0.15, executor 0, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:44:30 INFO TaskSetManager: Starting task 3.0 in stage 38.0 (TID 64) (172.18.0.15, executor 0, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:44:30 INFO TaskSetManager: Finished task 0.0 in stage 38.0 (TID 61) in 456 ms on 172.18.0.15 (executor 0) (1/4)
26/02/13 11:44:30 INFO TaskSetManager: Finished task 1.0 in stage 38.0 (TID 62) in 452 ms on 172.18.0.15 (executor 0) (2/4)
26/02/13 11:44:30 INFO TaskSetManager: Finished task 2.0 in stage 38.0 (TID 63) in 133 ms on 172.18.0.15 (executor 0) (3/4)
26/02/13 11:44:30 INFO TaskSetManager: Finished task 3.0 in stage 38.0 (TID 64) in 171 ms on 172.18.0.15 (executor 0) (4/4)
26/02/13 11:44:30 INFO TaskSchedulerImpl: Removed TaskSet 38.0, whose tasks have all completed, from pool 
26/02/13 11:44:30 INFO DAGScheduler: ResultStage 38 (start at NativeMethodAccessorImpl.java:0) finished in 0.633 s
26/02/13 11:44:30 INFO DAGScheduler: Job 23 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:44:30 INFO TaskSchedulerImpl: Killing all running tasks in stage 38: Stage finished
26/02/13 11:44:30 INFO DAGScheduler: Job 23 finished: start at NativeMethodAccessorImpl.java:0, took 0.657246 s
26/02/13 11:44:31 INFO MemoryStore: Block broadcast_35_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.9 MiB)
26/02/13 11:44:31 INFO BlockManagerInfo: Added broadcast_35_piece0 in memory on spark-master:40291 (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 11:44:31 INFO SparkContext: Created broadcast 35 from start at NativeMethodAccessorImpl.java:0
26/02/13 11:44:31 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 7, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@394385cb]. The input RDD has 3 partitions.
26/02/13 11:44:31 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:44:31 INFO DAGScheduler: Got job 24 (start at NativeMethodAccessorImpl.java:0) with 3 output partitions
26/02/13 11:44:31 INFO DAGScheduler: Final stage: ResultStage 39 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:44:31 INFO DAGScheduler: Parents of final stage: List()
26/02/13 11:44:31 INFO DAGScheduler: Missing parents: List()
26/02/13 11:44:31 INFO DAGScheduler: Submitting ResultStage 39 (MapPartitionsRDD[126] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:44:31 INFO MemoryStore: Block broadcast_36 stored as values in memory (estimated size 49.9 KiB, free 433.9 MiB)
26/02/13 11:44:31 INFO MemoryStore: Block broadcast_36_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.9 MiB)
26/02/13 11:44:31 INFO BlockManagerInfo: Added broadcast_36_piece0 in memory on spark-master:40291 (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 11:44:31 INFO SparkContext: Created broadcast 36 from broadcast at DAGScheduler.scala:1585
26/02/13 11:44:31 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 39 (MapPartitionsRDD[126] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
26/02/13 11:44:31 INFO TaskSchedulerImpl: Adding task set 39.0 with 3 tasks resource profile 0
26/02/13 11:44:31 INFO TaskSetManager: Starting task 1.0 in stage 39.0 (TID 65) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:44:31 INFO TaskSetManager: Starting task 0.0 in stage 39.0 (TID 66) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:44:31 INFO TaskSetManager: Starting task 2.0 in stage 39.0 (TID 67) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:44:31 INFO BlockManagerInfo: Added broadcast_36_piece0 in memory on 172.18.0.14:40939 (size: 19.5 KiB, free: 434.4 MiB)
26/02/13 11:44:31 INFO BlockManagerInfo: Added broadcast_36_piece0 in memory on 172.18.0.15:34959 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 11:44:31 INFO BlockManagerInfo: Added broadcast_35_piece0 in memory on 172.18.0.15:34959 (size: 107.2 KiB, free: 434.0 MiB)
26/02/13 11:44:31 INFO BlockManagerInfo: Added broadcast_35_piece0 in memory on 172.18.0.14:40939 (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 11:44:32 INFO TaskSetManager: Finished task 1.0 in stage 39.0 (TID 65) in 961 ms on 172.18.0.15 (executor 0) (1/3)
26/02/13 11:44:32 INFO TaskSetManager: Finished task 0.0 in stage 39.0 (TID 66) in 1080 ms on 172.18.0.14 (executor 1) (2/3)
26/02/13 11:44:32 INFO TaskSetManager: Finished task 2.0 in stage 39.0 (TID 67) in 1088 ms on 172.18.0.14 (executor 1) (3/3)
26/02/13 11:44:32 INFO TaskSchedulerImpl: Removed TaskSet 39.0, whose tasks have all completed, from pool 
26/02/13 11:44:32 INFO DAGScheduler: ResultStage 39 (start at NativeMethodAccessorImpl.java:0) finished in 1.111 s
26/02/13 11:44:32 INFO DAGScheduler: Job 24 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:44:32 INFO TaskSchedulerImpl: Killing all running tasks in stage 39: Stage finished
26/02/13 11:44:32 INFO DAGScheduler: Job 24 finished: start at NativeMethodAccessorImpl.java:0, took 1.123621 s
26/02/13 11:44:32 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 7, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@394385cb] is committing.
26/02/13 11:44:32 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 7, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@394385cb] committed.
26/02/13 11:44:32 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/7 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.7.f650d502-d7be-47d9-97c8-96f23fef22ff.tmp
26/02/13 11:44:32 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.7.f650d502-d7be-47d9-97c8-96f23fef22ff.tmp to file:/tmp/spark-checkpoint-enrichment/commits/7
26/02/13 11:44:32 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "fc5c6452-bd70-4c90-becf-772d3bebdb57",
  "name" : null,
  "timestamp" : "2026-02-13T11:44:29.162Z",
  "batchId" : 7,
  "numInputRows" : 8,
  "inputRowsPerSecond" : 533.3333333333334,
  "processedRowsPerSecond" : 2.5244556642473968,
  "durationMs" : {
    "addBatch" : 2410,
    "commitOffsets" : 117,
    "getBatch" : 1,
    "latestOffset" : 4,
    "queryPlanning" : 349,
    "triggerExecution" : 3169,
    "walCommit" : 275
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 3030,
        "1" : 3360,
        "0" : 5206
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 3031,
        "1" : 3366,
        "0" : 5207
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 3031,
        "1" : 3366,
        "0" : 5207
      }
    },
    "numInputRows" : 8,
    "inputRowsPerSecond" : 533.3333333333334,
    "processedRowsPerSecond" : 2.5244556642473968,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@7fe9b5de",
    "numOutputRows" : 3
  }
}
26/02/13 11:44:32 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/8 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.8.f469d4d1-1fa3-446b-96b2-a430f70a2dca.tmp
26/02/13 11:44:32 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.8.f469d4d1-1fa3-446b-96b2-a430f70a2dca.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/8
26/02/13 11:44:32 INFO MicroBatchExecution: Committed offsets for batch 8. Metadata OffsetSeqMetadata(0,1770983072356,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 11:44:32 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:44:32 INFO BlockManagerInfo: Removed broadcast_35_piece0 on spark-master:40291 in memory (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 11:44:32 INFO BlockManagerInfo: Removed broadcast_35_piece0 on 172.18.0.14:40939 in memory (size: 107.2 KiB, free: 434.4 MiB)
26/02/13 11:44:32 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:44:32 INFO BlockManagerInfo: Removed broadcast_35_piece0 on 172.18.0.15:34959 in memory (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 11:44:32 INFO BlockManagerInfo: Removed broadcast_36_piece0 on spark-master:40291 in memory (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 11:44:32 INFO BlockManagerInfo: Removed broadcast_36_piece0 on 172.18.0.14:40939 in memory (size: 19.5 KiB, free: 434.4 MiB)
26/02/13 11:44:32 INFO BlockManagerInfo: Removed broadcast_36_piece0 on 172.18.0.15:34959 in memory (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 11:44:32 INFO BlockManagerInfo: Removed broadcast_34_piece0 on spark-master:40291 in memory (size: 33.3 KiB, free: 434.4 MiB)
26/02/13 11:44:32 INFO BlockManagerInfo: Removed broadcast_34_piece0 on 172.18.0.15:34959 in memory (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 11:44:32 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:44:32 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:44:33 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:44:33 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:44:33 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#8783 - origin_code.nullCount#8782) > 0)
26/02/13 11:44:33 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#8788 - destination_code.nullCount#8787) > 0)
26/02/13 11:44:33 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#8818 - callsign.nullCount#8817) > 0)
26/02/13 11:44:33 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:44:33 INFO DAGScheduler: Got job 25 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 11:44:33 INFO DAGScheduler: Final stage: ResultStage 41 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:44:33 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 40)
26/02/13 11:44:33 INFO DAGScheduler: Missing parents: List()
26/02/13 11:44:33 INFO DAGScheduler: Submitting ResultStage 41 (MapPartitionsRDD[131] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:44:33 INFO MemoryStore: Block broadcast_37 stored as values in memory (estimated size 99.3 KiB, free 434.1 MiB)
26/02/13 11:44:33 INFO MemoryStore: Block broadcast_37_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 434.0 MiB)
26/02/13 11:44:33 INFO BlockManagerInfo: Added broadcast_37_piece0 in memory on spark-master:40291 (size: 33.3 KiB, free: 434.3 MiB)
26/02/13 11:44:33 INFO SparkContext: Created broadcast 37 from broadcast at DAGScheduler.scala:1585
26/02/13 11:44:33 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 41 (MapPartitionsRDD[131] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 11:44:33 INFO TaskSchedulerImpl: Adding task set 41.0 with 4 tasks resource profile 0
26/02/13 11:44:33 INFO TaskSetManager: Starting task 0.0 in stage 41.0 (TID 68) (172.18.0.15, executor 0, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:44:33 INFO TaskSetManager: Starting task 1.0 in stage 41.0 (TID 69) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:44:33 INFO BlockManagerInfo: Added broadcast_37_piece0 in memory on 172.18.0.15:34959 (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 11:44:33 INFO TaskSetManager: Starting task 2.0 in stage 41.0 (TID 70) (172.18.0.15, executor 0, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:44:33 INFO TaskSetManager: Finished task 0.0 in stage 41.0 (TID 68) in 97 ms on 172.18.0.15 (executor 0) (1/4)
26/02/13 11:44:33 INFO TaskSetManager: Starting task 3.0 in stage 41.0 (TID 71) (172.18.0.15, executor 0, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:44:33 INFO TaskSetManager: Finished task 1.0 in stage 41.0 (TID 69) in 112 ms on 172.18.0.15 (executor 0) (2/4)
26/02/13 11:44:33 INFO TaskSetManager: Finished task 2.0 in stage 41.0 (TID 70) in 86 ms on 172.18.0.15 (executor 0) (3/4)
26/02/13 11:44:33 INFO TaskSetManager: Finished task 3.0 in stage 41.0 (TID 71) in 203 ms on 172.18.0.15 (executor 0) (4/4)
26/02/13 11:44:33 INFO TaskSchedulerImpl: Removed TaskSet 41.0, whose tasks have all completed, from pool 
26/02/13 11:44:33 INFO DAGScheduler: ResultStage 41 (start at NativeMethodAccessorImpl.java:0) finished in 0.346 s
26/02/13 11:44:33 INFO DAGScheduler: Job 25 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:44:33 INFO TaskSchedulerImpl: Killing all running tasks in stage 41: Stage finished
26/02/13 11:44:33 INFO DAGScheduler: Job 25 finished: start at NativeMethodAccessorImpl.java:0, took 0.362130 s
26/02/13 11:44:33 INFO MemoryStore: Block broadcast_38_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.9 MiB)
26/02/13 11:44:33 INFO BlockManagerInfo: Added broadcast_38_piece0 in memory on spark-master:40291 (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 11:44:33 INFO SparkContext: Created broadcast 38 from start at NativeMethodAccessorImpl.java:0
26/02/13 11:44:33 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 8, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@4f7c070b]. The input RDD has 3 partitions.
26/02/13 11:44:33 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:44:33 INFO DAGScheduler: Got job 26 (start at NativeMethodAccessorImpl.java:0) with 3 output partitions
26/02/13 11:44:33 INFO DAGScheduler: Final stage: ResultStage 42 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:44:33 INFO DAGScheduler: Parents of final stage: List()
26/02/13 11:44:33 INFO DAGScheduler: Missing parents: List()
26/02/13 11:44:33 INFO DAGScheduler: Submitting ResultStage 42 (MapPartitionsRDD[137] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:44:33 INFO MemoryStore: Block broadcast_39 stored as values in memory (estimated size 49.9 KiB, free 433.9 MiB)
26/02/13 11:44:33 INFO MemoryStore: Block broadcast_39_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.9 MiB)
26/02/13 11:44:33 INFO BlockManagerInfo: Added broadcast_39_piece0 in memory on spark-master:40291 (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 11:44:33 INFO SparkContext: Created broadcast 39 from broadcast at DAGScheduler.scala:1585
26/02/13 11:44:33 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 42 (MapPartitionsRDD[137] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
26/02/13 11:44:33 INFO TaskSchedulerImpl: Adding task set 42.0 with 3 tasks resource profile 0
26/02/13 11:44:33 INFO TaskSetManager: Starting task 0.0 in stage 42.0 (TID 72) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:44:33 INFO TaskSetManager: Starting task 1.0 in stage 42.0 (TID 73) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:44:33 INFO TaskSetManager: Starting task 2.0 in stage 42.0 (TID 74) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:44:33 INFO BlockManagerInfo: Added broadcast_39_piece0 in memory on 172.18.0.15:34959 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 11:44:33 INFO BlockManagerInfo: Added broadcast_38_piece0 in memory on 172.18.0.15:34959 (size: 107.2 KiB, free: 434.0 MiB)
26/02/13 11:44:33 INFO BlockManagerInfo: Added broadcast_39_piece0 in memory on 172.18.0.14:40939 (size: 19.5 KiB, free: 434.4 MiB)
26/02/13 11:44:33 INFO BlockManagerInfo: Added broadcast_38_piece0 in memory on 172.18.0.14:40939 (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 11:44:34 INFO TaskSetManager: Finished task 1.0 in stage 42.0 (TID 73) in 601 ms on 172.18.0.15 (executor 0) (1/3)
26/02/13 11:44:34 INFO TaskSetManager: Finished task 2.0 in stage 42.0 (TID 74) in 674 ms on 172.18.0.14 (executor 1) (2/3)
26/02/13 11:44:34 INFO TaskSetManager: Finished task 0.0 in stage 42.0 (TID 72) in 686 ms on 172.18.0.14 (executor 1) (3/3)
26/02/13 11:44:34 INFO TaskSchedulerImpl: Removed TaskSet 42.0, whose tasks have all completed, from pool 
26/02/13 11:44:34 INFO DAGScheduler: ResultStage 42 (start at NativeMethodAccessorImpl.java:0) finished in 0.704 s
26/02/13 11:44:34 INFO DAGScheduler: Job 26 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:44:34 INFO TaskSchedulerImpl: Killing all running tasks in stage 42: Stage finished
26/02/13 11:44:34 INFO DAGScheduler: Job 26 finished: start at NativeMethodAccessorImpl.java:0, took 0.712881 s
26/02/13 11:44:34 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 8, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@4f7c070b] is committing.
26/02/13 11:44:34 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 8, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@4f7c070b] committed.
26/02/13 11:44:34 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/8 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.8.2ede9530-371e-46f6-8c56-428b253d8dfb.tmp
26/02/13 11:44:34 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.8.2ede9530-371e-46f6-8c56-428b253d8dfb.tmp to file:/tmp/spark-checkpoint-enrichment/commits/8
26/02/13 11:44:34 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "fc5c6452-bd70-4c90-becf-772d3bebdb57",
  "name" : null,
  "timestamp" : "2026-02-13T11:44:32.347Z",
  "batchId" : 8,
  "numInputRows" : 230,
  "inputRowsPerSecond" : 72.21350078492935,
  "processedRowsPerSecond" : 110.47070124879924,
  "durationMs" : {
    "addBatch" : 1665,
    "commitOffsets" : 110,
    "getBatch" : 0,
    "latestOffset" : 9,
    "queryPlanning" : 145,
    "triggerExecution" : 2082,
    "walCommit" : 150
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 3031,
        "1" : 3366,
        "0" : 5207
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 3106,
        "1" : 3444,
        "0" : 5284
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 3106,
        "1" : 3444,
        "0" : 5284
      }
    },
    "numInputRows" : 230,
    "inputRowsPerSecond" : 72.21350078492935,
    "processedRowsPerSecond" : 110.47070124879924,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@7fe9b5de",
    "numOutputRows" : 48
  }
}
26/02/13 11:44:43 INFO BlockManagerInfo: Removed broadcast_37_piece0 on spark-master:40291 in memory (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 11:44:43 INFO BlockManagerInfo: Removed broadcast_37_piece0 on 172.18.0.15:34959 in memory (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 11:44:43 INFO BlockManagerInfo: Removed broadcast_39_piece0 on spark-master:40291 in memory (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 11:44:43 INFO BlockManagerInfo: Removed broadcast_39_piece0 on 172.18.0.15:34959 in memory (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 11:44:43 INFO BlockManagerInfo: Removed broadcast_39_piece0 on 172.18.0.14:40939 in memory (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 11:44:44 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 11:44:45 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/9 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.9.1b2e8d3d-ddf4-4a46-9295-9996f5073f36.tmp
26/02/13 11:44:45 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.9.1b2e8d3d-ddf4-4a46-9295-9996f5073f36.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/9
26/02/13 11:44:45 INFO MicroBatchExecution: Committed offsets for batch 9. Metadata OffsetSeqMetadata(0,1770983085550,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 11:44:45 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:44:45 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:44:45 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:44:45 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:44:46 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:44:46 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:44:46 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#9637 - origin_code.nullCount#9636) > 0)
26/02/13 11:44:46 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#9642 - destination_code.nullCount#9641) > 0)
26/02/13 11:44:46 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#9672 - callsign.nullCount#9671) > 0)
26/02/13 11:44:46 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:44:46 INFO DAGScheduler: Got job 27 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 11:44:46 INFO DAGScheduler: Final stage: ResultStage 44 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:44:46 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 43)
26/02/13 11:44:46 INFO DAGScheduler: Missing parents: List()
26/02/13 11:44:46 INFO DAGScheduler: Submitting ResultStage 44 (MapPartitionsRDD[142] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:44:46 INFO MemoryStore: Block broadcast_40 stored as values in memory (estimated size 99.3 KiB, free 434.0 MiB)
26/02/13 11:44:46 INFO MemoryStore: Block broadcast_40_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 433.9 MiB)
26/02/13 11:44:46 INFO BlockManagerInfo: Added broadcast_40_piece0 in memory on spark-master:40291 (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 11:44:46 INFO SparkContext: Created broadcast 40 from broadcast at DAGScheduler.scala:1585
26/02/13 11:44:46 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 44 (MapPartitionsRDD[142] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 11:44:46 INFO TaskSchedulerImpl: Adding task set 44.0 with 4 tasks resource profile 0
26/02/13 11:44:46 INFO TaskSetManager: Starting task 0.0 in stage 44.0 (TID 75) (172.18.0.15, executor 0, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:44:46 INFO TaskSetManager: Starting task 1.0 in stage 44.0 (TID 76) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:44:46 INFO BlockManagerInfo: Added broadcast_40_piece0 in memory on 172.18.0.15:34959 (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 11:44:46 INFO TaskSetManager: Starting task 2.0 in stage 44.0 (TID 77) (172.18.0.15, executor 0, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:44:46 INFO TaskSetManager: Starting task 3.0 in stage 44.0 (TID 78) (172.18.0.15, executor 0, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:44:46 INFO TaskSetManager: Finished task 0.0 in stage 44.0 (TID 75) in 146 ms on 172.18.0.15 (executor 0) (1/4)
26/02/13 11:44:46 INFO TaskSetManager: Finished task 1.0 in stage 44.0 (TID 76) in 146 ms on 172.18.0.15 (executor 0) (2/4)
26/02/13 11:44:46 INFO TaskSetManager: Finished task 2.0 in stage 44.0 (TID 77) in 49 ms on 172.18.0.15 (executor 0) (3/4)
26/02/13 11:44:46 INFO TaskSetManager: Finished task 3.0 in stage 44.0 (TID 78) in 71 ms on 172.18.0.15 (executor 0) (4/4)
26/02/13 11:44:46 INFO TaskSchedulerImpl: Removed TaskSet 44.0, whose tasks have all completed, from pool 
26/02/13 11:44:46 INFO DAGScheduler: ResultStage 44 (start at NativeMethodAccessorImpl.java:0) finished in 0.255 s
26/02/13 11:44:46 INFO DAGScheduler: Job 27 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:44:46 INFO TaskSchedulerImpl: Killing all running tasks in stage 44: Stage finished
26/02/13 11:44:46 INFO DAGScheduler: Job 27 finished: start at NativeMethodAccessorImpl.java:0, took 0.281724 s
26/02/13 11:44:46 INFO MemoryStore: Block broadcast_41_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.8 MiB)
26/02/13 11:44:46 INFO BlockManagerInfo: Added broadcast_41_piece0 in memory on spark-master:40291 (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 11:44:46 INFO SparkContext: Created broadcast 41 from start at NativeMethodAccessorImpl.java:0
26/02/13 11:44:46 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 9, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@2f392d51]. The input RDD has 1 partitions.
26/02/13 11:44:46 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:44:46 INFO DAGScheduler: Got job 28 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/02/13 11:44:46 INFO DAGScheduler: Final stage: ResultStage 45 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:44:46 INFO DAGScheduler: Parents of final stage: List()
26/02/13 11:44:46 INFO DAGScheduler: Missing parents: List()
26/02/13 11:44:46 INFO DAGScheduler: Submitting ResultStage 45 (MapPartitionsRDD[148] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:44:46 INFO MemoryStore: Block broadcast_42 stored as values in memory (estimated size 49.9 KiB, free 433.8 MiB)
26/02/13 11:44:46 INFO MemoryStore: Block broadcast_42_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.8 MiB)
26/02/13 11:44:46 INFO BlockManagerInfo: Added broadcast_42_piece0 in memory on spark-master:40291 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 11:44:46 INFO SparkContext: Created broadcast 42 from broadcast at DAGScheduler.scala:1585
26/02/13 11:44:46 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 45 (MapPartitionsRDD[148] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/02/13 11:44:46 INFO TaskSchedulerImpl: Adding task set 45.0 with 1 tasks resource profile 0
26/02/13 11:44:46 INFO TaskSetManager: Starting task 0.0 in stage 45.0 (TID 79) (172.18.0.15, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:44:46 INFO BlockManagerInfo: Added broadcast_42_piece0 in memory on 172.18.0.15:34959 (size: 19.5 KiB, free: 434.0 MiB)
26/02/13 11:44:46 INFO BlockManagerInfo: Added broadcast_41_piece0 in memory on 172.18.0.15:34959 (size: 107.2 KiB, free: 433.9 MiB)
26/02/13 11:44:47 INFO TaskSetManager: Finished task 0.0 in stage 45.0 (TID 79) in 728 ms on 172.18.0.15 (executor 0) (1/1)
26/02/13 11:44:47 INFO TaskSchedulerImpl: Removed TaskSet 45.0, whose tasks have all completed, from pool 
26/02/13 11:44:47 INFO DAGScheduler: ResultStage 45 (start at NativeMethodAccessorImpl.java:0) finished in 0.758 s
26/02/13 11:44:47 INFO DAGScheduler: Job 28 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:44:47 INFO TaskSchedulerImpl: Killing all running tasks in stage 45: Stage finished
26/02/13 11:44:47 INFO DAGScheduler: Job 28 finished: start at NativeMethodAccessorImpl.java:0, took 0.764743 s
26/02/13 11:44:47 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 9, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@2f392d51] is committing.
26/02/13 11:44:47 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 9, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@2f392d51] committed.
26/02/13 11:44:47 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/9 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.9.818fcba7-a945-463c-a2ac-99e99852be0d.tmp
26/02/13 11:44:47 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.9.818fcba7-a945-463c-a2ac-99e99852be0d.tmp to file:/tmp/spark-checkpoint-enrichment/commits/9
26/02/13 11:44:47 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "fc5c6452-bd70-4c90-becf-772d3bebdb57",
  "name" : null,
  "timestamp" : "2026-02-13T11:44:45.542Z",
  "batchId" : 9,
  "numInputRows" : 5,
  "inputRowsPerSecond" : 263.15789473684214,
  "processedRowsPerSecond" : 2.598752598752599,
  "durationMs" : {
    "addBatch" : 1437,
    "commitOffsets" : 184,
    "getBatch" : 0,
    "latestOffset" : 8,
    "queryPlanning" : 136,
    "triggerExecution" : 1924,
    "walCommit" : 158
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 3106,
        "1" : 3444,
        "0" : 5284
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 3106,
        "1" : 3449,
        "0" : 5284
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 3106,
        "1" : 3449,
        "0" : 5284
      }
    },
    "numInputRows" : 5,
    "inputRowsPerSecond" : 263.15789473684214,
    "processedRowsPerSecond" : 2.598752598752599,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@7fe9b5de",
    "numOutputRows" : 1
  }
}
26/02/13 11:44:47 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/10 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.10.febe525e-9f62-4ca7-87e0-7ff05e87e18a.tmp
26/02/13 11:44:47 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.10.febe525e-9f62-4ca7-87e0-7ff05e87e18a.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/10
26/02/13 11:44:47 INFO MicroBatchExecution: Committed offsets for batch 10. Metadata OffsetSeqMetadata(0,1770983087477,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 11:44:47 INFO BlockManagerInfo: Removed broadcast_40_piece0 on spark-master:40291 in memory (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 11:44:47 INFO BlockManagerInfo: Removed broadcast_40_piece0 on 172.18.0.15:34959 in memory (size: 33.3 KiB, free: 433.9 MiB)
26/02/13 11:44:47 INFO BlockManagerInfo: Removed broadcast_42_piece0 on spark-master:40291 in memory (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 11:44:47 INFO BlockManagerInfo: Removed broadcast_42_piece0 on 172.18.0.15:34959 in memory (size: 19.5 KiB, free: 434.0 MiB)
26/02/13 11:44:47 INFO BlockManagerInfo: Removed broadcast_41_piece0 on spark-master:40291 in memory (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 11:44:47 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:44:47 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:44:48 INFO BlockManagerInfo: Removed broadcast_41_piece0 on 172.18.0.15:34959 in memory (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 11:44:48 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:44:48 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:44:48 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:44:48 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:44:48 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#10491 - origin_code.nullCount#10490) > 0)
26/02/13 11:44:48 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#10496 - destination_code.nullCount#10495) > 0)
26/02/13 11:44:48 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#10526 - callsign.nullCount#10525) > 0)
26/02/13 11:44:48 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:44:48 INFO DAGScheduler: Got job 29 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 11:44:48 INFO DAGScheduler: Final stage: ResultStage 47 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:44:48 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 46)
26/02/13 11:44:48 INFO DAGScheduler: Missing parents: List()
26/02/13 11:44:48 INFO DAGScheduler: Submitting ResultStage 47 (MapPartitionsRDD[153] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:44:48 INFO MemoryStore: Block broadcast_43 stored as values in memory (estimated size 99.3 KiB, free 434.0 MiB)
26/02/13 11:44:48 INFO MemoryStore: Block broadcast_43_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 433.9 MiB)
26/02/13 11:44:48 INFO BlockManagerInfo: Added broadcast_43_piece0 in memory on spark-master:40291 (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 11:44:48 INFO SparkContext: Created broadcast 43 from broadcast at DAGScheduler.scala:1585
26/02/13 11:44:48 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 47 (MapPartitionsRDD[153] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 11:44:48 INFO TaskSchedulerImpl: Adding task set 47.0 with 4 tasks resource profile 0
26/02/13 11:44:48 INFO TaskSetManager: Starting task 0.0 in stage 47.0 (TID 80) (172.18.0.15, executor 0, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:44:48 INFO TaskSetManager: Starting task 1.0 in stage 47.0 (TID 81) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:44:48 INFO BlockManagerInfo: Added broadcast_43_piece0 in memory on 172.18.0.15:34959 (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 11:44:48 INFO TaskSetManager: Starting task 2.0 in stage 47.0 (TID 82) (172.18.0.15, executor 0, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:44:48 INFO TaskSetManager: Starting task 3.0 in stage 47.0 (TID 83) (172.18.0.15, executor 0, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:44:48 INFO TaskSetManager: Finished task 1.0 in stage 47.0 (TID 81) in 125 ms on 172.18.0.15 (executor 0) (1/4)
26/02/13 11:44:48 INFO TaskSetManager: Finished task 0.0 in stage 47.0 (TID 80) in 131 ms on 172.18.0.15 (executor 0) (2/4)
26/02/13 11:44:48 INFO TaskSetManager: Finished task 3.0 in stage 47.0 (TID 83) in 37 ms on 172.18.0.15 (executor 0) (3/4)
26/02/13 11:44:48 INFO TaskSetManager: Finished task 2.0 in stage 47.0 (TID 82) in 68 ms on 172.18.0.15 (executor 0) (4/4)
26/02/13 11:44:48 INFO TaskSchedulerImpl: Removed TaskSet 47.0, whose tasks have all completed, from pool 
26/02/13 11:44:48 INFO DAGScheduler: ResultStage 47 (start at NativeMethodAccessorImpl.java:0) finished in 0.246 s
26/02/13 11:44:48 INFO DAGScheduler: Job 29 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:44:48 INFO TaskSchedulerImpl: Killing all running tasks in stage 47: Stage finished
26/02/13 11:44:48 INFO DAGScheduler: Job 29 finished: start at NativeMethodAccessorImpl.java:0, took 0.270913 s
26/02/13 11:44:48 INFO MemoryStore: Block broadcast_44_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.8 MiB)
26/02/13 11:44:48 INFO BlockManagerInfo: Added broadcast_44_piece0 in memory on spark-master:40291 (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 11:44:48 INFO SparkContext: Created broadcast 44 from start at NativeMethodAccessorImpl.java:0
26/02/13 11:44:48 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 10, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@608e6b9e]. The input RDD has 3 partitions.
26/02/13 11:44:48 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:44:48 INFO DAGScheduler: Got job 30 (start at NativeMethodAccessorImpl.java:0) with 3 output partitions
26/02/13 11:44:48 INFO DAGScheduler: Final stage: ResultStage 48 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:44:48 INFO DAGScheduler: Parents of final stage: List()
26/02/13 11:44:48 INFO DAGScheduler: Missing parents: List()
26/02/13 11:44:48 INFO DAGScheduler: Submitting ResultStage 48 (MapPartitionsRDD[159] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:44:48 INFO MemoryStore: Block broadcast_45 stored as values in memory (estimated size 49.9 KiB, free 433.8 MiB)
26/02/13 11:44:48 INFO MemoryStore: Block broadcast_45_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.8 MiB)
26/02/13 11:44:48 INFO BlockManagerInfo: Added broadcast_45_piece0 in memory on spark-master:40291 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 11:44:48 INFO SparkContext: Created broadcast 45 from broadcast at DAGScheduler.scala:1585
26/02/13 11:44:48 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 48 (MapPartitionsRDD[159] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
26/02/13 11:44:48 INFO TaskSchedulerImpl: Adding task set 48.0 with 3 tasks resource profile 0
26/02/13 11:44:48 INFO TaskSetManager: Starting task 1.0 in stage 48.0 (TID 84) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:44:48 INFO TaskSetManager: Starting task 0.0 in stage 48.0 (TID 85) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:44:48 INFO TaskSetManager: Starting task 2.0 in stage 48.0 (TID 86) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:44:48 INFO BlockManagerInfo: Added broadcast_45_piece0 in memory on 172.18.0.15:34959 (size: 19.5 KiB, free: 434.0 MiB)
26/02/13 11:44:48 INFO BlockManagerInfo: Added broadcast_45_piece0 in memory on 172.18.0.14:40939 (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 11:44:48 INFO BlockManagerInfo: Added broadcast_44_piece0 in memory on 172.18.0.15:34959 (size: 107.2 KiB, free: 433.9 MiB)
26/02/13 11:44:49 INFO BlockManagerInfo: Added broadcast_44_piece0 in memory on 172.18.0.14:40939 (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 11:44:49 INFO TaskSetManager: Finished task 1.0 in stage 48.0 (TID 84) in 304 ms on 172.18.0.15 (executor 0) (1/3)
26/02/13 11:44:49 INFO TaskSetManager: Finished task 2.0 in stage 48.0 (TID 86) in 1114 ms on 172.18.0.14 (executor 1) (2/3)
26/02/13 11:44:50 INFO TaskSetManager: Finished task 0.0 in stage 48.0 (TID 85) in 1194 ms on 172.18.0.14 (executor 1) (3/3)
26/02/13 11:44:50 INFO TaskSchedulerImpl: Removed TaskSet 48.0, whose tasks have all completed, from pool 
26/02/13 11:44:50 INFO DAGScheduler: ResultStage 48 (start at NativeMethodAccessorImpl.java:0) finished in 1.217 s
26/02/13 11:44:50 INFO DAGScheduler: Job 30 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:44:50 INFO TaskSchedulerImpl: Killing all running tasks in stage 48: Stage finished
26/02/13 11:44:50 INFO DAGScheduler: Job 30 finished: start at NativeMethodAccessorImpl.java:0, took 1.242161 s
26/02/13 11:44:50 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 10, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@608e6b9e] is committing.
26/02/13 11:44:50 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 10, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@608e6b9e] committed.
26/02/13 11:44:50 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/10 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.10.9f80bea8-2b4e-4d68-b977-b7371dc9eda8.tmp
26/02/13 11:44:50 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.10.9f80bea8-2b4e-4d68-b977-b7371dc9eda8.tmp to file:/tmp/spark-checkpoint-enrichment/commits/10
26/02/13 11:44:50 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "fc5c6452-bd70-4c90-becf-772d3bebdb57",
  "name" : null,
  "timestamp" : "2026-02-13T11:44:47.468Z",
  "batchId" : 10,
  "numInputRows" : 230,
  "inputRowsPerSecond" : 119.41848390446522,
  "processedRowsPerSecond" : 82.17220435869953,
  "durationMs" : {
    "addBatch" : 2053,
    "commitOffsets" : 223,
    "getBatch" : 0,
    "latestOffset" : 9,
    "queryPlanning" : 258,
    "triggerExecution" : 2799,
    "walCommit" : 254
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 3106,
        "1" : 3449,
        "0" : 5284
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 3182,
        "1" : 3526,
        "0" : 5361
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 3182,
        "1" : 3526,
        "0" : 5361
      }
    },
    "numInputRows" : 230,
    "inputRowsPerSecond" : 119.41848390446522,
    "processedRowsPerSecond" : 82.17220435869953,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@7fe9b5de",
    "numOutputRows" : 48
  }
}
26/02/13 11:45:00 INFO BlockManagerInfo: Removed broadcast_45_piece0 on spark-master:40291 in memory (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 11:45:00 INFO BlockManagerInfo: Removed broadcast_45_piece0 on 172.18.0.14:40939 in memory (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 11:45:00 INFO BlockManagerInfo: Removed broadcast_45_piece0 on 172.18.0.15:34959 in memory (size: 19.5 KiB, free: 433.9 MiB)
26/02/13 11:45:00 INFO BlockManagerInfo: Removed broadcast_43_piece0 on spark-master:40291 in memory (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 11:45:00 INFO BlockManagerInfo: Removed broadcast_43_piece0 on 172.18.0.15:34959 in memory (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 11:45:00 INFO BlockManagerInfo: Removed broadcast_38_piece0 on spark-master:40291 in memory (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 11:45:00 INFO BlockManagerInfo: Removed broadcast_38_piece0 on 172.18.0.14:40939 in memory (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 11:45:00 INFO BlockManagerInfo: Removed broadcast_38_piece0 on 172.18.0.15:34959 in memory (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 11:45:00 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 11:45:01 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/11 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.11.7e498a65-5856-4d71-add7-1c32c3d636a8.tmp
26/02/13 11:45:01 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.11.7e498a65-5856-4d71-add7-1c32c3d636a8.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/11
26/02/13 11:45:01 INFO MicroBatchExecution: Committed offsets for batch 11. Metadata OffsetSeqMetadata(0,1770983101432,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 11:45:01 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:45:01 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:45:01 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:45:01 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:45:01 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:45:01 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:45:01 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#11345 - origin_code.nullCount#11344) > 0)
26/02/13 11:45:01 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#11350 - destination_code.nullCount#11349) > 0)
26/02/13 11:45:01 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#11380 - callsign.nullCount#11379) > 0)
26/02/13 11:45:01 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:45:01 INFO DAGScheduler: Got job 31 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 11:45:01 INFO DAGScheduler: Final stage: ResultStage 50 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:45:01 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 49)
26/02/13 11:45:01 INFO DAGScheduler: Missing parents: List()
26/02/13 11:45:01 INFO DAGScheduler: Submitting ResultStage 50 (MapPartitionsRDD[164] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:45:01 INFO MemoryStore: Block broadcast_46 stored as values in memory (estimated size 99.3 KiB, free 434.0 MiB)
26/02/13 11:45:01 INFO MemoryStore: Block broadcast_46_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 433.9 MiB)
26/02/13 11:45:01 INFO BlockManagerInfo: Added broadcast_46_piece0 in memory on spark-master:40291 (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 11:45:01 INFO SparkContext: Created broadcast 46 from broadcast at DAGScheduler.scala:1585
26/02/13 11:45:01 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 50 (MapPartitionsRDD[164] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 11:45:01 INFO TaskSchedulerImpl: Adding task set 50.0 with 4 tasks resource profile 0
26/02/13 11:45:01 INFO TaskSetManager: Starting task 0.0 in stage 50.0 (TID 87) (172.18.0.15, executor 0, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:45:01 INFO TaskSetManager: Starting task 1.0 in stage 50.0 (TID 88) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:45:01 INFO BlockManagerInfo: Added broadcast_46_piece0 in memory on 172.18.0.15:34959 (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 11:45:01 INFO TaskSetManager: Starting task 2.0 in stage 50.0 (TID 89) (172.18.0.15, executor 0, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:45:01 INFO TaskSetManager: Starting task 3.0 in stage 50.0 (TID 90) (172.18.0.15, executor 0, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:45:01 INFO TaskSetManager: Finished task 0.0 in stage 50.0 (TID 87) in 73 ms on 172.18.0.15 (executor 0) (1/4)
26/02/13 11:45:01 INFO TaskSetManager: Finished task 1.0 in stage 50.0 (TID 88) in 73 ms on 172.18.0.15 (executor 0) (2/4)
26/02/13 11:45:01 INFO TaskSetManager: Finished task 2.0 in stage 50.0 (TID 89) in 25 ms on 172.18.0.15 (executor 0) (3/4)
26/02/13 11:45:01 INFO TaskSetManager: Finished task 3.0 in stage 50.0 (TID 90) in 37 ms on 172.18.0.15 (executor 0) (4/4)
26/02/13 11:45:01 INFO DAGScheduler: ResultStage 50 (start at NativeMethodAccessorImpl.java:0) finished in 0.127 s
26/02/13 11:45:01 INFO TaskSchedulerImpl: Removed TaskSet 50.0, whose tasks have all completed, from pool 
26/02/13 11:45:01 INFO DAGScheduler: Job 31 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:45:01 INFO TaskSchedulerImpl: Killing all running tasks in stage 50: Stage finished
26/02/13 11:45:01 INFO DAGScheduler: Job 31 finished: start at NativeMethodAccessorImpl.java:0, took 0.130869 s
26/02/13 11:45:01 INFO MemoryStore: Block broadcast_47_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.8 MiB)
26/02/13 11:45:01 INFO BlockManagerInfo: Added broadcast_47_piece0 in memory on spark-master:40291 (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 11:45:01 INFO SparkContext: Created broadcast 47 from start at NativeMethodAccessorImpl.java:0
26/02/13 11:45:01 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 11, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@62719f0f]. The input RDD has 3 partitions.
26/02/13 11:45:01 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:45:01 INFO DAGScheduler: Got job 32 (start at NativeMethodAccessorImpl.java:0) with 3 output partitions
26/02/13 11:45:01 INFO DAGScheduler: Final stage: ResultStage 51 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:45:01 INFO DAGScheduler: Parents of final stage: List()
26/02/13 11:45:01 INFO DAGScheduler: Missing parents: List()
26/02/13 11:45:01 INFO DAGScheduler: Submitting ResultStage 51 (MapPartitionsRDD[170] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:45:01 INFO MemoryStore: Block broadcast_48 stored as values in memory (estimated size 49.9 KiB, free 433.8 MiB)
26/02/13 11:45:01 INFO MemoryStore: Block broadcast_48_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.8 MiB)
26/02/13 11:45:01 INFO BlockManagerInfo: Added broadcast_48_piece0 in memory on spark-master:40291 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 11:45:01 INFO SparkContext: Created broadcast 48 from broadcast at DAGScheduler.scala:1585
26/02/13 11:45:01 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 51 (MapPartitionsRDD[170] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
26/02/13 11:45:01 INFO TaskSchedulerImpl: Adding task set 51.0 with 3 tasks resource profile 0
26/02/13 11:45:01 INFO TaskSetManager: Starting task 1.0 in stage 51.0 (TID 91) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:45:01 INFO TaskSetManager: Starting task 0.0 in stage 51.0 (TID 92) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:45:01 INFO TaskSetManager: Starting task 2.0 in stage 51.0 (TID 93) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:45:01 INFO BlockManagerInfo: Added broadcast_48_piece0 in memory on 172.18.0.15:34959 (size: 19.5 KiB, free: 434.0 MiB)
26/02/13 11:45:01 INFO BlockManagerInfo: Added broadcast_48_piece0 in memory on 172.18.0.14:40939 (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 11:45:01 INFO BlockManagerInfo: Added broadcast_47_piece0 in memory on 172.18.0.15:34959 (size: 107.2 KiB, free: 433.9 MiB)
26/02/13 11:45:02 INFO BlockManagerInfo: Added broadcast_47_piece0 in memory on 172.18.0.14:40939 (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 11:45:02 INFO TaskSetManager: Finished task 1.0 in stage 51.0 (TID 91) in 677 ms on 172.18.0.15 (executor 0) (1/3)
26/02/13 11:45:02 INFO TaskSetManager: Finished task 0.0 in stage 51.0 (TID 92) in 755 ms on 172.18.0.14 (executor 1) (2/3)
26/02/13 11:45:02 INFO TaskSetManager: Finished task 2.0 in stage 51.0 (TID 93) in 759 ms on 172.18.0.14 (executor 1) (3/3)
26/02/13 11:45:02 INFO TaskSchedulerImpl: Removed TaskSet 51.0, whose tasks have all completed, from pool 
26/02/13 11:45:02 INFO DAGScheduler: ResultStage 51 (start at NativeMethodAccessorImpl.java:0) finished in 0.768 s
26/02/13 11:45:02 INFO DAGScheduler: Job 32 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:45:02 INFO TaskSchedulerImpl: Killing all running tasks in stage 51: Stage finished
26/02/13 11:45:02 INFO DAGScheduler: Job 32 finished: start at NativeMethodAccessorImpl.java:0, took 0.771569 s
26/02/13 11:45:02 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 11, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@62719f0f] is committing.
26/02/13 11:45:02 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 11, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@62719f0f] committed.
26/02/13 11:45:02 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/11 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.11.8fab3fe2-8f63-4cf0-846d-20fd6bd17d11.tmp
26/02/13 11:45:02 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.11.8fab3fe2-8f63-4cf0-846d-20fd6bd17d11.tmp to file:/tmp/spark-checkpoint-enrichment/commits/11
26/02/13 11:45:02 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "fc5c6452-bd70-4c90-becf-772d3bebdb57",
  "name" : null,
  "timestamp" : "2026-02-13T11:45:01.430Z",
  "batchId" : 11,
  "numInputRows" : 108,
  "inputRowsPerSecond" : 8307.692307692309,
  "processedRowsPerSecond" : 79.88165680473372,
  "durationMs" : {
    "addBatch" : 1111,
    "commitOffsets" : 83,
    "getBatch" : 0,
    "latestOffset" : 2,
    "queryPlanning" : 75,
    "triggerExecution" : 1352,
    "walCommit" : 79
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 3182,
        "1" : 3526,
        "0" : 5361
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 3216,
        "1" : 3563,
        "0" : 5398
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 3216,
        "1" : 3563,
        "0" : 5398
      }
    },
    "numInputRows" : 108,
    "inputRowsPerSecond" : 8307.692307692309,
    "processedRowsPerSecond" : 79.88165680473372,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@7fe9b5de",
    "numOutputRows" : 23
  }
}
26/02/13 11:45:02 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/12 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.12.ba04b08b-be9c-4eaf-9c3f-aafb7f278a50.tmp
26/02/13 11:45:02 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.12.ba04b08b-be9c-4eaf-9c3f-aafb7f278a50.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/12
26/02/13 11:45:02 INFO MicroBatchExecution: Committed offsets for batch 12. Metadata OffsetSeqMetadata(0,1770983102785,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 11:45:02 INFO BlockManagerInfo: Removed broadcast_46_piece0 on spark-master:40291 in memory (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 11:45:02 INFO BlockManagerInfo: Removed broadcast_46_piece0 on 172.18.0.15:34959 in memory (size: 33.3 KiB, free: 433.9 MiB)
26/02/13 11:45:02 INFO BlockManagerInfo: Removed broadcast_47_piece0 on spark-master:40291 in memory (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 11:45:02 INFO BlockManagerInfo: Removed broadcast_47_piece0 on 172.18.0.14:40939 in memory (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 11:45:02 INFO BlockManagerInfo: Removed broadcast_47_piece0 on 172.18.0.15:34959 in memory (size: 107.2 KiB, free: 434.0 MiB)
26/02/13 11:45:02 INFO BlockManagerInfo: Removed broadcast_48_piece0 on spark-master:40291 in memory (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 11:45:02 INFO BlockManagerInfo: Removed broadcast_48_piece0 on 172.18.0.15:34959 in memory (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 11:45:02 INFO BlockManagerInfo: Removed broadcast_48_piece0 on 172.18.0.14:40939 in memory (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 11:45:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:45:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:45:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:45:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:45:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:45:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:45:03 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#12199 - origin_code.nullCount#12198) > 0)
26/02/13 11:45:03 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#12204 - destination_code.nullCount#12203) > 0)
26/02/13 11:45:03 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#12234 - callsign.nullCount#12233) > 0)
26/02/13 11:45:03 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:45:03 INFO DAGScheduler: Got job 33 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 11:45:03 INFO DAGScheduler: Final stage: ResultStage 53 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:45:03 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 52)
26/02/13 11:45:03 INFO DAGScheduler: Missing parents: List()
26/02/13 11:45:03 INFO DAGScheduler: Submitting ResultStage 53 (MapPartitionsRDD[175] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:45:03 INFO MemoryStore: Block broadcast_49 stored as values in memory (estimated size 99.3 KiB, free 434.0 MiB)
26/02/13 11:45:03 INFO MemoryStore: Block broadcast_49_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 433.9 MiB)
26/02/13 11:45:03 INFO BlockManagerInfo: Added broadcast_49_piece0 in memory on spark-master:40291 (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 11:45:03 INFO SparkContext: Created broadcast 49 from broadcast at DAGScheduler.scala:1585
26/02/13 11:45:03 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 53 (MapPartitionsRDD[175] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 11:45:03 INFO TaskSchedulerImpl: Adding task set 53.0 with 4 tasks resource profile 0
26/02/13 11:45:03 INFO TaskSetManager: Starting task 0.0 in stage 53.0 (TID 94) (172.18.0.15, executor 0, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:45:03 INFO TaskSetManager: Starting task 1.0 in stage 53.0 (TID 95) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:45:03 INFO BlockManagerInfo: Added broadcast_49_piece0 in memory on 172.18.0.15:34959 (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 11:45:03 INFO TaskSetManager: Starting task 2.0 in stage 53.0 (TID 96) (172.18.0.15, executor 0, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:45:03 INFO TaskSetManager: Finished task 0.0 in stage 53.0 (TID 94) in 78 ms on 172.18.0.15 (executor 0) (1/4)
26/02/13 11:45:03 INFO TaskSetManager: Starting task 3.0 in stage 53.0 (TID 97) (172.18.0.15, executor 0, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:45:03 INFO TaskSetManager: Finished task 1.0 in stage 53.0 (TID 95) in 110 ms on 172.18.0.15 (executor 0) (2/4)
26/02/13 11:45:03 INFO TaskSetManager: Finished task 3.0 in stage 53.0 (TID 97) in 38 ms on 172.18.0.15 (executor 0) (3/4)
26/02/13 11:45:03 INFO TaskSetManager: Finished task 2.0 in stage 53.0 (TID 96) in 71 ms on 172.18.0.15 (executor 0) (4/4)
26/02/13 11:45:03 INFO TaskSchedulerImpl: Removed TaskSet 53.0, whose tasks have all completed, from pool 
26/02/13 11:45:03 INFO DAGScheduler: ResultStage 53 (start at NativeMethodAccessorImpl.java:0) finished in 0.171 s
26/02/13 11:45:03 INFO DAGScheduler: Job 33 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:45:03 INFO TaskSchedulerImpl: Killing all running tasks in stage 53: Stage finished
26/02/13 11:45:03 INFO DAGScheduler: Job 33 finished: start at NativeMethodAccessorImpl.java:0, took 0.177399 s
26/02/13 11:45:03 INFO MemoryStore: Block broadcast_50_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.8 MiB)
26/02/13 11:45:03 INFO BlockManagerInfo: Added broadcast_50_piece0 in memory on spark-master:40291 (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 11:45:03 INFO SparkContext: Created broadcast 50 from start at NativeMethodAccessorImpl.java:0
26/02/13 11:45:03 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 12, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@afa50cc]. The input RDD has 3 partitions.
26/02/13 11:45:03 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:45:03 INFO DAGScheduler: Got job 34 (start at NativeMethodAccessorImpl.java:0) with 3 output partitions
26/02/13 11:45:03 INFO DAGScheduler: Final stage: ResultStage 54 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:45:03 INFO DAGScheduler: Parents of final stage: List()
26/02/13 11:45:03 INFO DAGScheduler: Missing parents: List()
26/02/13 11:45:03 INFO DAGScheduler: Submitting ResultStage 54 (MapPartitionsRDD[181] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:45:03 INFO MemoryStore: Block broadcast_51 stored as values in memory (estimated size 49.9 KiB, free 433.8 MiB)
26/02/13 11:45:03 INFO MemoryStore: Block broadcast_51_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.8 MiB)
26/02/13 11:45:03 INFO BlockManagerInfo: Added broadcast_51_piece0 in memory on spark-master:40291 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 11:45:03 INFO SparkContext: Created broadcast 51 from broadcast at DAGScheduler.scala:1585
26/02/13 11:45:03 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 54 (MapPartitionsRDD[181] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
26/02/13 11:45:03 INFO TaskSchedulerImpl: Adding task set 54.0 with 3 tasks resource profile 0
26/02/13 11:45:03 INFO TaskSetManager: Starting task 0.0 in stage 54.0 (TID 98) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:45:03 INFO TaskSetManager: Starting task 1.0 in stage 54.0 (TID 99) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:45:03 INFO TaskSetManager: Starting task 2.0 in stage 54.0 (TID 100) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:45:03 INFO BlockManagerInfo: Added broadcast_51_piece0 in memory on 172.18.0.15:34959 (size: 19.5 KiB, free: 434.0 MiB)
26/02/13 11:45:03 INFO BlockManagerInfo: Added broadcast_51_piece0 in memory on 172.18.0.14:40939 (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 11:45:03 INFO BlockManagerInfo: Added broadcast_50_piece0 in memory on 172.18.0.14:40939 (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 11:45:03 INFO BlockManagerInfo: Added broadcast_50_piece0 in memory on 172.18.0.15:34959 (size: 107.2 KiB, free: 433.9 MiB)
26/02/13 11:45:03 INFO TaskSetManager: Finished task 1.0 in stage 54.0 (TID 99) in 261 ms on 172.18.0.15 (executor 0) (1/3)
26/02/13 11:45:03 INFO TaskSetManager: Finished task 2.0 in stage 54.0 (TID 100) in 309 ms on 172.18.0.14 (executor 1) (2/3)
26/02/13 11:45:03 INFO TaskSetManager: Finished task 0.0 in stage 54.0 (TID 98) in 312 ms on 172.18.0.14 (executor 1) (3/3)
26/02/13 11:45:03 INFO TaskSchedulerImpl: Removed TaskSet 54.0, whose tasks have all completed, from pool 
26/02/13 11:45:03 INFO DAGScheduler: ResultStage 54 (start at NativeMethodAccessorImpl.java:0) finished in 0.322 s
26/02/13 11:45:03 INFO DAGScheduler: Job 34 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:45:03 INFO TaskSchedulerImpl: Killing all running tasks in stage 54: Stage finished
26/02/13 11:45:03 INFO DAGScheduler: Job 34 finished: start at NativeMethodAccessorImpl.java:0, took 0.324472 s
26/02/13 11:45:03 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 12, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@afa50cc] is committing.
26/02/13 11:45:03 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 12, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@afa50cc] committed.
26/02/13 11:45:03 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/12 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.12.c019926a-170e-4b18-a2bc-86b1190fc79c.tmp
26/02/13 11:45:03 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.12.c019926a-170e-4b18-a2bc-86b1190fc79c.tmp to file:/tmp/spark-checkpoint-enrichment/commits/12
26/02/13 11:45:03 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "fc5c6452-bd70-4c90-becf-772d3bebdb57",
  "name" : null,
  "timestamp" : "2026-02-13T11:45:02.783Z",
  "batchId" : 12,
  "numInputRows" : 126,
  "inputRowsPerSecond" : 93.12638580931264,
  "processedRowsPerSecond" : 138.1578947368421,
  "durationMs" : {
    "addBatch" : 703,
    "commitOffsets" : 66,
    "getBatch" : 0,
    "latestOffset" : 2,
    "queryPlanning" : 73,
    "triggerExecution" : 912,
    "walCommit" : 66
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 3216,
        "1" : 3563,
        "0" : 5398
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 3257,
        "1" : 3608,
        "0" : 5438
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 3257,
        "1" : 3608,
        "0" : 5438
      }
    },
    "numInputRows" : 126,
    "inputRowsPerSecond" : 93.12638580931264,
    "processedRowsPerSecond" : 138.1578947368421,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@7fe9b5de",
    "numOutputRows" : 26
  }
}
26/02/13 11:45:10 INFO BlockManagerInfo: Removed broadcast_51_piece0 on spark-master:40291 in memory (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 11:45:10 INFO BlockManagerInfo: Removed broadcast_51_piece0 on 172.18.0.14:40939 in memory (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 11:45:10 INFO BlockManagerInfo: Removed broadcast_51_piece0 on 172.18.0.15:34959 in memory (size: 19.5 KiB, free: 433.9 MiB)
26/02/13 11:45:10 INFO BlockManagerInfo: Removed broadcast_44_piece0 on 172.18.0.15:34959 in memory (size: 107.2 KiB, free: 434.0 MiB)
26/02/13 11:45:10 INFO BlockManagerInfo: Removed broadcast_44_piece0 on spark-master:40291 in memory (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 11:45:10 INFO BlockManagerInfo: Removed broadcast_44_piece0 on 172.18.0.14:40939 in memory (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 11:45:10 INFO BlockManagerInfo: Removed broadcast_49_piece0 on spark-master:40291 in memory (size: 33.3 KiB, free: 434.3 MiB)
26/02/13 11:45:10 INFO BlockManagerInfo: Removed broadcast_49_piece0 on 172.18.0.15:34959 in memory (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 11:45:13 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 11:45:17 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/13 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.13.be633ed7-7a5e-4cef-a210-b35c76464f08.tmp
26/02/13 11:45:17 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.13.be633ed7-7a5e-4cef-a210-b35c76464f08.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/13
26/02/13 11:45:17 INFO MicroBatchExecution: Committed offsets for batch 13. Metadata OffsetSeqMetadata(0,1770983117620,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 11:45:17 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:45:17 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:45:17 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:45:17 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:45:17 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:45:17 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:45:17 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#13053 - origin_code.nullCount#13052) > 0)
26/02/13 11:45:17 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#13058 - destination_code.nullCount#13057) > 0)
26/02/13 11:45:17 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#13088 - callsign.nullCount#13087) > 0)
26/02/13 11:45:17 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:45:17 INFO DAGScheduler: Got job 35 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 11:45:17 INFO DAGScheduler: Final stage: ResultStage 56 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:45:17 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 55)
26/02/13 11:45:17 INFO DAGScheduler: Missing parents: List()
26/02/13 11:45:17 INFO DAGScheduler: Submitting ResultStage 56 (MapPartitionsRDD[186] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:45:17 INFO MemoryStore: Block broadcast_52 stored as values in memory (estimated size 99.3 KiB, free 434.0 MiB)
26/02/13 11:45:17 INFO MemoryStore: Block broadcast_52_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 433.9 MiB)
26/02/13 11:45:17 INFO BlockManagerInfo: Added broadcast_52_piece0 in memory on spark-master:40291 (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 11:45:17 INFO SparkContext: Created broadcast 52 from broadcast at DAGScheduler.scala:1585
26/02/13 11:45:17 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 56 (MapPartitionsRDD[186] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 11:45:17 INFO TaskSchedulerImpl: Adding task set 56.0 with 4 tasks resource profile 0
26/02/13 11:45:17 INFO TaskSetManager: Starting task 0.0 in stage 56.0 (TID 101) (172.18.0.15, executor 0, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:45:17 INFO TaskSetManager: Starting task 1.0 in stage 56.0 (TID 102) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:45:17 INFO BlockManagerInfo: Added broadcast_52_piece0 in memory on 172.18.0.15:34959 (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 11:45:17 INFO TaskSetManager: Starting task 2.0 in stage 56.0 (TID 103) (172.18.0.15, executor 0, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:45:17 INFO TaskSetManager: Starting task 3.0 in stage 56.0 (TID 104) (172.18.0.15, executor 0, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:45:17 INFO TaskSetManager: Finished task 1.0 in stage 56.0 (TID 102) in 32 ms on 172.18.0.15 (executor 0) (1/4)
26/02/13 11:45:17 INFO TaskSetManager: Finished task 0.0 in stage 56.0 (TID 101) in 33 ms on 172.18.0.15 (executor 0) (2/4)
26/02/13 11:45:17 INFO TaskSetManager: Finished task 2.0 in stage 56.0 (TID 103) in 31 ms on 172.18.0.15 (executor 0) (3/4)
26/02/13 11:45:17 INFO TaskSetManager: Finished task 3.0 in stage 56.0 (TID 104) in 30 ms on 172.18.0.15 (executor 0) (4/4)
26/02/13 11:45:17 INFO TaskSchedulerImpl: Removed TaskSet 56.0, whose tasks have all completed, from pool 
26/02/13 11:45:17 INFO DAGScheduler: ResultStage 56 (start at NativeMethodAccessorImpl.java:0) finished in 0.071 s
26/02/13 11:45:17 INFO DAGScheduler: Job 35 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:45:17 INFO TaskSchedulerImpl: Killing all running tasks in stage 56: Stage finished
26/02/13 11:45:17 INFO DAGScheduler: Job 35 finished: start at NativeMethodAccessorImpl.java:0, took 0.073580 s
26/02/13 11:45:17 INFO MemoryStore: Block broadcast_53_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.8 MiB)
26/02/13 11:45:17 INFO BlockManagerInfo: Added broadcast_53_piece0 in memory on spark-master:40291 (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 11:45:17 INFO SparkContext: Created broadcast 53 from start at NativeMethodAccessorImpl.java:0
26/02/13 11:45:17 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 13, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@63336d3]. The input RDD has 3 partitions.
26/02/13 11:45:17 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:45:17 INFO DAGScheduler: Got job 36 (start at NativeMethodAccessorImpl.java:0) with 3 output partitions
26/02/13 11:45:17 INFO DAGScheduler: Final stage: ResultStage 57 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:45:17 INFO DAGScheduler: Parents of final stage: List()
26/02/13 11:45:17 INFO DAGScheduler: Missing parents: List()
26/02/13 11:45:17 INFO DAGScheduler: Submitting ResultStage 57 (MapPartitionsRDD[192] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:45:17 INFO MemoryStore: Block broadcast_54 stored as values in memory (estimated size 49.9 KiB, free 433.8 MiB)
26/02/13 11:45:17 INFO MemoryStore: Block broadcast_54_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.8 MiB)
26/02/13 11:45:17 INFO BlockManagerInfo: Added broadcast_54_piece0 in memory on spark-master:40291 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 11:45:17 INFO SparkContext: Created broadcast 54 from broadcast at DAGScheduler.scala:1585
26/02/13 11:45:17 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 57 (MapPartitionsRDD[192] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
26/02/13 11:45:17 INFO TaskSchedulerImpl: Adding task set 57.0 with 3 tasks resource profile 0
26/02/13 11:45:17 INFO TaskSetManager: Starting task 0.0 in stage 57.0 (TID 105) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:45:17 INFO TaskSetManager: Starting task 1.0 in stage 57.0 (TID 106) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:45:17 INFO TaskSetManager: Starting task 2.0 in stage 57.0 (TID 107) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:45:17 INFO BlockManagerInfo: Added broadcast_54_piece0 in memory on 172.18.0.15:34959 (size: 19.5 KiB, free: 434.0 MiB)
26/02/13 11:45:17 INFO BlockManagerInfo: Added broadcast_54_piece0 in memory on 172.18.0.14:40939 (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 11:45:17 INFO BlockManagerInfo: Added broadcast_53_piece0 in memory on 172.18.0.15:34959 (size: 107.2 KiB, free: 433.9 MiB)
26/02/13 11:45:17 INFO BlockManagerInfo: Added broadcast_53_piece0 in memory on 172.18.0.14:40939 (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 11:45:18 INFO TaskSetManager: Finished task 1.0 in stage 57.0 (TID 106) in 633 ms on 172.18.0.15 (executor 0) (1/3)
26/02/13 11:45:18 INFO TaskSetManager: Finished task 2.0 in stage 57.0 (TID 107) in 638 ms on 172.18.0.14 (executor 1) (2/3)
26/02/13 11:45:18 INFO TaskSetManager: Finished task 0.0 in stage 57.0 (TID 105) in 642 ms on 172.18.0.14 (executor 1) (3/3)
26/02/13 11:45:18 INFO TaskSchedulerImpl: Removed TaskSet 57.0, whose tasks have all completed, from pool 
26/02/13 11:45:18 INFO DAGScheduler: ResultStage 57 (start at NativeMethodAccessorImpl.java:0) finished in 0.656 s
26/02/13 11:45:18 INFO DAGScheduler: Job 36 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:45:18 INFO TaskSchedulerImpl: Killing all running tasks in stage 57: Stage finished
26/02/13 11:45:18 INFO DAGScheduler: Job 36 finished: start at NativeMethodAccessorImpl.java:0, took 0.657951 s
26/02/13 11:45:18 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 13, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@63336d3] is committing.
26/02/13 11:45:18 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 13, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@63336d3] committed.
26/02/13 11:45:18 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/13 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.13.4fdfd983-7a9d-486c-97b3-812754f3be20.tmp
26/02/13 11:45:18 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.13.4fdfd983-7a9d-486c-97b3-812754f3be20.tmp to file:/tmp/spark-checkpoint-enrichment/commits/13
26/02/13 11:45:18 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "fc5c6452-bd70-4c90-becf-772d3bebdb57",
  "name" : null,
  "timestamp" : "2026-02-13T11:45:17.618Z",
  "batchId" : 13,
  "numInputRows" : 110,
  "inputRowsPerSecond" : 9166.666666666666,
  "processedRowsPerSecond" : 103.18949343339587,
  "durationMs" : {
    "addBatch" : 854,
    "commitOffsets" : 81,
    "getBatch" : 0,
    "latestOffset" : 2,
    "queryPlanning" : 46,
    "triggerExecution" : 1066,
    "walCommit" : 82
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 3257,
        "1" : 3608,
        "0" : 5438
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 3291,
        "1" : 3646,
        "0" : 5476
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 3291,
        "1" : 3646,
        "0" : 5476
      }
    },
    "numInputRows" : 110,
    "inputRowsPerSecond" : 9166.666666666666,
    "processedRowsPerSecond" : 103.18949343339587,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@7fe9b5de",
    "numOutputRows" : 24
  }
}
26/02/13 11:45:18 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/14 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.14.0ce1d183-961e-4d01-9c05-cc2d5628c08d.tmp
26/02/13 11:45:18 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.14.0ce1d183-961e-4d01-9c05-cc2d5628c08d.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/14
26/02/13 11:45:18 INFO MicroBatchExecution: Committed offsets for batch 14. Metadata OffsetSeqMetadata(0,1770983118686,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 11:45:18 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:45:18 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:45:18 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:45:18 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:45:18 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:45:18 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:45:18 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#13907 - origin_code.nullCount#13906) > 0)
26/02/13 11:45:18 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#13912 - destination_code.nullCount#13911) > 0)
26/02/13 11:45:18 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#13942 - callsign.nullCount#13941) > 0)
26/02/13 11:45:18 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:45:18 INFO DAGScheduler: Got job 37 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 11:45:18 INFO DAGScheduler: Final stage: ResultStage 59 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:45:18 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 58)
26/02/13 11:45:18 INFO DAGScheduler: Missing parents: List()
26/02/13 11:45:18 INFO DAGScheduler: Submitting ResultStage 59 (MapPartitionsRDD[197] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:45:18 INFO MemoryStore: Block broadcast_55 stored as values in memory (estimated size 99.3 KiB, free 433.7 MiB)
26/02/13 11:45:18 INFO MemoryStore: Block broadcast_55_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 433.6 MiB)
26/02/13 11:45:18 INFO BlockManagerInfo: Added broadcast_55_piece0 in memory on spark-master:40291 (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 11:45:18 INFO SparkContext: Created broadcast 55 from broadcast at DAGScheduler.scala:1585
26/02/13 11:45:18 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 59 (MapPartitionsRDD[197] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 11:45:18 INFO TaskSchedulerImpl: Adding task set 59.0 with 4 tasks resource profile 0
26/02/13 11:45:18 INFO TaskSetManager: Starting task 0.0 in stage 59.0 (TID 108) (172.18.0.15, executor 0, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:45:18 INFO TaskSetManager: Starting task 1.0 in stage 59.0 (TID 109) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:45:18 INFO BlockManagerInfo: Added broadcast_55_piece0 in memory on 172.18.0.15:34959 (size: 33.3 KiB, free: 433.9 MiB)
26/02/13 11:45:18 INFO TaskSetManager: Starting task 2.0 in stage 59.0 (TID 110) (172.18.0.15, executor 0, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:45:18 INFO TaskSetManager: Finished task 0.0 in stage 59.0 (TID 108) in 19 ms on 172.18.0.15 (executor 0) (1/4)
26/02/13 11:45:18 INFO TaskSetManager: Starting task 3.0 in stage 59.0 (TID 111) (172.18.0.15, executor 0, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:45:18 INFO TaskSetManager: Finished task 1.0 in stage 59.0 (TID 109) in 30 ms on 172.18.0.15 (executor 0) (2/4)
26/02/13 11:45:18 INFO BlockManagerInfo: Removed broadcast_54_piece0 on spark-master:40291 in memory (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 11:45:18 INFO BlockManagerInfo: Removed broadcast_54_piece0 on 172.18.0.14:40939 in memory (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 11:45:18 INFO BlockManagerInfo: Removed broadcast_54_piece0 on 172.18.0.15:34959 in memory (size: 19.5 KiB, free: 433.9 MiB)
26/02/13 11:45:18 INFO BlockManagerInfo: Removed broadcast_50_piece0 on spark-master:40291 in memory (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 11:45:18 INFO BlockManagerInfo: Removed broadcast_50_piece0 on 172.18.0.15:34959 in memory (size: 107.2 KiB, free: 434.0 MiB)
26/02/13 11:45:18 INFO BlockManagerInfo: Removed broadcast_50_piece0 on 172.18.0.14:40939 in memory (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 11:45:18 INFO TaskSetManager: Finished task 2.0 in stage 59.0 (TID 110) in 42 ms on 172.18.0.15 (executor 0) (3/4)
26/02/13 11:45:18 INFO BlockManagerInfo: Removed broadcast_52_piece0 on spark-master:40291 in memory (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 11:45:18 INFO BlockManagerInfo: Removed broadcast_52_piece0 on 172.18.0.15:34959 in memory (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 11:45:18 INFO TaskSetManager: Finished task 3.0 in stage 59.0 (TID 111) in 43 ms on 172.18.0.15 (executor 0) (4/4)
26/02/13 11:45:18 INFO TaskSchedulerImpl: Removed TaskSet 59.0, whose tasks have all completed, from pool 
26/02/13 11:45:18 INFO DAGScheduler: ResultStage 59 (start at NativeMethodAccessorImpl.java:0) finished in 0.078 s
26/02/13 11:45:18 INFO DAGScheduler: Job 37 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:45:18 INFO TaskSchedulerImpl: Killing all running tasks in stage 59: Stage finished
26/02/13 11:45:18 INFO DAGScheduler: Job 37 finished: start at NativeMethodAccessorImpl.java:0, took 0.080390 s
26/02/13 11:45:18 INFO BlockManagerInfo: Removed broadcast_53_piece0 on spark-master:40291 in memory (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 11:45:18 INFO BlockManagerInfo: Removed broadcast_53_piece0 on 172.18.0.14:40939 in memory (size: 107.2 KiB, free: 434.4 MiB)
26/02/13 11:45:18 INFO BlockManagerInfo: Removed broadcast_53_piece0 on 172.18.0.15:34959 in memory (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 11:45:18 INFO MemoryStore: Block broadcast_56_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.9 MiB)
26/02/13 11:45:18 INFO BlockManagerInfo: Added broadcast_56_piece0 in memory on spark-master:40291 (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 11:45:18 INFO SparkContext: Created broadcast 56 from start at NativeMethodAccessorImpl.java:0
26/02/13 11:45:18 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 14, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@104f28a7]. The input RDD has 3 partitions.
26/02/13 11:45:18 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:45:18 INFO DAGScheduler: Got job 38 (start at NativeMethodAccessorImpl.java:0) with 3 output partitions
26/02/13 11:45:18 INFO DAGScheduler: Final stage: ResultStage 60 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:45:18 INFO DAGScheduler: Parents of final stage: List()
26/02/13 11:45:18 INFO DAGScheduler: Missing parents: List()
26/02/13 11:45:18 INFO DAGScheduler: Submitting ResultStage 60 (MapPartitionsRDD[203] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:45:19 INFO MemoryStore: Block broadcast_57 stored as values in memory (estimated size 49.9 KiB, free 433.9 MiB)
26/02/13 11:45:19 INFO MemoryStore: Block broadcast_57_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.9 MiB)
26/02/13 11:45:19 INFO BlockManagerInfo: Added broadcast_57_piece0 in memory on spark-master:40291 (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 11:45:19 INFO SparkContext: Created broadcast 57 from broadcast at DAGScheduler.scala:1585
26/02/13 11:45:19 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 60 (MapPartitionsRDD[203] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
26/02/13 11:45:19 INFO TaskSchedulerImpl: Adding task set 60.0 with 3 tasks resource profile 0
26/02/13 11:45:19 INFO TaskSetManager: Starting task 0.0 in stage 60.0 (TID 112) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:45:19 INFO TaskSetManager: Starting task 1.0 in stage 60.0 (TID 113) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:45:19 INFO TaskSetManager: Starting task 2.0 in stage 60.0 (TID 114) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:45:19 INFO BlockManagerInfo: Added broadcast_57_piece0 in memory on 172.18.0.14:40939 (size: 19.5 KiB, free: 434.4 MiB)
26/02/13 11:45:19 INFO BlockManagerInfo: Added broadcast_57_piece0 in memory on 172.18.0.15:34959 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 11:45:19 INFO BlockManagerInfo: Added broadcast_56_piece0 in memory on 172.18.0.15:34959 (size: 107.2 KiB, free: 434.0 MiB)
26/02/13 11:45:19 INFO BlockManagerInfo: Added broadcast_56_piece0 in memory on 172.18.0.14:40939 (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 11:45:19 INFO TaskSetManager: Finished task 1.0 in stage 60.0 (TID 113) in 88 ms on 172.18.0.15 (executor 0) (1/3)
26/02/13 11:45:19 INFO TaskSetManager: Finished task 0.0 in stage 60.0 (TID 112) in 115 ms on 172.18.0.14 (executor 1) (2/3)
26/02/13 11:45:19 INFO TaskSetManager: Finished task 2.0 in stage 60.0 (TID 114) in 120 ms on 172.18.0.14 (executor 1) (3/3)
26/02/13 11:45:19 INFO TaskSchedulerImpl: Removed TaskSet 60.0, whose tasks have all completed, from pool 
26/02/13 11:45:19 INFO DAGScheduler: ResultStage 60 (start at NativeMethodAccessorImpl.java:0) finished in 0.132 s
26/02/13 11:45:19 INFO DAGScheduler: Job 38 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:45:19 INFO TaskSchedulerImpl: Killing all running tasks in stage 60: Stage finished
26/02/13 11:45:19 INFO DAGScheduler: Job 38 finished: start at NativeMethodAccessorImpl.java:0, took 0.133708 s
26/02/13 11:45:19 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 14, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@104f28a7] is committing.
26/02/13 11:45:19 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 14, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@104f28a7] committed.
26/02/13 11:45:19 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/14 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.14.b528d90b-dfb0-4bca-b908-ec74b416cac2.tmp
26/02/13 11:45:19 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.14.b528d90b-dfb0-4bca-b908-ec74b416cac2.tmp to file:/tmp/spark-checkpoint-enrichment/commits/14
26/02/13 11:45:19 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "fc5c6452-bd70-4c90-becf-772d3bebdb57",
  "name" : null,
  "timestamp" : "2026-02-13T11:45:18.685Z",
  "batchId" : 14,
  "numInputRows" : 124,
  "inputRowsPerSecond" : 116.2136832239925,
  "processedRowsPerSecond" : 239.84526112185685,
  "durationMs" : {
    "addBatch" : 351,
    "commitOffsets" : 70,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 37,
    "triggerExecution" : 517,
    "walCommit" : 56
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 3291,
        "1" : 3646,
        "0" : 5476
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 3332,
        "1" : 3690,
        "0" : 5515
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 3332,
        "1" : 3690,
        "0" : 5515
      }
    },
    "numInputRows" : 124,
    "inputRowsPerSecond" : 116.2136832239925,
    "processedRowsPerSecond" : 239.84526112185685,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@7fe9b5de",
    "numOutputRows" : 25
  }
}
26/02/13 11:45:27 INFO BlockManagerInfo: Removed broadcast_57_piece0 on spark-master:40291 in memory (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 11:45:27 INFO BlockManagerInfo: Removed broadcast_57_piece0 on 172.18.0.15:34959 in memory (size: 19.5 KiB, free: 434.0 MiB)
26/02/13 11:45:27 INFO BlockManagerInfo: Removed broadcast_57_piece0 on 172.18.0.14:40939 in memory (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 11:45:29 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 11:45:34 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/15 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.15.692f001d-7424-4591-a39f-2c2dbd1beed4.tmp
26/02/13 11:45:34 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.15.692f001d-7424-4591-a39f-2c2dbd1beed4.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/15
26/02/13 11:45:34 INFO MicroBatchExecution: Committed offsets for batch 15. Metadata OffsetSeqMetadata(0,1770983134396,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 11:45:34 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:45:34 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:45:34 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:45:34 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:45:34 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:45:34 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:45:34 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#14761 - origin_code.nullCount#14760) > 0)
26/02/13 11:45:34 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#14766 - destination_code.nullCount#14765) > 0)
26/02/13 11:45:34 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#14796 - callsign.nullCount#14795) > 0)
26/02/13 11:45:34 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:45:34 INFO DAGScheduler: Got job 39 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 11:45:34 INFO DAGScheduler: Final stage: ResultStage 62 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:45:34 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 61)
26/02/13 11:45:34 INFO DAGScheduler: Missing parents: List()
26/02/13 11:45:34 INFO DAGScheduler: Submitting ResultStage 62 (MapPartitionsRDD[208] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:45:34 INFO MemoryStore: Block broadcast_58 stored as values in memory (estimated size 99.3 KiB, free 433.8 MiB)
26/02/13 11:45:34 INFO MemoryStore: Block broadcast_58_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 433.8 MiB)
26/02/13 11:45:34 INFO BlockManagerInfo: Added broadcast_58_piece0 in memory on spark-master:40291 (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 11:45:34 INFO SparkContext: Created broadcast 58 from broadcast at DAGScheduler.scala:1585
26/02/13 11:45:34 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 62 (MapPartitionsRDD[208] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 11:45:34 INFO TaskSchedulerImpl: Adding task set 62.0 with 4 tasks resource profile 0
26/02/13 11:45:34 INFO TaskSetManager: Starting task 0.0 in stage 62.0 (TID 115) (172.18.0.15, executor 0, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:45:34 INFO TaskSetManager: Starting task 1.0 in stage 62.0 (TID 116) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:45:34 INFO BlockManagerInfo: Added broadcast_58_piece0 in memory on 172.18.0.15:34959 (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 11:45:34 INFO TaskSetManager: Starting task 2.0 in stage 62.0 (TID 117) (172.18.0.15, executor 0, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:45:34 INFO TaskSetManager: Finished task 0.0 in stage 62.0 (TID 115) in 30 ms on 172.18.0.15 (executor 0) (1/4)
26/02/13 11:45:34 INFO TaskSetManager: Starting task 3.0 in stage 62.0 (TID 118) (172.18.0.15, executor 0, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:45:34 INFO TaskSetManager: Finished task 1.0 in stage 62.0 (TID 116) in 36 ms on 172.18.0.15 (executor 0) (2/4)
26/02/13 11:45:34 INFO BlockManagerInfo: Removed broadcast_55_piece0 on spark-master:40291 in memory (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 11:45:34 INFO TaskSetManager: Finished task 2.0 in stage 62.0 (TID 117) in 29 ms on 172.18.0.15 (executor 0) (3/4)
26/02/13 11:45:34 INFO TaskSetManager: Finished task 3.0 in stage 62.0 (TID 118) in 22 ms on 172.18.0.15 (executor 0) (4/4)
26/02/13 11:45:34 INFO TaskSchedulerImpl: Removed TaskSet 62.0, whose tasks have all completed, from pool 
26/02/13 11:45:34 INFO BlockManagerInfo: Removed broadcast_55_piece0 on 172.18.0.15:34959 in memory (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 11:45:34 INFO DAGScheduler: ResultStage 62 (start at NativeMethodAccessorImpl.java:0) finished in 0.069 s
26/02/13 11:45:34 INFO DAGScheduler: Job 39 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:45:34 INFO TaskSchedulerImpl: Killing all running tasks in stage 62: Stage finished
26/02/13 11:45:34 INFO DAGScheduler: Job 39 finished: start at NativeMethodAccessorImpl.java:0, took 0.071823 s
26/02/13 11:45:34 INFO MemoryStore: Block broadcast_59_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.8 MiB)
26/02/13 11:45:34 INFO BlockManagerInfo: Added broadcast_59_piece0 in memory on spark-master:40291 (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 11:45:34 INFO SparkContext: Created broadcast 59 from start at NativeMethodAccessorImpl.java:0
26/02/13 11:45:34 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 15, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@3c857c7]. The input RDD has 3 partitions.
26/02/13 11:45:34 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:45:34 INFO DAGScheduler: Got job 40 (start at NativeMethodAccessorImpl.java:0) with 3 output partitions
26/02/13 11:45:34 INFO DAGScheduler: Final stage: ResultStage 63 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:45:34 INFO DAGScheduler: Parents of final stage: List()
26/02/13 11:45:34 INFO DAGScheduler: Missing parents: List()
26/02/13 11:45:34 INFO DAGScheduler: Submitting ResultStage 63 (MapPartitionsRDD[214] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:45:34 INFO MemoryStore: Block broadcast_60 stored as values in memory (estimated size 49.9 KiB, free 433.8 MiB)
26/02/13 11:45:34 INFO MemoryStore: Block broadcast_60_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.8 MiB)
26/02/13 11:45:34 INFO BlockManagerInfo: Added broadcast_60_piece0 in memory on spark-master:40291 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 11:45:34 INFO SparkContext: Created broadcast 60 from broadcast at DAGScheduler.scala:1585
26/02/13 11:45:34 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 63 (MapPartitionsRDD[214] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
26/02/13 11:45:34 INFO TaskSchedulerImpl: Adding task set 63.0 with 3 tasks resource profile 0
26/02/13 11:45:34 INFO TaskSetManager: Starting task 1.0 in stage 63.0 (TID 119) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:45:34 INFO TaskSetManager: Starting task 0.0 in stage 63.0 (TID 120) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:45:34 INFO TaskSetManager: Starting task 2.0 in stage 63.0 (TID 121) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:45:34 INFO BlockManagerInfo: Added broadcast_60_piece0 in memory on 172.18.0.14:40939 (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 11:45:34 INFO BlockManagerInfo: Added broadcast_60_piece0 in memory on 172.18.0.15:34959 (size: 19.5 KiB, free: 434.0 MiB)
26/02/13 11:45:34 INFO BlockManagerInfo: Added broadcast_59_piece0 in memory on 172.18.0.14:40939 (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 11:45:34 INFO BlockManagerInfo: Added broadcast_59_piece0 in memory on 172.18.0.15:34959 (size: 107.2 KiB, free: 433.9 MiB)
26/02/13 11:45:35 INFO TaskSetManager: Finished task 0.0 in stage 63.0 (TID 120) in 611 ms on 172.18.0.14 (executor 1) (1/3)
26/02/13 11:45:35 INFO TaskSetManager: Finished task 2.0 in stage 63.0 (TID 121) in 618 ms on 172.18.0.14 (executor 1) (2/3)
26/02/13 11:45:35 INFO TaskSetManager: Finished task 1.0 in stage 63.0 (TID 119) in 638 ms on 172.18.0.15 (executor 0) (3/3)
26/02/13 11:45:35 INFO TaskSchedulerImpl: Removed TaskSet 63.0, whose tasks have all completed, from pool 
26/02/13 11:45:35 INFO DAGScheduler: ResultStage 63 (start at NativeMethodAccessorImpl.java:0) finished in 0.650 s
26/02/13 11:45:35 INFO DAGScheduler: Job 40 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:45:35 INFO TaskSchedulerImpl: Killing all running tasks in stage 63: Stage finished
26/02/13 11:45:35 INFO DAGScheduler: Job 40 finished: start at NativeMethodAccessorImpl.java:0, took 0.652150 s
26/02/13 11:45:35 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 15, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@3c857c7] is committing.
26/02/13 11:45:35 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 15, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@3c857c7] committed.
26/02/13 11:45:35 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/15 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.15.d3bc2db4-c8fb-44de-8984-1ba08a16fe00.tmp
26/02/13 11:45:35 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.15.d3bc2db4-c8fb-44de-8984-1ba08a16fe00.tmp to file:/tmp/spark-checkpoint-enrichment/commits/15
26/02/13 11:45:35 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "fc5c6452-bd70-4c90-becf-772d3bebdb57",
  "name" : null,
  "timestamp" : "2026-02-13T11:45:34.393Z",
  "batchId" : 15,
  "numInputRows" : 53,
  "inputRowsPerSecond" : 4076.923076923077,
  "processedRowsPerSecond" : 51.10896817743491,
  "durationMs" : {
    "addBatch" : 872,
    "commitOffsets" : 59,
    "getBatch" : 0,
    "latestOffset" : 3,
    "queryPlanning" : 37,
    "triggerExecution" : 1037,
    "walCommit" : 66
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 3332,
        "1" : 3690,
        "0" : 5515
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 3344,
        "1" : 3713,
        "0" : 5533
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 3344,
        "1" : 3713,
        "0" : 5533
      }
    },
    "numInputRows" : 53,
    "inputRowsPerSecond" : 4076.923076923077,
    "processedRowsPerSecond" : 51.10896817743491,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@7fe9b5de",
    "numOutputRows" : 14
  }
}
26/02/13 11:45:35 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/16 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.16.79d60c3b-b295-4da2-88c7-837c439843df.tmp
26/02/13 11:45:35 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.16.79d60c3b-b295-4da2-88c7-837c439843df.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/16
26/02/13 11:45:35 INFO MicroBatchExecution: Committed offsets for batch 16. Metadata OffsetSeqMetadata(0,1770983135432,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 11:45:35 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:45:35 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:45:35 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:45:35 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:45:35 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:45:35 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:45:35 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#15615 - origin_code.nullCount#15614) > 0)
26/02/13 11:45:35 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#15620 - destination_code.nullCount#15619) > 0)
26/02/13 11:45:35 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#15650 - callsign.nullCount#15649) > 0)
26/02/13 11:45:35 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:45:35 INFO DAGScheduler: Got job 41 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 11:45:35 INFO DAGScheduler: Final stage: ResultStage 65 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:45:35 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 64)
26/02/13 11:45:35 INFO DAGScheduler: Missing parents: List()
26/02/13 11:45:35 INFO DAGScheduler: Submitting ResultStage 65 (MapPartitionsRDD[219] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:45:35 INFO MemoryStore: Block broadcast_61 stored as values in memory (estimated size 99.3 KiB, free 433.7 MiB)
26/02/13 11:45:35 INFO MemoryStore: Block broadcast_61_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 433.6 MiB)
26/02/13 11:45:35 INFO BlockManagerInfo: Added broadcast_61_piece0 in memory on spark-master:40291 (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 11:45:35 INFO SparkContext: Created broadcast 61 from broadcast at DAGScheduler.scala:1585
26/02/13 11:45:35 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 65 (MapPartitionsRDD[219] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 11:45:35 INFO TaskSchedulerImpl: Adding task set 65.0 with 4 tasks resource profile 0
26/02/13 11:45:35 INFO TaskSetManager: Starting task 0.0 in stage 65.0 (TID 122) (172.18.0.15, executor 0, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:45:35 INFO TaskSetManager: Starting task 1.0 in stage 65.0 (TID 123) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:45:35 INFO BlockManagerInfo: Added broadcast_61_piece0 in memory on 172.18.0.15:34959 (size: 33.3 KiB, free: 433.9 MiB)
26/02/13 11:45:35 INFO TaskSetManager: Starting task 2.0 in stage 65.0 (TID 124) (172.18.0.15, executor 0, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:45:35 INFO TaskSetManager: Finished task 1.0 in stage 65.0 (TID 123) in 33 ms on 172.18.0.15 (executor 0) (1/4)
26/02/13 11:45:35 INFO TaskSetManager: Starting task 3.0 in stage 65.0 (TID 125) (172.18.0.15, executor 0, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:45:35 INFO TaskSetManager: Finished task 0.0 in stage 65.0 (TID 122) in 41 ms on 172.18.0.15 (executor 0) (2/4)
26/02/13 11:45:35 INFO TaskSetManager: Finished task 2.0 in stage 65.0 (TID 124) in 35 ms on 172.18.0.15 (executor 0) (3/4)
26/02/13 11:45:35 INFO BlockManagerInfo: Removed broadcast_59_piece0 on spark-master:40291 in memory (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 11:45:35 INFO TaskSetManager: Finished task 3.0 in stage 65.0 (TID 125) in 30 ms on 172.18.0.15 (executor 0) (4/4)
26/02/13 11:45:35 INFO BlockManagerInfo: Removed broadcast_59_piece0 on 172.18.0.14:40939 in memory (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 11:45:35 INFO TaskSchedulerImpl: Removed TaskSet 65.0, whose tasks have all completed, from pool 
26/02/13 11:45:35 INFO BlockManagerInfo: Removed broadcast_59_piece0 on 172.18.0.15:34959 in memory (size: 107.2 KiB, free: 434.0 MiB)
26/02/13 11:45:35 INFO DAGScheduler: ResultStage 65 (start at NativeMethodAccessorImpl.java:0) finished in 0.088 s
26/02/13 11:45:35 INFO DAGScheduler: Job 41 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:45:35 INFO TaskSchedulerImpl: Killing all running tasks in stage 65: Stage finished
26/02/13 11:45:35 INFO DAGScheduler: Job 41 finished: start at NativeMethodAccessorImpl.java:0, took 0.095534 s
26/02/13 11:45:35 INFO BlockManagerInfo: Removed broadcast_56_piece0 on spark-master:40291 in memory (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 11:45:35 INFO BlockManagerInfo: Removed broadcast_56_piece0 on 172.18.0.15:34959 in memory (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 11:45:35 INFO BlockManagerInfo: Removed broadcast_56_piece0 on 172.18.0.14:40939 in memory (size: 107.2 KiB, free: 434.4 MiB)
26/02/13 11:45:35 INFO MemoryStore: Block broadcast_62_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.7 MiB)
26/02/13 11:45:35 INFO BlockManagerInfo: Removed broadcast_58_piece0 on 172.18.0.15:34959 in memory (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 11:45:35 INFO BlockManagerInfo: Added broadcast_62_piece0 in memory on spark-master:40291 (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 11:45:35 INFO BlockManagerInfo: Removed broadcast_58_piece0 on spark-master:40291 in memory (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 11:45:35 INFO SparkContext: Created broadcast 62 from start at NativeMethodAccessorImpl.java:0
26/02/13 11:45:35 INFO BlockManagerInfo: Removed broadcast_60_piece0 on spark-master:40291 in memory (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 11:45:35 INFO BlockManagerInfo: Removed broadcast_60_piece0 on 172.18.0.14:40939 in memory (size: 19.5 KiB, free: 434.4 MiB)
26/02/13 11:45:35 INFO BlockManagerInfo: Removed broadcast_60_piece0 on 172.18.0.15:34959 in memory (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 11:45:35 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 16, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@71ab0dc4]. The input RDD has 3 partitions.
26/02/13 11:45:35 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:45:35 INFO DAGScheduler: Got job 42 (start at NativeMethodAccessorImpl.java:0) with 3 output partitions
26/02/13 11:45:35 INFO DAGScheduler: Final stage: ResultStage 66 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:45:35 INFO DAGScheduler: Parents of final stage: List()
26/02/13 11:45:35 INFO DAGScheduler: Missing parents: List()
26/02/13 11:45:35 INFO DAGScheduler: Submitting ResultStage 66 (MapPartitionsRDD[225] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:45:35 INFO MemoryStore: Block broadcast_63 stored as values in memory (estimated size 49.9 KiB, free 433.9 MiB)
26/02/13 11:45:35 INFO MemoryStore: Block broadcast_63_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.9 MiB)
26/02/13 11:45:35 INFO BlockManagerInfo: Added broadcast_63_piece0 in memory on spark-master:40291 (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 11:45:35 INFO SparkContext: Created broadcast 63 from broadcast at DAGScheduler.scala:1585
26/02/13 11:45:35 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 66 (MapPartitionsRDD[225] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
26/02/13 11:45:35 INFO TaskSchedulerImpl: Adding task set 66.0 with 3 tasks resource profile 0
26/02/13 11:45:35 INFO TaskSetManager: Starting task 1.0 in stage 66.0 (TID 126) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:45:35 INFO TaskSetManager: Starting task 0.0 in stage 66.0 (TID 127) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:45:35 INFO TaskSetManager: Starting task 2.0 in stage 66.0 (TID 128) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:45:35 INFO BlockManagerInfo: Added broadcast_63_piece0 in memory on 172.18.0.14:40939 (size: 19.5 KiB, free: 434.4 MiB)
26/02/13 11:45:35 INFO BlockManagerInfo: Added broadcast_63_piece0 in memory on 172.18.0.15:34959 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 11:45:35 INFO BlockManagerInfo: Added broadcast_62_piece0 in memory on 172.18.0.14:40939 (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 11:45:35 INFO BlockManagerInfo: Added broadcast_62_piece0 in memory on 172.18.0.15:34959 (size: 107.2 KiB, free: 434.0 MiB)
26/02/13 11:45:35 INFO TaskSetManager: Finished task 1.0 in stage 66.0 (TID 126) in 149 ms on 172.18.0.15 (executor 0) (1/3)
26/02/13 11:45:36 INFO TaskSetManager: Finished task 0.0 in stage 66.0 (TID 127) in 202 ms on 172.18.0.14 (executor 1) (2/3)
26/02/13 11:45:36 INFO TaskSetManager: Finished task 2.0 in stage 66.0 (TID 128) in 210 ms on 172.18.0.14 (executor 1) (3/3)
26/02/13 11:45:36 INFO TaskSchedulerImpl: Removed TaskSet 66.0, whose tasks have all completed, from pool 
26/02/13 11:45:36 INFO DAGScheduler: ResultStage 66 (start at NativeMethodAccessorImpl.java:0) finished in 0.219 s
26/02/13 11:45:36 INFO DAGScheduler: Job 42 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:45:36 INFO TaskSchedulerImpl: Killing all running tasks in stage 66: Stage finished
26/02/13 11:45:36 INFO DAGScheduler: Job 42 finished: start at NativeMethodAccessorImpl.java:0, took 0.223432 s
26/02/13 11:45:36 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 16, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@71ab0dc4] is committing.
26/02/13 11:45:36 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 16, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@71ab0dc4] committed.
26/02/13 11:45:36 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/16 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.16.41b1a0dc-2654-48ff-b0f5-17cb1068fcf7.tmp
26/02/13 11:45:36 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.16.41b1a0dc-2654-48ff-b0f5-17cb1068fcf7.tmp to file:/tmp/spark-checkpoint-enrichment/commits/16
26/02/13 11:45:36 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "fc5c6452-bd70-4c90-becf-772d3bebdb57",
  "name" : null,
  "timestamp" : "2026-02-13T11:45:35.431Z",
  "batchId" : 16,
  "numInputRows" : 181,
  "inputRowsPerSecond" : 174.37379576107898,
  "processedRowsPerSecond" : 262.3188405797102,
  "durationMs" : {
    "addBatch" : 476,
    "commitOffsets" : 104,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 51,
    "triggerExecution" : 690,
    "walCommit" : 56
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 3344,
        "1" : 3713,
        "0" : 5533
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 3407,
        "1" : 3770,
        "0" : 5594
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 3407,
        "1" : 3770,
        "0" : 5594
      }
    },
    "numInputRows" : 181,
    "inputRowsPerSecond" : 174.37379576107898,
    "processedRowsPerSecond" : 262.3188405797102,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@7fe9b5de",
    "numOutputRows" : 35
  }
}
26/02/13 11:45:44 INFO BlockManagerInfo: Removed broadcast_63_piece0 on spark-master:40291 in memory (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 11:45:44 INFO BlockManagerInfo: Removed broadcast_63_piece0 on 172.18.0.15:34959 in memory (size: 19.5 KiB, free: 434.0 MiB)
26/02/13 11:45:44 INFO BlockManagerInfo: Removed broadcast_63_piece0 on 172.18.0.14:40939 in memory (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 11:45:46 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 11:45:53 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/17 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.17.9b64b512-f89d-4899-9852-90057e06e5f9.tmp
26/02/13 11:45:53 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.17.9b64b512-f89d-4899-9852-90057e06e5f9.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/17
26/02/13 11:45:53 INFO MicroBatchExecution: Committed offsets for batch 17. Metadata OffsetSeqMetadata(0,1770983153120,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 11:45:53 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:45:53 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:45:53 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:45:53 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:45:53 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:45:53 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:45:53 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#16469 - origin_code.nullCount#16468) > 0)
26/02/13 11:45:53 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#16474 - destination_code.nullCount#16473) > 0)
26/02/13 11:45:53 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#16504 - callsign.nullCount#16503) > 0)
26/02/13 11:45:53 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:45:53 INFO DAGScheduler: Got job 43 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 11:45:53 INFO DAGScheduler: Final stage: ResultStage 68 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:45:53 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 67)
26/02/13 11:45:53 INFO DAGScheduler: Missing parents: List()
26/02/13 11:45:53 INFO DAGScheduler: Submitting ResultStage 68 (MapPartitionsRDD[230] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:45:53 INFO MemoryStore: Block broadcast_64 stored as values in memory (estimated size 99.3 KiB, free 433.8 MiB)
26/02/13 11:45:53 INFO MemoryStore: Block broadcast_64_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 433.8 MiB)
26/02/13 11:45:53 INFO BlockManagerInfo: Added broadcast_64_piece0 in memory on spark-master:40291 (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 11:45:53 INFO SparkContext: Created broadcast 64 from broadcast at DAGScheduler.scala:1585
26/02/13 11:45:53 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 68 (MapPartitionsRDD[230] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 11:45:53 INFO TaskSchedulerImpl: Adding task set 68.0 with 4 tasks resource profile 0
26/02/13 11:45:53 INFO TaskSetManager: Starting task 0.0 in stage 68.0 (TID 129) (172.18.0.15, executor 0, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:45:53 INFO TaskSetManager: Starting task 1.0 in stage 68.0 (TID 130) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:45:53 INFO BlockManagerInfo: Added broadcast_64_piece0 in memory on 172.18.0.15:34959 (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 11:45:53 INFO TaskSetManager: Starting task 2.0 in stage 68.0 (TID 131) (172.18.0.15, executor 0, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:45:53 INFO TaskSetManager: Starting task 3.0 in stage 68.0 (TID 132) (172.18.0.15, executor 0, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:45:53 INFO TaskSetManager: Finished task 1.0 in stage 68.0 (TID 130) in 43 ms on 172.18.0.15 (executor 0) (1/4)
26/02/13 11:45:53 INFO TaskSetManager: Finished task 0.0 in stage 68.0 (TID 129) in 43 ms on 172.18.0.15 (executor 0) (2/4)
26/02/13 11:45:53 INFO BlockManagerInfo: Removed broadcast_61_piece0 on spark-master:40291 in memory (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 11:45:53 INFO TaskSetManager: Finished task 3.0 in stage 68.0 (TID 132) in 41 ms on 172.18.0.15 (executor 0) (3/4)
26/02/13 11:45:53 INFO TaskSetManager: Finished task 2.0 in stage 68.0 (TID 131) in 44 ms on 172.18.0.15 (executor 0) (4/4)
26/02/13 11:45:53 INFO TaskSchedulerImpl: Removed TaskSet 68.0, whose tasks have all completed, from pool 
26/02/13 11:45:53 INFO DAGScheduler: ResultStage 68 (start at NativeMethodAccessorImpl.java:0) finished in 0.098 s
26/02/13 11:45:53 INFO DAGScheduler: Job 43 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:45:53 INFO TaskSchedulerImpl: Killing all running tasks in stage 68: Stage finished
26/02/13 11:45:53 INFO BlockManagerInfo: Removed broadcast_61_piece0 on 172.18.0.15:34959 in memory (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 11:45:53 INFO DAGScheduler: Job 43 finished: start at NativeMethodAccessorImpl.java:0, took 0.106022 s
26/02/13 11:45:53 INFO MemoryStore: Block broadcast_65_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.8 MiB)
26/02/13 11:45:53 INFO BlockManagerInfo: Added broadcast_65_piece0 in memory on spark-master:40291 (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 11:45:53 INFO SparkContext: Created broadcast 65 from start at NativeMethodAccessorImpl.java:0
26/02/13 11:45:53 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 17, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@58c0bac2]. The input RDD has 2 partitions.
26/02/13 11:45:53 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:45:53 INFO DAGScheduler: Got job 44 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/02/13 11:45:53 INFO DAGScheduler: Final stage: ResultStage 69 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:45:53 INFO DAGScheduler: Parents of final stage: List()
26/02/13 11:45:53 INFO DAGScheduler: Missing parents: List()
26/02/13 11:45:53 INFO DAGScheduler: Submitting ResultStage 69 (MapPartitionsRDD[236] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:45:53 INFO MemoryStore: Block broadcast_66 stored as values in memory (estimated size 49.9 KiB, free 433.8 MiB)
26/02/13 11:45:53 INFO MemoryStore: Block broadcast_66_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.8 MiB)
26/02/13 11:45:53 INFO BlockManagerInfo: Added broadcast_66_piece0 in memory on spark-master:40291 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 11:45:53 INFO SparkContext: Created broadcast 66 from broadcast at DAGScheduler.scala:1585
26/02/13 11:45:53 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 69 (MapPartitionsRDD[236] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/02/13 11:45:53 INFO TaskSchedulerImpl: Adding task set 69.0 with 2 tasks resource profile 0
26/02/13 11:45:53 INFO TaskSetManager: Starting task 0.0 in stage 69.0 (TID 133) (172.18.0.15, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:45:53 INFO TaskSetManager: Starting task 1.0 in stage 69.0 (TID 134) (172.18.0.14, executor 1, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:45:53 INFO BlockManagerInfo: Added broadcast_66_piece0 in memory on 172.18.0.15:34959 (size: 19.5 KiB, free: 434.0 MiB)
26/02/13 11:45:53 INFO BlockManagerInfo: Added broadcast_66_piece0 in memory on 172.18.0.14:40939 (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 11:45:53 INFO BlockManagerInfo: Added broadcast_65_piece0 in memory on 172.18.0.15:34959 (size: 107.2 KiB, free: 433.9 MiB)
26/02/13 11:45:53 INFO BlockManagerInfo: Added broadcast_65_piece0 in memory on 172.18.0.14:40939 (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 11:45:54 INFO TaskSetManager: Finished task 0.0 in stage 69.0 (TID 133) in 578 ms on 172.18.0.15 (executor 0) (1/2)
26/02/13 11:45:54 INFO TaskSetManager: Finished task 1.0 in stage 69.0 (TID 134) in 584 ms on 172.18.0.14 (executor 1) (2/2)
26/02/13 11:45:54 INFO TaskSchedulerImpl: Removed TaskSet 69.0, whose tasks have all completed, from pool 
26/02/13 11:45:54 INFO DAGScheduler: ResultStage 69 (start at NativeMethodAccessorImpl.java:0) finished in 0.591 s
26/02/13 11:45:54 INFO DAGScheduler: Job 44 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:45:54 INFO TaskSchedulerImpl: Killing all running tasks in stage 69: Stage finished
26/02/13 11:45:54 INFO DAGScheduler: Job 44 finished: start at NativeMethodAccessorImpl.java:0, took 0.594297 s
26/02/13 11:45:54 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 17, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@58c0bac2] is committing.
26/02/13 11:45:54 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 17, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@58c0bac2] committed.
26/02/13 11:45:54 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/17 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.17.63ed92ea-6d64-4abb-8634-b16384d87e0a.tmp
26/02/13 11:45:54 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.17.63ed92ea-6d64-4abb-8634-b16384d87e0a.tmp to file:/tmp/spark-checkpoint-enrichment/commits/17
26/02/13 11:45:54 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "fc5c6452-bd70-4c90-becf-772d3bebdb57",
  "name" : null,
  "timestamp" : "2026-02-13T11:45:53.119Z",
  "batchId" : 17,
  "numInputRows" : 3,
  "inputRowsPerSecond" : 230.76923076923077,
  "processedRowsPerSecond" : 2.982107355864811,
  "durationMs" : {
    "addBatch" : 821,
    "commitOffsets" : 66,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 42,
    "triggerExecution" : 1006,
    "walCommit" : 74
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 3407,
        "1" : 3770,
        "0" : 5594
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 3407,
        "1" : 3772,
        "0" : 5595
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 3407,
        "1" : 3772,
        "0" : 5595
      }
    },
    "numInputRows" : 3,
    "inputRowsPerSecond" : 230.76923076923077,
    "processedRowsPerSecond" : 2.982107355864811,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@7fe9b5de",
    "numOutputRows" : 1
  }
}
26/02/13 11:45:54 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/18 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.18.b958a4fe-a642-4d6c-af9c-3e6573dee3cd.tmp
26/02/13 11:45:54 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.18.b958a4fe-a642-4d6c-af9c-3e6573dee3cd.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/18
26/02/13 11:45:54 INFO MicroBatchExecution: Committed offsets for batch 18. Metadata OffsetSeqMetadata(0,1770983154128,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 11:45:54 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:45:54 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:45:54 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:45:54 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:45:54 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:45:54 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:45:54 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#17323 - origin_code.nullCount#17322) > 0)
26/02/13 11:45:54 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#17328 - destination_code.nullCount#17327) > 0)
26/02/13 11:45:54 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#17358 - callsign.nullCount#17357) > 0)
26/02/13 11:45:54 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:45:54 INFO DAGScheduler: Got job 45 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 11:45:54 INFO DAGScheduler: Final stage: ResultStage 71 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:45:54 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 70)
26/02/13 11:45:54 INFO DAGScheduler: Missing parents: List()
26/02/13 11:45:54 INFO DAGScheduler: Submitting ResultStage 71 (MapPartitionsRDD[241] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:45:54 INFO MemoryStore: Block broadcast_67 stored as values in memory (estimated size 99.3 KiB, free 433.7 MiB)
26/02/13 11:45:54 INFO MemoryStore: Block broadcast_67_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 433.6 MiB)
26/02/13 11:45:54 INFO BlockManagerInfo: Added broadcast_67_piece0 in memory on spark-master:40291 (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 11:45:54 INFO SparkContext: Created broadcast 67 from broadcast at DAGScheduler.scala:1585
26/02/13 11:45:54 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 71 (MapPartitionsRDD[241] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 11:45:54 INFO TaskSchedulerImpl: Adding task set 71.0 with 4 tasks resource profile 0
26/02/13 11:45:54 INFO TaskSetManager: Starting task 0.0 in stage 71.0 (TID 135) (172.18.0.15, executor 0, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:45:54 INFO TaskSetManager: Starting task 1.0 in stage 71.0 (TID 136) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:45:54 INFO BlockManagerInfo: Added broadcast_67_piece0 in memory on 172.18.0.15:34959 (size: 33.3 KiB, free: 433.9 MiB)
26/02/13 11:45:54 INFO TaskSetManager: Starting task 2.0 in stage 71.0 (TID 137) (172.18.0.15, executor 0, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:45:54 INFO TaskSetManager: Finished task 1.0 in stage 71.0 (TID 136) in 23 ms on 172.18.0.15 (executor 0) (1/4)
26/02/13 11:45:54 INFO TaskSetManager: Starting task 3.0 in stage 71.0 (TID 138) (172.18.0.15, executor 0, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:45:54 INFO TaskSetManager: Finished task 0.0 in stage 71.0 (TID 135) in 32 ms on 172.18.0.15 (executor 0) (2/4)
26/02/13 11:45:54 INFO TaskSetManager: Finished task 2.0 in stage 71.0 (TID 137) in 18 ms on 172.18.0.15 (executor 0) (3/4)
26/02/13 11:45:54 INFO TaskSetManager: Finished task 3.0 in stage 71.0 (TID 138) in 21 ms on 172.18.0.15 (executor 0) (4/4)
26/02/13 11:45:54 INFO TaskSchedulerImpl: Removed TaskSet 71.0, whose tasks have all completed, from pool 
26/02/13 11:45:54 INFO DAGScheduler: ResultStage 71 (start at NativeMethodAccessorImpl.java:0) finished in 0.062 s
26/02/13 11:45:54 INFO DAGScheduler: Job 45 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:45:54 INFO TaskSchedulerImpl: Killing all running tasks in stage 71: Stage finished
26/02/13 11:45:54 INFO DAGScheduler: Job 45 finished: start at NativeMethodAccessorImpl.java:0, took 0.064973 s
26/02/13 11:45:54 INFO BlockManagerInfo: Removed broadcast_66_piece0 on spark-master:40291 in memory (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 11:45:54 INFO BlockManagerInfo: Removed broadcast_66_piece0 on 172.18.0.15:34959 in memory (size: 19.5 KiB, free: 433.9 MiB)
26/02/13 11:45:54 INFO BlockManagerInfo: Removed broadcast_66_piece0 on 172.18.0.14:40939 in memory (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 11:45:54 INFO MemoryStore: Block broadcast_68_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.6 MiB)
26/02/13 11:45:54 INFO BlockManagerInfo: Added broadcast_68_piece0 in memory on spark-master:40291 (size: 107.2 KiB, free: 434.0 MiB)
26/02/13 11:45:54 INFO SparkContext: Created broadcast 68 from start at NativeMethodAccessorImpl.java:0
26/02/13 11:45:54 INFO BlockManagerInfo: Removed broadcast_62_piece0 on spark-master:40291 in memory (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 11:45:54 INFO BlockManagerInfo: Removed broadcast_62_piece0 on 172.18.0.14:40939 in memory (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 11:45:54 INFO BlockManagerInfo: Removed broadcast_62_piece0 on 172.18.0.15:34959 in memory (size: 107.2 KiB, free: 434.0 MiB)
26/02/13 11:45:54 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 18, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@2b033da1]. The input RDD has 3 partitions.
26/02/13 11:45:54 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:45:54 INFO DAGScheduler: Got job 46 (start at NativeMethodAccessorImpl.java:0) with 3 output partitions
26/02/13 11:45:54 INFO DAGScheduler: Final stage: ResultStage 72 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:45:54 INFO DAGScheduler: Parents of final stage: List()
26/02/13 11:45:54 INFO DAGScheduler: Missing parents: List()
26/02/13 11:45:54 INFO DAGScheduler: Submitting ResultStage 72 (MapPartitionsRDD[247] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:45:54 INFO MemoryStore: Block broadcast_69 stored as values in memory (estimated size 49.9 KiB, free 433.7 MiB)
26/02/13 11:45:54 INFO BlockManagerInfo: Removed broadcast_64_piece0 on spark-master:40291 in memory (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 11:45:54 INFO MemoryStore: Block broadcast_69_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.8 MiB)
26/02/13 11:45:54 INFO BlockManagerInfo: Removed broadcast_64_piece0 on 172.18.0.15:34959 in memory (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 11:45:54 INFO BlockManagerInfo: Added broadcast_69_piece0 in memory on spark-master:40291 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 11:45:54 INFO SparkContext: Created broadcast 69 from broadcast at DAGScheduler.scala:1585
26/02/13 11:45:54 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 72 (MapPartitionsRDD[247] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
26/02/13 11:45:54 INFO TaskSchedulerImpl: Adding task set 72.0 with 3 tasks resource profile 0
26/02/13 11:45:54 INFO TaskSetManager: Starting task 1.0 in stage 72.0 (TID 139) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:45:54 INFO BlockManagerInfo: Removed broadcast_67_piece0 on spark-master:40291 in memory (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 11:45:54 INFO TaskSetManager: Starting task 0.0 in stage 72.0 (TID 140) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:45:54 INFO TaskSetManager: Starting task 2.0 in stage 72.0 (TID 141) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:45:54 INFO BlockManagerInfo: Removed broadcast_67_piece0 on 172.18.0.15:34959 in memory (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 11:45:54 INFO BlockManagerInfo: Removed broadcast_65_piece0 on spark-master:40291 in memory (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 11:45:54 INFO BlockManagerInfo: Removed broadcast_65_piece0 on 172.18.0.14:40939 in memory (size: 107.2 KiB, free: 434.4 MiB)
26/02/13 11:45:54 INFO BlockManagerInfo: Removed broadcast_65_piece0 on 172.18.0.15:34959 in memory (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 11:45:54 INFO BlockManagerInfo: Added broadcast_69_piece0 in memory on 172.18.0.15:34959 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 11:45:54 INFO BlockManagerInfo: Added broadcast_69_piece0 in memory on 172.18.0.14:40939 (size: 19.5 KiB, free: 434.4 MiB)
26/02/13 11:45:54 INFO BlockManagerInfo: Added broadcast_68_piece0 in memory on 172.18.0.14:40939 (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 11:45:54 INFO BlockManagerInfo: Added broadcast_68_piece0 in memory on 172.18.0.15:34959 (size: 107.2 KiB, free: 434.0 MiB)
26/02/13 11:45:54 INFO TaskSetManager: Finished task 2.0 in stage 72.0 (TID 141) in 201 ms on 172.18.0.14 (executor 1) (1/3)
26/02/13 11:45:54 INFO TaskSetManager: Finished task 1.0 in stage 72.0 (TID 139) in 253 ms on 172.18.0.15 (executor 0) (2/3)
26/02/13 11:45:55 INFO TaskSetManager: Finished task 0.0 in stage 72.0 (TID 140) in 692 ms on 172.18.0.14 (executor 1) (3/3)
26/02/13 11:45:55 INFO TaskSchedulerImpl: Removed TaskSet 72.0, whose tasks have all completed, from pool 
26/02/13 11:45:55 INFO DAGScheduler: ResultStage 72 (start at NativeMethodAccessorImpl.java:0) finished in 0.710 s
26/02/13 11:45:55 INFO DAGScheduler: Job 46 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:45:55 INFO TaskSchedulerImpl: Killing all running tasks in stage 72: Stage finished
26/02/13 11:45:55 INFO DAGScheduler: Job 46 finished: start at NativeMethodAccessorImpl.java:0, took 0.712404 s
26/02/13 11:45:55 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 18, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@2b033da1] is committing.
26/02/13 11:45:55 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 18, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@2b033da1] committed.
26/02/13 11:45:55 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/18 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.18.1d15c671-a6df-42ba-8d5f-c85413546b30.tmp
26/02/13 11:45:55 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.18.1d15c671-a6df-42ba-8d5f-c85413546b30.tmp to file:/tmp/spark-checkpoint-enrichment/commits/18
26/02/13 11:45:55 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "fc5c6452-bd70-4c90-becf-772d3bebdb57",
  "name" : null,
  "timestamp" : "2026-02-13T11:45:54.126Z",
  "batchId" : 18,
  "numInputRows" : 233,
  "inputRowsPerSecond" : 231.38033763654423,
  "processedRowsPerSecond" : 208.78136200716844,
  "durationMs" : {
    "addBatch" : 932,
    "commitOffsets" : 76,
    "getBatch" : 0,
    "latestOffset" : 2,
    "queryPlanning" : 37,
    "triggerExecution" : 1116,
    "walCommit" : 67
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 3407,
        "1" : 3772,
        "0" : 5595
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 3483,
        "1" : 3853,
        "0" : 5671
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 3483,
        "1" : 3853,
        "0" : 5671
      }
    },
    "numInputRows" : 233,
    "inputRowsPerSecond" : 231.38033763654423,
    "processedRowsPerSecond" : 208.78136200716844,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@7fe9b5de",
    "numOutputRows" : 50
  }
}
26/02/13 11:46:04 INFO BlockManagerInfo: Removed broadcast_69_piece0 on spark-master:40291 in memory (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 11:46:04 INFO BlockManagerInfo: Removed broadcast_69_piece0 on 172.18.0.14:40939 in memory (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 11:46:04 INFO BlockManagerInfo: Removed broadcast_69_piece0 on 172.18.0.15:34959 in memory (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 11:46:05 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 11:46:09 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/19 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.19.52cb5559-3100-44f5-8889-d2142207de9b.tmp
26/02/13 11:46:09 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.19.52cb5559-3100-44f5-8889-d2142207de9b.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/19
26/02/13 11:46:09 INFO MicroBatchExecution: Committed offsets for batch 19. Metadata OffsetSeqMetadata(0,1770983169444,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 11:46:09 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:46:09 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:46:09 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:46:09 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:46:09 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:46:09 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:46:09 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#18177 - origin_code.nullCount#18176) > 0)
26/02/13 11:46:09 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#18182 - destination_code.nullCount#18181) > 0)
26/02/13 11:46:09 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#18212 - callsign.nullCount#18211) > 0)
26/02/13 11:46:09 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:46:09 INFO DAGScheduler: Got job 47 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 11:46:09 INFO DAGScheduler: Final stage: ResultStage 74 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:46:09 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 73)
26/02/13 11:46:09 INFO DAGScheduler: Missing parents: List()
26/02/13 11:46:09 INFO DAGScheduler: Submitting ResultStage 74 (MapPartitionsRDD[252] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:46:09 INFO MemoryStore: Block broadcast_70 stored as values in memory (estimated size 99.3 KiB, free 434.0 MiB)
26/02/13 11:46:09 INFO MemoryStore: Block broadcast_70_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 433.9 MiB)
26/02/13 11:46:09 INFO BlockManagerInfo: Added broadcast_70_piece0 in memory on spark-master:40291 (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 11:46:09 INFO SparkContext: Created broadcast 70 from broadcast at DAGScheduler.scala:1585
26/02/13 11:46:09 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 74 (MapPartitionsRDD[252] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 11:46:09 INFO TaskSchedulerImpl: Adding task set 74.0 with 4 tasks resource profile 0
26/02/13 11:46:09 INFO TaskSetManager: Starting task 0.0 in stage 74.0 (TID 142) (172.18.0.15, executor 0, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:46:09 INFO TaskSetManager: Starting task 1.0 in stage 74.0 (TID 143) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:46:10 INFO BlockManagerInfo: Added broadcast_70_piece0 in memory on 172.18.0.15:34959 (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 11:46:10 INFO TaskSetManager: Starting task 2.0 in stage 74.0 (TID 144) (172.18.0.15, executor 0, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:46:10 INFO TaskSetManager: Finished task 0.0 in stage 74.0 (TID 142) in 71 ms on 172.18.0.15 (executor 0) (1/4)
26/02/13 11:46:10 INFO TaskSetManager: Starting task 3.0 in stage 74.0 (TID 145) (172.18.0.15, executor 0, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:46:10 INFO TaskSetManager: Finished task 1.0 in stage 74.0 (TID 143) in 86 ms on 172.18.0.15 (executor 0) (2/4)
26/02/13 11:46:10 INFO TaskSetManager: Finished task 2.0 in stage 74.0 (TID 144) in 53 ms on 172.18.0.15 (executor 0) (3/4)
26/02/13 11:46:10 INFO TaskSetManager: Finished task 3.0 in stage 74.0 (TID 145) in 35 ms on 172.18.0.15 (executor 0) (4/4)
26/02/13 11:46:10 INFO TaskSchedulerImpl: Removed TaskSet 74.0, whose tasks have all completed, from pool 
26/02/13 11:46:10 INFO DAGScheduler: ResultStage 74 (start at NativeMethodAccessorImpl.java:0) finished in 0.152 s
26/02/13 11:46:10 INFO DAGScheduler: Job 47 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:46:10 INFO TaskSchedulerImpl: Killing all running tasks in stage 74: Stage finished
26/02/13 11:46:10 INFO DAGScheduler: Job 47 finished: start at NativeMethodAccessorImpl.java:0, took 0.157695 s
26/02/13 11:46:10 INFO MemoryStore: Block broadcast_71_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.8 MiB)
26/02/13 11:46:10 INFO BlockManagerInfo: Added broadcast_71_piece0 in memory on spark-master:40291 (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 11:46:10 INFO SparkContext: Created broadcast 71 from start at NativeMethodAccessorImpl.java:0
26/02/13 11:46:10 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 19, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@6453e68f]. The input RDD has 3 partitions.
26/02/13 11:46:10 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:46:10 INFO DAGScheduler: Got job 48 (start at NativeMethodAccessorImpl.java:0) with 3 output partitions
26/02/13 11:46:10 INFO DAGScheduler: Final stage: ResultStage 75 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:46:10 INFO DAGScheduler: Parents of final stage: List()
26/02/13 11:46:10 INFO DAGScheduler: Missing parents: List()
26/02/13 11:46:10 INFO DAGScheduler: Submitting ResultStage 75 (MapPartitionsRDD[258] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:46:10 INFO MemoryStore: Block broadcast_72 stored as values in memory (estimated size 49.9 KiB, free 433.8 MiB)
26/02/13 11:46:10 INFO MemoryStore: Block broadcast_72_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.8 MiB)
26/02/13 11:46:10 INFO BlockManagerInfo: Added broadcast_72_piece0 in memory on spark-master:40291 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 11:46:10 INFO SparkContext: Created broadcast 72 from broadcast at DAGScheduler.scala:1585
26/02/13 11:46:10 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 75 (MapPartitionsRDD[258] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
26/02/13 11:46:10 INFO TaskSchedulerImpl: Adding task set 75.0 with 3 tasks resource profile 0
26/02/13 11:46:10 INFO TaskSetManager: Starting task 0.0 in stage 75.0 (TID 146) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:46:10 INFO TaskSetManager: Starting task 1.0 in stage 75.0 (TID 147) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:46:10 INFO TaskSetManager: Starting task 2.0 in stage 75.0 (TID 148) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:46:10 INFO BlockManagerInfo: Added broadcast_72_piece0 in memory on 172.18.0.14:40939 (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 11:46:10 INFO BlockManagerInfo: Added broadcast_72_piece0 in memory on 172.18.0.15:34959 (size: 19.5 KiB, free: 434.0 MiB)
26/02/13 11:46:10 INFO BlockManagerInfo: Added broadcast_71_piece0 in memory on 172.18.0.15:34959 (size: 107.2 KiB, free: 433.9 MiB)
26/02/13 11:46:10 INFO BlockManagerInfo: Added broadcast_71_piece0 in memory on 172.18.0.14:40939 (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 11:46:10 INFO TaskSetManager: Finished task 1.0 in stage 75.0 (TID 147) in 637 ms on 172.18.0.15 (executor 0) (1/3)
26/02/13 11:46:10 INFO TaskSetManager: Finished task 2.0 in stage 75.0 (TID 148) in 641 ms on 172.18.0.14 (executor 1) (2/3)
26/02/13 11:46:10 INFO TaskSetManager: Finished task 0.0 in stage 75.0 (TID 146) in 648 ms on 172.18.0.14 (executor 1) (3/3)
26/02/13 11:46:10 INFO TaskSchedulerImpl: Removed TaskSet 75.0, whose tasks have all completed, from pool 
26/02/13 11:46:10 INFO DAGScheduler: ResultStage 75 (start at NativeMethodAccessorImpl.java:0) finished in 0.655 s
26/02/13 11:46:10 INFO DAGScheduler: Job 48 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:46:10 INFO TaskSchedulerImpl: Killing all running tasks in stage 75: Stage finished
26/02/13 11:46:10 INFO DAGScheduler: Job 48 finished: start at NativeMethodAccessorImpl.java:0, took 0.658773 s
26/02/13 11:46:10 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 19, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@6453e68f] is committing.
26/02/13 11:46:10 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 19, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@6453e68f] committed.
26/02/13 11:46:10 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/19 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.19.79de971a-ea9d-470a-8234-f2448eb35a7c.tmp
26/02/13 11:46:10 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.19.79de971a-ea9d-470a-8234-f2448eb35a7c.tmp to file:/tmp/spark-checkpoint-enrichment/commits/19
26/02/13 11:46:10 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "fc5c6452-bd70-4c90-becf-772d3bebdb57",
  "name" : null,
  "timestamp" : "2026-02-13T11:46:09.439Z",
  "batchId" : 19,
  "numInputRows" : 107,
  "inputRowsPerSecond" : 6294.117647058823,
  "processedRowsPerSecond" : 74.30555555555556,
  "durationMs" : {
    "addBatch" : 1145,
    "commitOffsets" : 91,
    "getBatch" : 0,
    "latestOffset" : 5,
    "queryPlanning" : 80,
    "triggerExecution" : 1440,
    "walCommit" : 118
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 3483,
        "1" : 3853,
        "0" : 5671
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 3516,
        "1" : 3890,
        "0" : 5708
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 3516,
        "1" : 3890,
        "0" : 5708
      }
    },
    "numInputRows" : 107,
    "inputRowsPerSecond" : 6294.117647058823,
    "processedRowsPerSecond" : 74.30555555555556,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@7fe9b5de",
    "numOutputRows" : 26
  }
}
26/02/13 11:46:10 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/20 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.20.23b0eb55-b9d8-499f-8378-2a0a500d951f.tmp
26/02/13 11:46:10 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.20.23b0eb55-b9d8-499f-8378-2a0a500d951f.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/20
26/02/13 11:46:10 INFO MicroBatchExecution: Committed offsets for batch 20. Metadata OffsetSeqMetadata(0,1770983170882,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 11:46:11 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:46:11 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:46:11 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:46:11 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:46:11 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:46:11 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:46:11 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#19031 - origin_code.nullCount#19030) > 0)
26/02/13 11:46:11 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#19036 - destination_code.nullCount#19035) > 0)
26/02/13 11:46:11 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#19066 - callsign.nullCount#19065) > 0)
26/02/13 11:46:11 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:46:11 INFO DAGScheduler: Got job 49 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 11:46:11 INFO DAGScheduler: Final stage: ResultStage 77 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:46:11 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 76)
26/02/13 11:46:11 INFO DAGScheduler: Missing parents: List()
26/02/13 11:46:11 INFO DAGScheduler: Submitting ResultStage 77 (MapPartitionsRDD[263] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:46:11 INFO MemoryStore: Block broadcast_73 stored as values in memory (estimated size 99.3 KiB, free 433.7 MiB)
26/02/13 11:46:11 INFO MemoryStore: Block broadcast_73_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 433.6 MiB)
26/02/13 11:46:11 INFO BlockManagerInfo: Added broadcast_73_piece0 in memory on spark-master:40291 (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 11:46:11 INFO SparkContext: Created broadcast 73 from broadcast at DAGScheduler.scala:1585
26/02/13 11:46:11 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 77 (MapPartitionsRDD[263] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 11:46:11 INFO TaskSchedulerImpl: Adding task set 77.0 with 4 tasks resource profile 0
26/02/13 11:46:11 INFO TaskSetManager: Starting task 0.0 in stage 77.0 (TID 149) (172.18.0.15, executor 0, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:46:11 INFO TaskSetManager: Starting task 1.0 in stage 77.0 (TID 150) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:46:11 INFO BlockManagerInfo: Added broadcast_73_piece0 in memory on 172.18.0.15:34959 (size: 33.3 KiB, free: 433.9 MiB)
26/02/13 11:46:11 INFO TaskSetManager: Starting task 2.0 in stage 77.0 (TID 151) (172.18.0.15, executor 0, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:46:11 INFO TaskSetManager: Finished task 1.0 in stage 77.0 (TID 150) in 30 ms on 172.18.0.15 (executor 0) (1/4)
26/02/13 11:46:11 INFO TaskSetManager: Starting task 3.0 in stage 77.0 (TID 152) (172.18.0.15, executor 0, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:46:11 INFO TaskSetManager: Finished task 0.0 in stage 77.0 (TID 149) in 33 ms on 172.18.0.15 (executor 0) (2/4)
26/02/13 11:46:11 INFO TaskSetManager: Finished task 2.0 in stage 77.0 (TID 151) in 22 ms on 172.18.0.15 (executor 0) (3/4)
26/02/13 11:46:11 INFO TaskSetManager: Finished task 3.0 in stage 77.0 (TID 152) in 22 ms on 172.18.0.15 (executor 0) (4/4)
26/02/13 11:46:11 INFO TaskSchedulerImpl: Removed TaskSet 77.0, whose tasks have all completed, from pool 
26/02/13 11:46:11 INFO DAGScheduler: ResultStage 77 (start at NativeMethodAccessorImpl.java:0) finished in 0.066 s
26/02/13 11:46:11 INFO DAGScheduler: Job 49 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:46:11 INFO TaskSchedulerImpl: Killing all running tasks in stage 77: Stage finished
26/02/13 11:46:11 INFO DAGScheduler: Job 49 finished: start at NativeMethodAccessorImpl.java:0, took 0.070979 s
26/02/13 11:46:11 INFO BlockManagerInfo: Removed broadcast_73_piece0 on spark-master:40291 in memory (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 11:46:11 INFO BlockManagerInfo: Removed broadcast_73_piece0 on 172.18.0.15:34959 in memory (size: 33.3 KiB, free: 433.9 MiB)
26/02/13 11:46:11 INFO MemoryStore: Block broadcast_74_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.7 MiB)
26/02/13 11:46:11 INFO BlockManagerInfo: Added broadcast_74_piece0 in memory on spark-master:40291 (size: 107.2 KiB, free: 434.0 MiB)
26/02/13 11:46:11 INFO SparkContext: Created broadcast 74 from start at NativeMethodAccessorImpl.java:0
26/02/13 11:46:11 INFO BlockManagerInfo: Removed broadcast_68_piece0 on spark-master:40291 in memory (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 11:46:11 INFO BlockManagerInfo: Removed broadcast_68_piece0 on 172.18.0.15:34959 in memory (size: 107.2 KiB, free: 434.0 MiB)
26/02/13 11:46:11 INFO BlockManagerInfo: Removed broadcast_68_piece0 on 172.18.0.14:40939 in memory (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 11:46:11 INFO BlockManagerInfo: Removed broadcast_70_piece0 on spark-master:40291 in memory (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 11:46:11 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 20, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@49de0f0e]. The input RDD has 3 partitions.
26/02/13 11:46:11 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:46:11 INFO BlockManagerInfo: Removed broadcast_70_piece0 on 172.18.0.15:34959 in memory (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 11:46:11 INFO DAGScheduler: Got job 50 (start at NativeMethodAccessorImpl.java:0) with 3 output partitions
26/02/13 11:46:11 INFO DAGScheduler: Final stage: ResultStage 78 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:46:11 INFO DAGScheduler: Parents of final stage: List()
26/02/13 11:46:11 INFO DAGScheduler: Missing parents: List()
26/02/13 11:46:11 INFO DAGScheduler: Submitting ResultStage 78 (MapPartitionsRDD[269] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:46:11 INFO BlockManagerInfo: Removed broadcast_71_piece0 on spark-master:40291 in memory (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 11:46:11 INFO BlockManagerInfo: Removed broadcast_71_piece0 on 172.18.0.15:34959 in memory (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 11:46:11 INFO MemoryStore: Block broadcast_75 stored as values in memory (estimated size 49.9 KiB, free 433.9 MiB)
26/02/13 11:46:11 INFO BlockManagerInfo: Removed broadcast_71_piece0 on 172.18.0.14:40939 in memory (size: 107.2 KiB, free: 434.4 MiB)
26/02/13 11:46:11 INFO MemoryStore: Block broadcast_75_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.9 MiB)
26/02/13 11:46:11 INFO BlockManagerInfo: Added broadcast_75_piece0 in memory on spark-master:40291 (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 11:46:11 INFO SparkContext: Created broadcast 75 from broadcast at DAGScheduler.scala:1585
26/02/13 11:46:11 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 78 (MapPartitionsRDD[269] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
26/02/13 11:46:11 INFO TaskSchedulerImpl: Adding task set 78.0 with 3 tasks resource profile 0
26/02/13 11:46:11 INFO TaskSetManager: Starting task 1.0 in stage 78.0 (TID 153) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:46:11 INFO TaskSetManager: Starting task 0.0 in stage 78.0 (TID 154) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:46:11 INFO TaskSetManager: Starting task 2.0 in stage 78.0 (TID 155) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:46:11 INFO BlockManagerInfo: Removed broadcast_72_piece0 on spark-master:40291 in memory (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 11:46:11 INFO BlockManagerInfo: Removed broadcast_72_piece0 on 172.18.0.15:34959 in memory (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 11:46:11 INFO BlockManagerInfo: Removed broadcast_72_piece0 on 172.18.0.14:40939 in memory (size: 19.5 KiB, free: 434.4 MiB)
26/02/13 11:46:11 INFO BlockManagerInfo: Added broadcast_75_piece0 in memory on 172.18.0.15:34959 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 11:46:11 INFO BlockManagerInfo: Added broadcast_75_piece0 in memory on 172.18.0.14:40939 (size: 19.5 KiB, free: 434.4 MiB)
26/02/13 11:46:11 INFO BlockManagerInfo: Added broadcast_74_piece0 in memory on 172.18.0.14:40939 (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 11:46:11 INFO BlockManagerInfo: Added broadcast_74_piece0 in memory on 172.18.0.15:34959 (size: 107.2 KiB, free: 434.0 MiB)
26/02/13 11:46:11 INFO TaskSetManager: Finished task 0.0 in stage 78.0 (TID 154) in 113 ms on 172.18.0.14 (executor 1) (1/3)
26/02/13 11:46:11 INFO TaskSetManager: Finished task 2.0 in stage 78.0 (TID 155) in 115 ms on 172.18.0.14 (executor 1) (2/3)
26/02/13 11:46:11 INFO TaskSetManager: Finished task 1.0 in stage 78.0 (TID 153) in 138 ms on 172.18.0.15 (executor 0) (3/3)
26/02/13 11:46:11 INFO TaskSchedulerImpl: Removed TaskSet 78.0, whose tasks have all completed, from pool 
26/02/13 11:46:11 INFO DAGScheduler: ResultStage 78 (start at NativeMethodAccessorImpl.java:0) finished in 0.161 s
26/02/13 11:46:11 INFO DAGScheduler: Job 50 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:46:11 INFO TaskSchedulerImpl: Killing all running tasks in stage 78: Stage finished
26/02/13 11:46:11 INFO DAGScheduler: Job 50 finished: start at NativeMethodAccessorImpl.java:0, took 0.167267 s
26/02/13 11:46:11 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 20, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@49de0f0e] is committing.
26/02/13 11:46:11 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 20, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@49de0f0e] committed.
26/02/13 11:46:11 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/20 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.20.c30c4ff8-eacc-4056-af38-42d4c1cb7797.tmp
26/02/13 11:46:11 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.20.c30c4ff8-eacc-4056-af38-42d4c1cb7797.tmp to file:/tmp/spark-checkpoint-enrichment/commits/20
26/02/13 11:46:11 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "fc5c6452-bd70-4c90-becf-772d3bebdb57",
  "name" : null,
  "timestamp" : "2026-02-13T11:46:10.881Z",
  "batchId" : 20,
  "numInputRows" : 131,
  "inputRowsPerSecond" : 90.84604715672677,
  "processedRowsPerSecond" : 214.75409836065575,
  "durationMs" : {
    "addBatch" : 407,
    "commitOffsets" : 73,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 62,
    "triggerExecution" : 610,
    "walCommit" : 67
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 3516,
        "1" : 3890,
        "0" : 5708
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 3560,
        "1" : 3935,
        "0" : 5750
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 3560,
        "1" : 3935,
        "0" : 5750
      }
    },
    "numInputRows" : 131,
    "inputRowsPerSecond" : 90.84604715672677,
    "processedRowsPerSecond" : 214.75409836065575,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@7fe9b5de",
    "numOutputRows" : 26
  }
}
26/02/13 11:46:20 INFO BlockManagerInfo: Removed broadcast_75_piece0 on spark-master:40291 in memory (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 11:46:20 INFO BlockManagerInfo: Removed broadcast_75_piece0 on 172.18.0.14:40939 in memory (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 11:46:20 INFO BlockManagerInfo: Removed broadcast_75_piece0 on 172.18.0.15:34959 in memory (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 11:46:21 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 11:46:25 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/21 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.21.925253b6-0110-465b-8704-37b9129b4728.tmp
26/02/13 11:46:25 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.21.925253b6-0110-465b-8704-37b9129b4728.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/21
26/02/13 11:46:25 INFO MicroBatchExecution: Committed offsets for batch 21. Metadata OffsetSeqMetadata(0,1770983185790,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 11:46:25 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:46:25 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:46:25 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:46:25 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:46:26 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:46:26 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:46:26 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#19885 - origin_code.nullCount#19884) > 0)
26/02/13 11:46:26 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#19890 - destination_code.nullCount#19889) > 0)
26/02/13 11:46:26 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#19920 - callsign.nullCount#19919) > 0)
26/02/13 11:46:26 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:46:26 INFO DAGScheduler: Got job 51 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 11:46:26 INFO DAGScheduler: Final stage: ResultStage 80 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:46:26 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 79)
26/02/13 11:46:26 INFO DAGScheduler: Missing parents: List()
26/02/13 11:46:26 INFO DAGScheduler: Submitting ResultStage 80 (MapPartitionsRDD[274] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:46:26 INFO MemoryStore: Block broadcast_76 stored as values in memory (estimated size 99.3 KiB, free 434.0 MiB)
26/02/13 11:46:26 INFO MemoryStore: Block broadcast_76_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 433.9 MiB)
26/02/13 11:46:26 INFO BlockManagerInfo: Added broadcast_76_piece0 in memory on spark-master:40291 (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 11:46:26 INFO SparkContext: Created broadcast 76 from broadcast at DAGScheduler.scala:1585
26/02/13 11:46:26 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 80 (MapPartitionsRDD[274] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 11:46:26 INFO TaskSchedulerImpl: Adding task set 80.0 with 4 tasks resource profile 0
26/02/13 11:46:26 INFO TaskSetManager: Starting task 0.0 in stage 80.0 (TID 156) (172.18.0.15, executor 0, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:46:26 INFO TaskSetManager: Starting task 1.0 in stage 80.0 (TID 157) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:46:26 INFO BlockManagerInfo: Added broadcast_76_piece0 in memory on 172.18.0.15:34959 (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 11:46:26 INFO TaskSetManager: Starting task 2.0 in stage 80.0 (TID 158) (172.18.0.15, executor 0, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:46:26 INFO TaskSetManager: Finished task 0.0 in stage 80.0 (TID 156) in 47 ms on 172.18.0.15 (executor 0) (1/4)
26/02/13 11:46:26 INFO TaskSetManager: Starting task 3.0 in stage 80.0 (TID 159) (172.18.0.15, executor 0, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:46:26 INFO TaskSetManager: Finished task 1.0 in stage 80.0 (TID 157) in 56 ms on 172.18.0.15 (executor 0) (2/4)
26/02/13 11:46:26 INFO TaskSetManager: Finished task 2.0 in stage 80.0 (TID 158) in 39 ms on 172.18.0.15 (executor 0) (3/4)
26/02/13 11:46:26 INFO TaskSetManager: Finished task 3.0 in stage 80.0 (TID 159) in 36 ms on 172.18.0.15 (executor 0) (4/4)
26/02/13 11:46:26 INFO TaskSchedulerImpl: Removed TaskSet 80.0, whose tasks have all completed, from pool 
26/02/13 11:46:26 INFO DAGScheduler: ResultStage 80 (start at NativeMethodAccessorImpl.java:0) finished in 0.107 s
26/02/13 11:46:26 INFO DAGScheduler: Job 51 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:46:26 INFO TaskSchedulerImpl: Killing all running tasks in stage 80: Stage finished
26/02/13 11:46:26 INFO DAGScheduler: Job 51 finished: start at NativeMethodAccessorImpl.java:0, took 0.111714 s
26/02/13 11:46:26 INFO MemoryStore: Block broadcast_77_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.8 MiB)
26/02/13 11:46:26 INFO BlockManagerInfo: Added broadcast_77_piece0 in memory on spark-master:40291 (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 11:46:26 INFO SparkContext: Created broadcast 77 from start at NativeMethodAccessorImpl.java:0
26/02/13 11:46:26 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 21, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@25c82fed]. The input RDD has 3 partitions.
26/02/13 11:46:26 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:46:26 INFO DAGScheduler: Got job 52 (start at NativeMethodAccessorImpl.java:0) with 3 output partitions
26/02/13 11:46:26 INFO DAGScheduler: Final stage: ResultStage 81 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:46:26 INFO DAGScheduler: Parents of final stage: List()
26/02/13 11:46:26 INFO DAGScheduler: Missing parents: List()
26/02/13 11:46:26 INFO DAGScheduler: Submitting ResultStage 81 (MapPartitionsRDD[280] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:46:26 INFO MemoryStore: Block broadcast_78 stored as values in memory (estimated size 49.9 KiB, free 433.8 MiB)
26/02/13 11:46:26 INFO MemoryStore: Block broadcast_78_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.8 MiB)
26/02/13 11:46:26 INFO BlockManagerInfo: Added broadcast_78_piece0 in memory on spark-master:40291 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 11:46:26 INFO SparkContext: Created broadcast 78 from broadcast at DAGScheduler.scala:1585
26/02/13 11:46:26 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 81 (MapPartitionsRDD[280] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
26/02/13 11:46:26 INFO TaskSchedulerImpl: Adding task set 81.0 with 3 tasks resource profile 0
26/02/13 11:46:26 INFO TaskSetManager: Starting task 1.0 in stage 81.0 (TID 160) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:46:26 INFO TaskSetManager: Starting task 0.0 in stage 81.0 (TID 161) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:46:26 INFO TaskSetManager: Starting task 2.0 in stage 81.0 (TID 162) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:46:26 INFO BlockManagerInfo: Added broadcast_78_piece0 in memory on 172.18.0.15:34959 (size: 19.5 KiB, free: 434.0 MiB)
26/02/13 11:46:26 INFO BlockManagerInfo: Added broadcast_78_piece0 in memory on 172.18.0.14:40939 (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 11:46:26 INFO BlockManagerInfo: Added broadcast_77_piece0 in memory on 172.18.0.15:34959 (size: 107.2 KiB, free: 433.9 MiB)
26/02/13 11:46:26 INFO BlockManagerInfo: Added broadcast_77_piece0 in memory on 172.18.0.14:40939 (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 11:46:26 INFO TaskSetManager: Finished task 1.0 in stage 81.0 (TID 160) in 575 ms on 172.18.0.15 (executor 0) (1/3)
26/02/13 11:46:26 INFO TaskSetManager: Finished task 0.0 in stage 81.0 (TID 161) in 584 ms on 172.18.0.14 (executor 1) (2/3)
26/02/13 11:46:26 INFO TaskSetManager: Finished task 2.0 in stage 81.0 (TID 162) in 584 ms on 172.18.0.14 (executor 1) (3/3)
26/02/13 11:46:26 INFO TaskSchedulerImpl: Removed TaskSet 81.0, whose tasks have all completed, from pool 
26/02/13 11:46:26 INFO DAGScheduler: ResultStage 81 (start at NativeMethodAccessorImpl.java:0) finished in 0.590 s
26/02/13 11:46:26 INFO DAGScheduler: Job 52 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:46:26 INFO TaskSchedulerImpl: Killing all running tasks in stage 81: Stage finished
26/02/13 11:46:26 INFO DAGScheduler: Job 52 finished: start at NativeMethodAccessorImpl.java:0, took 0.591853 s
26/02/13 11:46:26 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 21, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@25c82fed] is committing.
26/02/13 11:46:26 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 21, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@25c82fed] committed.
26/02/13 11:46:26 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/21 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.21.5e6addf5-22fa-4906-ab9a-2c6636098673.tmp
26/02/13 11:46:26 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.21.5e6addf5-22fa-4906-ab9a-2c6636098673.tmp to file:/tmp/spark-checkpoint-enrichment/commits/21
26/02/13 11:46:26 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "fc5c6452-bd70-4c90-becf-772d3bebdb57",
  "name" : null,
  "timestamp" : "2026-02-13T11:46:25.787Z",
  "batchId" : 21,
  "numInputRows" : 8,
  "inputRowsPerSecond" : 666.6666666666666,
  "processedRowsPerSecond" : 7.504690431519699,
  "durationMs" : {
    "addBatch" : 853,
    "commitOffsets" : 60,
    "getBatch" : 0,
    "latestOffset" : 3,
    "queryPlanning" : 56,
    "triggerExecution" : 1066,
    "walCommit" : 94
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 3560,
        "1" : 3935,
        "0" : 5750
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 3562,
        "1" : 3940,
        "0" : 5751
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 3562,
        "1" : 3940,
        "0" : 5751
      }
    },
    "numInputRows" : 8,
    "inputRowsPerSecond" : 666.6666666666666,
    "processedRowsPerSecond" : 7.504690431519699,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@7fe9b5de",
    "numOutputRows" : 3
  }
}
26/02/13 11:46:26 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/22 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.22.7dfdeef8-15a8-4156-9f3e-32c7cd101881.tmp
26/02/13 11:46:26 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.22.7dfdeef8-15a8-4156-9f3e-32c7cd101881.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/22
26/02/13 11:46:26 INFO MicroBatchExecution: Committed offsets for batch 22. Metadata OffsetSeqMetadata(0,1770983186855,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 11:46:26 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:46:26 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:46:26 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:46:26 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:46:26 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:46:26 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:46:27 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#20739 - origin_code.nullCount#20738) > 0)
26/02/13 11:46:27 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#20744 - destination_code.nullCount#20743) > 0)
26/02/13 11:46:27 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#20774 - callsign.nullCount#20773) > 0)
26/02/13 11:46:27 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:46:27 INFO DAGScheduler: Got job 53 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 11:46:27 INFO DAGScheduler: Final stage: ResultStage 83 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:46:27 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 82)
26/02/13 11:46:27 INFO DAGScheduler: Missing parents: List()
26/02/13 11:46:27 INFO DAGScheduler: Submitting ResultStage 83 (MapPartitionsRDD[285] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:46:27 INFO MemoryStore: Block broadcast_79 stored as values in memory (estimated size 99.3 KiB, free 433.7 MiB)
26/02/13 11:46:27 INFO MemoryStore: Block broadcast_79_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 433.6 MiB)
26/02/13 11:46:27 INFO BlockManagerInfo: Added broadcast_79_piece0 in memory on spark-master:40291 (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 11:46:27 INFO SparkContext: Created broadcast 79 from broadcast at DAGScheduler.scala:1585
26/02/13 11:46:27 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 83 (MapPartitionsRDD[285] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 11:46:27 INFO TaskSchedulerImpl: Adding task set 83.0 with 4 tasks resource profile 0
26/02/13 11:46:27 INFO TaskSetManager: Starting task 0.0 in stage 83.0 (TID 163) (172.18.0.15, executor 0, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:46:27 INFO TaskSetManager: Starting task 1.0 in stage 83.0 (TID 164) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:46:27 INFO BlockManagerInfo: Added broadcast_79_piece0 in memory on 172.18.0.15:34959 (size: 33.3 KiB, free: 433.9 MiB)
26/02/13 11:46:27 INFO TaskSetManager: Starting task 2.0 in stage 83.0 (TID 165) (172.18.0.15, executor 0, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:46:27 INFO TaskSetManager: Finished task 1.0 in stage 83.0 (TID 164) in 29 ms on 172.18.0.15 (executor 0) (1/4)
26/02/13 11:46:27 INFO TaskSetManager: Starting task 3.0 in stage 83.0 (TID 166) (172.18.0.15, executor 0, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:46:27 INFO TaskSetManager: Finished task 0.0 in stage 83.0 (TID 163) in 31 ms on 172.18.0.15 (executor 0) (2/4)
26/02/13 11:46:27 INFO TaskSetManager: Finished task 2.0 in stage 83.0 (TID 165) in 21 ms on 172.18.0.15 (executor 0) (3/4)
26/02/13 11:46:27 INFO TaskSetManager: Finished task 3.0 in stage 83.0 (TID 166) in 22 ms on 172.18.0.15 (executor 0) (4/4)
26/02/13 11:46:27 INFO TaskSchedulerImpl: Removed TaskSet 83.0, whose tasks have all completed, from pool 
26/02/13 11:46:27 INFO DAGScheduler: ResultStage 83 (start at NativeMethodAccessorImpl.java:0) finished in 0.065 s
26/02/13 11:46:27 INFO DAGScheduler: Job 53 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:46:27 INFO TaskSchedulerImpl: Killing all running tasks in stage 83: Stage finished
26/02/13 11:46:27 INFO DAGScheduler: Job 53 finished: start at NativeMethodAccessorImpl.java:0, took 0.068510 s
26/02/13 11:46:27 INFO BlockManagerInfo: Removed broadcast_74_piece0 on spark-master:40291 in memory (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 11:46:27 INFO MemoryStore: Block broadcast_80_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.6 MiB)
26/02/13 11:46:27 INFO BlockManagerInfo: Removed broadcast_74_piece0 on 172.18.0.15:34959 in memory (size: 107.2 KiB, free: 434.0 MiB)
26/02/13 11:46:27 INFO BlockManagerInfo: Added broadcast_80_piece0 in memory on spark-master:40291 (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 11:46:27 INFO SparkContext: Created broadcast 80 from start at NativeMethodAccessorImpl.java:0
26/02/13 11:46:27 INFO BlockManagerInfo: Removed broadcast_74_piece0 on 172.18.0.14:40939 in memory (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 11:46:27 INFO BlockManagerInfo: Removed broadcast_76_piece0 on spark-master:40291 in memory (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 11:46:27 INFO BlockManagerInfo: Removed broadcast_76_piece0 on 172.18.0.15:34959 in memory (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 11:46:27 INFO BlockManagerInfo: Removed broadcast_77_piece0 on spark-master:40291 in memory (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 11:46:27 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 22, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@7115b75d]. The input RDD has 3 partitions.
26/02/13 11:46:27 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:46:27 INFO BlockManagerInfo: Removed broadcast_77_piece0 on 172.18.0.15:34959 in memory (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 11:46:27 INFO DAGScheduler: Got job 54 (start at NativeMethodAccessorImpl.java:0) with 3 output partitions
26/02/13 11:46:27 INFO DAGScheduler: Final stage: ResultStage 84 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:46:27 INFO DAGScheduler: Parents of final stage: List()
26/02/13 11:46:27 INFO DAGScheduler: Missing parents: List()
26/02/13 11:46:27 INFO DAGScheduler: Submitting ResultStage 84 (MapPartitionsRDD[291] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:46:27 INFO BlockManagerInfo: Removed broadcast_77_piece0 on 172.18.0.14:40939 in memory (size: 107.2 KiB, free: 434.4 MiB)
26/02/13 11:46:27 INFO MemoryStore: Block broadcast_81 stored as values in memory (estimated size 49.9 KiB, free 433.8 MiB)
26/02/13 11:46:27 INFO BlockManagerInfo: Removed broadcast_79_piece0 on spark-master:40291 in memory (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 11:46:27 INFO MemoryStore: Block broadcast_81_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.8 MiB)
26/02/13 11:46:27 INFO BlockManagerInfo: Removed broadcast_79_piece0 on 172.18.0.15:34959 in memory (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 11:46:27 INFO BlockManagerInfo: Added broadcast_81_piece0 in memory on spark-master:40291 (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 11:46:27 INFO SparkContext: Created broadcast 81 from broadcast at DAGScheduler.scala:1585
26/02/13 11:46:27 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 84 (MapPartitionsRDD[291] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
26/02/13 11:46:27 INFO TaskSchedulerImpl: Adding task set 84.0 with 3 tasks resource profile 0
26/02/13 11:46:27 INFO TaskSetManager: Starting task 1.0 in stage 84.0 (TID 167) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:46:27 INFO TaskSetManager: Starting task 0.0 in stage 84.0 (TID 168) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:46:27 INFO BlockManagerInfo: Removed broadcast_78_piece0 on spark-master:40291 in memory (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 11:46:27 INFO TaskSetManager: Starting task 2.0 in stage 84.0 (TID 169) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:46:27 INFO BlockManagerInfo: Removed broadcast_78_piece0 on 172.18.0.15:34959 in memory (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 11:46:27 INFO BlockManagerInfo: Removed broadcast_78_piece0 on 172.18.0.14:40939 in memory (size: 19.5 KiB, free: 434.4 MiB)
26/02/13 11:46:27 INFO BlockManagerInfo: Added broadcast_81_piece0 in memory on 172.18.0.14:40939 (size: 19.5 KiB, free: 434.4 MiB)
26/02/13 11:46:27 INFO BlockManagerInfo: Added broadcast_81_piece0 in memory on 172.18.0.15:34959 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 11:46:27 INFO BlockManagerInfo: Added broadcast_80_piece0 in memory on 172.18.0.14:40939 (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 11:46:27 INFO BlockManagerInfo: Added broadcast_80_piece0 in memory on 172.18.0.15:34959 (size: 107.2 KiB, free: 434.0 MiB)
26/02/13 11:46:27 INFO TaskSetManager: Finished task 2.0 in stage 84.0 (TID 169) in 143 ms on 172.18.0.14 (executor 1) (1/3)
26/02/13 11:46:27 INFO TaskSetManager: Finished task 0.0 in stage 84.0 (TID 168) in 147 ms on 172.18.0.14 (executor 1) (2/3)
26/02/13 11:46:27 INFO TaskSetManager: Finished task 1.0 in stage 84.0 (TID 167) in 153 ms on 172.18.0.15 (executor 0) (3/3)
26/02/13 11:46:27 INFO TaskSchedulerImpl: Removed TaskSet 84.0, whose tasks have all completed, from pool 
26/02/13 11:46:27 INFO DAGScheduler: ResultStage 84 (start at NativeMethodAccessorImpl.java:0) finished in 0.169 s
26/02/13 11:46:27 INFO DAGScheduler: Job 54 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:46:27 INFO TaskSchedulerImpl: Killing all running tasks in stage 84: Stage finished
26/02/13 11:46:27 INFO DAGScheduler: Job 54 finished: start at NativeMethodAccessorImpl.java:0, took 0.173235 s
26/02/13 11:46:27 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 22, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@7115b75d] is committing.
26/02/13 11:46:27 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 22, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@7115b75d] committed.
26/02/13 11:46:27 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/22 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.22.64d9902b-407b-42d5-81c9-c7de8ea685b8.tmp
26/02/13 11:46:27 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.22.64d9902b-407b-42d5-81c9-c7de8ea685b8.tmp to file:/tmp/spark-checkpoint-enrichment/commits/22
26/02/13 11:46:27 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "fc5c6452-bd70-4c90-becf-772d3bebdb57",
  "name" : null,
  "timestamp" : "2026-02-13T11:46:26.853Z",
  "batchId" : 22,
  "numInputRows" : 227,
  "inputRowsPerSecond" : 212.94559099437146,
  "processedRowsPerSecond" : 432.38095238095235,
  "durationMs" : {
    "addBatch" : 369,
    "commitOffsets" : 64,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 30,
    "triggerExecution" : 525,
    "walCommit" : 59
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 3562,
        "1" : 3940,
        "0" : 5751
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 3636,
        "1" : 4017,
        "0" : 5827
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 3636,
        "1" : 4017,
        "0" : 5827
      }
    },
    "numInputRows" : 227,
    "inputRowsPerSecond" : 212.94559099437146,
    "processedRowsPerSecond" : 432.38095238095235,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@7fe9b5de",
    "numOutputRows" : 51
  }
}
26/02/13 11:46:36 INFO BlockManagerInfo: Removed broadcast_81_piece0 on spark-master:40291 in memory (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 11:46:36 INFO BlockManagerInfo: Removed broadcast_81_piece0 on 172.18.0.15:34959 in memory (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 11:46:36 INFO BlockManagerInfo: Removed broadcast_81_piece0 on 172.18.0.14:40939 in memory (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 11:46:37 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 11:46:44 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/23 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.23.ebba65ef-05d8-41a4-a694-d6cd587d13ea.tmp
26/02/13 11:46:44 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.23.ebba65ef-05d8-41a4-a694-d6cd587d13ea.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/23
26/02/13 11:46:44 INFO MicroBatchExecution: Committed offsets for batch 23. Metadata OffsetSeqMetadata(0,1770983204060,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 11:46:44 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:46:44 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:46:44 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:46:44 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:46:44 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:46:44 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:46:44 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#21593 - origin_code.nullCount#21592) > 0)
26/02/13 11:46:44 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#21598 - destination_code.nullCount#21597) > 0)
26/02/13 11:46:44 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#21628 - callsign.nullCount#21627) > 0)
26/02/13 11:46:44 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:46:44 INFO DAGScheduler: Got job 55 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 11:46:44 INFO DAGScheduler: Final stage: ResultStage 86 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:46:44 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 85)
26/02/13 11:46:44 INFO DAGScheduler: Missing parents: List()
26/02/13 11:46:44 INFO DAGScheduler: Submitting ResultStage 86 (MapPartitionsRDD[296] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:46:44 INFO MemoryStore: Block broadcast_82 stored as values in memory (estimated size 99.3 KiB, free 434.0 MiB)
26/02/13 11:46:44 INFO MemoryStore: Block broadcast_82_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 433.9 MiB)
26/02/13 11:46:44 INFO BlockManagerInfo: Added broadcast_82_piece0 in memory on spark-master:40291 (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 11:46:44 INFO SparkContext: Created broadcast 82 from broadcast at DAGScheduler.scala:1585
26/02/13 11:46:44 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 86 (MapPartitionsRDD[296] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 11:46:44 INFO TaskSchedulerImpl: Adding task set 86.0 with 4 tasks resource profile 0
26/02/13 11:46:44 INFO TaskSetManager: Starting task 0.0 in stage 86.0 (TID 170) (172.18.0.15, executor 0, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:46:44 INFO TaskSetManager: Starting task 1.0 in stage 86.0 (TID 171) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:46:44 INFO BlockManagerInfo: Added broadcast_82_piece0 in memory on 172.18.0.15:34959 (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 11:46:44 INFO TaskSetManager: Starting task 2.0 in stage 86.0 (TID 172) (172.18.0.15, executor 0, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:46:44 INFO TaskSetManager: Finished task 0.0 in stage 86.0 (TID 170) in 32 ms on 172.18.0.15 (executor 0) (1/4)
26/02/13 11:46:44 INFO TaskSetManager: Starting task 3.0 in stage 86.0 (TID 173) (172.18.0.15, executor 0, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:46:44 INFO TaskSetManager: Finished task 1.0 in stage 86.0 (TID 171) in 36 ms on 172.18.0.15 (executor 0) (2/4)
26/02/13 11:46:44 INFO TaskSetManager: Finished task 2.0 in stage 86.0 (TID 172) in 19 ms on 172.18.0.15 (executor 0) (3/4)
26/02/13 11:46:44 INFO TaskSetManager: Finished task 3.0 in stage 86.0 (TID 173) in 26 ms on 172.18.0.15 (executor 0) (4/4)
26/02/13 11:46:44 INFO TaskSchedulerImpl: Removed TaskSet 86.0, whose tasks have all completed, from pool 
26/02/13 11:46:44 INFO DAGScheduler: ResultStage 86 (start at NativeMethodAccessorImpl.java:0) finished in 0.073 s
26/02/13 11:46:44 INFO DAGScheduler: Job 55 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:46:44 INFO TaskSchedulerImpl: Killing all running tasks in stage 86: Stage finished
26/02/13 11:46:44 INFO DAGScheduler: Job 55 finished: start at NativeMethodAccessorImpl.java:0, took 0.076517 s
26/02/13 11:46:44 INFO MemoryStore: Block broadcast_83_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.8 MiB)
26/02/13 11:46:44 INFO BlockManagerInfo: Added broadcast_83_piece0 in memory on spark-master:40291 (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 11:46:44 INFO SparkContext: Created broadcast 83 from start at NativeMethodAccessorImpl.java:0
26/02/13 11:46:44 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 23, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@58e5ea87]. The input RDD has 3 partitions.
26/02/13 11:46:44 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:46:44 INFO DAGScheduler: Got job 56 (start at NativeMethodAccessorImpl.java:0) with 3 output partitions
26/02/13 11:46:44 INFO DAGScheduler: Final stage: ResultStage 87 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:46:44 INFO DAGScheduler: Parents of final stage: List()
26/02/13 11:46:44 INFO DAGScheduler: Missing parents: List()
26/02/13 11:46:44 INFO DAGScheduler: Submitting ResultStage 87 (MapPartitionsRDD[302] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:46:44 INFO MemoryStore: Block broadcast_84 stored as values in memory (estimated size 49.9 KiB, free 433.8 MiB)
26/02/13 11:46:44 INFO MemoryStore: Block broadcast_84_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.8 MiB)
26/02/13 11:46:44 INFO BlockManagerInfo: Added broadcast_84_piece0 in memory on spark-master:40291 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 11:46:44 INFO SparkContext: Created broadcast 84 from broadcast at DAGScheduler.scala:1585
26/02/13 11:46:44 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 87 (MapPartitionsRDD[302] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
26/02/13 11:46:44 INFO TaskSchedulerImpl: Adding task set 87.0 with 3 tasks resource profile 0
26/02/13 11:46:44 INFO TaskSetManager: Starting task 1.0 in stage 87.0 (TID 174) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:46:44 INFO TaskSetManager: Starting task 0.0 in stage 87.0 (TID 175) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:46:44 INFO TaskSetManager: Starting task 2.0 in stage 87.0 (TID 176) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:46:44 INFO BlockManagerInfo: Added broadcast_84_piece0 in memory on 172.18.0.15:34959 (size: 19.5 KiB, free: 434.0 MiB)
26/02/13 11:46:44 INFO BlockManagerInfo: Added broadcast_83_piece0 in memory on 172.18.0.15:34959 (size: 107.2 KiB, free: 433.9 MiB)
26/02/13 11:46:44 INFO BlockManagerInfo: Added broadcast_84_piece0 in memory on 172.18.0.14:40939 (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 11:46:44 INFO BlockManagerInfo: Added broadcast_83_piece0 in memory on 172.18.0.14:40939 (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 11:46:45 INFO TaskSetManager: Finished task 1.0 in stage 87.0 (TID 174) in 596 ms on 172.18.0.15 (executor 0) (1/3)
26/02/13 11:46:45 INFO TaskSetManager: Finished task 0.0 in stage 87.0 (TID 175) in 633 ms on 172.18.0.14 (executor 1) (2/3)
26/02/13 11:46:45 INFO TaskSetManager: Finished task 2.0 in stage 87.0 (TID 176) in 633 ms on 172.18.0.14 (executor 1) (3/3)
26/02/13 11:46:45 INFO TaskSchedulerImpl: Removed TaskSet 87.0, whose tasks have all completed, from pool 
26/02/13 11:46:45 INFO DAGScheduler: ResultStage 87 (start at NativeMethodAccessorImpl.java:0) finished in 0.643 s
26/02/13 11:46:45 INFO DAGScheduler: Job 56 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:46:45 INFO TaskSchedulerImpl: Killing all running tasks in stage 87: Stage finished
26/02/13 11:46:45 INFO DAGScheduler: Job 56 finished: start at NativeMethodAccessorImpl.java:0, took 0.645925 s
26/02/13 11:46:45 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 23, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@58e5ea87] is committing.
26/02/13 11:46:45 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 23, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@58e5ea87] committed.
26/02/13 11:46:45 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/23 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.23.eefcfc47-7f3d-47f2-a370-14974703fd96.tmp
26/02/13 11:46:45 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.23.eefcfc47-7f3d-47f2-a370-14974703fd96.tmp to file:/tmp/spark-checkpoint-enrichment/commits/23
26/02/13 11:46:45 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "fc5c6452-bd70-4c90-becf-772d3bebdb57",
  "name" : null,
  "timestamp" : "2026-02-13T11:46:44.058Z",
  "batchId" : 23,
  "numInputRows" : 111,
  "inputRowsPerSecond" : 9250.0,
  "processedRowsPerSecond" : 102.6827012025902,
  "durationMs" : {
    "addBatch" : 870,
    "commitOffsets" : 70,
    "getBatch" : 0,
    "latestOffset" : 2,
    "queryPlanning" : 59,
    "triggerExecution" : 1081,
    "walCommit" : 80
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 3636,
        "1" : 4017,
        "0" : 5827
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 3673,
        "1" : 4054,
        "0" : 5864
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 3673,
        "1" : 4054,
        "0" : 5864
      }
    },
    "numInputRows" : 111,
    "inputRowsPerSecond" : 9250.0,
    "processedRowsPerSecond" : 102.6827012025902,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@7fe9b5de",
    "numOutputRows" : 26
  }
}
26/02/13 11:46:45 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/24 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.24.facbdbe2-5518-4fd2-855a-cbdefa1c5e70.tmp
26/02/13 11:46:45 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.24.facbdbe2-5518-4fd2-855a-cbdefa1c5e70.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/24
26/02/13 11:46:45 INFO MicroBatchExecution: Committed offsets for batch 24. Metadata OffsetSeqMetadata(0,1770983205142,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 11:46:45 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:46:45 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:46:45 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:46:45 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:46:45 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:46:45 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:46:45 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#22447 - origin_code.nullCount#22446) > 0)
26/02/13 11:46:45 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#22452 - destination_code.nullCount#22451) > 0)
26/02/13 11:46:45 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#22482 - callsign.nullCount#22481) > 0)
26/02/13 11:46:45 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:46:45 INFO DAGScheduler: Got job 57 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 11:46:45 INFO DAGScheduler: Final stage: ResultStage 89 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:46:45 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 88)
26/02/13 11:46:45 INFO DAGScheduler: Missing parents: List()
26/02/13 11:46:45 INFO DAGScheduler: Submitting ResultStage 89 (MapPartitionsRDD[307] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:46:45 INFO MemoryStore: Block broadcast_85 stored as values in memory (estimated size 99.3 KiB, free 433.7 MiB)
26/02/13 11:46:45 INFO MemoryStore: Block broadcast_85_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 433.6 MiB)
26/02/13 11:46:45 INFO BlockManagerInfo: Added broadcast_85_piece0 in memory on spark-master:40291 (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 11:46:45 INFO SparkContext: Created broadcast 85 from broadcast at DAGScheduler.scala:1585
26/02/13 11:46:45 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 89 (MapPartitionsRDD[307] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 11:46:45 INFO TaskSchedulerImpl: Adding task set 89.0 with 4 tasks resource profile 0
26/02/13 11:46:45 INFO TaskSetManager: Starting task 0.0 in stage 89.0 (TID 177) (172.18.0.15, executor 0, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:46:45 INFO TaskSetManager: Starting task 1.0 in stage 89.0 (TID 178) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:46:45 INFO BlockManagerInfo: Added broadcast_85_piece0 in memory on 172.18.0.15:34959 (size: 33.3 KiB, free: 433.9 MiB)
26/02/13 11:46:45 INFO TaskSetManager: Starting task 2.0 in stage 89.0 (TID 179) (172.18.0.15, executor 0, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:46:45 INFO TaskSetManager: Starting task 3.0 in stage 89.0 (TID 180) (172.18.0.15, executor 0, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:46:45 INFO TaskSetManager: Finished task 1.0 in stage 89.0 (TID 178) in 29 ms on 172.18.0.15 (executor 0) (1/4)
26/02/13 11:46:45 INFO TaskSetManager: Finished task 0.0 in stage 89.0 (TID 177) in 31 ms on 172.18.0.15 (executor 0) (2/4)
26/02/13 11:46:45 INFO TaskSetManager: Finished task 2.0 in stage 89.0 (TID 179) in 21 ms on 172.18.0.15 (executor 0) (3/4)
26/02/13 11:46:45 INFO TaskSetManager: Finished task 3.0 in stage 89.0 (TID 180) in 24 ms on 172.18.0.15 (executor 0) (4/4)
26/02/13 11:46:45 INFO TaskSchedulerImpl: Removed TaskSet 89.0, whose tasks have all completed, from pool 
26/02/13 11:46:45 INFO DAGScheduler: ResultStage 89 (start at NativeMethodAccessorImpl.java:0) finished in 0.061 s
26/02/13 11:46:45 INFO DAGScheduler: Job 57 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:46:45 INFO TaskSchedulerImpl: Killing all running tasks in stage 89: Stage finished
26/02/13 11:46:45 INFO DAGScheduler: Job 57 finished: start at NativeMethodAccessorImpl.java:0, took 0.063608 s
26/02/13 11:46:45 INFO BlockManagerInfo: Removed broadcast_83_piece0 on spark-master:40291 in memory (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 11:46:45 INFO MemoryStore: Block broadcast_86_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.6 MiB)
26/02/13 11:46:45 INFO BlockManagerInfo: Added broadcast_86_piece0 in memory on spark-master:40291 (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 11:46:45 INFO SparkContext: Created broadcast 86 from start at NativeMethodAccessorImpl.java:0
26/02/13 11:46:45 INFO BlockManagerInfo: Removed broadcast_83_piece0 on 172.18.0.15:34959 in memory (size: 107.2 KiB, free: 434.0 MiB)
26/02/13 11:46:45 INFO BlockManagerInfo: Removed broadcast_83_piece0 on 172.18.0.14:40939 in memory (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 11:46:45 INFO BlockManagerInfo: Removed broadcast_84_piece0 on spark-master:40291 in memory (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 11:46:45 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 24, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@6d706f73]. The input RDD has 3 partitions.
26/02/13 11:46:45 INFO BlockManagerInfo: Removed broadcast_84_piece0 on 172.18.0.15:34959 in memory (size: 19.5 KiB, free: 434.0 MiB)
26/02/13 11:46:45 INFO BlockManagerInfo: Removed broadcast_84_piece0 on 172.18.0.14:40939 in memory (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 11:46:45 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:46:45 INFO DAGScheduler: Got job 58 (start at NativeMethodAccessorImpl.java:0) with 3 output partitions
26/02/13 11:46:45 INFO DAGScheduler: Final stage: ResultStage 90 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:46:45 INFO DAGScheduler: Parents of final stage: List()
26/02/13 11:46:45 INFO DAGScheduler: Missing parents: List()
26/02/13 11:46:45 INFO DAGScheduler: Submitting ResultStage 90 (MapPartitionsRDD[313] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:46:45 INFO MemoryStore: Block broadcast_87 stored as values in memory (estimated size 49.9 KiB, free 433.7 MiB)
26/02/13 11:46:45 INFO MemoryStore: Block broadcast_87_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.6 MiB)
26/02/13 11:46:45 INFO BlockManagerInfo: Added broadcast_87_piece0 in memory on spark-master:40291 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 11:46:45 INFO SparkContext: Created broadcast 87 from broadcast at DAGScheduler.scala:1585
26/02/13 11:46:45 INFO BlockManagerInfo: Removed broadcast_82_piece0 on spark-master:40291 in memory (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 11:46:45 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 90 (MapPartitionsRDD[313] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
26/02/13 11:46:45 INFO BlockManagerInfo: Removed broadcast_82_piece0 on 172.18.0.15:34959 in memory (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 11:46:45 INFO TaskSchedulerImpl: Adding task set 90.0 with 3 tasks resource profile 0
26/02/13 11:46:45 INFO TaskSetManager: Starting task 0.0 in stage 90.0 (TID 181) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:46:45 INFO TaskSetManager: Starting task 1.0 in stage 90.0 (TID 182) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:46:45 INFO TaskSetManager: Starting task 2.0 in stage 90.0 (TID 183) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:46:45 INFO BlockManagerInfo: Removed broadcast_85_piece0 on spark-master:40291 in memory (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 11:46:45 INFO BlockManagerInfo: Removed broadcast_85_piece0 on 172.18.0.15:34959 in memory (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 11:46:45 INFO BlockManagerInfo: Added broadcast_87_piece0 in memory on 172.18.0.15:34959 (size: 19.5 KiB, free: 434.0 MiB)
26/02/13 11:46:45 INFO BlockManagerInfo: Added broadcast_87_piece0 in memory on 172.18.0.14:40939 (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 11:46:45 INFO BlockManagerInfo: Added broadcast_86_piece0 in memory on 172.18.0.15:34959 (size: 107.2 KiB, free: 433.9 MiB)
26/02/13 11:46:45 INFO BlockManagerInfo: Added broadcast_86_piece0 in memory on 172.18.0.14:40939 (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 11:46:45 INFO TaskSetManager: Finished task 0.0 in stage 90.0 (TID 181) in 113 ms on 172.18.0.14 (executor 1) (1/3)
26/02/13 11:46:45 INFO TaskSetManager: Finished task 2.0 in stage 90.0 (TID 183) in 116 ms on 172.18.0.14 (executor 1) (2/3)
26/02/13 11:46:45 INFO TaskSetManager: Finished task 1.0 in stage 90.0 (TID 182) in 130 ms on 172.18.0.15 (executor 0) (3/3)
26/02/13 11:46:45 INFO TaskSchedulerImpl: Removed TaskSet 90.0, whose tasks have all completed, from pool 
26/02/13 11:46:45 INFO DAGScheduler: ResultStage 90 (start at NativeMethodAccessorImpl.java:0) finished in 0.149 s
26/02/13 11:46:45 INFO DAGScheduler: Job 58 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:46:45 INFO TaskSchedulerImpl: Killing all running tasks in stage 90: Stage finished
26/02/13 11:46:45 INFO DAGScheduler: Job 58 finished: start at NativeMethodAccessorImpl.java:0, took 0.154208 s
26/02/13 11:46:45 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 24, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@6d706f73] is committing.
26/02/13 11:46:45 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 24, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@6d706f73] committed.
26/02/13 11:46:45 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/24 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.24.8c25b775-5914-4a36-8bd9-d6a2ee4f34a7.tmp
26/02/13 11:46:45 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.24.8c25b775-5914-4a36-8bd9-d6a2ee4f34a7.tmp to file:/tmp/spark-checkpoint-enrichment/commits/24
26/02/13 11:46:45 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "fc5c6452-bd70-4c90-becf-772d3bebdb57",
  "name" : null,
  "timestamp" : "2026-02-13T11:46:45.140Z",
  "batchId" : 24,
  "numInputRows" : 125,
  "inputRowsPerSecond" : 115.5268022181146,
  "processedRowsPerSecond" : 218.53146853146856,
  "durationMs" : {
    "addBatch" : 359,
    "commitOffsets" : 98,
    "getBatch" : 0,
    "latestOffset" : 2,
    "queryPlanning" : 50,
    "triggerExecution" : 572,
    "walCommit" : 62
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 3673,
        "1" : 4054,
        "0" : 5864
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 3712,
        "1" : 4100,
        "0" : 5904
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 3712,
        "1" : 4100,
        "0" : 5904
      }
    },
    "numInputRows" : 125,
    "inputRowsPerSecond" : 115.5268022181146,
    "processedRowsPerSecond" : 218.53146853146856,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@7fe9b5de",
    "numOutputRows" : 28
  }
}
26/02/13 11:46:54 INFO BlockManagerInfo: Removed broadcast_87_piece0 on spark-master:40291 in memory (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 11:46:54 INFO BlockManagerInfo: Removed broadcast_87_piece0 on 172.18.0.14:40939 in memory (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 11:46:54 INFO BlockManagerInfo: Removed broadcast_87_piece0 on 172.18.0.15:34959 in memory (size: 19.5 KiB, free: 434.0 MiB)
26/02/13 11:46:55 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 11:47:00 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/25 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.25.95711984-4d0b-4a05-81de-728d9973918e.tmp
26/02/13 11:47:00 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.25.95711984-4d0b-4a05-81de-728d9973918e.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/25
26/02/13 11:47:00 INFO MicroBatchExecution: Committed offsets for batch 25. Metadata OffsetSeqMetadata(0,1770983220615,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 11:47:00 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:47:00 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:47:00 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:47:00 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:47:00 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:47:00 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:47:00 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#23301 - origin_code.nullCount#23300) > 0)
26/02/13 11:47:00 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#23306 - destination_code.nullCount#23305) > 0)
26/02/13 11:47:00 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#23336 - callsign.nullCount#23335) > 0)
26/02/13 11:47:00 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:47:00 INFO DAGScheduler: Got job 59 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 11:47:00 INFO DAGScheduler: Final stage: ResultStage 92 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:47:00 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 91)
26/02/13 11:47:00 INFO DAGScheduler: Missing parents: List()
26/02/13 11:47:00 INFO DAGScheduler: Submitting ResultStage 92 (MapPartitionsRDD[318] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:47:00 INFO MemoryStore: Block broadcast_88 stored as values in memory (estimated size 99.3 KiB, free 433.9 MiB)
26/02/13 11:47:00 INFO MemoryStore: Block broadcast_88_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 433.8 MiB)
26/02/13 11:47:00 INFO BlockManagerInfo: Added broadcast_88_piece0 in memory on spark-master:40291 (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 11:47:00 INFO SparkContext: Created broadcast 88 from broadcast at DAGScheduler.scala:1585
26/02/13 11:47:00 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 92 (MapPartitionsRDD[318] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 11:47:00 INFO TaskSchedulerImpl: Adding task set 92.0 with 4 tasks resource profile 0
26/02/13 11:47:00 INFO TaskSetManager: Starting task 0.0 in stage 92.0 (TID 184) (172.18.0.15, executor 0, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:47:00 INFO TaskSetManager: Starting task 1.0 in stage 92.0 (TID 185) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:47:00 INFO BlockManagerInfo: Added broadcast_88_piece0 in memory on 172.18.0.15:34959 (size: 33.3 KiB, free: 433.9 MiB)
26/02/13 11:47:00 INFO TaskSetManager: Starting task 2.0 in stage 92.0 (TID 186) (172.18.0.15, executor 0, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:47:00 INFO TaskSetManager: Finished task 0.0 in stage 92.0 (TID 184) in 21 ms on 172.18.0.15 (executor 0) (1/4)
26/02/13 11:47:00 INFO TaskSetManager: Starting task 3.0 in stage 92.0 (TID 187) (172.18.0.15, executor 0, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:47:00 INFO TaskSetManager: Finished task 1.0 in stage 92.0 (TID 185) in 24 ms on 172.18.0.15 (executor 0) (2/4)
26/02/13 11:47:00 INFO TaskSetManager: Finished task 2.0 in stage 92.0 (TID 186) in 19 ms on 172.18.0.15 (executor 0) (3/4)
26/02/13 11:47:00 INFO TaskSetManager: Finished task 3.0 in stage 92.0 (TID 187) in 20 ms on 172.18.0.15 (executor 0) (4/4)
26/02/13 11:47:00 INFO TaskSchedulerImpl: Removed TaskSet 92.0, whose tasks have all completed, from pool 
26/02/13 11:47:00 INFO DAGScheduler: ResultStage 92 (start at NativeMethodAccessorImpl.java:0) finished in 0.054 s
26/02/13 11:47:00 INFO DAGScheduler: Job 59 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:47:00 INFO TaskSchedulerImpl: Killing all running tasks in stage 92: Stage finished
26/02/13 11:47:00 INFO DAGScheduler: Job 59 finished: start at NativeMethodAccessorImpl.java:0, took 0.055867 s
26/02/13 11:47:00 INFO MemoryStore: Block broadcast_89_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.8 MiB)
26/02/13 11:47:00 INFO BlockManagerInfo: Removed broadcast_80_piece0 on spark-master:40291 in memory (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 11:47:00 INFO BlockManagerInfo: Added broadcast_89_piece0 in memory on spark-master:40291 (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 11:47:00 INFO SparkContext: Created broadcast 89 from start at NativeMethodAccessorImpl.java:0
26/02/13 11:47:00 INFO BlockManagerInfo: Removed broadcast_80_piece0 on 172.18.0.15:34959 in memory (size: 107.2 KiB, free: 434.0 MiB)
26/02/13 11:47:00 INFO BlockManagerInfo: Removed broadcast_80_piece0 on 172.18.0.14:40939 in memory (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 11:47:00 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 25, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@6e6fc607]. The input RDD has 3 partitions.
26/02/13 11:47:00 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:47:00 INFO DAGScheduler: Got job 60 (start at NativeMethodAccessorImpl.java:0) with 3 output partitions
26/02/13 11:47:00 INFO DAGScheduler: Final stage: ResultStage 93 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:47:00 INFO DAGScheduler: Parents of final stage: List()
26/02/13 11:47:00 INFO DAGScheduler: Missing parents: List()
26/02/13 11:47:00 INFO DAGScheduler: Submitting ResultStage 93 (MapPartitionsRDD[324] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:47:00 INFO MemoryStore: Block broadcast_90 stored as values in memory (estimated size 49.9 KiB, free 433.8 MiB)
26/02/13 11:47:00 INFO MemoryStore: Block broadcast_90_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.8 MiB)
26/02/13 11:47:00 INFO BlockManagerInfo: Added broadcast_90_piece0 in memory on spark-master:40291 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 11:47:00 INFO SparkContext: Created broadcast 90 from broadcast at DAGScheduler.scala:1585
26/02/13 11:47:00 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 93 (MapPartitionsRDD[324] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
26/02/13 11:47:00 INFO TaskSchedulerImpl: Adding task set 93.0 with 3 tasks resource profile 0
26/02/13 11:47:00 INFO TaskSetManager: Starting task 0.0 in stage 93.0 (TID 188) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:47:00 INFO TaskSetManager: Starting task 1.0 in stage 93.0 (TID 189) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:47:00 INFO TaskSetManager: Starting task 2.0 in stage 93.0 (TID 190) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:47:00 INFO BlockManagerInfo: Added broadcast_90_piece0 in memory on 172.18.0.15:34959 (size: 19.5 KiB, free: 434.0 MiB)
26/02/13 11:47:00 INFO BlockManagerInfo: Added broadcast_90_piece0 in memory on 172.18.0.14:40939 (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 11:47:00 INFO BlockManagerInfo: Added broadcast_89_piece0 in memory on 172.18.0.15:34959 (size: 107.2 KiB, free: 433.9 MiB)
26/02/13 11:47:00 INFO BlockManagerInfo: Added broadcast_89_piece0 in memory on 172.18.0.14:40939 (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 11:47:01 INFO TaskSetManager: Finished task 1.0 in stage 93.0 (TID 189) in 591 ms on 172.18.0.15 (executor 0) (1/3)
26/02/13 11:47:01 INFO TaskSetManager: Finished task 0.0 in stage 93.0 (TID 188) in 599 ms on 172.18.0.14 (executor 1) (2/3)
26/02/13 11:47:01 INFO TaskSetManager: Finished task 2.0 in stage 93.0 (TID 190) in 605 ms on 172.18.0.14 (executor 1) (3/3)
26/02/13 11:47:01 INFO TaskSchedulerImpl: Removed TaskSet 93.0, whose tasks have all completed, from pool 
26/02/13 11:47:01 INFO DAGScheduler: ResultStage 93 (start at NativeMethodAccessorImpl.java:0) finished in 0.614 s
26/02/13 11:47:01 INFO DAGScheduler: Job 60 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:47:01 INFO TaskSchedulerImpl: Killing all running tasks in stage 93: Stage finished
26/02/13 11:47:01 INFO DAGScheduler: Job 60 finished: start at NativeMethodAccessorImpl.java:0, took 0.618662 s
26/02/13 11:47:01 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 25, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@6e6fc607] is committing.
26/02/13 11:47:01 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 25, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@6e6fc607] committed.
26/02/13 11:47:01 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/25 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.25.d7ae452a-f46e-4774-b88e-6672433c824d.tmp
26/02/13 11:47:01 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.25.d7ae452a-f46e-4774-b88e-6672433c824d.tmp to file:/tmp/spark-checkpoint-enrichment/commits/25
26/02/13 11:47:01 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "fc5c6452-bd70-4c90-becf-772d3bebdb57",
  "name" : null,
  "timestamp" : "2026-02-13T11:47:00.613Z",
  "batchId" : 25,
  "numInputRows" : 48,
  "inputRowsPerSecond" : 4000.0,
  "processedRowsPerSecond" : 48.78048780487805,
  "durationMs" : {
    "addBatch" : 810,
    "commitOffsets" : 63,
    "getBatch" : 0,
    "latestOffset" : 2,
    "queryPlanning" : 41,
    "triggerExecution" : 984,
    "walCommit" : 67
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 3712,
        "1" : 4100,
        "0" : 5904
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 3723,
        "1" : 4123,
        "0" : 5918
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 3723,
        "1" : 4123,
        "0" : 5918
      }
    },
    "numInputRows" : 48,
    "inputRowsPerSecond" : 4000.0,
    "processedRowsPerSecond" : 48.78048780487805,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@7fe9b5de",
    "numOutputRows" : 13
  }
}
26/02/13 11:47:01 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/26 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.26.51bf49bc-bd07-4c4d-afd7-09e0ed42ca93.tmp
26/02/13 11:47:01 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.26.51bf49bc-bd07-4c4d-afd7-09e0ed42ca93.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/26
26/02/13 11:47:01 INFO MicroBatchExecution: Committed offsets for batch 26. Metadata OffsetSeqMetadata(0,1770983221599,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 11:47:01 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:47:01 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:47:01 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:47:01 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:47:01 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:47:01 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:47:01 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#24155 - origin_code.nullCount#24154) > 0)
26/02/13 11:47:01 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#24160 - destination_code.nullCount#24159) > 0)
26/02/13 11:47:01 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#24190 - callsign.nullCount#24189) > 0)
26/02/13 11:47:01 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:47:01 INFO DAGScheduler: Got job 61 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 11:47:01 INFO DAGScheduler: Final stage: ResultStage 95 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:47:01 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 94)
26/02/13 11:47:01 INFO DAGScheduler: Missing parents: List()
26/02/13 11:47:01 INFO DAGScheduler: Submitting ResultStage 95 (MapPartitionsRDD[329] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:47:01 INFO MemoryStore: Block broadcast_91 stored as values in memory (estimated size 99.3 KiB, free 433.7 MiB)
26/02/13 11:47:01 INFO MemoryStore: Block broadcast_91_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 433.6 MiB)
26/02/13 11:47:01 INFO BlockManagerInfo: Added broadcast_91_piece0 in memory on spark-master:40291 (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 11:47:01 INFO SparkContext: Created broadcast 91 from broadcast at DAGScheduler.scala:1585
26/02/13 11:47:01 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 95 (MapPartitionsRDD[329] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 11:47:01 INFO TaskSchedulerImpl: Adding task set 95.0 with 4 tasks resource profile 0
26/02/13 11:47:01 INFO TaskSetManager: Starting task 0.0 in stage 95.0 (TID 191) (172.18.0.15, executor 0, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:47:01 INFO TaskSetManager: Starting task 1.0 in stage 95.0 (TID 192) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:47:01 INFO BlockManagerInfo: Added broadcast_91_piece0 in memory on 172.18.0.15:34959 (size: 33.3 KiB, free: 433.9 MiB)
26/02/13 11:47:01 INFO TaskSetManager: Starting task 2.0 in stage 95.0 (TID 193) (172.18.0.15, executor 0, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:47:01 INFO TaskSetManager: Finished task 0.0 in stage 95.0 (TID 191) in 21 ms on 172.18.0.15 (executor 0) (1/4)
26/02/13 11:47:01 INFO TaskSetManager: Starting task 3.0 in stage 95.0 (TID 194) (172.18.0.15, executor 0, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:47:01 INFO TaskSetManager: Finished task 1.0 in stage 95.0 (TID 192) in 22 ms on 172.18.0.15 (executor 0) (2/4)
26/02/13 11:47:01 INFO TaskSetManager: Finished task 2.0 in stage 95.0 (TID 193) in 24 ms on 172.18.0.15 (executor 0) (3/4)
26/02/13 11:47:01 INFO TaskSetManager: Finished task 3.0 in stage 95.0 (TID 194) in 26 ms on 172.18.0.15 (executor 0) (4/4)
26/02/13 11:47:01 INFO TaskSchedulerImpl: Removed TaskSet 95.0, whose tasks have all completed, from pool 
26/02/13 11:47:01 INFO DAGScheduler: ResultStage 95 (start at NativeMethodAccessorImpl.java:0) finished in 0.055 s
26/02/13 11:47:01 INFO DAGScheduler: Job 61 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:47:01 INFO TaskSchedulerImpl: Killing all running tasks in stage 95: Stage finished
26/02/13 11:47:01 INFO DAGScheduler: Job 61 finished: start at NativeMethodAccessorImpl.java:0, took 0.057015 s
26/02/13 11:47:01 INFO MemoryStore: Block broadcast_92_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.5 MiB)
26/02/13 11:47:01 INFO BlockManagerInfo: Added broadcast_92_piece0 in memory on spark-master:40291 (size: 107.2 KiB, free: 434.0 MiB)
26/02/13 11:47:01 INFO SparkContext: Created broadcast 92 from start at NativeMethodAccessorImpl.java:0
26/02/13 11:47:01 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 26, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@2516cf92]. The input RDD has 3 partitions.
26/02/13 11:47:01 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:47:01 INFO DAGScheduler: Got job 62 (start at NativeMethodAccessorImpl.java:0) with 3 output partitions
26/02/13 11:47:01 INFO DAGScheduler: Final stage: ResultStage 96 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:47:01 INFO DAGScheduler: Parents of final stage: List()
26/02/13 11:47:01 INFO DAGScheduler: Missing parents: List()
26/02/13 11:47:01 INFO DAGScheduler: Submitting ResultStage 96 (MapPartitionsRDD[335] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:47:01 INFO BlockManagerInfo: Removed broadcast_90_piece0 on spark-master:40291 in memory (size: 19.5 KiB, free: 434.0 MiB)
26/02/13 11:47:01 INFO MemoryStore: Block broadcast_93 stored as values in memory (estimated size 49.9 KiB, free 433.5 MiB)
26/02/13 11:47:01 INFO BlockManagerInfo: Removed broadcast_90_piece0 on 172.18.0.14:40939 in memory (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 11:47:01 INFO MemoryStore: Block broadcast_93_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.5 MiB)
26/02/13 11:47:01 INFO BlockManagerInfo: Added broadcast_93_piece0 in memory on spark-master:40291 (size: 19.5 KiB, free: 434.0 MiB)
26/02/13 11:47:01 INFO BlockManagerInfo: Removed broadcast_90_piece0 on 172.18.0.15:34959 in memory (size: 19.5 KiB, free: 433.9 MiB)
26/02/13 11:47:01 INFO SparkContext: Created broadcast 93 from broadcast at DAGScheduler.scala:1585
26/02/13 11:47:01 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 96 (MapPartitionsRDD[335] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
26/02/13 11:47:01 INFO TaskSchedulerImpl: Adding task set 96.0 with 3 tasks resource profile 0
26/02/13 11:47:01 INFO TaskSetManager: Starting task 1.0 in stage 96.0 (TID 195) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:47:01 INFO TaskSetManager: Starting task 0.0 in stage 96.0 (TID 196) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:47:01 INFO TaskSetManager: Starting task 2.0 in stage 96.0 (TID 197) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:47:01 INFO BlockManagerInfo: Removed broadcast_88_piece0 on spark-master:40291 in memory (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 11:47:01 INFO BlockManagerInfo: Removed broadcast_88_piece0 on 172.18.0.15:34959 in memory (size: 33.3 KiB, free: 433.9 MiB)
26/02/13 11:47:01 INFO BlockManagerInfo: Removed broadcast_91_piece0 on spark-master:40291 in memory (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 11:47:01 INFO BlockManagerInfo: Added broadcast_93_piece0 in memory on 172.18.0.15:34959 (size: 19.5 KiB, free: 433.9 MiB)
26/02/13 11:47:01 INFO BlockManagerInfo: Removed broadcast_91_piece0 on 172.18.0.15:34959 in memory (size: 33.3 KiB, free: 433.9 MiB)
26/02/13 11:47:01 INFO BlockManagerInfo: Added broadcast_93_piece0 in memory on 172.18.0.14:40939 (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 11:47:01 INFO BlockManagerInfo: Removed broadcast_89_piece0 on spark-master:40291 in memory (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 11:47:01 INFO BlockManagerInfo: Removed broadcast_89_piece0 on 172.18.0.15:34959 in memory (size: 107.2 KiB, free: 434.0 MiB)
26/02/13 11:47:01 INFO BlockManagerInfo: Removed broadcast_89_piece0 on 172.18.0.14:40939 in memory (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 11:47:01 INFO BlockManagerInfo: Added broadcast_92_piece0 in memory on 172.18.0.15:34959 (size: 107.2 KiB, free: 433.9 MiB)
26/02/13 11:47:01 INFO BlockManagerInfo: Added broadcast_92_piece0 in memory on 172.18.0.14:40939 (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 11:47:01 INFO TaskSetManager: Finished task 1.0 in stage 96.0 (TID 195) in 70 ms on 172.18.0.15 (executor 0) (1/3)
26/02/13 11:47:01 INFO TaskSetManager: Finished task 0.0 in stage 96.0 (TID 196) in 104 ms on 172.18.0.14 (executor 1) (2/3)
26/02/13 11:47:01 INFO TaskSetManager: Finished task 2.0 in stage 96.0 (TID 197) in 105 ms on 172.18.0.14 (executor 1) (3/3)
26/02/13 11:47:01 INFO TaskSchedulerImpl: Removed TaskSet 96.0, whose tasks have all completed, from pool 
26/02/13 11:47:01 INFO DAGScheduler: ResultStage 96 (start at NativeMethodAccessorImpl.java:0) finished in 0.114 s
26/02/13 11:47:01 INFO DAGScheduler: Job 62 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:47:01 INFO TaskSchedulerImpl: Killing all running tasks in stage 96: Stage finished
26/02/13 11:47:01 INFO DAGScheduler: Job 62 finished: start at NativeMethodAccessorImpl.java:0, took 0.120090 s
26/02/13 11:47:01 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 26, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@2516cf92] is committing.
26/02/13 11:47:01 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 26, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@2516cf92] committed.
26/02/13 11:47:01 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/26 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.26.0c41a9c6-130d-4362-a815-3777f97bed21.tmp
26/02/13 11:47:02 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.26.0c41a9c6-130d-4362-a815-3777f97bed21.tmp to file:/tmp/spark-checkpoint-enrichment/commits/26
26/02/13 11:47:02 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "fc5c6452-bd70-4c90-becf-772d3bebdb57",
  "name" : null,
  "timestamp" : "2026-02-13T11:47:01.598Z",
  "batchId" : 26,
  "numInputRows" : 191,
  "inputRowsPerSecond" : 193.90862944162436,
  "processedRowsPerSecond" : 440.0921658986175,
  "durationMs" : {
    "addBatch" : 272,
    "commitOffsets" : 62,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 37,
    "triggerExecution" : 434,
    "walCommit" : 60
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 3723,
        "1" : 4123,
        "0" : 5918
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 3789,
        "1" : 4184,
        "0" : 5982
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 3789,
        "1" : 4184,
        "0" : 5982
      }
    },
    "numInputRows" : 191,
    "inputRowsPerSecond" : 193.90862944162436,
    "processedRowsPerSecond" : 440.0921658986175,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@7fe9b5de",
    "numOutputRows" : 42
  }
}
26/02/13 11:47:02 INFO BlockManagerInfo: Removed broadcast_93_piece0 on spark-master:40291 in memory (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 11:47:02 INFO BlockManagerInfo: Removed broadcast_93_piece0 on 172.18.0.15:34959 in memory (size: 19.5 KiB, free: 434.0 MiB)
26/02/13 11:47:02 INFO BlockManagerInfo: Removed broadcast_93_piece0 on 172.18.0.14:40939 in memory (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 11:47:12 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 11:47:17 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/27 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.27.b113738e-4ccb-425c-87af-2bcae2a9ecd1.tmp
26/02/13 11:47:17 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.27.b113738e-4ccb-425c-87af-2bcae2a9ecd1.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/27
26/02/13 11:47:17 INFO MicroBatchExecution: Committed offsets for batch 27. Metadata OffsetSeqMetadata(0,1770983236997,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 11:47:17 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:47:17 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:47:17 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:47:17 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:47:17 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:47:17 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:47:17 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#25009 - origin_code.nullCount#25008) > 0)
26/02/13 11:47:17 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#25014 - destination_code.nullCount#25013) > 0)
26/02/13 11:47:17 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#25044 - callsign.nullCount#25043) > 0)
26/02/13 11:47:17 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:47:17 INFO DAGScheduler: Got job 63 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 11:47:17 INFO DAGScheduler: Final stage: ResultStage 98 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:47:17 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 97)
26/02/13 11:47:17 INFO DAGScheduler: Missing parents: List()
26/02/13 11:47:17 INFO DAGScheduler: Submitting ResultStage 98 (MapPartitionsRDD[340] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:47:17 INFO MemoryStore: Block broadcast_94 stored as values in memory (estimated size 99.3 KiB, free 433.9 MiB)
26/02/13 11:47:17 INFO MemoryStore: Block broadcast_94_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 433.8 MiB)
26/02/13 11:47:17 INFO BlockManagerInfo: Added broadcast_94_piece0 in memory on spark-master:40291 (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 11:47:17 INFO SparkContext: Created broadcast 94 from broadcast at DAGScheduler.scala:1585
26/02/13 11:47:17 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 98 (MapPartitionsRDD[340] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 11:47:17 INFO TaskSchedulerImpl: Adding task set 98.0 with 4 tasks resource profile 0
26/02/13 11:47:17 INFO TaskSetManager: Starting task 0.0 in stage 98.0 (TID 198) (172.18.0.15, executor 0, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:47:17 INFO TaskSetManager: Starting task 1.0 in stage 98.0 (TID 199) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:47:17 INFO BlockManagerInfo: Added broadcast_94_piece0 in memory on 172.18.0.15:34959 (size: 33.3 KiB, free: 433.9 MiB)
26/02/13 11:47:17 INFO TaskSetManager: Starting task 2.0 in stage 98.0 (TID 200) (172.18.0.15, executor 0, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:47:17 INFO TaskSetManager: Starting task 3.0 in stage 98.0 (TID 201) (172.18.0.15, executor 0, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:47:17 INFO TaskSetManager: Finished task 1.0 in stage 98.0 (TID 199) in 24 ms on 172.18.0.15 (executor 0) (1/4)
26/02/13 11:47:17 INFO TaskSetManager: Finished task 0.0 in stage 98.0 (TID 198) in 24 ms on 172.18.0.15 (executor 0) (2/4)
26/02/13 11:47:17 INFO TaskSetManager: Finished task 3.0 in stage 98.0 (TID 201) in 15 ms on 172.18.0.15 (executor 0) (3/4)
26/02/13 11:47:17 INFO TaskSetManager: Finished task 2.0 in stage 98.0 (TID 200) in 20 ms on 172.18.0.15 (executor 0) (4/4)
26/02/13 11:47:17 INFO TaskSchedulerImpl: Removed TaskSet 98.0, whose tasks have all completed, from pool 
26/02/13 11:47:17 INFO DAGScheduler: ResultStage 98 (start at NativeMethodAccessorImpl.java:0) finished in 0.055 s
26/02/13 11:47:17 INFO DAGScheduler: Job 63 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:47:17 INFO TaskSchedulerImpl: Killing all running tasks in stage 98: Stage finished
26/02/13 11:47:17 INFO DAGScheduler: Job 63 finished: start at NativeMethodAccessorImpl.java:0, took 0.058191 s
26/02/13 11:47:17 INFO BlockManagerInfo: Removed broadcast_94_piece0 on spark-master:40291 in memory (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 11:47:17 INFO BlockManagerInfo: Removed broadcast_94_piece0 on 172.18.0.15:34959 in memory (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 11:47:17 INFO MemoryStore: Block broadcast_95_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.9 MiB)
26/02/13 11:47:17 INFO BlockManagerInfo: Added broadcast_95_piece0 in memory on spark-master:40291 (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 11:47:17 INFO SparkContext: Created broadcast 95 from start at NativeMethodAccessorImpl.java:0
26/02/13 11:47:17 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 27, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@62eb6cd6]. The input RDD has 3 partitions.
26/02/13 11:47:17 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:47:17 INFO DAGScheduler: Got job 64 (start at NativeMethodAccessorImpl.java:0) with 3 output partitions
26/02/13 11:47:17 INFO DAGScheduler: Final stage: ResultStage 99 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:47:17 INFO DAGScheduler: Parents of final stage: List()
26/02/13 11:47:17 INFO DAGScheduler: Missing parents: List()
26/02/13 11:47:17 INFO DAGScheduler: Submitting ResultStage 99 (MapPartitionsRDD[346] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:47:17 INFO MemoryStore: Block broadcast_96 stored as values in memory (estimated size 49.9 KiB, free 433.8 MiB)
26/02/13 11:47:17 INFO MemoryStore: Block broadcast_96_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.8 MiB)
26/02/13 11:47:17 INFO BlockManagerInfo: Added broadcast_96_piece0 in memory on spark-master:40291 (size: 19.5 KiB, free: 434.0 MiB)
26/02/13 11:47:17 INFO SparkContext: Created broadcast 96 from broadcast at DAGScheduler.scala:1585
26/02/13 11:47:17 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 99 (MapPartitionsRDD[346] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
26/02/13 11:47:17 INFO TaskSchedulerImpl: Adding task set 99.0 with 3 tasks resource profile 0
26/02/13 11:47:17 INFO TaskSetManager: Starting task 0.0 in stage 99.0 (TID 202) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:47:17 INFO TaskSetManager: Starting task 1.0 in stage 99.0 (TID 203) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:47:17 INFO TaskSetManager: Starting task 2.0 in stage 99.0 (TID 204) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:47:17 INFO BlockManagerInfo: Added broadcast_96_piece0 in memory on 172.18.0.15:34959 (size: 19.5 KiB, free: 433.9 MiB)
26/02/13 11:47:17 INFO BlockManagerInfo: Added broadcast_96_piece0 in memory on 172.18.0.14:40939 (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 11:47:17 INFO BlockManagerInfo: Added broadcast_95_piece0 in memory on 172.18.0.15:34959 (size: 107.2 KiB, free: 433.8 MiB)
26/02/13 11:47:17 INFO BlockManagerInfo: Added broadcast_95_piece0 in memory on 172.18.0.14:40939 (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 11:47:17 INFO BlockManagerInfo: Removed broadcast_86_piece0 on spark-master:40291 in memory (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 11:47:17 INFO BlockManagerInfo: Removed broadcast_86_piece0 on 172.18.0.15:34959 in memory (size: 107.2 KiB, free: 433.9 MiB)
26/02/13 11:47:17 INFO BlockManagerInfo: Removed broadcast_86_piece0 on 172.18.0.14:40939 in memory (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 11:47:17 INFO BlockManagerInfo: Removed broadcast_92_piece0 on spark-master:40291 in memory (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 11:47:17 INFO BlockManagerInfo: Removed broadcast_92_piece0 on 172.18.0.14:40939 in memory (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 11:47:17 INFO BlockManagerInfo: Removed broadcast_92_piece0 on 172.18.0.15:34959 in memory (size: 107.2 KiB, free: 434.0 MiB)
26/02/13 11:47:17 INFO TaskSetManager: Finished task 1.0 in stage 99.0 (TID 203) in 610 ms on 172.18.0.15 (executor 0) (1/3)
26/02/13 11:47:17 INFO TaskSetManager: Finished task 0.0 in stage 99.0 (TID 202) in 653 ms on 172.18.0.14 (executor 1) (2/3)
26/02/13 11:47:17 INFO TaskSetManager: Finished task 2.0 in stage 99.0 (TID 204) in 657 ms on 172.18.0.14 (executor 1) (3/3)
26/02/13 11:47:17 INFO TaskSchedulerImpl: Removed TaskSet 99.0, whose tasks have all completed, from pool 
26/02/13 11:47:17 INFO DAGScheduler: ResultStage 99 (start at NativeMethodAccessorImpl.java:0) finished in 0.668 s
26/02/13 11:47:17 INFO DAGScheduler: Job 64 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:47:17 INFO TaskSchedulerImpl: Killing all running tasks in stage 99: Stage finished
26/02/13 11:47:17 INFO DAGScheduler: Job 64 finished: start at NativeMethodAccessorImpl.java:0, took 0.670109 s
26/02/13 11:47:17 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 27, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@62eb6cd6] is committing.
26/02/13 11:47:17 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 27, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@62eb6cd6] committed.
26/02/13 11:47:17 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/27 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.27.4dd6069b-f0b0-4de4-89d5-4da6f495f1ff.tmp
26/02/13 11:47:18 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.27.4dd6069b-f0b0-4de4-89d5-4da6f495f1ff.tmp to file:/tmp/spark-checkpoint-enrichment/commits/27
26/02/13 11:47:18 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "fc5c6452-bd70-4c90-becf-772d3bebdb57",
  "name" : null,
  "timestamp" : "2026-02-13T11:47:16.995Z",
  "batchId" : 27,
  "numInputRows" : 59,
  "inputRowsPerSecond" : 4916.666666666667,
  "processedRowsPerSecond" : 56.94980694980695,
  "durationMs" : {
    "addBatch" : 857,
    "commitOffsets" : 64,
    "getBatch" : 0,
    "latestOffset" : 2,
    "queryPlanning" : 41,
    "triggerExecution" : 1036,
    "walCommit" : 71
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 3789,
        "1" : 4184,
        "0" : 5982
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 3803,
        "1" : 4209,
        "0" : 6002
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 3803,
        "1" : 4209,
        "0" : 6002
      }
    },
    "numInputRows" : 59,
    "inputRowsPerSecond" : 4916.666666666667,
    "processedRowsPerSecond" : 56.94980694980695,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@7fe9b5de",
    "numOutputRows" : 16
  }
}
26/02/13 11:47:18 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/28 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.28.203dd9e1-7557-426c-8841-08eb43df009c.tmp
26/02/13 11:47:18 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.28.203dd9e1-7557-426c-8841-08eb43df009c.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/28
26/02/13 11:47:18 INFO MicroBatchExecution: Committed offsets for batch 28. Metadata OffsetSeqMetadata(0,1770983238033,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 11:47:18 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:47:18 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:47:18 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:47:18 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:47:18 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:47:18 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:47:18 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#25863 - origin_code.nullCount#25862) > 0)
26/02/13 11:47:18 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#25868 - destination_code.nullCount#25867) > 0)
26/02/13 11:47:18 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#25898 - callsign.nullCount#25897) > 0)
26/02/13 11:47:18 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:47:18 INFO DAGScheduler: Got job 65 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 11:47:18 INFO DAGScheduler: Final stage: ResultStage 101 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:47:18 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 100)
26/02/13 11:47:18 INFO DAGScheduler: Missing parents: List()
26/02/13 11:47:18 INFO DAGScheduler: Submitting ResultStage 101 (MapPartitionsRDD[351] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:47:18 INFO MemoryStore: Block broadcast_97 stored as values in memory (estimated size 99.3 KiB, free 433.9 MiB)
26/02/13 11:47:18 INFO MemoryStore: Block broadcast_97_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 433.9 MiB)
26/02/13 11:47:18 INFO BlockManagerInfo: Added broadcast_97_piece0 in memory on spark-master:40291 (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 11:47:18 INFO SparkContext: Created broadcast 97 from broadcast at DAGScheduler.scala:1585
26/02/13 11:47:18 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 101 (MapPartitionsRDD[351] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 11:47:18 INFO TaskSchedulerImpl: Adding task set 101.0 with 4 tasks resource profile 0
26/02/13 11:47:18 INFO TaskSetManager: Starting task 0.0 in stage 101.0 (TID 205) (172.18.0.15, executor 0, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:47:18 INFO TaskSetManager: Starting task 1.0 in stage 101.0 (TID 206) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:47:18 INFO BlockManagerInfo: Added broadcast_97_piece0 in memory on 172.18.0.15:34959 (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 11:47:18 INFO TaskSetManager: Starting task 2.0 in stage 101.0 (TID 207) (172.18.0.15, executor 0, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:47:18 INFO TaskSetManager: Finished task 1.0 in stage 101.0 (TID 206) in 28 ms on 172.18.0.15 (executor 0) (1/4)
26/02/13 11:47:18 INFO TaskSetManager: Starting task 3.0 in stage 101.0 (TID 208) (172.18.0.15, executor 0, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:47:18 INFO TaskSetManager: Finished task 0.0 in stage 101.0 (TID 205) in 32 ms on 172.18.0.15 (executor 0) (2/4)
26/02/13 11:47:18 INFO TaskSetManager: Finished task 2.0 in stage 101.0 (TID 207) in 21 ms on 172.18.0.15 (executor 0) (3/4)
26/02/13 11:47:18 INFO TaskSetManager: Finished task 3.0 in stage 101.0 (TID 208) in 21 ms on 172.18.0.15 (executor 0) (4/4)
26/02/13 11:47:18 INFO TaskSchedulerImpl: Removed TaskSet 101.0, whose tasks have all completed, from pool 
26/02/13 11:47:18 INFO DAGScheduler: ResultStage 101 (start at NativeMethodAccessorImpl.java:0) finished in 0.059 s
26/02/13 11:47:18 INFO DAGScheduler: Job 65 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:47:18 INFO TaskSchedulerImpl: Killing all running tasks in stage 101: Stage finished
26/02/13 11:47:18 INFO DAGScheduler: Job 65 finished: start at NativeMethodAccessorImpl.java:0, took 0.062507 s
26/02/13 11:47:18 INFO MemoryStore: Block broadcast_98_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.8 MiB)
26/02/13 11:47:18 INFO BlockManagerInfo: Added broadcast_98_piece0 in memory on spark-master:40291 (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 11:47:18 INFO SparkContext: Created broadcast 98 from start at NativeMethodAccessorImpl.java:0
26/02/13 11:47:18 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 28, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@2d647aa9]. The input RDD has 3 partitions.
26/02/13 11:47:18 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:47:18 INFO DAGScheduler: Got job 66 (start at NativeMethodAccessorImpl.java:0) with 3 output partitions
26/02/13 11:47:18 INFO DAGScheduler: Final stage: ResultStage 102 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:47:18 INFO DAGScheduler: Parents of final stage: List()
26/02/13 11:47:18 INFO DAGScheduler: Missing parents: List()
26/02/13 11:47:18 INFO DAGScheduler: Submitting ResultStage 102 (MapPartitionsRDD[357] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:47:18 INFO MemoryStore: Block broadcast_99 stored as values in memory (estimated size 49.9 KiB, free 433.7 MiB)
26/02/13 11:47:18 INFO MemoryStore: Block broadcast_99_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.7 MiB)
26/02/13 11:47:18 INFO BlockManagerInfo: Added broadcast_99_piece0 in memory on spark-master:40291 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 11:47:18 INFO SparkContext: Created broadcast 99 from broadcast at DAGScheduler.scala:1585
26/02/13 11:47:18 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 102 (MapPartitionsRDD[357] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
26/02/13 11:47:18 INFO TaskSchedulerImpl: Adding task set 102.0 with 3 tasks resource profile 0
26/02/13 11:47:18 INFO TaskSetManager: Starting task 1.0 in stage 102.0 (TID 209) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:47:18 INFO TaskSetManager: Starting task 0.0 in stage 102.0 (TID 210) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:47:18 INFO TaskSetManager: Starting task 2.0 in stage 102.0 (TID 211) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:47:18 INFO BlockManagerInfo: Added broadcast_99_piece0 in memory on 172.18.0.14:40939 (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 11:47:18 INFO BlockManagerInfo: Added broadcast_99_piece0 in memory on 172.18.0.15:34959 (size: 19.5 KiB, free: 434.0 MiB)
26/02/13 11:47:18 INFO BlockManagerInfo: Added broadcast_98_piece0 in memory on 172.18.0.14:40939 (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 11:47:18 INFO BlockManagerInfo: Added broadcast_98_piece0 in memory on 172.18.0.15:34959 (size: 107.2 KiB, free: 433.9 MiB)
26/02/13 11:47:18 INFO TaskSetManager: Finished task 2.0 in stage 102.0 (TID 211) in 104 ms on 172.18.0.14 (executor 1) (1/3)
26/02/13 11:47:18 INFO TaskSetManager: Finished task 0.0 in stage 102.0 (TID 210) in 120 ms on 172.18.0.14 (executor 1) (2/3)
26/02/13 11:47:18 INFO TaskSetManager: Finished task 1.0 in stage 102.0 (TID 209) in 121 ms on 172.18.0.15 (executor 0) (3/3)
26/02/13 11:47:18 INFO TaskSchedulerImpl: Removed TaskSet 102.0, whose tasks have all completed, from pool 
26/02/13 11:47:18 INFO DAGScheduler: ResultStage 102 (start at NativeMethodAccessorImpl.java:0) finished in 0.131 s
26/02/13 11:47:18 INFO DAGScheduler: Job 66 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:47:18 INFO TaskSchedulerImpl: Killing all running tasks in stage 102: Stage finished
26/02/13 11:47:18 INFO DAGScheduler: Job 66 finished: start at NativeMethodAccessorImpl.java:0, took 0.135032 s
26/02/13 11:47:18 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 28, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@2d647aa9] is committing.
26/02/13 11:47:18 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 28, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@2d647aa9] committed.
26/02/13 11:47:18 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/28 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.28.5717ccf4-d8cd-4c36-a39b-adac3598dd09.tmp
26/02/13 11:47:18 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.28.5717ccf4-d8cd-4c36-a39b-adac3598dd09.tmp to file:/tmp/spark-checkpoint-enrichment/commits/28
26/02/13 11:47:18 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "fc5c6452-bd70-4c90-becf-772d3bebdb57",
  "name" : null,
  "timestamp" : "2026-02-13T11:47:18.032Z",
  "batchId" : 28,
  "numInputRows" : 179,
  "inputRowsPerSecond" : 172.61330761812923,
  "processedRowsPerSecond" : 346.8992248062015,
  "durationMs" : {
    "addBatch" : 335,
    "commitOffsets" : 65,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 44,
    "triggerExecution" : 516,
    "walCommit" : 69
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 3803,
        "1" : 4209,
        "0" : 6002
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 3866,
        "1" : 4268,
        "0" : 6059
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 3866,
        "1" : 4268,
        "0" : 6059
      }
    },
    "numInputRows" : 179,
    "inputRowsPerSecond" : 172.61330761812923,
    "processedRowsPerSecond" : 346.8992248062015,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@7fe9b5de",
    "numOutputRows" : 40
  }
}
26/02/13 11:47:20 INFO BlockManagerInfo: Removed broadcast_97_piece0 on spark-master:40291 in memory (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 11:47:20 INFO BlockManagerInfo: Removed broadcast_97_piece0 on 172.18.0.15:34959 in memory (size: 33.3 KiB, free: 433.9 MiB)
26/02/13 11:47:20 INFO BlockManagerInfo: Removed broadcast_96_piece0 on spark-master:40291 in memory (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 11:47:20 INFO BlockManagerInfo: Removed broadcast_96_piece0 on 172.18.0.15:34959 in memory (size: 19.5 KiB, free: 433.9 MiB)
26/02/13 11:47:20 INFO BlockManagerInfo: Removed broadcast_96_piece0 on 172.18.0.14:40939 in memory (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 11:47:20 INFO BlockManagerInfo: Removed broadcast_95_piece0 on spark-master:40291 in memory (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 11:47:20 INFO BlockManagerInfo: Removed broadcast_95_piece0 on 172.18.0.15:34959 in memory (size: 107.2 KiB, free: 434.0 MiB)
26/02/13 11:47:20 INFO BlockManagerInfo: Removed broadcast_95_piece0 on 172.18.0.14:40939 in memory (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 11:47:20 INFO BlockManagerInfo: Removed broadcast_99_piece0 on spark-master:40291 in memory (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 11:47:20 INFO BlockManagerInfo: Removed broadcast_99_piece0 on 172.18.0.15:34959 in memory (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 11:47:20 INFO BlockManagerInfo: Removed broadcast_99_piece0 on 172.18.0.14:40939 in memory (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 11:47:28 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 11:47:33 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/29 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.29.d84c7fa4-5cf7-416e-ac3c-74a25356dd9f.tmp
26/02/13 11:47:33 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.29.d84c7fa4-5cf7-416e-ac3c-74a25356dd9f.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/29
26/02/13 11:47:33 INFO MicroBatchExecution: Committed offsets for batch 29. Metadata OffsetSeqMetadata(0,1770983253211,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 11:47:33 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:47:33 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:47:33 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:47:33 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:47:33 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:47:33 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:47:33 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#26717 - origin_code.nullCount#26716) > 0)
26/02/13 11:47:33 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#26722 - destination_code.nullCount#26721) > 0)
26/02/13 11:47:33 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#26752 - callsign.nullCount#26751) > 0)
26/02/13 11:47:33 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:47:33 INFO DAGScheduler: Got job 67 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 11:47:33 INFO DAGScheduler: Final stage: ResultStage 104 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:47:33 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 103)
26/02/13 11:47:33 INFO DAGScheduler: Missing parents: List()
26/02/13 11:47:33 INFO DAGScheduler: Submitting ResultStage 104 (MapPartitionsRDD[362] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:47:33 INFO MemoryStore: Block broadcast_100 stored as values in memory (estimated size 99.3 KiB, free 434.0 MiB)
26/02/13 11:47:33 INFO MemoryStore: Block broadcast_100_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 433.9 MiB)
26/02/13 11:47:33 INFO BlockManagerInfo: Added broadcast_100_piece0 in memory on spark-master:40291 (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 11:47:33 INFO SparkContext: Created broadcast 100 from broadcast at DAGScheduler.scala:1585
26/02/13 11:47:33 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 104 (MapPartitionsRDD[362] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 11:47:33 INFO TaskSchedulerImpl: Adding task set 104.0 with 4 tasks resource profile 0
26/02/13 11:47:33 INFO TaskSetManager: Starting task 0.0 in stage 104.0 (TID 212) (172.18.0.15, executor 0, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:47:33 INFO TaskSetManager: Starting task 1.0 in stage 104.0 (TID 213) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:47:33 INFO BlockManagerInfo: Added broadcast_100_piece0 in memory on 172.18.0.15:34959 (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 11:47:33 INFO TaskSetManager: Starting task 2.0 in stage 104.0 (TID 214) (172.18.0.15, executor 0, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:47:33 INFO TaskSetManager: Finished task 0.0 in stage 104.0 (TID 212) in 26 ms on 172.18.0.15 (executor 0) (1/4)
26/02/13 11:47:33 INFO TaskSetManager: Starting task 3.0 in stage 104.0 (TID 215) (172.18.0.15, executor 0, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:47:33 INFO TaskSetManager: Finished task 1.0 in stage 104.0 (TID 213) in 32 ms on 172.18.0.15 (executor 0) (2/4)
26/02/13 11:47:33 INFO TaskSetManager: Finished task 2.0 in stage 104.0 (TID 214) in 23 ms on 172.18.0.15 (executor 0) (3/4)
26/02/13 11:47:33 INFO TaskSetManager: Finished task 3.0 in stage 104.0 (TID 215) in 20 ms on 172.18.0.15 (executor 0) (4/4)
26/02/13 11:47:33 INFO TaskSchedulerImpl: Removed TaskSet 104.0, whose tasks have all completed, from pool 
26/02/13 11:47:33 INFO DAGScheduler: ResultStage 104 (start at NativeMethodAccessorImpl.java:0) finished in 0.057 s
26/02/13 11:47:33 INFO DAGScheduler: Job 67 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:47:33 INFO TaskSchedulerImpl: Killing all running tasks in stage 104: Stage finished
26/02/13 11:47:33 INFO DAGScheduler: Job 67 finished: start at NativeMethodAccessorImpl.java:0, took 0.059757 s
26/02/13 11:47:33 INFO MemoryStore: Block broadcast_101_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.8 MiB)
26/02/13 11:47:33 INFO BlockManagerInfo: Added broadcast_101_piece0 in memory on spark-master:40291 (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 11:47:33 INFO SparkContext: Created broadcast 101 from start at NativeMethodAccessorImpl.java:0
26/02/13 11:47:33 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 29, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@20027307]. The input RDD has 3 partitions.
26/02/13 11:47:33 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:47:33 INFO DAGScheduler: Got job 68 (start at NativeMethodAccessorImpl.java:0) with 3 output partitions
26/02/13 11:47:33 INFO DAGScheduler: Final stage: ResultStage 105 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:47:33 INFO DAGScheduler: Parents of final stage: List()
26/02/13 11:47:33 INFO DAGScheduler: Missing parents: List()
26/02/13 11:47:33 INFO DAGScheduler: Submitting ResultStage 105 (MapPartitionsRDD[368] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:47:33 INFO MemoryStore: Block broadcast_102 stored as values in memory (estimated size 49.9 KiB, free 433.8 MiB)
26/02/13 11:47:33 INFO MemoryStore: Block broadcast_102_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.8 MiB)
26/02/13 11:47:33 INFO BlockManagerInfo: Added broadcast_102_piece0 in memory on spark-master:40291 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 11:47:33 INFO SparkContext: Created broadcast 102 from broadcast at DAGScheduler.scala:1585
26/02/13 11:47:33 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 105 (MapPartitionsRDD[368] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
26/02/13 11:47:33 INFO TaskSchedulerImpl: Adding task set 105.0 with 3 tasks resource profile 0
26/02/13 11:47:33 INFO TaskSetManager: Starting task 1.0 in stage 105.0 (TID 216) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:47:33 INFO TaskSetManager: Starting task 0.0 in stage 105.0 (TID 217) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:47:33 INFO TaskSetManager: Starting task 2.0 in stage 105.0 (TID 218) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:47:33 INFO BlockManagerInfo: Added broadcast_102_piece0 in memory on 172.18.0.15:34959 (size: 19.5 KiB, free: 434.0 MiB)
26/02/13 11:47:33 INFO BlockManagerInfo: Added broadcast_102_piece0 in memory on 172.18.0.14:40939 (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 11:47:33 INFO BlockManagerInfo: Added broadcast_101_piece0 in memory on 172.18.0.15:34959 (size: 107.2 KiB, free: 433.9 MiB)
26/02/13 11:47:33 INFO BlockManagerInfo: Added broadcast_101_piece0 in memory on 172.18.0.14:40939 (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 11:47:34 INFO TaskSetManager: Finished task 1.0 in stage 105.0 (TID 216) in 565 ms on 172.18.0.15 (executor 0) (1/3)
26/02/13 11:47:34 INFO TaskSetManager: Finished task 0.0 in stage 105.0 (TID 217) in 576 ms on 172.18.0.14 (executor 1) (2/3)
26/02/13 11:47:34 INFO TaskSetManager: Finished task 2.0 in stage 105.0 (TID 218) in 575 ms on 172.18.0.14 (executor 1) (3/3)
26/02/13 11:47:34 INFO TaskSchedulerImpl: Removed TaskSet 105.0, whose tasks have all completed, from pool 
26/02/13 11:47:34 INFO DAGScheduler: ResultStage 105 (start at NativeMethodAccessorImpl.java:0) finished in 0.581 s
26/02/13 11:47:34 INFO DAGScheduler: Job 68 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:47:34 INFO TaskSchedulerImpl: Killing all running tasks in stage 105: Stage finished
26/02/13 11:47:34 INFO DAGScheduler: Job 68 finished: start at NativeMethodAccessorImpl.java:0, took 0.581848 s
26/02/13 11:47:34 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 29, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@20027307] is committing.
26/02/13 11:47:34 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 29, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@20027307] committed.
26/02/13 11:47:34 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/29 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.29.432a5567-6a27-4e72-af7c-b63916ccf269.tmp
26/02/13 11:47:34 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.29.432a5567-6a27-4e72-af7c-b63916ccf269.tmp to file:/tmp/spark-checkpoint-enrichment/commits/29
26/02/13 11:47:34 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "fc5c6452-bd70-4c90-becf-772d3bebdb57",
  "name" : null,
  "timestamp" : "2026-02-13T11:47:33.209Z",
  "batchId" : 29,
  "numInputRows" : 98,
  "inputRowsPerSecond" : 8166.666666666666,
  "processedRowsPerSecond" : 103.70370370370371,
  "durationMs" : {
    "addBatch" : 733,
    "commitOffsets" : 60,
    "getBatch" : 0,
    "latestOffset" : 2,
    "queryPlanning" : 48,
    "triggerExecution" : 945,
    "walCommit" : 101
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 3866,
        "1" : 4268,
        "0" : 6059
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 3895,
        "1" : 4305,
        "0" : 6091
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 3895,
        "1" : 4305,
        "0" : 6091
      }
    },
    "numInputRows" : 98,
    "inputRowsPerSecond" : 8166.666666666666,
    "processedRowsPerSecond" : 103.70370370370371,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@7fe9b5de",
    "numOutputRows" : 24
  }
}
26/02/13 11:47:34 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/30 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.30.77bd210c-b70b-440a-be5b-e4d49a0a7fa6.tmp
26/02/13 11:47:34 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.30.77bd210c-b70b-440a-be5b-e4d49a0a7fa6.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/30
26/02/13 11:47:34 INFO MicroBatchExecution: Committed offsets for batch 30. Metadata OffsetSeqMetadata(0,1770983254156,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 11:47:34 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:47:34 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:47:34 INFO BlockManagerInfo: Removed broadcast_101_piece0 on spark-master:40291 in memory (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 11:47:34 INFO BlockManagerInfo: Removed broadcast_101_piece0 on 172.18.0.14:40939 in memory (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 11:47:34 INFO BlockManagerInfo: Removed broadcast_101_piece0 on 172.18.0.15:34959 in memory (size: 107.2 KiB, free: 434.0 MiB)
26/02/13 11:47:34 INFO BlockManagerInfo: Removed broadcast_98_piece0 on spark-master:40291 in memory (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 11:47:34 INFO BlockManagerInfo: Removed broadcast_98_piece0 on 172.18.0.15:34959 in memory (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 11:47:34 INFO BlockManagerInfo: Removed broadcast_98_piece0 on 172.18.0.14:40939 in memory (size: 107.2 KiB, free: 434.4 MiB)
26/02/13 11:47:34 INFO BlockManagerInfo: Removed broadcast_102_piece0 on spark-master:40291 in memory (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 11:47:34 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:47:34 INFO BlockManagerInfo: Removed broadcast_102_piece0 on 172.18.0.15:34959 in memory (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 11:47:34 INFO BlockManagerInfo: Removed broadcast_102_piece0 on 172.18.0.14:40939 in memory (size: 19.5 KiB, free: 434.4 MiB)
26/02/13 11:47:34 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:47:34 INFO BlockManagerInfo: Removed broadcast_100_piece0 on 172.18.0.15:34959 in memory (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 11:47:34 INFO BlockManagerInfo: Removed broadcast_100_piece0 on spark-master:40291 in memory (size: 33.3 KiB, free: 434.4 MiB)
26/02/13 11:47:34 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:47:34 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:47:34 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#27571 - origin_code.nullCount#27570) > 0)
26/02/13 11:47:34 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#27576 - destination_code.nullCount#27575) > 0)
26/02/13 11:47:34 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#27606 - callsign.nullCount#27605) > 0)
26/02/13 11:47:34 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:47:34 INFO DAGScheduler: Got job 69 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 11:47:34 INFO DAGScheduler: Final stage: ResultStage 107 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:47:34 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 106)
26/02/13 11:47:34 INFO DAGScheduler: Missing parents: List()
26/02/13 11:47:34 INFO DAGScheduler: Submitting ResultStage 107 (MapPartitionsRDD[373] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:47:34 INFO MemoryStore: Block broadcast_103 stored as values in memory (estimated size 99.3 KiB, free 434.1 MiB)
26/02/13 11:47:34 INFO MemoryStore: Block broadcast_103_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 434.0 MiB)
26/02/13 11:47:34 INFO BlockManagerInfo: Added broadcast_103_piece0 in memory on spark-master:40291 (size: 33.3 KiB, free: 434.3 MiB)
26/02/13 11:47:34 INFO SparkContext: Created broadcast 103 from broadcast at DAGScheduler.scala:1585
26/02/13 11:47:34 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 107 (MapPartitionsRDD[373] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 11:47:34 INFO TaskSchedulerImpl: Adding task set 107.0 with 4 tasks resource profile 0
26/02/13 11:47:34 INFO TaskSetManager: Starting task 0.0 in stage 107.0 (TID 219) (172.18.0.15, executor 0, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:47:34 INFO TaskSetManager: Starting task 1.0 in stage 107.0 (TID 220) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:47:34 INFO BlockManagerInfo: Added broadcast_103_piece0 in memory on 172.18.0.15:34959 (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 11:47:34 INFO TaskSetManager: Starting task 2.0 in stage 107.0 (TID 221) (172.18.0.15, executor 0, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:47:34 INFO TaskSetManager: Starting task 3.0 in stage 107.0 (TID 222) (172.18.0.15, executor 0, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:47:34 INFO TaskSetManager: Finished task 1.0 in stage 107.0 (TID 220) in 31 ms on 172.18.0.15 (executor 0) (1/4)
26/02/13 11:47:34 INFO TaskSetManager: Finished task 0.0 in stage 107.0 (TID 219) in 32 ms on 172.18.0.15 (executor 0) (2/4)
26/02/13 11:47:34 INFO TaskSetManager: Finished task 3.0 in stage 107.0 (TID 222) in 22 ms on 172.18.0.15 (executor 0) (3/4)
26/02/13 11:47:34 INFO TaskSetManager: Finished task 2.0 in stage 107.0 (TID 221) in 30 ms on 172.18.0.15 (executor 0) (4/4)
26/02/13 11:47:34 INFO TaskSchedulerImpl: Removed TaskSet 107.0, whose tasks have all completed, from pool 
26/02/13 11:47:34 INFO DAGScheduler: ResultStage 107 (start at NativeMethodAccessorImpl.java:0) finished in 0.072 s
26/02/13 11:47:34 INFO DAGScheduler: Job 69 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:47:34 INFO TaskSchedulerImpl: Killing all running tasks in stage 107: Stage finished
26/02/13 11:47:34 INFO DAGScheduler: Job 69 finished: start at NativeMethodAccessorImpl.java:0, took 0.086407 s
26/02/13 11:47:34 INFO MemoryStore: Block broadcast_104_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.9 MiB)
26/02/13 11:47:34 INFO BlockManagerInfo: Added broadcast_104_piece0 in memory on spark-master:40291 (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 11:47:34 INFO SparkContext: Created broadcast 104 from start at NativeMethodAccessorImpl.java:0
26/02/13 11:47:34 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 30, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@3136d9d0]. The input RDD has 3 partitions.
26/02/13 11:47:34 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:47:34 INFO DAGScheduler: Got job 70 (start at NativeMethodAccessorImpl.java:0) with 3 output partitions
26/02/13 11:47:34 INFO DAGScheduler: Final stage: ResultStage 108 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:47:34 INFO DAGScheduler: Parents of final stage: List()
26/02/13 11:47:34 INFO DAGScheduler: Missing parents: List()
26/02/13 11:47:34 INFO DAGScheduler: Submitting ResultStage 108 (MapPartitionsRDD[379] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:47:34 INFO MemoryStore: Block broadcast_105 stored as values in memory (estimated size 49.9 KiB, free 433.9 MiB)
26/02/13 11:47:34 INFO MemoryStore: Block broadcast_105_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.9 MiB)
26/02/13 11:47:34 INFO BlockManagerInfo: Added broadcast_105_piece0 in memory on spark-master:40291 (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 11:47:34 INFO SparkContext: Created broadcast 105 from broadcast at DAGScheduler.scala:1585
26/02/13 11:47:34 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 108 (MapPartitionsRDD[379] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
26/02/13 11:47:34 INFO TaskSchedulerImpl: Adding task set 108.0 with 3 tasks resource profile 0
26/02/13 11:47:34 INFO TaskSetManager: Starting task 0.0 in stage 108.0 (TID 223) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:47:34 INFO TaskSetManager: Starting task 1.0 in stage 108.0 (TID 224) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:47:34 INFO TaskSetManager: Starting task 2.0 in stage 108.0 (TID 225) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:47:34 INFO BlockManagerInfo: Added broadcast_105_piece0 in memory on 172.18.0.15:34959 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 11:47:34 INFO BlockManagerInfo: Added broadcast_105_piece0 in memory on 172.18.0.14:40939 (size: 19.5 KiB, free: 434.4 MiB)
26/02/13 11:47:34 INFO BlockManagerInfo: Added broadcast_104_piece0 in memory on 172.18.0.15:34959 (size: 107.2 KiB, free: 434.0 MiB)
26/02/13 11:47:34 INFO BlockManagerInfo: Added broadcast_104_piece0 in memory on 172.18.0.14:40939 (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 11:47:34 INFO TaskSetManager: Finished task 1.0 in stage 108.0 (TID 224) in 103 ms on 172.18.0.15 (executor 0) (1/3)
26/02/13 11:47:34 INFO TaskSetManager: Finished task 0.0 in stage 108.0 (TID 223) in 124 ms on 172.18.0.14 (executor 1) (2/3)
26/02/13 11:47:34 INFO TaskSetManager: Finished task 2.0 in stage 108.0 (TID 225) in 141 ms on 172.18.0.14 (executor 1) (3/3)
26/02/13 11:47:34 INFO TaskSchedulerImpl: Removed TaskSet 108.0, whose tasks have all completed, from pool 
26/02/13 11:47:34 INFO DAGScheduler: ResultStage 108 (start at NativeMethodAccessorImpl.java:0) finished in 0.151 s
26/02/13 11:47:34 INFO DAGScheduler: Job 70 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:47:34 INFO TaskSchedulerImpl: Killing all running tasks in stage 108: Stage finished
26/02/13 11:47:34 INFO DAGScheduler: Job 70 finished: start at NativeMethodAccessorImpl.java:0, took 0.152868 s
26/02/13 11:47:34 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 30, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@3136d9d0] is committing.
26/02/13 11:47:34 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 30, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@3136d9d0] committed.
26/02/13 11:47:34 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/30 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.30.a80451f0-c2c2-4fa3-856c-9c93da6654d3.tmp
26/02/13 11:47:34 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.30.a80451f0-c2c2-4fa3-856c-9c93da6654d3.tmp to file:/tmp/spark-checkpoint-enrichment/commits/30
26/02/13 11:47:34 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "fc5c6452-bd70-4c90-becf-772d3bebdb57",
  "name" : null,
  "timestamp" : "2026-02-13T11:47:34.155Z",
  "batchId" : 30,
  "numInputRows" : 137,
  "inputRowsPerSecond" : 144.82029598308668,
  "processedRowsPerSecond" : 258.49056603773585,
  "durationMs" : {
    "addBatch" : 376,
    "commitOffsets" : 64,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 30,
    "triggerExecution" : 530,
    "walCommit" : 59
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 3895,
        "1" : 4305,
        "0" : 6091
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 3942,
        "1" : 4350,
        "0" : 6136
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 3942,
        "1" : 4350,
        "0" : 6136
      }
    },
    "numInputRows" : 137,
    "inputRowsPerSecond" : 144.82029598308668,
    "processedRowsPerSecond" : 258.49056603773585,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@7fe9b5de",
    "numOutputRows" : 32
  }
}
26/02/13 11:47:44 INFO BlockManagerInfo: Removed broadcast_103_piece0 on spark-master:40291 in memory (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 11:47:44 INFO BlockManagerInfo: Removed broadcast_103_piece0 on 172.18.0.15:34959 in memory (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 11:47:44 INFO BlockManagerInfo: Removed broadcast_105_piece0 on spark-master:40291 in memory (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 11:47:44 INFO BlockManagerInfo: Removed broadcast_105_piece0 on 172.18.0.15:34959 in memory (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 11:47:44 INFO BlockManagerInfo: Removed broadcast_105_piece0 on 172.18.0.14:40939 in memory (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 11:47:44 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 11:47:49 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/31 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.31.326765a8-c4c8-4452-a79e-e99e3ce8002e.tmp
26/02/13 11:47:49 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.31.326765a8-c4c8-4452-a79e-e99e3ce8002e.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/31
26/02/13 11:47:49 INFO MicroBatchExecution: Committed offsets for batch 31. Metadata OffsetSeqMetadata(0,1770983269550,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 11:47:49 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:47:49 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:47:49 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:47:49 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:47:49 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:47:49 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:47:49 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#28425 - origin_code.nullCount#28424) > 0)
26/02/13 11:47:49 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#28430 - destination_code.nullCount#28429) > 0)
26/02/13 11:47:49 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#28460 - callsign.nullCount#28459) > 0)
26/02/13 11:47:49 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:47:49 INFO DAGScheduler: Got job 71 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 11:47:49 INFO DAGScheduler: Final stage: ResultStage 110 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:47:49 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 109)
26/02/13 11:47:49 INFO DAGScheduler: Missing parents: List()
26/02/13 11:47:49 INFO DAGScheduler: Submitting ResultStage 110 (MapPartitionsRDD[384] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:47:49 INFO MemoryStore: Block broadcast_106 stored as values in memory (estimated size 99.3 KiB, free 434.0 MiB)
26/02/13 11:47:49 INFO MemoryStore: Block broadcast_106_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 433.9 MiB)
26/02/13 11:47:49 INFO BlockManagerInfo: Added broadcast_106_piece0 in memory on spark-master:40291 (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 11:47:49 INFO SparkContext: Created broadcast 106 from broadcast at DAGScheduler.scala:1585
26/02/13 11:47:49 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 110 (MapPartitionsRDD[384] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 11:47:49 INFO TaskSchedulerImpl: Adding task set 110.0 with 4 tasks resource profile 0
26/02/13 11:47:49 INFO TaskSetManager: Starting task 0.0 in stage 110.0 (TID 226) (172.18.0.15, executor 0, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:47:49 INFO TaskSetManager: Starting task 1.0 in stage 110.0 (TID 227) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:47:49 INFO BlockManagerInfo: Added broadcast_106_piece0 in memory on 172.18.0.15:34959 (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 11:47:49 INFO TaskSetManager: Starting task 2.0 in stage 110.0 (TID 228) (172.18.0.15, executor 0, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:47:49 INFO TaskSetManager: Finished task 0.0 in stage 110.0 (TID 226) in 25 ms on 172.18.0.15 (executor 0) (1/4)
26/02/13 11:47:49 INFO TaskSetManager: Starting task 3.0 in stage 110.0 (TID 229) (172.18.0.15, executor 0, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:47:49 INFO TaskSetManager: Finished task 1.0 in stage 110.0 (TID 227) in 28 ms on 172.18.0.15 (executor 0) (2/4)
26/02/13 11:47:49 INFO TaskSetManager: Finished task 3.0 in stage 110.0 (TID 229) in 18 ms on 172.18.0.15 (executor 0) (3/4)
26/02/13 11:47:49 INFO TaskSetManager: Finished task 2.0 in stage 110.0 (TID 228) in 23 ms on 172.18.0.15 (executor 0) (4/4)
26/02/13 11:47:49 INFO TaskSchedulerImpl: Removed TaskSet 110.0, whose tasks have all completed, from pool 
26/02/13 11:47:49 INFO DAGScheduler: ResultStage 110 (start at NativeMethodAccessorImpl.java:0) finished in 0.060 s
26/02/13 11:47:49 INFO DAGScheduler: Job 71 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:47:49 INFO TaskSchedulerImpl: Killing all running tasks in stage 110: Stage finished
26/02/13 11:47:49 INFO DAGScheduler: Job 71 finished: start at NativeMethodAccessorImpl.java:0, took 0.077561 s
26/02/13 11:47:49 INFO MemoryStore: Block broadcast_107_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.8 MiB)
26/02/13 11:47:49 INFO BlockManagerInfo: Added broadcast_107_piece0 in memory on spark-master:40291 (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 11:47:49 INFO SparkContext: Created broadcast 107 from start at NativeMethodAccessorImpl.java:0
26/02/13 11:47:49 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 31, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@2f7644ac]. The input RDD has 3 partitions.
26/02/13 11:47:49 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:47:49 INFO DAGScheduler: Got job 72 (start at NativeMethodAccessorImpl.java:0) with 3 output partitions
26/02/13 11:47:49 INFO DAGScheduler: Final stage: ResultStage 111 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:47:49 INFO DAGScheduler: Parents of final stage: List()
26/02/13 11:47:49 INFO DAGScheduler: Missing parents: List()
26/02/13 11:47:49 INFO DAGScheduler: Submitting ResultStage 111 (MapPartitionsRDD[390] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:47:49 INFO MemoryStore: Block broadcast_108 stored as values in memory (estimated size 49.9 KiB, free 433.8 MiB)
26/02/13 11:47:49 INFO MemoryStore: Block broadcast_108_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.8 MiB)
26/02/13 11:47:49 INFO BlockManagerInfo: Added broadcast_108_piece0 in memory on spark-master:40291 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 11:47:49 INFO SparkContext: Created broadcast 108 from broadcast at DAGScheduler.scala:1585
26/02/13 11:47:49 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 111 (MapPartitionsRDD[390] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
26/02/13 11:47:49 INFO TaskSchedulerImpl: Adding task set 111.0 with 3 tasks resource profile 0
26/02/13 11:47:49 INFO TaskSetManager: Starting task 0.0 in stage 111.0 (TID 230) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:47:49 INFO TaskSetManager: Starting task 1.0 in stage 111.0 (TID 231) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:47:49 INFO TaskSetManager: Starting task 2.0 in stage 111.0 (TID 232) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:47:49 INFO BlockManagerInfo: Added broadcast_108_piece0 in memory on 172.18.0.15:34959 (size: 19.5 KiB, free: 434.0 MiB)
26/02/13 11:47:49 INFO BlockManagerInfo: Added broadcast_108_piece0 in memory on 172.18.0.14:40939 (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 11:47:49 INFO BlockManagerInfo: Added broadcast_107_piece0 in memory on 172.18.0.15:34959 (size: 107.2 KiB, free: 433.9 MiB)
26/02/13 11:47:49 INFO BlockManagerInfo: Added broadcast_107_piece0 in memory on 172.18.0.14:40939 (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 11:47:50 INFO TaskSetManager: Finished task 1.0 in stage 111.0 (TID 231) in 573 ms on 172.18.0.15 (executor 0) (1/3)
26/02/13 11:47:50 INFO TaskSetManager: Finished task 2.0 in stage 111.0 (TID 232) in 583 ms on 172.18.0.14 (executor 1) (2/3)
26/02/13 11:47:50 INFO TaskSetManager: Finished task 0.0 in stage 111.0 (TID 230) in 586 ms on 172.18.0.14 (executor 1) (3/3)
26/02/13 11:47:50 INFO TaskSchedulerImpl: Removed TaskSet 111.0, whose tasks have all completed, from pool 
26/02/13 11:47:50 INFO DAGScheduler: ResultStage 111 (start at NativeMethodAccessorImpl.java:0) finished in 0.594 s
26/02/13 11:47:50 INFO DAGScheduler: Job 72 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:47:50 INFO TaskSchedulerImpl: Killing all running tasks in stage 111: Stage finished
26/02/13 11:47:50 INFO DAGScheduler: Job 72 finished: start at NativeMethodAccessorImpl.java:0, took 0.596296 s
26/02/13 11:47:50 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 31, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@2f7644ac] is committing.
26/02/13 11:47:50 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 31, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@2f7644ac] committed.
26/02/13 11:47:50 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/31 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.31.9e5ddf67-1030-41b4-b8be-bade42769f35.tmp
26/02/13 11:47:50 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.31.9e5ddf67-1030-41b4-b8be-bade42769f35.tmp to file:/tmp/spark-checkpoint-enrichment/commits/31
26/02/13 11:47:50 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "fc5c6452-bd70-4c90-becf-772d3bebdb57",
  "name" : null,
  "timestamp" : "2026-02-13T11:47:49.549Z",
  "batchId" : 31,
  "numInputRows" : 7,
  "inputRowsPerSecond" : 583.3333333333334,
  "processedRowsPerSecond" : 7.391763463569166,
  "durationMs" : {
    "addBatch" : 764,
    "commitOffsets" : 68,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 30,
    "triggerExecution" : 947,
    "walCommit" : 83
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 3942,
        "1" : 4350,
        "0" : 6136
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 3943,
        "1" : 4355,
        "0" : 6137
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 3943,
        "1" : 4355,
        "0" : 6137
      }
    },
    "numInputRows" : 7,
    "inputRowsPerSecond" : 583.3333333333334,
    "processedRowsPerSecond" : 7.391763463569166,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@7fe9b5de",
    "numOutputRows" : 3
  }
}
26/02/13 11:47:50 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/32 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.32.74d67947-f6d7-432b-a4de-acc6be724e8c.tmp
26/02/13 11:47:50 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.32.74d67947-f6d7-432b-a4de-acc6be724e8c.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/32
26/02/13 11:47:50 INFO MicroBatchExecution: Committed offsets for batch 32. Metadata OffsetSeqMetadata(0,1770983270498,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 11:47:50 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:47:50 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:47:50 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:47:50 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:47:50 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:47:50 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:47:50 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#29279 - origin_code.nullCount#29278) > 0)
26/02/13 11:47:50 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#29284 - destination_code.nullCount#29283) > 0)
26/02/13 11:47:50 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#29314 - callsign.nullCount#29313) > 0)
26/02/13 11:47:50 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:47:50 INFO DAGScheduler: Got job 73 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 11:47:50 INFO DAGScheduler: Final stage: ResultStage 113 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:47:50 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 112)
26/02/13 11:47:50 INFO DAGScheduler: Missing parents: List()
26/02/13 11:47:50 INFO DAGScheduler: Submitting ResultStage 113 (MapPartitionsRDD[395] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:47:50 INFO MemoryStore: Block broadcast_109 stored as values in memory (estimated size 99.3 KiB, free 433.7 MiB)
26/02/13 11:47:50 INFO MemoryStore: Block broadcast_109_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 433.6 MiB)
26/02/13 11:47:50 INFO BlockManagerInfo: Added broadcast_109_piece0 in memory on spark-master:40291 (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 11:47:50 INFO SparkContext: Created broadcast 109 from broadcast at DAGScheduler.scala:1585
26/02/13 11:47:50 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 113 (MapPartitionsRDD[395] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 11:47:50 INFO TaskSchedulerImpl: Adding task set 113.0 with 4 tasks resource profile 0
26/02/13 11:47:50 INFO TaskSetManager: Starting task 0.0 in stage 113.0 (TID 233) (172.18.0.15, executor 0, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:47:50 INFO TaskSetManager: Starting task 1.0 in stage 113.0 (TID 234) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:47:50 INFO BlockManagerInfo: Added broadcast_109_piece0 in memory on 172.18.0.15:34959 (size: 33.3 KiB, free: 433.9 MiB)
26/02/13 11:47:50 INFO TaskSetManager: Starting task 2.0 in stage 113.0 (TID 235) (172.18.0.15, executor 0, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:47:50 INFO TaskSetManager: Finished task 1.0 in stage 113.0 (TID 234) in 21 ms on 172.18.0.15 (executor 0) (1/4)
26/02/13 11:47:50 INFO TaskSetManager: Starting task 3.0 in stage 113.0 (TID 236) (172.18.0.15, executor 0, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:47:50 INFO TaskSetManager: Finished task 0.0 in stage 113.0 (TID 233) in 33 ms on 172.18.0.15 (executor 0) (2/4)
26/02/13 11:47:50 INFO TaskSetManager: Finished task 2.0 in stage 113.0 (TID 235) in 26 ms on 172.18.0.15 (executor 0) (3/4)
26/02/13 11:47:50 INFO TaskSetManager: Finished task 3.0 in stage 113.0 (TID 236) in 41 ms on 172.18.0.15 (executor 0) (4/4)
26/02/13 11:47:50 INFO TaskSchedulerImpl: Removed TaskSet 113.0, whose tasks have all completed, from pool 
26/02/13 11:47:50 INFO DAGScheduler: ResultStage 113 (start at NativeMethodAccessorImpl.java:0) finished in 0.082 s
26/02/13 11:47:50 INFO DAGScheduler: Job 73 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:47:50 INFO TaskSchedulerImpl: Killing all running tasks in stage 113: Stage finished
26/02/13 11:47:50 INFO DAGScheduler: Job 73 finished: start at NativeMethodAccessorImpl.java:0, took 0.085256 s
26/02/13 11:47:50 INFO MemoryStore: Block broadcast_110_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.5 MiB)
26/02/13 11:47:50 INFO BlockManagerInfo: Added broadcast_110_piece0 in memory on spark-master:40291 (size: 107.2 KiB, free: 434.0 MiB)
26/02/13 11:47:50 INFO SparkContext: Created broadcast 110 from start at NativeMethodAccessorImpl.java:0
26/02/13 11:47:50 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 32, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@46ca8a89]. The input RDD has 3 partitions.
26/02/13 11:47:50 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:47:50 INFO DAGScheduler: Got job 74 (start at NativeMethodAccessorImpl.java:0) with 3 output partitions
26/02/13 11:47:50 INFO DAGScheduler: Final stage: ResultStage 114 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:47:50 INFO DAGScheduler: Parents of final stage: List()
26/02/13 11:47:50 INFO DAGScheduler: Missing parents: List()
26/02/13 11:47:50 INFO DAGScheduler: Submitting ResultStage 114 (MapPartitionsRDD[401] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:47:50 INFO MemoryStore: Block broadcast_111 stored as values in memory (estimated size 49.9 KiB, free 433.5 MiB)
26/02/13 11:47:50 INFO MemoryStore: Block broadcast_111_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.5 MiB)
26/02/13 11:47:50 INFO BlockManagerInfo: Removed broadcast_108_piece0 on spark-master:40291 in memory (size: 19.5 KiB, free: 434.0 MiB)
26/02/13 11:47:50 INFO BlockManagerInfo: Added broadcast_111_piece0 in memory on spark-master:40291 (size: 19.5 KiB, free: 434.0 MiB)
26/02/13 11:47:50 INFO SparkContext: Created broadcast 111 from broadcast at DAGScheduler.scala:1585
26/02/13 11:47:50 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 114 (MapPartitionsRDD[401] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
26/02/13 11:47:50 INFO TaskSchedulerImpl: Adding task set 114.0 with 3 tasks resource profile 0
26/02/13 11:47:50 INFO BlockManagerInfo: Removed broadcast_108_piece0 on 172.18.0.15:34959 in memory (size: 19.5 KiB, free: 433.9 MiB)
26/02/13 11:47:50 INFO BlockManagerInfo: Removed broadcast_108_piece0 on 172.18.0.14:40939 in memory (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 11:47:50 INFO TaskSetManager: Starting task 1.0 in stage 114.0 (TID 237) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:47:50 INFO TaskSetManager: Starting task 0.0 in stage 114.0 (TID 238) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:47:50 INFO TaskSetManager: Starting task 2.0 in stage 114.0 (TID 239) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:47:50 INFO BlockManagerInfo: Removed broadcast_107_piece0 on spark-master:40291 in memory (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 11:47:50 INFO BlockManagerInfo: Removed broadcast_107_piece0 on 172.18.0.14:40939 in memory (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 11:47:50 INFO BlockManagerInfo: Removed broadcast_107_piece0 on 172.18.0.15:34959 in memory (size: 107.2 KiB, free: 434.0 MiB)
26/02/13 11:47:50 INFO BlockManagerInfo: Added broadcast_111_piece0 in memory on 172.18.0.14:40939 (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 11:47:50 INFO BlockManagerInfo: Added broadcast_111_piece0 in memory on 172.18.0.15:34959 (size: 19.5 KiB, free: 434.0 MiB)
26/02/13 11:47:50 INFO BlockManagerInfo: Removed broadcast_106_piece0 on spark-master:40291 in memory (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 11:47:50 INFO BlockManagerInfo: Removed broadcast_106_piece0 on 172.18.0.15:34959 in memory (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 11:47:50 INFO BlockManagerInfo: Removed broadcast_109_piece0 on spark-master:40291 in memory (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 11:47:50 INFO BlockManagerInfo: Removed broadcast_109_piece0 on 172.18.0.15:34959 in memory (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 11:47:50 INFO BlockManagerInfo: Added broadcast_110_piece0 in memory on 172.18.0.14:40939 (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 11:47:50 INFO BlockManagerInfo: Added broadcast_110_piece0 in memory on 172.18.0.15:34959 (size: 107.2 KiB, free: 433.9 MiB)
26/02/13 11:47:50 INFO TaskSetManager: Finished task 1.0 in stage 114.0 (TID 237) in 140 ms on 172.18.0.15 (executor 0) (1/3)
26/02/13 11:47:51 INFO TaskSetManager: Finished task 2.0 in stage 114.0 (TID 239) in 169 ms on 172.18.0.14 (executor 1) (2/3)
26/02/13 11:47:51 INFO TaskSetManager: Finished task 0.0 in stage 114.0 (TID 238) in 170 ms on 172.18.0.14 (executor 1) (3/3)
26/02/13 11:47:51 INFO TaskSchedulerImpl: Removed TaskSet 114.0, whose tasks have all completed, from pool 
26/02/13 11:47:51 INFO DAGScheduler: ResultStage 114 (start at NativeMethodAccessorImpl.java:0) finished in 0.186 s
26/02/13 11:47:51 INFO DAGScheduler: Job 74 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:47:51 INFO TaskSchedulerImpl: Killing all running tasks in stage 114: Stage finished
26/02/13 11:47:51 INFO DAGScheduler: Job 74 finished: start at NativeMethodAccessorImpl.java:0, took 0.188789 s
26/02/13 11:47:51 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 32, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@46ca8a89] is committing.
26/02/13 11:47:51 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 32, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@46ca8a89] committed.
26/02/13 11:47:51 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/32 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.32.33f1977b-6a23-41c0-82e5-7295af8a7493.tmp
26/02/13 11:47:51 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.32.33f1977b-6a23-41c0-82e5-7295af8a7493.tmp to file:/tmp/spark-checkpoint-enrichment/commits/32
26/02/13 11:47:51 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "fc5c6452-bd70-4c90-becf-772d3bebdb57",
  "name" : null,
  "timestamp" : "2026-02-13T11:47:50.497Z",
  "batchId" : 32,
  "numInputRows" : 232,
  "inputRowsPerSecond" : 244.72573839662448,
  "processedRowsPerSecond" : 398.6254295532646,
  "durationMs" : {
    "addBatch" : 409,
    "commitOffsets" : 71,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 36,
    "triggerExecution" : 582,
    "walCommit" : 64
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 3943,
        "1" : 4355,
        "0" : 6137
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 4019,
        "1" : 4433,
        "0" : 6215
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 4019,
        "1" : 4433,
        "0" : 6215
      }
    },
    "numInputRows" : 232,
    "inputRowsPerSecond" : 244.72573839662448,
    "processedRowsPerSecond" : 398.6254295532646,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@7fe9b5de",
    "numOutputRows" : 53
  }
}
26/02/13 11:47:51 INFO BlockManagerInfo: Removed broadcast_111_piece0 on spark-master:40291 in memory (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 11:47:51 INFO BlockManagerInfo: Removed broadcast_111_piece0 on 172.18.0.15:34959 in memory (size: 19.5 KiB, free: 434.0 MiB)
26/02/13 11:47:51 INFO BlockManagerInfo: Removed broadcast_111_piece0 on 172.18.0.14:40939 in memory (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 11:48:01 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 11:48:02 INFO BlockManagerInfo: Removed broadcast_104_piece0 on spark-master:40291 in memory (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 11:48:02 INFO BlockManagerInfo: Removed broadcast_104_piece0 on 172.18.0.15:34959 in memory (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 11:48:02 INFO BlockManagerInfo: Removed broadcast_104_piece0 on 172.18.0.14:40939 in memory (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 11:48:06 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/33 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.33.c2362ad5-1ede-44a3-95d9-d3391bb2cf30.tmp
26/02/13 11:48:06 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.33.c2362ad5-1ede-44a3-95d9-d3391bb2cf30.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/33
26/02/13 11:48:06 INFO MicroBatchExecution: Committed offsets for batch 33. Metadata OffsetSeqMetadata(0,1770983286263,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 11:48:06 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:48:06 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:48:06 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:48:06 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:48:06 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:48:06 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:48:06 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#30133 - origin_code.nullCount#30132) > 0)
26/02/13 11:48:06 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#30138 - destination_code.nullCount#30137) > 0)
26/02/13 11:48:06 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#30168 - callsign.nullCount#30167) > 0)
26/02/13 11:48:06 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:48:06 INFO DAGScheduler: Got job 75 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 11:48:06 INFO DAGScheduler: Final stage: ResultStage 116 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:48:06 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 115)
26/02/13 11:48:06 INFO DAGScheduler: Missing parents: List()
26/02/13 11:48:06 INFO DAGScheduler: Submitting ResultStage 116 (MapPartitionsRDD[406] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:48:06 INFO MemoryStore: Block broadcast_112 stored as values in memory (estimated size 99.3 KiB, free 434.0 MiB)
26/02/13 11:48:06 INFO MemoryStore: Block broadcast_112_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 433.9 MiB)
26/02/13 11:48:06 INFO BlockManagerInfo: Added broadcast_112_piece0 in memory on spark-master:40291 (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 11:48:06 INFO SparkContext: Created broadcast 112 from broadcast at DAGScheduler.scala:1585
26/02/13 11:48:06 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 116 (MapPartitionsRDD[406] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 11:48:06 INFO TaskSchedulerImpl: Adding task set 116.0 with 4 tasks resource profile 0
26/02/13 11:48:06 INFO TaskSetManager: Starting task 0.0 in stage 116.0 (TID 240) (172.18.0.15, executor 0, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:48:06 INFO TaskSetManager: Starting task 1.0 in stage 116.0 (TID 241) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:48:06 INFO BlockManagerInfo: Added broadcast_112_piece0 in memory on 172.18.0.15:34959 (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 11:48:06 INFO TaskSetManager: Starting task 2.0 in stage 116.0 (TID 242) (172.18.0.15, executor 0, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:48:06 INFO TaskSetManager: Finished task 0.0 in stage 116.0 (TID 240) in 19 ms on 172.18.0.15 (executor 0) (1/4)
26/02/13 11:48:06 INFO TaskSetManager: Starting task 3.0 in stage 116.0 (TID 243) (172.18.0.15, executor 0, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:48:06 INFO TaskSetManager: Finished task 1.0 in stage 116.0 (TID 241) in 25 ms on 172.18.0.15 (executor 0) (2/4)
26/02/13 11:48:06 INFO TaskSetManager: Finished task 2.0 in stage 116.0 (TID 242) in 17 ms on 172.18.0.15 (executor 0) (3/4)
26/02/13 11:48:06 INFO TaskSetManager: Finished task 3.0 in stage 116.0 (TID 243) in 18 ms on 172.18.0.15 (executor 0) (4/4)
26/02/13 11:48:06 INFO TaskSchedulerImpl: Removed TaskSet 116.0, whose tasks have all completed, from pool 
26/02/13 11:48:06 INFO DAGScheduler: ResultStage 116 (start at NativeMethodAccessorImpl.java:0) finished in 0.052 s
26/02/13 11:48:06 INFO DAGScheduler: Job 75 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:48:06 INFO TaskSchedulerImpl: Killing all running tasks in stage 116: Stage finished
26/02/13 11:48:06 INFO DAGScheduler: Job 75 finished: start at NativeMethodAccessorImpl.java:0, took 0.055546 s
26/02/13 11:48:06 INFO MemoryStore: Block broadcast_113_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.8 MiB)
26/02/13 11:48:06 INFO BlockManagerInfo: Added broadcast_113_piece0 in memory on spark-master:40291 (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 11:48:06 INFO SparkContext: Created broadcast 113 from start at NativeMethodAccessorImpl.java:0
26/02/13 11:48:06 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 33, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@194ff63a]. The input RDD has 3 partitions.
26/02/13 11:48:06 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:48:06 INFO DAGScheduler: Got job 76 (start at NativeMethodAccessorImpl.java:0) with 3 output partitions
26/02/13 11:48:06 INFO DAGScheduler: Final stage: ResultStage 117 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:48:06 INFO DAGScheduler: Parents of final stage: List()
26/02/13 11:48:06 INFO DAGScheduler: Missing parents: List()
26/02/13 11:48:06 INFO DAGScheduler: Submitting ResultStage 117 (MapPartitionsRDD[412] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:48:06 INFO MemoryStore: Block broadcast_114 stored as values in memory (estimated size 49.9 KiB, free 433.8 MiB)
26/02/13 11:48:06 INFO MemoryStore: Block broadcast_114_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.8 MiB)
26/02/13 11:48:06 INFO BlockManagerInfo: Added broadcast_114_piece0 in memory on spark-master:40291 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 11:48:06 INFO SparkContext: Created broadcast 114 from broadcast at DAGScheduler.scala:1585
26/02/13 11:48:06 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 117 (MapPartitionsRDD[412] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
26/02/13 11:48:06 INFO TaskSchedulerImpl: Adding task set 117.0 with 3 tasks resource profile 0
26/02/13 11:48:06 INFO TaskSetManager: Starting task 0.0 in stage 117.0 (TID 244) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:48:06 INFO TaskSetManager: Starting task 1.0 in stage 117.0 (TID 245) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:48:06 INFO TaskSetManager: Starting task 2.0 in stage 117.0 (TID 246) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:48:06 INFO BlockManagerInfo: Added broadcast_114_piece0 in memory on 172.18.0.15:34959 (size: 19.5 KiB, free: 434.0 MiB)
26/02/13 11:48:06 INFO BlockManagerInfo: Added broadcast_114_piece0 in memory on 172.18.0.14:40939 (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 11:48:06 INFO BlockManagerInfo: Added broadcast_113_piece0 in memory on 172.18.0.15:34959 (size: 107.2 KiB, free: 433.9 MiB)
26/02/13 11:48:06 INFO BlockManagerInfo: Added broadcast_113_piece0 in memory on 172.18.0.14:40939 (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 11:48:07 INFO TaskSetManager: Finished task 0.0 in stage 117.0 (TID 244) in 585 ms on 172.18.0.14 (executor 1) (1/3)
26/02/13 11:48:07 INFO TaskSetManager: Finished task 2.0 in stage 117.0 (TID 246) in 585 ms on 172.18.0.14 (executor 1) (2/3)
26/02/13 11:48:07 INFO TaskSetManager: Finished task 1.0 in stage 117.0 (TID 245) in 591 ms on 172.18.0.15 (executor 0) (3/3)
26/02/13 11:48:07 INFO TaskSchedulerImpl: Removed TaskSet 117.0, whose tasks have all completed, from pool 
26/02/13 11:48:07 INFO DAGScheduler: ResultStage 117 (start at NativeMethodAccessorImpl.java:0) finished in 0.597 s
26/02/13 11:48:07 INFO DAGScheduler: Job 76 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:48:07 INFO TaskSchedulerImpl: Killing all running tasks in stage 117: Stage finished
26/02/13 11:48:07 INFO DAGScheduler: Job 76 finished: start at NativeMethodAccessorImpl.java:0, took 0.598423 s
26/02/13 11:48:07 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 33, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@194ff63a] is committing.
26/02/13 11:48:07 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 33, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@194ff63a] committed.
26/02/13 11:48:07 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/33 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.33.d0ab2dad-b651-4548-91f9-a59cd1f0c566.tmp
26/02/13 11:48:07 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.33.d0ab2dad-b651-4548-91f9-a59cd1f0c566.tmp to file:/tmp/spark-checkpoint-enrichment/commits/33
26/02/13 11:48:07 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "fc5c6452-bd70-4c90-becf-772d3bebdb57",
  "name" : null,
  "timestamp" : "2026-02-13T11:48:06.262Z",
  "batchId" : 33,
  "numInputRows" : 83,
  "inputRowsPerSecond" : 6916.666666666667,
  "processedRowsPerSecond" : 85.83247156153051,
  "durationMs" : {
    "addBatch" : 744,
    "commitOffsets" : 58,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 50,
    "triggerExecution" : 967,
    "walCommit" : 114
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 4019,
        "1" : 4433,
        "0" : 6215
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 4046,
        "1" : 4465,
        "0" : 6239
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 4046,
        "1" : 4465,
        "0" : 6239
      }
    },
    "numInputRows" : 83,
    "inputRowsPerSecond" : 6916.666666666667,
    "processedRowsPerSecond" : 85.83247156153051,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@7fe9b5de",
    "numOutputRows" : 22
  }
}
26/02/13 11:48:07 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/34 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.34.91afead5-6e46-42cb-8e85-f3d72050bd7d.tmp
26/02/13 11:48:07 INFO BlockManagerInfo: Removed broadcast_112_piece0 on spark-master:40291 in memory (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 11:48:07 INFO BlockManagerInfo: Removed broadcast_112_piece0 on 172.18.0.15:34959 in memory (size: 33.3 KiB, free: 433.9 MiB)
26/02/13 11:48:07 INFO BlockManagerInfo: Removed broadcast_114_piece0 on spark-master:40291 in memory (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 11:48:07 INFO BlockManagerInfo: Removed broadcast_114_piece0 on 172.18.0.15:34959 in memory (size: 19.5 KiB, free: 434.0 MiB)
26/02/13 11:48:07 INFO BlockManagerInfo: Removed broadcast_114_piece0 on 172.18.0.14:40939 in memory (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 11:48:07 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.34.91afead5-6e46-42cb-8e85-f3d72050bd7d.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/34
26/02/13 11:48:07 INFO MicroBatchExecution: Committed offsets for batch 34. Metadata OffsetSeqMetadata(0,1770983287232,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 11:48:07 INFO BlockManagerInfo: Removed broadcast_113_piece0 on spark-master:40291 in memory (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 11:48:07 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:48:07 INFO BlockManagerInfo: Removed broadcast_113_piece0 on 172.18.0.15:34959 in memory (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 11:48:07 INFO BlockManagerInfo: Removed broadcast_113_piece0 on 172.18.0.14:40939 in memory (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 11:48:07 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:48:07 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:48:07 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:48:07 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:48:07 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:48:07 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#30987 - origin_code.nullCount#30986) > 0)
26/02/13 11:48:07 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#30992 - destination_code.nullCount#30991) > 0)
26/02/13 11:48:07 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#31022 - callsign.nullCount#31021) > 0)
26/02/13 11:48:07 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:48:07 INFO DAGScheduler: Got job 77 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 11:48:07 INFO DAGScheduler: Final stage: ResultStage 119 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:48:07 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 118)
26/02/13 11:48:07 INFO DAGScheduler: Missing parents: List()
26/02/13 11:48:07 INFO DAGScheduler: Submitting ResultStage 119 (MapPartitionsRDD[417] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:48:07 INFO MemoryStore: Block broadcast_115 stored as values in memory (estimated size 99.3 KiB, free 434.0 MiB)
26/02/13 11:48:07 INFO MemoryStore: Block broadcast_115_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 433.9 MiB)
26/02/13 11:48:07 INFO BlockManagerInfo: Added broadcast_115_piece0 in memory on spark-master:40291 (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 11:48:07 INFO SparkContext: Created broadcast 115 from broadcast at DAGScheduler.scala:1585
26/02/13 11:48:07 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 119 (MapPartitionsRDD[417] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 11:48:07 INFO TaskSchedulerImpl: Adding task set 119.0 with 4 tasks resource profile 0
26/02/13 11:48:07 INFO TaskSetManager: Starting task 0.0 in stage 119.0 (TID 247) (172.18.0.15, executor 0, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:48:07 INFO TaskSetManager: Starting task 1.0 in stage 119.0 (TID 248) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:48:07 INFO BlockManagerInfo: Added broadcast_115_piece0 in memory on 172.18.0.15:34959 (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 11:48:07 INFO TaskSetManager: Starting task 2.0 in stage 119.0 (TID 249) (172.18.0.15, executor 0, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:48:07 INFO TaskSetManager: Starting task 3.0 in stage 119.0 (TID 250) (172.18.0.15, executor 0, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:48:07 INFO TaskSetManager: Finished task 0.0 in stage 119.0 (TID 247) in 29 ms on 172.18.0.15 (executor 0) (1/4)
26/02/13 11:48:07 INFO TaskSetManager: Finished task 1.0 in stage 119.0 (TID 248) in 30 ms on 172.18.0.15 (executor 0) (2/4)
26/02/13 11:48:07 INFO TaskSetManager: Finished task 2.0 in stage 119.0 (TID 249) in 18 ms on 172.18.0.15 (executor 0) (3/4)
26/02/13 11:48:07 INFO TaskSetManager: Finished task 3.0 in stage 119.0 (TID 250) in 20 ms on 172.18.0.15 (executor 0) (4/4)
26/02/13 11:48:07 INFO TaskSchedulerImpl: Removed TaskSet 119.0, whose tasks have all completed, from pool 
26/02/13 11:48:07 INFO DAGScheduler: ResultStage 119 (start at NativeMethodAccessorImpl.java:0) finished in 0.056 s
26/02/13 11:48:07 INFO DAGScheduler: Job 77 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:48:07 INFO TaskSchedulerImpl: Killing all running tasks in stage 119: Stage finished
26/02/13 11:48:07 INFO DAGScheduler: Job 77 finished: start at NativeMethodAccessorImpl.java:0, took 0.059046 s
26/02/13 11:48:07 INFO BlockManagerInfo: Removed broadcast_110_piece0 on spark-master:40291 in memory (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 11:48:07 INFO MemoryStore: Block broadcast_116_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.9 MiB)
26/02/13 11:48:07 INFO BlockManagerInfo: Added broadcast_116_piece0 in memory on spark-master:40291 (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 11:48:07 INFO SparkContext: Created broadcast 116 from start at NativeMethodAccessorImpl.java:0
26/02/13 11:48:07 INFO BlockManagerInfo: Removed broadcast_110_piece0 on 172.18.0.14:40939 in memory (size: 107.2 KiB, free: 434.4 MiB)
26/02/13 11:48:07 INFO BlockManagerInfo: Removed broadcast_110_piece0 on 172.18.0.15:34959 in memory (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 11:48:07 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 34, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@cdca956]. The input RDD has 3 partitions.
26/02/13 11:48:07 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:48:07 INFO DAGScheduler: Got job 78 (start at NativeMethodAccessorImpl.java:0) with 3 output partitions
26/02/13 11:48:07 INFO DAGScheduler: Final stage: ResultStage 120 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:48:07 INFO DAGScheduler: Parents of final stage: List()
26/02/13 11:48:07 INFO DAGScheduler: Missing parents: List()
26/02/13 11:48:07 INFO DAGScheduler: Submitting ResultStage 120 (MapPartitionsRDD[423] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:48:07 INFO MemoryStore: Block broadcast_117 stored as values in memory (estimated size 49.9 KiB, free 433.9 MiB)
26/02/13 11:48:07 INFO MemoryStore: Block broadcast_117_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.9 MiB)
26/02/13 11:48:07 INFO BlockManagerInfo: Added broadcast_117_piece0 in memory on spark-master:40291 (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 11:48:07 INFO SparkContext: Created broadcast 117 from broadcast at DAGScheduler.scala:1585
26/02/13 11:48:07 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 120 (MapPartitionsRDD[423] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
26/02/13 11:48:07 INFO TaskSchedulerImpl: Adding task set 120.0 with 3 tasks resource profile 0
26/02/13 11:48:07 INFO TaskSetManager: Starting task 0.0 in stage 120.0 (TID 251) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:48:07 INFO TaskSetManager: Starting task 1.0 in stage 120.0 (TID 252) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:48:07 INFO TaskSetManager: Starting task 2.0 in stage 120.0 (TID 253) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:48:07 INFO BlockManagerInfo: Added broadcast_117_piece0 in memory on 172.18.0.15:34959 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 11:48:07 INFO BlockManagerInfo: Added broadcast_117_piece0 in memory on 172.18.0.14:40939 (size: 19.5 KiB, free: 434.4 MiB)
26/02/13 11:48:07 INFO BlockManagerInfo: Added broadcast_116_piece0 in memory on 172.18.0.15:34959 (size: 107.2 KiB, free: 434.0 MiB)
26/02/13 11:48:07 INFO BlockManagerInfo: Added broadcast_116_piece0 in memory on 172.18.0.14:40939 (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 11:48:07 INFO TaskSetManager: Finished task 1.0 in stage 120.0 (TID 252) in 81 ms on 172.18.0.15 (executor 0) (1/3)
26/02/13 11:48:07 INFO TaskSetManager: Finished task 0.0 in stage 120.0 (TID 251) in 84 ms on 172.18.0.14 (executor 1) (2/3)
26/02/13 11:48:07 INFO TaskSetManager: Finished task 2.0 in stage 120.0 (TID 253) in 88 ms on 172.18.0.14 (executor 1) (3/3)
26/02/13 11:48:07 INFO TaskSchedulerImpl: Removed TaskSet 120.0, whose tasks have all completed, from pool 
26/02/13 11:48:07 INFO DAGScheduler: ResultStage 120 (start at NativeMethodAccessorImpl.java:0) finished in 0.096 s
26/02/13 11:48:07 INFO DAGScheduler: Job 78 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:48:07 INFO TaskSchedulerImpl: Killing all running tasks in stage 120: Stage finished
26/02/13 11:48:07 INFO DAGScheduler: Job 78 finished: start at NativeMethodAccessorImpl.java:0, took 0.097619 s
26/02/13 11:48:07 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 34, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@cdca956] is committing.
26/02/13 11:48:07 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 34, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@cdca956] committed.
26/02/13 11:48:07 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/34 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.34.e1976fca-eae9-40ee-9729-f82a2a90f44b.tmp
26/02/13 11:48:07 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.34.e1976fca-eae9-40ee-9729-f82a2a90f44b.tmp to file:/tmp/spark-checkpoint-enrichment/commits/34
26/02/13 11:48:07 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "fc5c6452-bd70-4c90-becf-772d3bebdb57",
  "name" : null,
  "timestamp" : "2026-02-13T11:48:07.231Z",
  "batchId" : 34,
  "numInputRows" : 155,
  "inputRowsPerSecond" : 159.95872033023736,
  "processedRowsPerSecond" : 377.12895377128956,
  "durationMs" : {
    "addBatch" : 260,
    "commitOffsets" : 55,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 32,
    "triggerExecution" : 411,
    "walCommit" : 62
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 4046,
        "1" : 4465,
        "0" : 6239
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 4096,
        "1" : 4516,
        "0" : 6293
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 4096,
        "1" : 4516,
        "0" : 6293
      }
    },
    "numInputRows" : 155,
    "inputRowsPerSecond" : 159.95872033023736,
    "processedRowsPerSecond" : 377.12895377128956,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@7fe9b5de",
    "numOutputRows" : 33
  }
}
26/02/13 11:48:17 INFO BlockManagerInfo: Removed broadcast_117_piece0 on spark-master:40291 in memory (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 11:48:17 INFO BlockManagerInfo: Removed broadcast_117_piece0 on 172.18.0.15:34959 in memory (size: 19.5 KiB, free: 434.0 MiB)
26/02/13 11:48:17 INFO BlockManagerInfo: Removed broadcast_117_piece0 on 172.18.0.14:40939 in memory (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 11:48:17 INFO BlockManagerInfo: Removed broadcast_115_piece0 on spark-master:40291 in memory (size: 33.3 KiB, free: 434.3 MiB)
26/02/13 11:48:17 INFO BlockManagerInfo: Removed broadcast_115_piece0 on 172.18.0.15:34959 in memory (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 11:48:17 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 11:48:22 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/35 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.35.85f0509f-284d-4dd4-a391-fa22fb28f20c.tmp
26/02/13 11:48:22 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.35.85f0509f-284d-4dd4-a391-fa22fb28f20c.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/35
26/02/13 11:48:22 INFO MicroBatchExecution: Committed offsets for batch 35. Metadata OffsetSeqMetadata(0,1770983302451,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 11:48:22 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:48:22 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:48:22 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:48:22 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:48:22 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:48:22 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:48:22 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#31841 - origin_code.nullCount#31840) > 0)
26/02/13 11:48:22 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#31846 - destination_code.nullCount#31845) > 0)
26/02/13 11:48:22 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#31876 - callsign.nullCount#31875) > 0)
26/02/13 11:48:22 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:48:22 INFO DAGScheduler: Got job 79 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 11:48:22 INFO DAGScheduler: Final stage: ResultStage 122 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:48:22 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 121)
26/02/13 11:48:22 INFO DAGScheduler: Missing parents: List()
26/02/13 11:48:22 INFO DAGScheduler: Submitting ResultStage 122 (MapPartitionsRDD[428] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:48:22 INFO MemoryStore: Block broadcast_118 stored as values in memory (estimated size 99.3 KiB, free 434.0 MiB)
26/02/13 11:48:22 INFO BlockManagerInfo: Removed broadcast_116_piece0 on spark-master:40291 in memory (size: 107.2 KiB, free: 434.4 MiB)
26/02/13 11:48:22 INFO MemoryStore: Block broadcast_118_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 434.0 MiB)
26/02/13 11:48:22 INFO BlockManagerInfo: Added broadcast_118_piece0 in memory on spark-master:40291 (size: 33.3 KiB, free: 434.3 MiB)
26/02/13 11:48:22 INFO SparkContext: Created broadcast 118 from broadcast at DAGScheduler.scala:1585
26/02/13 11:48:22 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 122 (MapPartitionsRDD[428] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 11:48:22 INFO TaskSchedulerImpl: Adding task set 122.0 with 4 tasks resource profile 0
26/02/13 11:48:22 INFO BlockManagerInfo: Removed broadcast_116_piece0 on 172.18.0.14:40939 in memory (size: 107.2 KiB, free: 434.4 MiB)
26/02/13 11:48:22 INFO BlockManagerInfo: Removed broadcast_116_piece0 on 172.18.0.15:34959 in memory (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 11:48:22 INFO TaskSetManager: Starting task 0.0 in stage 122.0 (TID 254) (172.18.0.15, executor 0, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:48:22 INFO TaskSetManager: Starting task 1.0 in stage 122.0 (TID 255) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:48:22 INFO BlockManagerInfo: Added broadcast_118_piece0 in memory on 172.18.0.15:34959 (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 11:48:22 INFO TaskSetManager: Starting task 2.0 in stage 122.0 (TID 256) (172.18.0.15, executor 0, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:48:22 INFO TaskSetManager: Finished task 1.0 in stage 122.0 (TID 255) in 26 ms on 172.18.0.15 (executor 0) (1/4)
26/02/13 11:48:22 INFO TaskSetManager: Starting task 3.0 in stage 122.0 (TID 257) (172.18.0.15, executor 0, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:48:22 INFO TaskSetManager: Finished task 0.0 in stage 122.0 (TID 254) in 45 ms on 172.18.0.15 (executor 0) (2/4)
26/02/13 11:48:22 INFO TaskSetManager: Finished task 2.0 in stage 122.0 (TID 256) in 40 ms on 172.18.0.15 (executor 0) (3/4)
26/02/13 11:48:22 INFO TaskSetManager: Finished task 3.0 in stage 122.0 (TID 257) in 44 ms on 172.18.0.15 (executor 0) (4/4)
26/02/13 11:48:22 INFO TaskSchedulerImpl: Removed TaskSet 122.0, whose tasks have all completed, from pool 
26/02/13 11:48:22 INFO DAGScheduler: ResultStage 122 (start at NativeMethodAccessorImpl.java:0) finished in 0.104 s
26/02/13 11:48:22 INFO DAGScheduler: Job 79 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:48:22 INFO TaskSchedulerImpl: Killing all running tasks in stage 122: Stage finished
26/02/13 11:48:22 INFO DAGScheduler: Job 79 finished: start at NativeMethodAccessorImpl.java:0, took 0.109617 s
26/02/13 11:48:22 INFO MemoryStore: Block broadcast_119_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.9 MiB)
26/02/13 11:48:22 INFO BlockManagerInfo: Added broadcast_119_piece0 in memory on spark-master:40291 (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 11:48:22 INFO SparkContext: Created broadcast 119 from start at NativeMethodAccessorImpl.java:0
26/02/13 11:48:22 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 35, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@69bb636c]. The input RDD has 3 partitions.
26/02/13 11:48:22 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:48:22 INFO DAGScheduler: Got job 80 (start at NativeMethodAccessorImpl.java:0) with 3 output partitions
26/02/13 11:48:22 INFO DAGScheduler: Final stage: ResultStage 123 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:48:22 INFO DAGScheduler: Parents of final stage: List()
26/02/13 11:48:22 INFO DAGScheduler: Missing parents: List()
26/02/13 11:48:22 INFO DAGScheduler: Submitting ResultStage 123 (MapPartitionsRDD[434] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:48:22 INFO MemoryStore: Block broadcast_120 stored as values in memory (estimated size 49.9 KiB, free 433.9 MiB)
26/02/13 11:48:22 INFO MemoryStore: Block broadcast_120_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.9 MiB)
26/02/13 11:48:22 INFO BlockManagerInfo: Added broadcast_120_piece0 in memory on spark-master:40291 (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 11:48:22 INFO SparkContext: Created broadcast 120 from broadcast at DAGScheduler.scala:1585
26/02/13 11:48:22 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 123 (MapPartitionsRDD[434] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
26/02/13 11:48:22 INFO TaskSchedulerImpl: Adding task set 123.0 with 3 tasks resource profile 0
26/02/13 11:48:22 INFO TaskSetManager: Starting task 1.0 in stage 123.0 (TID 258) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:48:22 INFO TaskSetManager: Starting task 0.0 in stage 123.0 (TID 259) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:48:22 INFO TaskSetManager: Starting task 2.0 in stage 123.0 (TID 260) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:48:22 INFO BlockManagerInfo: Added broadcast_120_piece0 in memory on 172.18.0.15:34959 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 11:48:22 INFO BlockManagerInfo: Added broadcast_120_piece0 in memory on 172.18.0.14:40939 (size: 19.5 KiB, free: 434.4 MiB)
26/02/13 11:48:22 INFO BlockManagerInfo: Added broadcast_119_piece0 in memory on 172.18.0.15:34959 (size: 107.2 KiB, free: 434.0 MiB)
26/02/13 11:48:22 INFO BlockManagerInfo: Added broadcast_119_piece0 in memory on 172.18.0.14:40939 (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 11:48:23 INFO TaskSetManager: Finished task 1.0 in stage 123.0 (TID 258) in 607 ms on 172.18.0.15 (executor 0) (1/3)
26/02/13 11:48:23 INFO TaskSetManager: Finished task 0.0 in stage 123.0 (TID 259) in 633 ms on 172.18.0.14 (executor 1) (2/3)
26/02/13 11:48:23 INFO TaskSetManager: Finished task 2.0 in stage 123.0 (TID 260) in 643 ms on 172.18.0.14 (executor 1) (3/3)
26/02/13 11:48:23 INFO TaskSchedulerImpl: Removed TaskSet 123.0, whose tasks have all completed, from pool 
26/02/13 11:48:23 INFO DAGScheduler: ResultStage 123 (start at NativeMethodAccessorImpl.java:0) finished in 0.656 s
26/02/13 11:48:23 INFO DAGScheduler: Job 80 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:48:23 INFO TaskSchedulerImpl: Killing all running tasks in stage 123: Stage finished
26/02/13 11:48:23 INFO DAGScheduler: Job 80 finished: start at NativeMethodAccessorImpl.java:0, took 0.658913 s
26/02/13 11:48:23 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 35, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@69bb636c] is committing.
26/02/13 11:48:23 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 35, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@69bb636c] committed.
26/02/13 11:48:23 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/35 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.35.5bcf8b0a-6f3a-4e2b-b6d7-e7cb6aba4855.tmp
26/02/13 11:48:23 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.35.5bcf8b0a-6f3a-4e2b-b6d7-e7cb6aba4855.tmp to file:/tmp/spark-checkpoint-enrichment/commits/35
26/02/13 11:48:23 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "fc5c6452-bd70-4c90-becf-772d3bebdb57",
  "name" : null,
  "timestamp" : "2026-02-13T11:48:22.450Z",
  "batchId" : 35,
  "numInputRows" : 109,
  "inputRowsPerSecond" : 9083.333333333334,
  "processedRowsPerSecond" : 102.44360902255639,
  "durationMs" : {
    "addBatch" : 891,
    "commitOffsets" : 65,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 40,
    "triggerExecution" : 1064,
    "walCommit" : 66
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 4096,
        "1" : 4516,
        "0" : 6293
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 4131,
        "1" : 4553,
        "0" : 6330
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 4131,
        "1" : 4553,
        "0" : 6330
      }
    },
    "numInputRows" : 109,
    "inputRowsPerSecond" : 9083.333333333334,
    "processedRowsPerSecond" : 102.44360902255639,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@7fe9b5de",
    "numOutputRows" : 26
  }
}
26/02/13 11:48:23 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/36 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.36.b336976b-43cd-4a0a-927f-56f6329b2659.tmp
26/02/13 11:48:23 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.36.b336976b-43cd-4a0a-927f-56f6329b2659.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/36
26/02/13 11:48:23 INFO MicroBatchExecution: Committed offsets for batch 36. Metadata OffsetSeqMetadata(0,1770983303516,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 11:48:23 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:48:23 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:48:23 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:48:23 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:48:23 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:48:23 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:48:23 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#32695 - origin_code.nullCount#32694) > 0)
26/02/13 11:48:23 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#32700 - destination_code.nullCount#32699) > 0)
26/02/13 11:48:23 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#32730 - callsign.nullCount#32729) > 0)
26/02/13 11:48:23 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:48:23 INFO DAGScheduler: Got job 81 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 11:48:23 INFO DAGScheduler: Final stage: ResultStage 125 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:48:23 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 124)
26/02/13 11:48:23 INFO DAGScheduler: Missing parents: List()
26/02/13 11:48:23 INFO DAGScheduler: Submitting ResultStage 125 (MapPartitionsRDD[439] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:48:23 INFO MemoryStore: Block broadcast_121 stored as values in memory (estimated size 99.3 KiB, free 433.8 MiB)
26/02/13 11:48:23 INFO MemoryStore: Block broadcast_121_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 433.7 MiB)
26/02/13 11:48:23 INFO BlockManagerInfo: Added broadcast_121_piece0 in memory on spark-master:40291 (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 11:48:23 INFO SparkContext: Created broadcast 121 from broadcast at DAGScheduler.scala:1585
26/02/13 11:48:23 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 125 (MapPartitionsRDD[439] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 11:48:23 INFO TaskSchedulerImpl: Adding task set 125.0 with 4 tasks resource profile 0
26/02/13 11:48:23 INFO TaskSetManager: Starting task 0.0 in stage 125.0 (TID 261) (172.18.0.15, executor 0, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:48:23 INFO TaskSetManager: Starting task 1.0 in stage 125.0 (TID 262) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:48:23 INFO BlockManagerInfo: Added broadcast_121_piece0 in memory on 172.18.0.15:34959 (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 11:48:23 INFO TaskSetManager: Starting task 2.0 in stage 125.0 (TID 263) (172.18.0.15, executor 0, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:48:23 INFO TaskSetManager: Finished task 0.0 in stage 125.0 (TID 261) in 32 ms on 172.18.0.15 (executor 0) (1/4)
26/02/13 11:48:23 INFO TaskSetManager: Starting task 3.0 in stage 125.0 (TID 264) (172.18.0.15, executor 0, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:48:23 INFO TaskSetManager: Finished task 1.0 in stage 125.0 (TID 262) in 40 ms on 172.18.0.15 (executor 0) (2/4)
26/02/13 11:48:23 INFO TaskSetManager: Finished task 2.0 in stage 125.0 (TID 263) in 19 ms on 172.18.0.15 (executor 0) (3/4)
26/02/13 11:48:23 INFO TaskSetManager: Finished task 3.0 in stage 125.0 (TID 264) in 22 ms on 172.18.0.15 (executor 0) (4/4)
26/02/13 11:48:23 INFO TaskSchedulerImpl: Removed TaskSet 125.0, whose tasks have all completed, from pool 
26/02/13 11:48:23 INFO DAGScheduler: ResultStage 125 (start at NativeMethodAccessorImpl.java:0) finished in 0.071 s
26/02/13 11:48:23 INFO DAGScheduler: Job 81 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:48:23 INFO TaskSchedulerImpl: Killing all running tasks in stage 125: Stage finished
26/02/13 11:48:23 INFO DAGScheduler: Job 81 finished: start at NativeMethodAccessorImpl.java:0, took 0.076453 s
26/02/13 11:48:23 INFO MemoryStore: Block broadcast_122_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.6 MiB)
26/02/13 11:48:23 INFO BlockManagerInfo: Added broadcast_122_piece0 in memory on spark-master:40291 (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 11:48:23 INFO SparkContext: Created broadcast 122 from start at NativeMethodAccessorImpl.java:0
26/02/13 11:48:23 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 36, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@668780c]. The input RDD has 3 partitions.
26/02/13 11:48:23 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:48:23 INFO DAGScheduler: Got job 82 (start at NativeMethodAccessorImpl.java:0) with 3 output partitions
26/02/13 11:48:23 INFO DAGScheduler: Final stage: ResultStage 126 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:48:23 INFO DAGScheduler: Parents of final stage: List()
26/02/13 11:48:23 INFO DAGScheduler: Missing parents: List()
26/02/13 11:48:23 INFO DAGScheduler: Submitting ResultStage 126 (MapPartitionsRDD[445] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:48:23 INFO MemoryStore: Block broadcast_123 stored as values in memory (estimated size 49.9 KiB, free 433.6 MiB)
26/02/13 11:48:23 INFO MemoryStore: Block broadcast_123_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.6 MiB)
26/02/13 11:48:23 INFO BlockManagerInfo: Added broadcast_123_piece0 in memory on spark-master:40291 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 11:48:23 INFO SparkContext: Created broadcast 123 from broadcast at DAGScheduler.scala:1585
26/02/13 11:48:23 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 126 (MapPartitionsRDD[445] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
26/02/13 11:48:23 INFO TaskSchedulerImpl: Adding task set 126.0 with 3 tasks resource profile 0
26/02/13 11:48:23 INFO BlockManagerInfo: Removed broadcast_119_piece0 on spark-master:40291 in memory (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 11:48:23 INFO TaskSetManager: Starting task 0.0 in stage 126.0 (TID 265) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:48:23 INFO TaskSetManager: Starting task 1.0 in stage 126.0 (TID 266) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:48:23 INFO TaskSetManager: Starting task 2.0 in stage 126.0 (TID 267) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:48:23 INFO BlockManagerInfo: Removed broadcast_119_piece0 on 172.18.0.14:40939 in memory (size: 107.2 KiB, free: 434.4 MiB)
26/02/13 11:48:23 INFO BlockManagerInfo: Removed broadcast_119_piece0 on 172.18.0.15:34959 in memory (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 11:48:23 INFO BlockManagerInfo: Removed broadcast_118_piece0 on spark-master:40291 in memory (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 11:48:23 INFO BlockManagerInfo: Added broadcast_123_piece0 in memory on 172.18.0.14:40939 (size: 19.5 KiB, free: 434.4 MiB)
26/02/13 11:48:23 INFO BlockManagerInfo: Removed broadcast_118_piece0 on 172.18.0.15:34959 in memory (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 11:48:23 INFO BlockManagerInfo: Added broadcast_123_piece0 in memory on 172.18.0.15:34959 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 11:48:23 INFO BlockManagerInfo: Removed broadcast_120_piece0 on spark-master:40291 in memory (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 11:48:23 INFO BlockManagerInfo: Removed broadcast_120_piece0 on 172.18.0.14:40939 in memory (size: 19.5 KiB, free: 434.4 MiB)
26/02/13 11:48:23 INFO BlockManagerInfo: Removed broadcast_120_piece0 on 172.18.0.15:34959 in memory (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 11:48:23 INFO BlockManagerInfo: Removed broadcast_121_piece0 on spark-master:40291 in memory (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 11:48:23 INFO BlockManagerInfo: Added broadcast_122_piece0 in memory on 172.18.0.15:34959 (size: 107.2 KiB, free: 434.0 MiB)
26/02/13 11:48:23 INFO BlockManagerInfo: Removed broadcast_121_piece0 on 172.18.0.15:34959 in memory (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 11:48:23 INFO BlockManagerInfo: Added broadcast_122_piece0 in memory on 172.18.0.14:40939 (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 11:48:24 INFO TaskSetManager: Finished task 0.0 in stage 126.0 (TID 265) in 194 ms on 172.18.0.14 (executor 1) (1/3)
26/02/13 11:48:24 INFO TaskSetManager: Finished task 2.0 in stage 126.0 (TID 267) in 208 ms on 172.18.0.14 (executor 1) (2/3)
26/02/13 11:48:24 INFO TaskSetManager: Finished task 1.0 in stage 126.0 (TID 266) in 252 ms on 172.18.0.15 (executor 0) (3/3)
26/02/13 11:48:24 INFO TaskSchedulerImpl: Removed TaskSet 126.0, whose tasks have all completed, from pool 
26/02/13 11:48:24 INFO DAGScheduler: ResultStage 126 (start at NativeMethodAccessorImpl.java:0) finished in 0.276 s
26/02/13 11:48:24 INFO DAGScheduler: Job 82 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:48:24 INFO TaskSchedulerImpl: Killing all running tasks in stage 126: Stage finished
26/02/13 11:48:24 INFO DAGScheduler: Job 82 finished: start at NativeMethodAccessorImpl.java:0, took 0.279370 s
26/02/13 11:48:24 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 36, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@668780c] is committing.
26/02/13 11:48:24 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 36, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@668780c] committed.
26/02/13 11:48:24 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/36 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.36.cf916e8b-c8b6-4498-8f43-b09dccc04749.tmp
26/02/13 11:48:24 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.36.cf916e8b-c8b6-4498-8f43-b09dccc04749.tmp to file:/tmp/spark-checkpoint-enrichment/commits/36
26/02/13 11:48:24 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "fc5c6452-bd70-4c90-becf-772d3bebdb57",
  "name" : null,
  "timestamp" : "2026-02-13T11:48:23.515Z",
  "batchId" : 36,
  "numInputRows" : 129,
  "inputRowsPerSecond" : 121.12676056338029,
  "processedRowsPerSecond" : 186.68596237337195,
  "durationMs" : {
    "addBatch" : 485,
    "commitOffsets" : 95,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 47,
    "triggerExecution" : 691,
    "walCommit" : 62
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 4131,
        "1" : 4553,
        "0" : 6330
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 4174,
        "1" : 4599,
        "0" : 6370
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 4174,
        "1" : 4599,
        "0" : 6370
      }
    },
    "numInputRows" : 129,
    "inputRowsPerSecond" : 121.12676056338029,
    "processedRowsPerSecond" : 186.68596237337195,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@7fe9b5de",
    "numOutputRows" : 29
  }
}
26/02/13 11:48:24 INFO BlockManagerInfo: Removed broadcast_123_piece0 on spark-master:40291 in memory (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 11:48:24 INFO BlockManagerInfo: Removed broadcast_123_piece0 on 172.18.0.14:40939 in memory (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 11:48:24 INFO BlockManagerInfo: Removed broadcast_123_piece0 on 172.18.0.15:34959 in memory (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 11:48:28 INFO NetworkClient: [AdminClient clientId=adminclient-1] Node -1 disconnected.
26/02/13 11:48:34 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 11:48:38 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/37 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.37.e2650bcd-7651-4cf8-b6a2-ed4371604a53.tmp
26/02/13 11:48:38 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.37.e2650bcd-7651-4cf8-b6a2-ed4371604a53.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/37
26/02/13 11:48:38 INFO MicroBatchExecution: Committed offsets for batch 37. Metadata OffsetSeqMetadata(0,1770983318634,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 11:48:38 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:48:38 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:48:38 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:48:38 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:48:38 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:48:38 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:48:38 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#33549 - origin_code.nullCount#33548) > 0)
26/02/13 11:48:38 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#33554 - destination_code.nullCount#33553) > 0)
26/02/13 11:48:38 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#33584 - callsign.nullCount#33583) > 0)
26/02/13 11:48:38 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:48:38 INFO DAGScheduler: Got job 83 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 11:48:38 INFO DAGScheduler: Final stage: ResultStage 128 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:48:38 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 127)
26/02/13 11:48:38 INFO DAGScheduler: Missing parents: List()
26/02/13 11:48:38 INFO DAGScheduler: Submitting ResultStage 128 (MapPartitionsRDD[450] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:48:38 INFO MemoryStore: Block broadcast_124 stored as values in memory (estimated size 99.3 KiB, free 434.0 MiB)
26/02/13 11:48:38 INFO MemoryStore: Block broadcast_124_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 433.9 MiB)
26/02/13 11:48:38 INFO BlockManagerInfo: Added broadcast_124_piece0 in memory on spark-master:40291 (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 11:48:38 INFO SparkContext: Created broadcast 124 from broadcast at DAGScheduler.scala:1585
26/02/13 11:48:38 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 128 (MapPartitionsRDD[450] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 11:48:38 INFO TaskSchedulerImpl: Adding task set 128.0 with 4 tasks resource profile 0
26/02/13 11:48:38 INFO TaskSetManager: Starting task 0.0 in stage 128.0 (TID 268) (172.18.0.15, executor 0, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:48:38 INFO TaskSetManager: Starting task 1.0 in stage 128.0 (TID 269) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:48:38 INFO BlockManagerInfo: Added broadcast_124_piece0 in memory on 172.18.0.15:34959 (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 11:48:38 INFO TaskSetManager: Starting task 2.0 in stage 128.0 (TID 270) (172.18.0.15, executor 0, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:48:38 INFO TaskSetManager: Finished task 0.0 in stage 128.0 (TID 268) in 22 ms on 172.18.0.15 (executor 0) (1/4)
26/02/13 11:48:38 INFO TaskSetManager: Starting task 3.0 in stage 128.0 (TID 271) (172.18.0.15, executor 0, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:48:38 INFO TaskSetManager: Finished task 1.0 in stage 128.0 (TID 269) in 24 ms on 172.18.0.15 (executor 0) (2/4)
26/02/13 11:48:38 INFO TaskSetManager: Finished task 2.0 in stage 128.0 (TID 270) in 15 ms on 172.18.0.15 (executor 0) (3/4)
26/02/13 11:48:38 INFO TaskSetManager: Finished task 3.0 in stage 128.0 (TID 271) in 13 ms on 172.18.0.15 (executor 0) (4/4)
26/02/13 11:48:38 INFO TaskSchedulerImpl: Removed TaskSet 128.0, whose tasks have all completed, from pool 
26/02/13 11:48:38 INFO DAGScheduler: ResultStage 128 (start at NativeMethodAccessorImpl.java:0) finished in 0.043 s
26/02/13 11:48:38 INFO DAGScheduler: Job 83 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:48:38 INFO TaskSchedulerImpl: Killing all running tasks in stage 128: Stage finished
26/02/13 11:48:38 INFO DAGScheduler: Job 83 finished: start at NativeMethodAccessorImpl.java:0, took 0.046013 s
26/02/13 11:48:38 INFO MemoryStore: Block broadcast_125_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.8 MiB)
26/02/13 11:48:38 INFO BlockManagerInfo: Added broadcast_125_piece0 in memory on spark-master:40291 (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 11:48:38 INFO SparkContext: Created broadcast 125 from start at NativeMethodAccessorImpl.java:0
26/02/13 11:48:38 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 37, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@7d2ddbb9]. The input RDD has 3 partitions.
26/02/13 11:48:38 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:48:38 INFO DAGScheduler: Got job 84 (start at NativeMethodAccessorImpl.java:0) with 3 output partitions
26/02/13 11:48:38 INFO DAGScheduler: Final stage: ResultStage 129 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:48:38 INFO DAGScheduler: Parents of final stage: List()
26/02/13 11:48:38 INFO DAGScheduler: Missing parents: List()
26/02/13 11:48:38 INFO DAGScheduler: Submitting ResultStage 129 (MapPartitionsRDD[456] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:48:38 INFO MemoryStore: Block broadcast_126 stored as values in memory (estimated size 49.9 KiB, free 433.8 MiB)
26/02/13 11:48:38 INFO MemoryStore: Block broadcast_126_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.8 MiB)
26/02/13 11:48:38 INFO BlockManagerInfo: Added broadcast_126_piece0 in memory on spark-master:40291 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 11:48:38 INFO SparkContext: Created broadcast 126 from broadcast at DAGScheduler.scala:1585
26/02/13 11:48:38 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 129 (MapPartitionsRDD[456] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
26/02/13 11:48:38 INFO TaskSchedulerImpl: Adding task set 129.0 with 3 tasks resource profile 0
26/02/13 11:48:38 INFO TaskSetManager: Starting task 1.0 in stage 129.0 (TID 272) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:48:38 INFO TaskSetManager: Starting task 0.0 in stage 129.0 (TID 273) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:48:38 INFO TaskSetManager: Starting task 2.0 in stage 129.0 (TID 274) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:48:38 INFO BlockManagerInfo: Added broadcast_126_piece0 in memory on 172.18.0.15:34959 (size: 19.5 KiB, free: 434.0 MiB)
26/02/13 11:48:38 INFO BlockManagerInfo: Added broadcast_126_piece0 in memory on 172.18.0.14:40939 (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 11:48:38 INFO BlockManagerInfo: Added broadcast_125_piece0 in memory on 172.18.0.15:34959 (size: 107.2 KiB, free: 433.9 MiB)
26/02/13 11:48:38 INFO BlockManagerInfo: Added broadcast_125_piece0 in memory on 172.18.0.14:40939 (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 11:48:39 INFO TaskSetManager: Finished task 0.0 in stage 129.0 (TID 273) in 597 ms on 172.18.0.14 (executor 1) (1/3)
26/02/13 11:48:39 INFO TaskSetManager: Finished task 2.0 in stage 129.0 (TID 274) in 604 ms on 172.18.0.14 (executor 1) (2/3)
26/02/13 11:48:39 INFO TaskSetManager: Finished task 1.0 in stage 129.0 (TID 272) in 613 ms on 172.18.0.15 (executor 0) (3/3)
26/02/13 11:48:39 INFO TaskSchedulerImpl: Removed TaskSet 129.0, whose tasks have all completed, from pool 
26/02/13 11:48:39 INFO DAGScheduler: ResultStage 129 (start at NativeMethodAccessorImpl.java:0) finished in 0.618 s
26/02/13 11:48:39 INFO DAGScheduler: Job 84 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:48:39 INFO TaskSchedulerImpl: Killing all running tasks in stage 129: Stage finished
26/02/13 11:48:39 INFO DAGScheduler: Job 84 finished: start at NativeMethodAccessorImpl.java:0, took 0.619820 s
26/02/13 11:48:39 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 37, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@7d2ddbb9] is committing.
26/02/13 11:48:39 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 37, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@7d2ddbb9] committed.
26/02/13 11:48:39 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/37 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.37.c5ddc8f7-20b5-4ba4-91e9-ca85e37659cb.tmp
26/02/13 11:48:39 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.37.c5ddc8f7-20b5-4ba4-91e9-ca85e37659cb.tmp to file:/tmp/spark-checkpoint-enrichment/commits/37
26/02/13 11:48:39 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "fc5c6452-bd70-4c90-becf-772d3bebdb57",
  "name" : null,
  "timestamp" : "2026-02-13T11:48:38.632Z",
  "batchId" : 37,
  "numInputRows" : 111,
  "inputRowsPerSecond" : 9250.0,
  "processedRowsPerSecond" : 116.10878661087867,
  "durationMs" : {
    "addBatch" : 753,
    "commitOffsets" : 69,
    "getBatch" : 0,
    "latestOffset" : 2,
    "queryPlanning" : 43,
    "triggerExecution" : 956,
    "walCommit" : 87
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 4174,
        "1" : 4599,
        "0" : 6370
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 4211,
        "1" : 4636,
        "0" : 6407
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 4211,
        "1" : 4636,
        "0" : 6407
      }
    },
    "numInputRows" : 111,
    "inputRowsPerSecond" : 9250.0,
    "processedRowsPerSecond" : 116.10878661087867,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@7fe9b5de",
    "numOutputRows" : 27
  }
}
26/02/13 11:48:39 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/38 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.38.25194d75-ea27-4105-aef3-5900da6eb42b.tmp
26/02/13 11:48:39 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.38.25194d75-ea27-4105-aef3-5900da6eb42b.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/38
26/02/13 11:48:39 INFO MicroBatchExecution: Committed offsets for batch 38. Metadata OffsetSeqMetadata(0,1770983319589,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 11:48:39 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:48:39 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:48:39 INFO BlockManagerInfo: Removed broadcast_125_piece0 on spark-master:40291 in memory (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 11:48:39 INFO BlockManagerInfo: Removed broadcast_125_piece0 on 172.18.0.15:34959 in memory (size: 107.2 KiB, free: 434.0 MiB)
26/02/13 11:48:39 INFO BlockManagerInfo: Removed broadcast_125_piece0 on 172.18.0.14:40939 in memory (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 11:48:39 INFO BlockManagerInfo: Removed broadcast_124_piece0 on spark-master:40291 in memory (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 11:48:39 INFO BlockManagerInfo: Removed broadcast_124_piece0 on 172.18.0.15:34959 in memory (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 11:48:39 INFO BlockManagerInfo: Removed broadcast_126_piece0 on spark-master:40291 in memory (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 11:48:39 INFO BlockManagerInfo: Removed broadcast_126_piece0 on 172.18.0.15:34959 in memory (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 11:48:39 INFO BlockManagerInfo: Removed broadcast_126_piece0 on 172.18.0.14:40939 in memory (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 11:48:39 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:48:39 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:48:39 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:48:39 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:48:39 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#34403 - origin_code.nullCount#34402) > 0)
26/02/13 11:48:39 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#34408 - destination_code.nullCount#34407) > 0)
26/02/13 11:48:39 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#34438 - callsign.nullCount#34437) > 0)
26/02/13 11:48:39 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:48:39 INFO DAGScheduler: Got job 85 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 11:48:39 INFO DAGScheduler: Final stage: ResultStage 131 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:48:39 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 130)
26/02/13 11:48:39 INFO DAGScheduler: Missing parents: List()
26/02/13 11:48:39 INFO DAGScheduler: Submitting ResultStage 131 (MapPartitionsRDD[461] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:48:39 INFO MemoryStore: Block broadcast_127 stored as values in memory (estimated size 99.3 KiB, free 434.0 MiB)
26/02/13 11:48:39 INFO MemoryStore: Block broadcast_127_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 433.9 MiB)
26/02/13 11:48:39 INFO BlockManagerInfo: Added broadcast_127_piece0 in memory on spark-master:40291 (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 11:48:39 INFO SparkContext: Created broadcast 127 from broadcast at DAGScheduler.scala:1585
26/02/13 11:48:39 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 131 (MapPartitionsRDD[461] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 11:48:39 INFO TaskSchedulerImpl: Adding task set 131.0 with 4 tasks resource profile 0
26/02/13 11:48:39 INFO TaskSetManager: Starting task 0.0 in stage 131.0 (TID 275) (172.18.0.15, executor 0, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:48:39 INFO TaskSetManager: Starting task 1.0 in stage 131.0 (TID 276) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:48:39 INFO BlockManagerInfo: Added broadcast_127_piece0 in memory on 172.18.0.15:34959 (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 11:48:39 INFO TaskSetManager: Starting task 2.0 in stage 131.0 (TID 277) (172.18.0.15, executor 0, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:48:39 INFO TaskSetManager: Starting task 3.0 in stage 131.0 (TID 278) (172.18.0.15, executor 0, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:48:39 INFO TaskSetManager: Finished task 0.0 in stage 131.0 (TID 275) in 26 ms on 172.18.0.15 (executor 0) (1/4)
26/02/13 11:48:39 INFO TaskSetManager: Finished task 1.0 in stage 131.0 (TID 276) in 27 ms on 172.18.0.15 (executor 0) (2/4)
26/02/13 11:48:39 INFO TaskSetManager: Finished task 2.0 in stage 131.0 (TID 277) in 18 ms on 172.18.0.15 (executor 0) (3/4)
26/02/13 11:48:39 INFO TaskSetManager: Finished task 3.0 in stage 131.0 (TID 278) in 35 ms on 172.18.0.15 (executor 0) (4/4)
26/02/13 11:48:39 INFO TaskSchedulerImpl: Removed TaskSet 131.0, whose tasks have all completed, from pool 
26/02/13 11:48:39 INFO DAGScheduler: ResultStage 131 (start at NativeMethodAccessorImpl.java:0) finished in 0.073 s
26/02/13 11:48:39 INFO BlockManagerInfo: Removed broadcast_122_piece0 on 172.18.0.14:40939 in memory (size: 107.2 KiB, free: 434.4 MiB)
26/02/13 11:48:39 INFO DAGScheduler: Job 85 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:48:39 INFO TaskSchedulerImpl: Killing all running tasks in stage 131: Stage finished
26/02/13 11:48:39 INFO DAGScheduler: Job 85 finished: start at NativeMethodAccessorImpl.java:0, took 0.079539 s
26/02/13 11:48:39 INFO BlockManagerInfo: Removed broadcast_122_piece0 on spark-master:40291 in memory (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 11:48:39 INFO BlockManagerInfo: Removed broadcast_122_piece0 on 172.18.0.15:34959 in memory (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 11:48:39 INFO MemoryStore: Block broadcast_128_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.9 MiB)
26/02/13 11:48:39 INFO BlockManagerInfo: Added broadcast_128_piece0 in memory on spark-master:40291 (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 11:48:39 INFO SparkContext: Created broadcast 128 from start at NativeMethodAccessorImpl.java:0
26/02/13 11:48:39 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 38, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@19a1bb03]. The input RDD has 3 partitions.
26/02/13 11:48:39 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:48:39 INFO DAGScheduler: Got job 86 (start at NativeMethodAccessorImpl.java:0) with 3 output partitions
26/02/13 11:48:39 INFO DAGScheduler: Final stage: ResultStage 132 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:48:39 INFO DAGScheduler: Parents of final stage: List()
26/02/13 11:48:39 INFO DAGScheduler: Missing parents: List()
26/02/13 11:48:39 INFO DAGScheduler: Submitting ResultStage 132 (MapPartitionsRDD[467] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:48:39 INFO MemoryStore: Block broadcast_129 stored as values in memory (estimated size 49.9 KiB, free 433.9 MiB)
26/02/13 11:48:39 INFO MemoryStore: Block broadcast_129_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.9 MiB)
26/02/13 11:48:39 INFO BlockManagerInfo: Added broadcast_129_piece0 in memory on spark-master:40291 (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 11:48:39 INFO SparkContext: Created broadcast 129 from broadcast at DAGScheduler.scala:1585
26/02/13 11:48:39 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 132 (MapPartitionsRDD[467] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
26/02/13 11:48:39 INFO TaskSchedulerImpl: Adding task set 132.0 with 3 tasks resource profile 0
26/02/13 11:48:39 INFO TaskSetManager: Starting task 1.0 in stage 132.0 (TID 279) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:48:39 INFO TaskSetManager: Starting task 0.0 in stage 132.0 (TID 280) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:48:39 INFO TaskSetManager: Starting task 2.0 in stage 132.0 (TID 281) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:48:39 INFO BlockManagerInfo: Added broadcast_129_piece0 in memory on 172.18.0.15:34959 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 11:48:39 INFO BlockManagerInfo: Added broadcast_129_piece0 in memory on 172.18.0.14:40939 (size: 19.5 KiB, free: 434.4 MiB)
26/02/13 11:48:39 INFO BlockManagerInfo: Added broadcast_128_piece0 in memory on 172.18.0.15:34959 (size: 107.2 KiB, free: 434.0 MiB)
26/02/13 11:48:40 INFO TaskSetManager: Finished task 1.0 in stage 132.0 (TID 279) in 86 ms on 172.18.0.15 (executor 0) (1/3)
26/02/13 11:48:40 INFO BlockManagerInfo: Added broadcast_128_piece0 in memory on 172.18.0.14:40939 (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 11:48:40 INFO TaskSetManager: Finished task 2.0 in stage 132.0 (TID 281) in 142 ms on 172.18.0.14 (executor 1) (2/3)
26/02/13 11:48:40 INFO TaskSetManager: Finished task 0.0 in stage 132.0 (TID 280) in 143 ms on 172.18.0.14 (executor 1) (3/3)
26/02/13 11:48:40 INFO TaskSchedulerImpl: Removed TaskSet 132.0, whose tasks have all completed, from pool 
26/02/13 11:48:40 INFO DAGScheduler: ResultStage 132 (start at NativeMethodAccessorImpl.java:0) finished in 0.155 s
26/02/13 11:48:40 INFO DAGScheduler: Job 86 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:48:40 INFO TaskSchedulerImpl: Killing all running tasks in stage 132: Stage finished
26/02/13 11:48:40 INFO DAGScheduler: Job 86 finished: start at NativeMethodAccessorImpl.java:0, took 0.159834 s
26/02/13 11:48:40 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 38, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@19a1bb03] is committing.
26/02/13 11:48:40 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 38, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@19a1bb03] committed.
26/02/13 11:48:40 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/38 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.38.366ef4d0-fb65-4fac-9d03-b689e3343f49.tmp
26/02/13 11:48:40 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.38.366ef4d0-fb65-4fac-9d03-b689e3343f49.tmp to file:/tmp/spark-checkpoint-enrichment/commits/38
26/02/13 11:48:40 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "fc5c6452-bd70-4c90-becf-772d3bebdb57",
  "name" : null,
  "timestamp" : "2026-02-13T11:48:39.588Z",
  "batchId" : 38,
  "numInputRows" : 129,
  "inputRowsPerSecond" : 134.93723849372387,
  "processedRowsPerSecond" : 229.94652406417111,
  "durationMs" : {
    "addBatch" : 369,
    "commitOffsets" : 76,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 47,
    "triggerExecution" : 561,
    "walCommit" : 65
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 4211,
        "1" : 4636,
        "0" : 6407
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 4254,
        "1" : 4682,
        "0" : 6447
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 4254,
        "1" : 4682,
        "0" : 6447
      }
    },
    "numInputRows" : 129,
    "inputRowsPerSecond" : 134.93723849372387,
    "processedRowsPerSecond" : 229.94652406417111,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@7fe9b5de",
    "numOutputRows" : 29
  }
}
26/02/13 11:48:49 INFO BlockManagerInfo: Removed broadcast_127_piece0 on spark-master:40291 in memory (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 11:48:49 INFO BlockManagerInfo: Removed broadcast_127_piece0 on 172.18.0.15:34959 in memory (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 11:48:49 INFO BlockManagerInfo: Removed broadcast_129_piece0 on spark-master:40291 in memory (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 11:48:49 INFO BlockManagerInfo: Removed broadcast_129_piece0 on 172.18.0.15:34959 in memory (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 11:48:49 INFO BlockManagerInfo: Removed broadcast_129_piece0 on 172.18.0.14:40939 in memory (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 11:48:50 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 11:48:55 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/39 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.39.562ca496-8589-475b-b4a4-2f14e4a7aaab.tmp
26/02/13 11:48:55 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.39.562ca496-8589-475b-b4a4-2f14e4a7aaab.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/39
26/02/13 11:48:55 INFO MicroBatchExecution: Committed offsets for batch 39. Metadata OffsetSeqMetadata(0,1770983335005,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 11:48:55 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:48:55 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:48:55 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:48:55 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:48:55 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:48:55 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:48:55 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#35257 - origin_code.nullCount#35256) > 0)
26/02/13 11:48:55 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#35262 - destination_code.nullCount#35261) > 0)
26/02/13 11:48:55 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#35292 - callsign.nullCount#35291) > 0)
26/02/13 11:48:55 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:48:55 INFO DAGScheduler: Got job 87 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 11:48:55 INFO DAGScheduler: Final stage: ResultStage 134 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:48:55 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 133)
26/02/13 11:48:55 INFO DAGScheduler: Missing parents: List()
26/02/13 11:48:55 INFO DAGScheduler: Submitting ResultStage 134 (MapPartitionsRDD[472] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:48:55 INFO MemoryStore: Block broadcast_130 stored as values in memory (estimated size 99.3 KiB, free 434.0 MiB)
26/02/13 11:48:55 INFO MemoryStore: Block broadcast_130_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 433.9 MiB)
26/02/13 11:48:55 INFO BlockManagerInfo: Added broadcast_130_piece0 in memory on spark-master:40291 (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 11:48:55 INFO SparkContext: Created broadcast 130 from broadcast at DAGScheduler.scala:1585
26/02/13 11:48:55 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 134 (MapPartitionsRDD[472] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 11:48:55 INFO TaskSchedulerImpl: Adding task set 134.0 with 4 tasks resource profile 0
26/02/13 11:48:55 INFO TaskSetManager: Starting task 0.0 in stage 134.0 (TID 282) (172.18.0.15, executor 0, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:48:55 INFO TaskSetManager: Starting task 1.0 in stage 134.0 (TID 283) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:48:55 INFO BlockManagerInfo: Added broadcast_130_piece0 in memory on 172.18.0.15:34959 (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 11:48:55 INFO TaskSetManager: Starting task 2.0 in stage 134.0 (TID 284) (172.18.0.15, executor 0, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:48:55 INFO TaskSetManager: Finished task 0.0 in stage 134.0 (TID 282) in 23 ms on 172.18.0.15 (executor 0) (1/4)
26/02/13 11:48:55 INFO TaskSetManager: Starting task 3.0 in stage 134.0 (TID 285) (172.18.0.15, executor 0, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:48:55 INFO TaskSetManager: Finished task 1.0 in stage 134.0 (TID 283) in 25 ms on 172.18.0.15 (executor 0) (2/4)
26/02/13 11:48:55 INFO TaskSetManager: Finished task 2.0 in stage 134.0 (TID 284) in 14 ms on 172.18.0.15 (executor 0) (3/4)
26/02/13 11:48:55 INFO TaskSetManager: Finished task 3.0 in stage 134.0 (TID 285) in 15 ms on 172.18.0.15 (executor 0) (4/4)
26/02/13 11:48:55 INFO TaskSchedulerImpl: Removed TaskSet 134.0, whose tasks have all completed, from pool 
26/02/13 11:48:55 INFO DAGScheduler: ResultStage 134 (start at NativeMethodAccessorImpl.java:0) finished in 0.055 s
26/02/13 11:48:55 INFO DAGScheduler: Job 87 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:48:55 INFO TaskSchedulerImpl: Killing all running tasks in stage 134: Stage finished
26/02/13 11:48:55 INFO DAGScheduler: Job 87 finished: start at NativeMethodAccessorImpl.java:0, took 0.058964 s
26/02/13 11:48:55 INFO MemoryStore: Block broadcast_131_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.8 MiB)
26/02/13 11:48:55 INFO BlockManagerInfo: Added broadcast_131_piece0 in memory on spark-master:40291 (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 11:48:55 INFO SparkContext: Created broadcast 131 from start at NativeMethodAccessorImpl.java:0
26/02/13 11:48:55 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 39, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@301cb570]. The input RDD has 2 partitions.
26/02/13 11:48:55 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:48:55 INFO DAGScheduler: Got job 88 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/02/13 11:48:55 INFO DAGScheduler: Final stage: ResultStage 135 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:48:55 INFO DAGScheduler: Parents of final stage: List()
26/02/13 11:48:55 INFO DAGScheduler: Missing parents: List()
26/02/13 11:48:55 INFO DAGScheduler: Submitting ResultStage 135 (MapPartitionsRDD[478] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:48:55 INFO MemoryStore: Block broadcast_132 stored as values in memory (estimated size 49.9 KiB, free 433.8 MiB)
26/02/13 11:48:55 INFO MemoryStore: Block broadcast_132_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.8 MiB)
26/02/13 11:48:55 INFO BlockManagerInfo: Added broadcast_132_piece0 in memory on spark-master:40291 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 11:48:55 INFO SparkContext: Created broadcast 132 from broadcast at DAGScheduler.scala:1585
26/02/13 11:48:55 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 135 (MapPartitionsRDD[478] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/02/13 11:48:55 INFO TaskSchedulerImpl: Adding task set 135.0 with 2 tasks resource profile 0
26/02/13 11:48:55 INFO TaskSetManager: Starting task 1.0 in stage 135.0 (TID 286) (172.18.0.14, executor 1, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:48:55 INFO TaskSetManager: Starting task 0.0 in stage 135.0 (TID 287) (172.18.0.15, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:48:55 INFO BlockManagerInfo: Added broadcast_132_piece0 in memory on 172.18.0.14:40939 (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 11:48:55 INFO BlockManagerInfo: Added broadcast_132_piece0 in memory on 172.18.0.15:34959 (size: 19.5 KiB, free: 434.0 MiB)
26/02/13 11:48:55 INFO BlockManagerInfo: Added broadcast_131_piece0 in memory on 172.18.0.14:40939 (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 11:48:55 INFO BlockManagerInfo: Added broadcast_131_piece0 in memory on 172.18.0.15:34959 (size: 107.2 KiB, free: 433.9 MiB)
26/02/13 11:48:55 INFO TaskSetManager: Finished task 1.0 in stage 135.0 (TID 286) in 572 ms on 172.18.0.14 (executor 1) (1/2)
26/02/13 11:48:55 INFO TaskSetManager: Finished task 0.0 in stage 135.0 (TID 287) in 573 ms on 172.18.0.15 (executor 0) (2/2)
26/02/13 11:48:55 INFO TaskSchedulerImpl: Removed TaskSet 135.0, whose tasks have all completed, from pool 
26/02/13 11:48:55 INFO DAGScheduler: ResultStage 135 (start at NativeMethodAccessorImpl.java:0) finished in 0.578 s
26/02/13 11:48:55 INFO DAGScheduler: Job 88 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:48:55 INFO TaskSchedulerImpl: Killing all running tasks in stage 135: Stage finished
26/02/13 11:48:55 INFO DAGScheduler: Job 88 finished: start at NativeMethodAccessorImpl.java:0, took 0.580179 s
26/02/13 11:48:55 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 39, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@301cb570] is committing.
26/02/13 11:48:55 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 39, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@301cb570] committed.
26/02/13 11:48:55 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/39 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.39.99b18085-56e3-4a29-8b2a-7cca76fb76cd.tmp
26/02/13 11:48:55 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.39.99b18085-56e3-4a29-8b2a-7cca76fb76cd.tmp to file:/tmp/spark-checkpoint-enrichment/commits/39
26/02/13 11:48:55 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "fc5c6452-bd70-4c90-becf-772d3bebdb57",
  "name" : null,
  "timestamp" : "2026-02-13T11:48:55.003Z",
  "batchId" : 39,
  "numInputRows" : 5,
  "inputRowsPerSecond" : 416.6666666666667,
  "processedRowsPerSecond" : 5.235602094240838,
  "durationMs" : {
    "addBatch" : 761,
    "commitOffsets" : 63,
    "getBatch" : 0,
    "latestOffset" : 2,
    "queryPlanning" : 45,
    "triggerExecution" : 955,
    "walCommit" : 83
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 4254,
        "1" : 4682,
        "0" : 6447
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 4254,
        "1" : 4685,
        "0" : 6449
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 4254,
        "1" : 4685,
        "0" : 6449
      }
    },
    "numInputRows" : 5,
    "inputRowsPerSecond" : 416.6666666666667,
    "processedRowsPerSecond" : 5.235602094240838,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@7fe9b5de",
    "numOutputRows" : 2
  }
}
26/02/13 11:48:55 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/40 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.40.f46d2959-7b33-4a3b-80bf-0ef6db2105f3.tmp
26/02/13 11:48:56 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.40.f46d2959-7b33-4a3b-80bf-0ef6db2105f3.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/40
26/02/13 11:48:56 INFO MicroBatchExecution: Committed offsets for batch 40. Metadata OffsetSeqMetadata(0,1770983335960,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 11:48:56 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:48:56 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:48:56 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:48:56 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:48:56 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:48:56 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:48:56 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#36111 - origin_code.nullCount#36110) > 0)
26/02/13 11:48:56 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#36116 - destination_code.nullCount#36115) > 0)
26/02/13 11:48:56 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#36146 - callsign.nullCount#36145) > 0)
26/02/13 11:48:56 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:48:56 INFO DAGScheduler: Got job 89 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 11:48:56 INFO DAGScheduler: Final stage: ResultStage 137 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:48:56 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 136)
26/02/13 11:48:56 INFO DAGScheduler: Missing parents: List()
26/02/13 11:48:56 INFO DAGScheduler: Submitting ResultStage 137 (MapPartitionsRDD[483] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:48:56 INFO MemoryStore: Block broadcast_133 stored as values in memory (estimated size 99.3 KiB, free 433.7 MiB)
26/02/13 11:48:56 INFO MemoryStore: Block broadcast_133_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 433.6 MiB)
26/02/13 11:48:56 INFO BlockManagerInfo: Added broadcast_133_piece0 in memory on spark-master:40291 (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 11:48:56 INFO SparkContext: Created broadcast 133 from broadcast at DAGScheduler.scala:1585
26/02/13 11:48:56 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 137 (MapPartitionsRDD[483] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 11:48:56 INFO TaskSchedulerImpl: Adding task set 137.0 with 4 tasks resource profile 0
26/02/13 11:48:56 INFO TaskSetManager: Starting task 0.0 in stage 137.0 (TID 288) (172.18.0.15, executor 0, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:48:56 INFO TaskSetManager: Starting task 1.0 in stage 137.0 (TID 289) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:48:56 INFO BlockManagerInfo: Added broadcast_133_piece0 in memory on 172.18.0.15:34959 (size: 33.3 KiB, free: 433.9 MiB)
26/02/13 11:48:56 INFO TaskSetManager: Starting task 2.0 in stage 137.0 (TID 290) (172.18.0.15, executor 0, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:48:56 INFO TaskSetManager: Finished task 1.0 in stage 137.0 (TID 289) in 21 ms on 172.18.0.15 (executor 0) (1/4)
26/02/13 11:48:56 INFO TaskSetManager: Starting task 3.0 in stage 137.0 (TID 291) (172.18.0.15, executor 0, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:48:56 INFO TaskSetManager: Finished task 0.0 in stage 137.0 (TID 288) in 33 ms on 172.18.0.15 (executor 0) (2/4)
26/02/13 11:48:56 INFO TaskSetManager: Finished task 2.0 in stage 137.0 (TID 290) in 12 ms on 172.18.0.15 (executor 0) (3/4)
26/02/13 11:48:56 INFO TaskSetManager: Finished task 3.0 in stage 137.0 (TID 291) in 16 ms on 172.18.0.15 (executor 0) (4/4)
26/02/13 11:48:56 INFO TaskSchedulerImpl: Removed TaskSet 137.0, whose tasks have all completed, from pool 
26/02/13 11:48:56 INFO DAGScheduler: ResultStage 137 (start at NativeMethodAccessorImpl.java:0) finished in 0.054 s
26/02/13 11:48:56 INFO DAGScheduler: Job 89 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:48:56 INFO TaskSchedulerImpl: Killing all running tasks in stage 137: Stage finished
26/02/13 11:48:56 INFO DAGScheduler: Job 89 finished: start at NativeMethodAccessorImpl.java:0, took 0.055856 s
26/02/13 11:48:56 INFO MemoryStore: Block broadcast_134_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.5 MiB)
26/02/13 11:48:56 INFO BlockManagerInfo: Added broadcast_134_piece0 in memory on spark-master:40291 (size: 107.2 KiB, free: 434.0 MiB)
26/02/13 11:48:56 INFO SparkContext: Created broadcast 134 from start at NativeMethodAccessorImpl.java:0
26/02/13 11:48:56 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 40, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@400ad89d]. The input RDD has 3 partitions.
26/02/13 11:48:56 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:48:56 INFO DAGScheduler: Got job 90 (start at NativeMethodAccessorImpl.java:0) with 3 output partitions
26/02/13 11:48:56 INFO DAGScheduler: Final stage: ResultStage 138 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:48:56 INFO DAGScheduler: Parents of final stage: List()
26/02/13 11:48:56 INFO DAGScheduler: Missing parents: List()
26/02/13 11:48:56 INFO DAGScheduler: Submitting ResultStage 138 (MapPartitionsRDD[489] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:48:56 INFO MemoryStore: Block broadcast_135 stored as values in memory (estimated size 49.9 KiB, free 433.5 MiB)
26/02/13 11:48:56 INFO MemoryStore: Block broadcast_135_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.5 MiB)
26/02/13 11:48:56 INFO BlockManagerInfo: Added broadcast_135_piece0 in memory on spark-master:40291 (size: 19.5 KiB, free: 433.9 MiB)
26/02/13 11:48:56 INFO SparkContext: Created broadcast 135 from broadcast at DAGScheduler.scala:1585
26/02/13 11:48:56 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 138 (MapPartitionsRDD[489] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
26/02/13 11:48:56 INFO TaskSchedulerImpl: Adding task set 138.0 with 3 tasks resource profile 0
26/02/13 11:48:56 INFO TaskSetManager: Starting task 0.0 in stage 138.0 (TID 292) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:48:56 INFO TaskSetManager: Starting task 1.0 in stage 138.0 (TID 293) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:48:56 INFO TaskSetManager: Starting task 2.0 in stage 138.0 (TID 294) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:48:56 INFO BlockManagerInfo: Added broadcast_135_piece0 in memory on 172.18.0.15:34959 (size: 19.5 KiB, free: 433.9 MiB)
26/02/13 11:48:56 INFO BlockManagerInfo: Added broadcast_135_piece0 in memory on 172.18.0.14:40939 (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 11:48:56 INFO BlockManagerInfo: Added broadcast_134_piece0 in memory on 172.18.0.14:40939 (size: 107.2 KiB, free: 434.0 MiB)
26/02/13 11:48:56 INFO BlockManagerInfo: Added broadcast_134_piece0 in memory on 172.18.0.15:34959 (size: 107.2 KiB, free: 433.7 MiB)
26/02/13 11:48:56 INFO TaskSetManager: Finished task 2.0 in stage 138.0 (TID 294) in 72 ms on 172.18.0.14 (executor 1) (1/3)
26/02/13 11:48:56 INFO TaskSetManager: Finished task 1.0 in stage 138.0 (TID 293) in 82 ms on 172.18.0.15 (executor 0) (2/3)
26/02/13 11:48:56 INFO TaskSetManager: Finished task 0.0 in stage 138.0 (TID 292) in 579 ms on 172.18.0.14 (executor 1) (3/3)
26/02/13 11:48:56 INFO TaskSchedulerImpl: Removed TaskSet 138.0, whose tasks have all completed, from pool 
26/02/13 11:48:56 INFO DAGScheduler: ResultStage 138 (start at NativeMethodAccessorImpl.java:0) finished in 0.585 s
26/02/13 11:48:56 INFO DAGScheduler: Job 90 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:48:56 INFO TaskSchedulerImpl: Killing all running tasks in stage 138: Stage finished
26/02/13 11:48:56 INFO DAGScheduler: Job 90 finished: start at NativeMethodAccessorImpl.java:0, took 0.587859 s
26/02/13 11:48:56 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 40, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@400ad89d] is committing.
26/02/13 11:48:56 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 40, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@400ad89d] committed.
26/02/13 11:48:56 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/40 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.40.d56c0dc5-70a6-4abf-a5b9-edb12e980fa9.tmp
26/02/13 11:48:56 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.40.d56c0dc5-70a6-4abf-a5b9-edb12e980fa9.tmp to file:/tmp/spark-checkpoint-enrichment/commits/40
26/02/13 11:48:56 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "fc5c6452-bd70-4c90-becf-772d3bebdb57",
  "name" : null,
  "timestamp" : "2026-02-13T11:48:55.959Z",
  "batchId" : 40,
  "numInputRows" : 235,
  "inputRowsPerSecond" : 245.81589958158997,
  "processedRowsPerSecond" : 257.9582875960483,
  "durationMs" : {
    "addBatch" : 749,
    "commitOffsets" : 62,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 35,
    "triggerExecution" : 911,
    "walCommit" : 64
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 4254,
        "1" : 4685,
        "0" : 6449
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 4334,
        "1" : 4765,
        "0" : 6524
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 4334,
        "1" : 4765,
        "0" : 6524
      }
    },
    "numInputRows" : 235,
    "inputRowsPerSecond" : 245.81589958158997,
    "processedRowsPerSecond" : 257.9582875960483,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@7fe9b5de",
    "numOutputRows" : 55
  }
}
26/02/13 11:48:58 INFO BlockManagerInfo: Removed broadcast_132_piece0 on spark-master:40291 in memory (size: 19.5 KiB, free: 434.0 MiB)
26/02/13 11:48:58 INFO BlockManagerInfo: Removed broadcast_132_piece0 on 172.18.0.15:34959 in memory (size: 19.5 KiB, free: 433.8 MiB)
26/02/13 11:48:58 INFO BlockManagerInfo: Removed broadcast_132_piece0 on 172.18.0.14:40939 in memory (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 11:48:58 INFO BlockManagerInfo: Removed broadcast_133_piece0 on spark-master:40291 in memory (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 11:48:58 INFO BlockManagerInfo: Removed broadcast_133_piece0 on 172.18.0.15:34959 in memory (size: 33.3 KiB, free: 433.8 MiB)
26/02/13 11:48:58 INFO BlockManagerInfo: Removed broadcast_130_piece0 on spark-master:40291 in memory (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 11:48:58 INFO BlockManagerInfo: Removed broadcast_130_piece0 on 172.18.0.15:34959 in memory (size: 33.3 KiB, free: 433.8 MiB)
26/02/13 11:48:58 INFO BlockManagerInfo: Removed broadcast_131_piece0 on spark-master:40291 in memory (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 11:48:58 INFO BlockManagerInfo: Removed broadcast_131_piece0 on 172.18.0.14:40939 in memory (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 11:48:58 INFO BlockManagerInfo: Removed broadcast_131_piece0 on 172.18.0.15:34959 in memory (size: 107.2 KiB, free: 433.9 MiB)
26/02/13 11:48:58 INFO BlockManagerInfo: Removed broadcast_135_piece0 on spark-master:40291 in memory (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 11:48:58 INFO BlockManagerInfo: Removed broadcast_135_piece0 on 172.18.0.15:34959 in memory (size: 19.5 KiB, free: 434.0 MiB)
26/02/13 11:48:58 INFO BlockManagerInfo: Removed broadcast_135_piece0 on 172.18.0.14:40939 in memory (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 11:49:06 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 11:49:11 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/41 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.41.05cb446d-2afa-4ae8-ae7d-4fc305135ff3.tmp
26/02/13 11:49:11 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.41.05cb446d-2afa-4ae8-ae7d-4fc305135ff3.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/41
26/02/13 11:49:11 INFO MicroBatchExecution: Committed offsets for batch 41. Metadata OffsetSeqMetadata(0,1770983351509,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 11:49:11 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:49:11 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:49:11 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:49:11 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:49:11 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:49:11 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:49:11 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#36965 - origin_code.nullCount#36964) > 0)
26/02/13 11:49:11 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#36970 - destination_code.nullCount#36969) > 0)
26/02/13 11:49:11 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#37000 - callsign.nullCount#36999) > 0)
26/02/13 11:49:11 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:49:11 INFO DAGScheduler: Got job 91 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 11:49:11 INFO DAGScheduler: Final stage: ResultStage 140 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:49:11 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 139)
26/02/13 11:49:11 INFO DAGScheduler: Missing parents: List()
26/02/13 11:49:11 INFO DAGScheduler: Submitting ResultStage 140 (MapPartitionsRDD[494] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:49:11 INFO MemoryStore: Block broadcast_136 stored as values in memory (estimated size 99.3 KiB, free 433.9 MiB)
26/02/13 11:49:11 INFO MemoryStore: Block broadcast_136_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 433.8 MiB)
26/02/13 11:49:11 INFO BlockManagerInfo: Added broadcast_136_piece0 in memory on spark-master:40291 (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 11:49:11 INFO SparkContext: Created broadcast 136 from broadcast at DAGScheduler.scala:1585
26/02/13 11:49:11 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 140 (MapPartitionsRDD[494] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 11:49:11 INFO TaskSchedulerImpl: Adding task set 140.0 with 4 tasks resource profile 0
26/02/13 11:49:11 INFO TaskSetManager: Starting task 0.0 in stage 140.0 (TID 295) (172.18.0.15, executor 0, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:49:11 INFO TaskSetManager: Starting task 1.0 in stage 140.0 (TID 296) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:49:11 INFO BlockManagerInfo: Added broadcast_136_piece0 in memory on 172.18.0.15:34959 (size: 33.3 KiB, free: 433.9 MiB)
26/02/13 11:49:11 INFO TaskSetManager: Starting task 2.0 in stage 140.0 (TID 297) (172.18.0.15, executor 0, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:49:11 INFO TaskSetManager: Starting task 3.0 in stage 140.0 (TID 298) (172.18.0.15, executor 0, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:49:11 INFO TaskSetManager: Finished task 0.0 in stage 140.0 (TID 295) in 25 ms on 172.18.0.15 (executor 0) (1/4)
26/02/13 11:49:11 INFO TaskSetManager: Finished task 1.0 in stage 140.0 (TID 296) in 26 ms on 172.18.0.15 (executor 0) (2/4)
26/02/13 11:49:11 INFO TaskSetManager: Finished task 2.0 in stage 140.0 (TID 297) in 29 ms on 172.18.0.15 (executor 0) (3/4)
26/02/13 11:49:11 INFO TaskSetManager: Finished task 3.0 in stage 140.0 (TID 298) in 29 ms on 172.18.0.15 (executor 0) (4/4)
26/02/13 11:49:11 INFO TaskSchedulerImpl: Removed TaskSet 140.0, whose tasks have all completed, from pool 
26/02/13 11:49:11 INFO DAGScheduler: ResultStage 140 (start at NativeMethodAccessorImpl.java:0) finished in 0.065 s
26/02/13 11:49:11 INFO DAGScheduler: Job 91 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:49:11 INFO TaskSchedulerImpl: Killing all running tasks in stage 140: Stage finished
26/02/13 11:49:11 INFO DAGScheduler: Job 91 finished: start at NativeMethodAccessorImpl.java:0, took 0.067703 s
26/02/13 11:49:11 INFO MemoryStore: Block broadcast_137_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.7 MiB)
26/02/13 11:49:11 INFO BlockManagerInfo: Added broadcast_137_piece0 in memory on spark-master:40291 (size: 107.2 KiB, free: 434.0 MiB)
26/02/13 11:49:11 INFO SparkContext: Created broadcast 137 from start at NativeMethodAccessorImpl.java:0
26/02/13 11:49:11 INFO BlockManagerInfo: Removed broadcast_134_piece0 on spark-master:40291 in memory (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 11:49:11 INFO BlockManagerInfo: Removed broadcast_134_piece0 on 172.18.0.14:40939 in memory (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 11:49:11 INFO BlockManagerInfo: Removed broadcast_134_piece0 on 172.18.0.15:34959 in memory (size: 107.2 KiB, free: 434.0 MiB)
26/02/13 11:49:11 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 41, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@2998c309]. The input RDD has 3 partitions.
26/02/13 11:49:11 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:49:11 INFO DAGScheduler: Got job 92 (start at NativeMethodAccessorImpl.java:0) with 3 output partitions
26/02/13 11:49:11 INFO DAGScheduler: Final stage: ResultStage 141 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:49:11 INFO DAGScheduler: Parents of final stage: List()
26/02/13 11:49:11 INFO DAGScheduler: Missing parents: List()
26/02/13 11:49:11 INFO DAGScheduler: Submitting ResultStage 141 (MapPartitionsRDD[500] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:49:11 INFO BlockManagerInfo: Removed broadcast_128_piece0 on spark-master:40291 in memory (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 11:49:11 INFO BlockManagerInfo: Removed broadcast_128_piece0 on 172.18.0.15:34959 in memory (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 11:49:11 INFO BlockManagerInfo: Removed broadcast_128_piece0 on 172.18.0.14:40939 in memory (size: 107.2 KiB, free: 434.4 MiB)
26/02/13 11:49:11 INFO MemoryStore: Block broadcast_138 stored as values in memory (estimated size 49.9 KiB, free 433.9 MiB)
26/02/13 11:49:11 INFO MemoryStore: Block broadcast_138_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.9 MiB)
26/02/13 11:49:11 INFO BlockManagerInfo: Added broadcast_138_piece0 in memory on spark-master:40291 (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 11:49:11 INFO SparkContext: Created broadcast 138 from broadcast at DAGScheduler.scala:1585
26/02/13 11:49:11 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 141 (MapPartitionsRDD[500] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
26/02/13 11:49:11 INFO TaskSchedulerImpl: Adding task set 141.0 with 3 tasks resource profile 0
26/02/13 11:49:11 INFO TaskSetManager: Starting task 1.0 in stage 141.0 (TID 299) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:49:11 INFO TaskSetManager: Starting task 0.0 in stage 141.0 (TID 300) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:49:11 INFO TaskSetManager: Starting task 2.0 in stage 141.0 (TID 301) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:49:11 INFO BlockManagerInfo: Added broadcast_138_piece0 in memory on 172.18.0.15:34959 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 11:49:11 INFO BlockManagerInfo: Added broadcast_138_piece0 in memory on 172.18.0.14:40939 (size: 19.5 KiB, free: 434.4 MiB)
26/02/13 11:49:11 INFO BlockManagerInfo: Added broadcast_137_piece0 in memory on 172.18.0.15:34959 (size: 107.2 KiB, free: 434.0 MiB)
26/02/13 11:49:11 INFO BlockManagerInfo: Added broadcast_137_piece0 in memory on 172.18.0.14:40939 (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 11:49:12 INFO TaskSetManager: Finished task 1.0 in stage 141.0 (TID 299) in 602 ms on 172.18.0.15 (executor 0) (1/3)
26/02/13 11:49:12 INFO TaskSetManager: Finished task 2.0 in stage 141.0 (TID 301) in 614 ms on 172.18.0.14 (executor 1) (2/3)
26/02/13 11:49:12 INFO TaskSetManager: Finished task 0.0 in stage 141.0 (TID 300) in 616 ms on 172.18.0.14 (executor 1) (3/3)
26/02/13 11:49:12 INFO TaskSchedulerImpl: Removed TaskSet 141.0, whose tasks have all completed, from pool 
26/02/13 11:49:12 INFO DAGScheduler: ResultStage 141 (start at NativeMethodAccessorImpl.java:0) finished in 0.628 s
26/02/13 11:49:12 INFO DAGScheduler: Job 92 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:49:12 INFO TaskSchedulerImpl: Killing all running tasks in stage 141: Stage finished
26/02/13 11:49:12 INFO DAGScheduler: Job 92 finished: start at NativeMethodAccessorImpl.java:0, took 0.634422 s
26/02/13 11:49:12 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 41, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@2998c309] is committing.
26/02/13 11:49:12 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 41, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@2998c309] committed.
26/02/13 11:49:12 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/41 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.41.f8c67499-805b-4899-9dec-b02355b51f11.tmp
26/02/13 11:49:12 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.41.f8c67499-805b-4899-9dec-b02355b51f11.tmp to file:/tmp/spark-checkpoint-enrichment/commits/41
26/02/13 11:49:12 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "fc5c6452-bd70-4c90-becf-772d3bebdb57",
  "name" : null,
  "timestamp" : "2026-02-13T11:49:11.507Z",
  "batchId" : 41,
  "numInputRows" : 86,
  "inputRowsPerSecond" : 7166.666666666667,
  "processedRowsPerSecond" : 81.67141500474834,
  "durationMs" : {
    "addBatch" : 823,
    "commitOffsets" : 66,
    "getBatch" : 0,
    "latestOffset" : 2,
    "queryPlanning" : 55,
    "triggerExecution" : 1053,
    "walCommit" : 107
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 4334,
        "1" : 4765,
        "0" : 6524
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 4364,
        "1" : 4797,
        "0" : 6548
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 4364,
        "1" : 4797,
        "0" : 6548
      }
    },
    "numInputRows" : 86,
    "inputRowsPerSecond" : 7166.666666666667,
    "processedRowsPerSecond" : 81.67141500474834,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@7fe9b5de",
    "numOutputRows" : 24
  }
}
26/02/13 11:49:12 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/42 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.42.230be92b-8b0a-4332-8c2a-55d9287c88fd.tmp
26/02/13 11:49:12 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.42.230be92b-8b0a-4332-8c2a-55d9287c88fd.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/42
26/02/13 11:49:12 INFO MicroBatchExecution: Committed offsets for batch 42. Metadata OffsetSeqMetadata(0,1770983352562,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 11:49:12 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:49:12 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:49:12 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:49:12 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:49:12 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:49:12 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:49:12 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#37819 - origin_code.nullCount#37818) > 0)
26/02/13 11:49:12 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#37824 - destination_code.nullCount#37823) > 0)
26/02/13 11:49:12 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#37854 - callsign.nullCount#37853) > 0)
26/02/13 11:49:12 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:49:12 INFO DAGScheduler: Got job 93 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 11:49:12 INFO DAGScheduler: Final stage: ResultStage 143 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:49:12 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 142)
26/02/13 11:49:12 INFO DAGScheduler: Missing parents: List()
26/02/13 11:49:12 INFO DAGScheduler: Submitting ResultStage 143 (MapPartitionsRDD[505] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:49:12 INFO MemoryStore: Block broadcast_139 stored as values in memory (estimated size 99.3 KiB, free 433.8 MiB)
26/02/13 11:49:12 INFO MemoryStore: Block broadcast_139_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 433.7 MiB)
26/02/13 11:49:12 INFO BlockManagerInfo: Added broadcast_139_piece0 in memory on spark-master:40291 (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 11:49:12 INFO SparkContext: Created broadcast 139 from broadcast at DAGScheduler.scala:1585
26/02/13 11:49:12 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 143 (MapPartitionsRDD[505] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 11:49:12 INFO TaskSchedulerImpl: Adding task set 143.0 with 4 tasks resource profile 0
26/02/13 11:49:12 INFO TaskSetManager: Starting task 0.0 in stage 143.0 (TID 302) (172.18.0.15, executor 0, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:49:12 INFO TaskSetManager: Starting task 1.0 in stage 143.0 (TID 303) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:49:12 INFO BlockManagerInfo: Added broadcast_139_piece0 in memory on 172.18.0.15:34959 (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 11:49:12 INFO TaskSetManager: Starting task 2.0 in stage 143.0 (TID 304) (172.18.0.15, executor 0, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:49:12 INFO TaskSetManager: Finished task 0.0 in stage 143.0 (TID 302) in 25 ms on 172.18.0.15 (executor 0) (1/4)
26/02/13 11:49:12 INFO TaskSetManager: Starting task 3.0 in stage 143.0 (TID 305) (172.18.0.15, executor 0, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:49:12 INFO TaskSetManager: Finished task 1.0 in stage 143.0 (TID 303) in 27 ms on 172.18.0.15 (executor 0) (2/4)
26/02/13 11:49:12 INFO TaskSetManager: Finished task 2.0 in stage 143.0 (TID 304) in 18 ms on 172.18.0.15 (executor 0) (3/4)
26/02/13 11:49:12 INFO TaskSetManager: Finished task 3.0 in stage 143.0 (TID 305) in 18 ms on 172.18.0.15 (executor 0) (4/4)
26/02/13 11:49:12 INFO TaskSchedulerImpl: Removed TaskSet 143.0, whose tasks have all completed, from pool 
26/02/13 11:49:12 INFO DAGScheduler: ResultStage 143 (start at NativeMethodAccessorImpl.java:0) finished in 0.051 s
26/02/13 11:49:12 INFO DAGScheduler: Job 93 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:49:12 INFO TaskSchedulerImpl: Killing all running tasks in stage 143: Stage finished
26/02/13 11:49:12 INFO DAGScheduler: Job 93 finished: start at NativeMethodAccessorImpl.java:0, took 0.053172 s
26/02/13 11:49:12 INFO MemoryStore: Block broadcast_140_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.6 MiB)
26/02/13 11:49:12 INFO BlockManagerInfo: Added broadcast_140_piece0 in memory on spark-master:40291 (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 11:49:12 INFO SparkContext: Created broadcast 140 from start at NativeMethodAccessorImpl.java:0
26/02/13 11:49:12 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 42, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@1ce5fd9a]. The input RDD has 3 partitions.
26/02/13 11:49:12 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:49:12 INFO DAGScheduler: Got job 94 (start at NativeMethodAccessorImpl.java:0) with 3 output partitions
26/02/13 11:49:12 INFO DAGScheduler: Final stage: ResultStage 144 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:49:12 INFO DAGScheduler: Parents of final stage: List()
26/02/13 11:49:12 INFO DAGScheduler: Missing parents: List()
26/02/13 11:49:12 INFO DAGScheduler: Submitting ResultStage 144 (MapPartitionsRDD[511] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:49:12 INFO MemoryStore: Block broadcast_141 stored as values in memory (estimated size 49.9 KiB, free 433.6 MiB)
26/02/13 11:49:12 INFO MemoryStore: Block broadcast_141_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.6 MiB)
26/02/13 11:49:12 INFO BlockManagerInfo: Added broadcast_141_piece0 in memory on spark-master:40291 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 11:49:12 INFO SparkContext: Created broadcast 141 from broadcast at DAGScheduler.scala:1585
26/02/13 11:49:12 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 144 (MapPartitionsRDD[511] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
26/02/13 11:49:12 INFO TaskSchedulerImpl: Adding task set 144.0 with 3 tasks resource profile 0
26/02/13 11:49:12 INFO TaskSetManager: Starting task 0.0 in stage 144.0 (TID 306) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:49:12 INFO TaskSetManager: Starting task 1.0 in stage 144.0 (TID 307) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:49:12 INFO TaskSetManager: Starting task 2.0 in stage 144.0 (TID 308) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:49:12 INFO BlockManagerInfo: Added broadcast_141_piece0 in memory on 172.18.0.15:34959 (size: 19.5 KiB, free: 434.0 MiB)
26/02/13 11:49:12 INFO BlockManagerInfo: Added broadcast_141_piece0 in memory on 172.18.0.14:40939 (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 11:49:12 INFO BlockManagerInfo: Added broadcast_140_piece0 in memory on 172.18.0.15:34959 (size: 107.2 KiB, free: 433.9 MiB)
26/02/13 11:49:12 INFO BlockManagerInfo: Added broadcast_140_piece0 in memory on 172.18.0.14:40939 (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 11:49:12 INFO TaskSetManager: Finished task 1.0 in stage 144.0 (TID 307) in 86 ms on 172.18.0.15 (executor 0) (1/3)
26/02/13 11:49:12 INFO TaskSetManager: Finished task 2.0 in stage 144.0 (TID 308) in 93 ms on 172.18.0.14 (executor 1) (2/3)
26/02/13 11:49:12 INFO TaskSetManager: Finished task 0.0 in stage 144.0 (TID 306) in 94 ms on 172.18.0.14 (executor 1) (3/3)
26/02/13 11:49:12 INFO TaskSchedulerImpl: Removed TaskSet 144.0, whose tasks have all completed, from pool 
26/02/13 11:49:12 INFO DAGScheduler: ResultStage 144 (start at NativeMethodAccessorImpl.java:0) finished in 0.102 s
26/02/13 11:49:12 INFO DAGScheduler: Job 94 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:49:12 INFO TaskSchedulerImpl: Killing all running tasks in stage 144: Stage finished
26/02/13 11:49:12 INFO DAGScheduler: Job 94 finished: start at NativeMethodAccessorImpl.java:0, took 0.103813 s
26/02/13 11:49:12 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 42, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@1ce5fd9a] is committing.
26/02/13 11:49:12 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 42, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@1ce5fd9a] committed.
26/02/13 11:49:12 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/42 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.42.03ee6a50-49b5-46f1-94b9-8b5c6d5f3e6a.tmp
26/02/13 11:49:13 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.42.03ee6a50-49b5-46f1-94b9-8b5c6d5f3e6a.tmp to file:/tmp/spark-checkpoint-enrichment/commits/42
26/02/13 11:49:13 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "fc5c6452-bd70-4c90-becf-772d3bebdb57",
  "name" : null,
  "timestamp" : "2026-02-13T11:49:12.561Z",
  "batchId" : 42,
  "numInputRows" : 155,
  "inputRowsPerSecond" : 147.05882352941177,
  "processedRowsPerSecond" : 322.9166666666667,
  "durationMs" : {
    "addBatch" : 281,
    "commitOffsets" : 69,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 49,
    "triggerExecution" : 480,
    "walCommit" : 79
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 4364,
        "1" : 4797,
        "0" : 6548
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 4415,
        "1" : 4850,
        "0" : 6599
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 4415,
        "1" : 4850,
        "0" : 6599
      }
    },
    "numInputRows" : 155,
    "inputRowsPerSecond" : 147.05882352941177,
    "processedRowsPerSecond" : 322.9166666666667,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@7fe9b5de",
    "numOutputRows" : 34
  }
}
26/02/13 11:49:15 INFO BlockManagerInfo: Removed broadcast_137_piece0 on spark-master:40291 in memory (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 11:49:15 INFO BlockManagerInfo: Removed broadcast_137_piece0 on 172.18.0.14:40939 in memory (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 11:49:15 INFO BlockManagerInfo: Removed broadcast_137_piece0 on 172.18.0.15:34959 in memory (size: 107.2 KiB, free: 434.0 MiB)
26/02/13 11:49:15 INFO BlockManagerInfo: Removed broadcast_139_piece0 on spark-master:40291 in memory (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 11:49:15 INFO BlockManagerInfo: Removed broadcast_139_piece0 on 172.18.0.15:34959 in memory (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 11:49:15 INFO BlockManagerInfo: Removed broadcast_138_piece0 on spark-master:40291 in memory (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 11:49:15 INFO BlockManagerInfo: Removed broadcast_138_piece0 on 172.18.0.15:34959 in memory (size: 19.5 KiB, free: 434.0 MiB)
26/02/13 11:49:15 INFO BlockManagerInfo: Removed broadcast_138_piece0 on 172.18.0.14:40939 in memory (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 11:49:15 INFO BlockManagerInfo: Removed broadcast_141_piece0 on spark-master:40291 in memory (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 11:49:15 INFO BlockManagerInfo: Removed broadcast_141_piece0 on 172.18.0.15:34959 in memory (size: 19.5 KiB, free: 434.0 MiB)
26/02/13 11:49:15 INFO BlockManagerInfo: Removed broadcast_141_piece0 on 172.18.0.14:40939 in memory (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 11:49:15 INFO BlockManagerInfo: Removed broadcast_136_piece0 on spark-master:40291 in memory (size: 33.3 KiB, free: 434.3 MiB)
26/02/13 11:49:15 INFO BlockManagerInfo: Removed broadcast_136_piece0 on 172.18.0.15:34959 in memory (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 11:49:23 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 11:49:31 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/43 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.43.f4d41920-b600-4bfe-a4f9-64309bb59d85.tmp
26/02/13 11:49:31 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.43.f4d41920-b600-4bfe-a4f9-64309bb59d85.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/43
26/02/13 11:49:31 INFO MicroBatchExecution: Committed offsets for batch 43. Metadata OffsetSeqMetadata(0,1770983371488,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 11:49:31 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:49:31 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:49:31 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:49:31 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:49:31 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:49:31 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:49:31 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#38673 - origin_code.nullCount#38672) > 0)
26/02/13 11:49:31 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#38678 - destination_code.nullCount#38677) > 0)
26/02/13 11:49:31 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#38708 - callsign.nullCount#38707) > 0)
26/02/13 11:49:31 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:49:31 INFO DAGScheduler: Got job 95 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 11:49:31 INFO DAGScheduler: Final stage: ResultStage 146 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:49:31 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 145)
26/02/13 11:49:31 INFO DAGScheduler: Missing parents: List()
26/02/13 11:49:31 INFO DAGScheduler: Submitting ResultStage 146 (MapPartitionsRDD[516] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:49:31 INFO MemoryStore: Block broadcast_142 stored as values in memory (estimated size 99.3 KiB, free 434.0 MiB)
26/02/13 11:49:31 INFO MemoryStore: Block broadcast_142_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 433.9 MiB)
26/02/13 11:49:31 INFO BlockManagerInfo: Added broadcast_142_piece0 in memory on spark-master:40291 (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 11:49:31 INFO SparkContext: Created broadcast 142 from broadcast at DAGScheduler.scala:1585
26/02/13 11:49:31 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 146 (MapPartitionsRDD[516] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 11:49:31 INFO TaskSchedulerImpl: Adding task set 146.0 with 4 tasks resource profile 0
26/02/13 11:49:31 INFO TaskSetManager: Starting task 0.0 in stage 146.0 (TID 309) (172.18.0.15, executor 0, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:49:31 INFO TaskSetManager: Starting task 1.0 in stage 146.0 (TID 310) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:49:31 INFO BlockManagerInfo: Added broadcast_142_piece0 in memory on 172.18.0.15:34959 (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 11:49:31 INFO TaskSetManager: Starting task 2.0 in stage 146.0 (TID 311) (172.18.0.15, executor 0, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:49:31 INFO TaskSetManager: Finished task 0.0 in stage 146.0 (TID 309) in 23 ms on 172.18.0.15 (executor 0) (1/4)
26/02/13 11:49:31 INFO TaskSetManager: Starting task 3.0 in stage 146.0 (TID 312) (172.18.0.15, executor 0, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:49:31 INFO TaskSetManager: Finished task 1.0 in stage 146.0 (TID 310) in 25 ms on 172.18.0.15 (executor 0) (2/4)
26/02/13 11:49:31 INFO TaskSetManager: Finished task 2.0 in stage 146.0 (TID 311) in 18 ms on 172.18.0.15 (executor 0) (3/4)
26/02/13 11:49:31 INFO TaskSetManager: Finished task 3.0 in stage 146.0 (TID 312) in 19 ms on 172.18.0.15 (executor 0) (4/4)
26/02/13 11:49:31 INFO TaskSchedulerImpl: Removed TaskSet 146.0, whose tasks have all completed, from pool 
26/02/13 11:49:31 INFO DAGScheduler: ResultStage 146 (start at NativeMethodAccessorImpl.java:0) finished in 0.055 s
26/02/13 11:49:31 INFO DAGScheduler: Job 95 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:49:31 INFO TaskSchedulerImpl: Killing all running tasks in stage 146: Stage finished
26/02/13 11:49:31 INFO DAGScheduler: Job 95 finished: start at NativeMethodAccessorImpl.java:0, took 0.057717 s
26/02/13 11:49:31 INFO MemoryStore: Block broadcast_143_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.8 MiB)
26/02/13 11:49:31 INFO BlockManagerInfo: Added broadcast_143_piece0 in memory on spark-master:40291 (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 11:49:31 INFO BlockManagerInfo: Removed broadcast_140_piece0 on spark-master:40291 in memory (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 11:49:31 INFO SparkContext: Created broadcast 143 from start at NativeMethodAccessorImpl.java:0
26/02/13 11:49:31 INFO BlockManagerInfo: Removed broadcast_140_piece0 on 172.18.0.15:34959 in memory (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 11:49:31 INFO BlockManagerInfo: Removed broadcast_140_piece0 on 172.18.0.14:40939 in memory (size: 107.2 KiB, free: 434.4 MiB)
26/02/13 11:49:31 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 43, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@453db3a6]. The input RDD has 3 partitions.
26/02/13 11:49:31 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:49:31 INFO DAGScheduler: Got job 96 (start at NativeMethodAccessorImpl.java:0) with 3 output partitions
26/02/13 11:49:31 INFO DAGScheduler: Final stage: ResultStage 147 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:49:31 INFO DAGScheduler: Parents of final stage: List()
26/02/13 11:49:31 INFO DAGScheduler: Missing parents: List()
26/02/13 11:49:31 INFO DAGScheduler: Submitting ResultStage 147 (MapPartitionsRDD[522] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:49:31 INFO MemoryStore: Block broadcast_144 stored as values in memory (estimated size 49.9 KiB, free 433.9 MiB)
26/02/13 11:49:31 INFO MemoryStore: Block broadcast_144_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.9 MiB)
26/02/13 11:49:31 INFO BlockManagerInfo: Added broadcast_144_piece0 in memory on spark-master:40291 (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 11:49:31 INFO SparkContext: Created broadcast 144 from broadcast at DAGScheduler.scala:1585
26/02/13 11:49:31 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 147 (MapPartitionsRDD[522] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
26/02/13 11:49:31 INFO TaskSchedulerImpl: Adding task set 147.0 with 3 tasks resource profile 0
26/02/13 11:49:31 INFO TaskSetManager: Starting task 1.0 in stage 147.0 (TID 313) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:49:31 INFO TaskSetManager: Starting task 0.0 in stage 147.0 (TID 314) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:49:31 INFO TaskSetManager: Starting task 2.0 in stage 147.0 (TID 315) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:49:31 INFO BlockManagerInfo: Added broadcast_144_piece0 in memory on 172.18.0.15:34959 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 11:49:31 INFO BlockManagerInfo: Added broadcast_144_piece0 in memory on 172.18.0.14:40939 (size: 19.5 KiB, free: 434.4 MiB)
26/02/13 11:49:31 INFO BlockManagerInfo: Added broadcast_143_piece0 in memory on 172.18.0.15:34959 (size: 107.2 KiB, free: 434.0 MiB)
26/02/13 11:49:31 INFO BlockManagerInfo: Added broadcast_143_piece0 in memory on 172.18.0.14:40939 (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 11:49:32 INFO TaskSetManager: Finished task 2.0 in stage 147.0 (TID 315) in 599 ms on 172.18.0.14 (executor 1) (1/3)
26/02/13 11:49:32 INFO TaskSetManager: Finished task 0.0 in stage 147.0 (TID 314) in 604 ms on 172.18.0.14 (executor 1) (2/3)
26/02/13 11:49:32 INFO TaskSetManager: Finished task 1.0 in stage 147.0 (TID 313) in 605 ms on 172.18.0.15 (executor 0) (3/3)
26/02/13 11:49:32 INFO TaskSchedulerImpl: Removed TaskSet 147.0, whose tasks have all completed, from pool 
26/02/13 11:49:32 INFO DAGScheduler: ResultStage 147 (start at NativeMethodAccessorImpl.java:0) finished in 0.615 s
26/02/13 11:49:32 INFO DAGScheduler: Job 96 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:49:32 INFO TaskSchedulerImpl: Killing all running tasks in stage 147: Stage finished
26/02/13 11:49:32 INFO DAGScheduler: Job 96 finished: start at NativeMethodAccessorImpl.java:0, took 0.617269 s
26/02/13 11:49:32 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 43, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@453db3a6] is committing.
26/02/13 11:49:32 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 43, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@453db3a6] committed.
26/02/13 11:49:32 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/43 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.43.f50d84af-f436-4c36-a6b7-07c4b7c2eee2.tmp
26/02/13 11:49:32 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.43.f50d84af-f436-4c36-a6b7-07c4b7c2eee2.tmp to file:/tmp/spark-checkpoint-enrichment/commits/43
26/02/13 11:49:32 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "fc5c6452-bd70-4c90-becf-772d3bebdb57",
  "name" : null,
  "timestamp" : "2026-02-13T11:49:31.487Z",
  "batchId" : 43,
  "numInputRows" : 108,
  "inputRowsPerSecond" : 8307.692307692309,
  "processedRowsPerSecond" : 111.34020618556701,
  "durationMs" : {
    "addBatch" : 798,
    "commitOffsets" : 61,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 47,
    "triggerExecution" : 970,
    "walCommit" : 63
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 4415,
        "1" : 4850,
        "0" : 6599
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 4449,
        "1" : 4887,
        "0" : 6636
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 4449,
        "1" : 4887,
        "0" : 6636
      }
    },
    "numInputRows" : 108,
    "inputRowsPerSecond" : 8307.692307692309,
    "processedRowsPerSecond" : 111.34020618556701,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@7fe9b5de",
    "numOutputRows" : 28
  }
}
26/02/13 11:49:32 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/44 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.44.12ba6641-3466-4e2a-b957-40b15ce545dd.tmp
26/02/13 11:49:32 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.44.12ba6641-3466-4e2a-b957-40b15ce545dd.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/44
26/02/13 11:49:32 INFO MicroBatchExecution: Committed offsets for batch 44. Metadata OffsetSeqMetadata(0,1770983372459,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 11:49:32 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:49:32 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:49:32 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:49:32 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:49:32 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:49:32 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:49:32 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#39527 - origin_code.nullCount#39526) > 0)
26/02/13 11:49:32 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#39532 - destination_code.nullCount#39531) > 0)
26/02/13 11:49:32 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#39562 - callsign.nullCount#39561) > 0)
26/02/13 11:49:32 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:49:32 INFO DAGScheduler: Got job 97 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 11:49:32 INFO DAGScheduler: Final stage: ResultStage 149 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:49:32 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 148)
26/02/13 11:49:32 INFO DAGScheduler: Missing parents: List()
26/02/13 11:49:32 INFO DAGScheduler: Submitting ResultStage 149 (MapPartitionsRDD[527] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:49:32 INFO MemoryStore: Block broadcast_145 stored as values in memory (estimated size 99.3 KiB, free 433.8 MiB)
26/02/13 11:49:32 INFO MemoryStore: Block broadcast_145_piece0 stored as bytes in memory (estimated size 33.4 KiB, free 433.7 MiB)
26/02/13 11:49:32 INFO BlockManagerInfo: Added broadcast_145_piece0 in memory on spark-master:40291 (size: 33.4 KiB, free: 434.2 MiB)
26/02/13 11:49:32 INFO SparkContext: Created broadcast 145 from broadcast at DAGScheduler.scala:1585
26/02/13 11:49:32 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 149 (MapPartitionsRDD[527] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 11:49:32 INFO TaskSchedulerImpl: Adding task set 149.0 with 4 tasks resource profile 0
26/02/13 11:49:32 INFO TaskSetManager: Starting task 0.0 in stage 149.0 (TID 316) (172.18.0.15, executor 0, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:49:32 INFO TaskSetManager: Starting task 1.0 in stage 149.0 (TID 317) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:49:32 INFO BlockManagerInfo: Added broadcast_145_piece0 in memory on 172.18.0.15:34959 (size: 33.4 KiB, free: 434.0 MiB)
26/02/13 11:49:32 INFO TaskSetManager: Starting task 2.0 in stage 149.0 (TID 318) (172.18.0.15, executor 0, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:49:32 INFO TaskSetManager: Starting task 3.0 in stage 149.0 (TID 319) (172.18.0.15, executor 0, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:49:32 INFO TaskSetManager: Finished task 0.0 in stage 149.0 (TID 316) in 27 ms on 172.18.0.15 (executor 0) (1/4)
26/02/13 11:49:32 INFO TaskSetManager: Finished task 1.0 in stage 149.0 (TID 317) in 27 ms on 172.18.0.15 (executor 0) (2/4)
26/02/13 11:49:32 INFO TaskSetManager: Finished task 2.0 in stage 149.0 (TID 318) in 19 ms on 172.18.0.15 (executor 0) (3/4)
26/02/13 11:49:32 INFO TaskSetManager: Finished task 3.0 in stage 149.0 (TID 319) in 20 ms on 172.18.0.15 (executor 0) (4/4)
26/02/13 11:49:32 INFO DAGScheduler: ResultStage 149 (start at NativeMethodAccessorImpl.java:0) finished in 0.052 s
26/02/13 11:49:32 INFO TaskSchedulerImpl: Removed TaskSet 149.0, whose tasks have all completed, from pool 
26/02/13 11:49:32 INFO DAGScheduler: Job 97 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:49:32 INFO TaskSchedulerImpl: Killing all running tasks in stage 149: Stage finished
26/02/13 11:49:32 INFO DAGScheduler: Job 97 finished: start at NativeMethodAccessorImpl.java:0, took 0.055012 s
26/02/13 11:49:32 INFO MemoryStore: Block broadcast_146_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.6 MiB)
26/02/13 11:49:32 INFO BlockManagerInfo: Added broadcast_146_piece0 in memory on spark-master:40291 (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 11:49:32 INFO SparkContext: Created broadcast 146 from start at NativeMethodAccessorImpl.java:0
26/02/13 11:49:32 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 44, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@3e8c693b]. The input RDD has 3 partitions.
26/02/13 11:49:32 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:49:32 INFO DAGScheduler: Got job 98 (start at NativeMethodAccessorImpl.java:0) with 3 output partitions
26/02/13 11:49:32 INFO DAGScheduler: Final stage: ResultStage 150 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:49:32 INFO DAGScheduler: Parents of final stage: List()
26/02/13 11:49:32 INFO DAGScheduler: Missing parents: List()
26/02/13 11:49:32 INFO DAGScheduler: Submitting ResultStage 150 (MapPartitionsRDD[533] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:49:32 INFO MemoryStore: Block broadcast_147 stored as values in memory (estimated size 49.9 KiB, free 433.6 MiB)
26/02/13 11:49:32 INFO MemoryStore: Block broadcast_147_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.6 MiB)
26/02/13 11:49:32 INFO BlockManagerInfo: Added broadcast_147_piece0 in memory on spark-master:40291 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 11:49:32 INFO SparkContext: Created broadcast 147 from broadcast at DAGScheduler.scala:1585
26/02/13 11:49:32 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 150 (MapPartitionsRDD[533] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
26/02/13 11:49:32 INFO TaskSchedulerImpl: Adding task set 150.0 with 3 tasks resource profile 0
26/02/13 11:49:32 INFO TaskSetManager: Starting task 0.0 in stage 150.0 (TID 320) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:49:32 INFO TaskSetManager: Starting task 1.0 in stage 150.0 (TID 321) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:49:32 INFO TaskSetManager: Starting task 2.0 in stage 150.0 (TID 322) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:49:32 INFO BlockManagerInfo: Added broadcast_147_piece0 in memory on 172.18.0.15:34959 (size: 19.5 KiB, free: 434.0 MiB)
26/02/13 11:49:32 INFO BlockManagerInfo: Added broadcast_147_piece0 in memory on 172.18.0.14:40939 (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 11:49:32 INFO BlockManagerInfo: Added broadcast_146_piece0 in memory on 172.18.0.15:34959 (size: 107.2 KiB, free: 433.9 MiB)
26/02/13 11:49:32 INFO BlockManagerInfo: Added broadcast_146_piece0 in memory on 172.18.0.14:40939 (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 11:49:32 INFO TaskSetManager: Finished task 0.0 in stage 150.0 (TID 320) in 62 ms on 172.18.0.14 (executor 1) (1/3)
26/02/13 11:49:32 INFO TaskSetManager: Finished task 2.0 in stage 150.0 (TID 322) in 63 ms on 172.18.0.14 (executor 1) (2/3)
26/02/13 11:49:32 INFO TaskSetManager: Finished task 1.0 in stage 150.0 (TID 321) in 79 ms on 172.18.0.15 (executor 0) (3/3)
26/02/13 11:49:32 INFO TaskSchedulerImpl: Removed TaskSet 150.0, whose tasks have all completed, from pool 
26/02/13 11:49:32 INFO DAGScheduler: ResultStage 150 (start at NativeMethodAccessorImpl.java:0) finished in 0.085 s
26/02/13 11:49:32 INFO DAGScheduler: Job 98 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:49:32 INFO TaskSchedulerImpl: Killing all running tasks in stage 150: Stage finished
26/02/13 11:49:32 INFO DAGScheduler: Job 98 finished: start at NativeMethodAccessorImpl.java:0, took 0.086836 s
26/02/13 11:49:32 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 44, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@3e8c693b] is committing.
26/02/13 11:49:32 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 44, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@3e8c693b] committed.
26/02/13 11:49:32 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/44 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.44.5284a808-787f-4c17-a966-f0dbed80d344.tmp
26/02/13 11:49:32 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.44.5284a808-787f-4c17-a966-f0dbed80d344.tmp to file:/tmp/spark-checkpoint-enrichment/commits/44
26/02/13 11:49:32 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "fc5c6452-bd70-4c90-becf-772d3bebdb57",
  "name" : null,
  "timestamp" : "2026-02-13T11:49:32.458Z",
  "batchId" : 44,
  "numInputRows" : 130,
  "inputRowsPerSecond" : 133.88259526261587,
  "processedRowsPerSecond" : 332.48081841432224,
  "durationMs" : {
    "addBatch" : 248,
    "commitOffsets" : 64,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 26,
    "triggerExecution" : 391,
    "walCommit" : 51
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 4449,
        "1" : 4887,
        "0" : 6636
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 4495,
        "1" : 4933,
        "0" : 6674
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 4495,
        "1" : 4933,
        "0" : 6674
      }
    },
    "numInputRows" : 130,
    "inputRowsPerSecond" : 133.88259526261587,
    "processedRowsPerSecond" : 332.48081841432224,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@7fe9b5de",
    "numOutputRows" : 29
  }
}
26/02/13 11:49:34 INFO BlockManagerInfo: Removed broadcast_144_piece0 on spark-master:40291 in memory (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 11:49:34 INFO BlockManagerInfo: Removed broadcast_144_piece0 on 172.18.0.15:34959 in memory (size: 19.5 KiB, free: 433.9 MiB)
26/02/13 11:49:34 INFO BlockManagerInfo: Removed broadcast_144_piece0 on 172.18.0.14:40939 in memory (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 11:49:34 INFO BlockManagerInfo: Removed broadcast_147_piece0 on spark-master:40291 in memory (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 11:49:34 INFO BlockManagerInfo: Removed broadcast_147_piece0 on 172.18.0.14:40939 in memory (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 11:49:34 INFO BlockManagerInfo: Removed broadcast_147_piece0 on 172.18.0.15:34959 in memory (size: 19.5 KiB, free: 433.9 MiB)
26/02/13 11:49:34 INFO BlockManagerInfo: Removed broadcast_143_piece0 on spark-master:40291 in memory (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 11:49:34 INFO BlockManagerInfo: Removed broadcast_143_piece0 on 172.18.0.15:34959 in memory (size: 107.2 KiB, free: 434.0 MiB)
26/02/13 11:49:34 INFO BlockManagerInfo: Removed broadcast_143_piece0 on 172.18.0.14:40939 in memory (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 11:49:34 INFO BlockManagerInfo: Removed broadcast_145_piece0 on spark-master:40291 in memory (size: 33.4 KiB, free: 434.2 MiB)
26/02/13 11:49:34 INFO BlockManagerInfo: Removed broadcast_145_piece0 on 172.18.0.15:34959 in memory (size: 33.4 KiB, free: 434.0 MiB)
26/02/13 11:49:34 INFO BlockManagerInfo: Removed broadcast_142_piece0 on spark-master:40291 in memory (size: 33.3 KiB, free: 434.3 MiB)
26/02/13 11:49:34 INFO BlockManagerInfo: Removed broadcast_142_piece0 on 172.18.0.15:34959 in memory (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 11:49:42 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 11:49:47 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/45 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.45.549a8382-7383-4a60-82a0-9e105681ff77.tmp
26/02/13 11:49:47 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.45.549a8382-7383-4a60-82a0-9e105681ff77.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/45
26/02/13 11:49:47 INFO MicroBatchExecution: Committed offsets for batch 45. Metadata OffsetSeqMetadata(0,1770983387648,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 11:49:47 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:49:47 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:49:47 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:49:47 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:49:47 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:49:47 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:49:47 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#40381 - origin_code.nullCount#40380) > 0)
26/02/13 11:49:47 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#40386 - destination_code.nullCount#40385) > 0)
26/02/13 11:49:47 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#40416 - callsign.nullCount#40415) > 0)
26/02/13 11:49:47 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:49:47 INFO DAGScheduler: Got job 99 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 11:49:47 INFO DAGScheduler: Final stage: ResultStage 152 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:49:47 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 151)
26/02/13 11:49:47 INFO DAGScheduler: Missing parents: List()
26/02/13 11:49:47 INFO DAGScheduler: Submitting ResultStage 152 (MapPartitionsRDD[538] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:49:47 INFO MemoryStore: Block broadcast_148 stored as values in memory (estimated size 99.3 KiB, free 434.0 MiB)
26/02/13 11:49:47 INFO MemoryStore: Block broadcast_148_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 433.9 MiB)
26/02/13 11:49:47 INFO BlockManagerInfo: Added broadcast_148_piece0 in memory on spark-master:40291 (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 11:49:47 INFO SparkContext: Created broadcast 148 from broadcast at DAGScheduler.scala:1585
26/02/13 11:49:47 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 152 (MapPartitionsRDD[538] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 11:49:47 INFO TaskSchedulerImpl: Adding task set 152.0 with 4 tasks resource profile 0
26/02/13 11:49:47 INFO TaskSetManager: Starting task 0.0 in stage 152.0 (TID 323) (172.18.0.15, executor 0, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:49:47 INFO TaskSetManager: Starting task 1.0 in stage 152.0 (TID 324) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:49:47 INFO BlockManagerInfo: Added broadcast_148_piece0 in memory on 172.18.0.15:34959 (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 11:49:47 INFO TaskSetManager: Starting task 2.0 in stage 152.0 (TID 325) (172.18.0.15, executor 0, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:49:47 INFO TaskSetManager: Starting task 3.0 in stage 152.0 (TID 326) (172.18.0.15, executor 0, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:49:47 INFO TaskSetManager: Finished task 0.0 in stage 152.0 (TID 323) in 25 ms on 172.18.0.15 (executor 0) (1/4)
26/02/13 11:49:47 INFO TaskSetManager: Finished task 1.0 in stage 152.0 (TID 324) in 26 ms on 172.18.0.15 (executor 0) (2/4)
26/02/13 11:49:47 INFO TaskSetManager: Finished task 3.0 in stage 152.0 (TID 326) in 17 ms on 172.18.0.15 (executor 0) (3/4)
26/02/13 11:49:47 INFO TaskSetManager: Finished task 2.0 in stage 152.0 (TID 325) in 20 ms on 172.18.0.15 (executor 0) (4/4)
26/02/13 11:49:47 INFO TaskSchedulerImpl: Removed TaskSet 152.0, whose tasks have all completed, from pool 
26/02/13 11:49:47 INFO DAGScheduler: ResultStage 152 (start at NativeMethodAccessorImpl.java:0) finished in 0.054 s
26/02/13 11:49:47 INFO DAGScheduler: Job 99 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:49:47 INFO TaskSchedulerImpl: Killing all running tasks in stage 152: Stage finished
26/02/13 11:49:47 INFO DAGScheduler: Job 99 finished: start at NativeMethodAccessorImpl.java:0, took 0.056791 s
26/02/13 11:49:47 INFO BlockManagerInfo: Removed broadcast_146_piece0 on spark-master:40291 in memory (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 11:49:47 INFO BlockManagerInfo: Removed broadcast_146_piece0 on 172.18.0.14:40939 in memory (size: 107.2 KiB, free: 434.4 MiB)
26/02/13 11:49:47 INFO BlockManagerInfo: Removed broadcast_146_piece0 on 172.18.0.15:34959 in memory (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 11:49:47 INFO MemoryStore: Block broadcast_149_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.9 MiB)
26/02/13 11:49:47 INFO BlockManagerInfo: Added broadcast_149_piece0 in memory on spark-master:40291 (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 11:49:47 INFO SparkContext: Created broadcast 149 from start at NativeMethodAccessorImpl.java:0
26/02/13 11:49:47 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 45, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@17637a73]. The input RDD has 3 partitions.
26/02/13 11:49:47 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:49:47 INFO DAGScheduler: Got job 100 (start at NativeMethodAccessorImpl.java:0) with 3 output partitions
26/02/13 11:49:47 INFO DAGScheduler: Final stage: ResultStage 153 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:49:47 INFO DAGScheduler: Parents of final stage: List()
26/02/13 11:49:47 INFO DAGScheduler: Missing parents: List()
26/02/13 11:49:47 INFO DAGScheduler: Submitting ResultStage 153 (MapPartitionsRDD[544] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:49:47 INFO MemoryStore: Block broadcast_150 stored as values in memory (estimated size 49.9 KiB, free 433.9 MiB)
26/02/13 11:49:47 INFO MemoryStore: Block broadcast_150_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.9 MiB)
26/02/13 11:49:47 INFO BlockManagerInfo: Added broadcast_150_piece0 in memory on spark-master:40291 (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 11:49:47 INFO SparkContext: Created broadcast 150 from broadcast at DAGScheduler.scala:1585
26/02/13 11:49:47 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 153 (MapPartitionsRDD[544] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
26/02/13 11:49:47 INFO TaskSchedulerImpl: Adding task set 153.0 with 3 tasks resource profile 0
26/02/13 11:49:47 INFO TaskSetManager: Starting task 1.0 in stage 153.0 (TID 327) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:49:47 INFO TaskSetManager: Starting task 0.0 in stage 153.0 (TID 328) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:49:47 INFO TaskSetManager: Starting task 2.0 in stage 153.0 (TID 329) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:49:48 INFO BlockManagerInfo: Added broadcast_150_piece0 in memory on 172.18.0.15:34959 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 11:49:48 INFO BlockManagerInfo: Added broadcast_150_piece0 in memory on 172.18.0.14:40939 (size: 19.5 KiB, free: 434.4 MiB)
26/02/13 11:49:48 INFO BlockManagerInfo: Added broadcast_149_piece0 in memory on 172.18.0.15:34959 (size: 107.2 KiB, free: 434.0 MiB)
26/02/13 11:49:48 INFO BlockManagerInfo: Added broadcast_149_piece0 in memory on 172.18.0.14:40939 (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 11:49:48 INFO TaskSetManager: Finished task 2.0 in stage 153.0 (TID 329) in 573 ms on 172.18.0.14 (executor 1) (1/3)
26/02/13 11:49:48 INFO TaskSetManager: Finished task 0.0 in stage 153.0 (TID 328) in 577 ms on 172.18.0.14 (executor 1) (2/3)
26/02/13 11:49:48 INFO TaskSetManager: Finished task 1.0 in stage 153.0 (TID 327) in 590 ms on 172.18.0.15 (executor 0) (3/3)
26/02/13 11:49:48 INFO TaskSchedulerImpl: Removed TaskSet 153.0, whose tasks have all completed, from pool 
26/02/13 11:49:48 INFO DAGScheduler: ResultStage 153 (start at NativeMethodAccessorImpl.java:0) finished in 0.596 s
26/02/13 11:49:48 INFO DAGScheduler: Job 100 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:49:48 INFO TaskSchedulerImpl: Killing all running tasks in stage 153: Stage finished
26/02/13 11:49:48 INFO DAGScheduler: Job 100 finished: start at NativeMethodAccessorImpl.java:0, took 0.598726 s
26/02/13 11:49:48 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 45, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@17637a73] is committing.
26/02/13 11:49:48 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 45, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@17637a73] committed.
26/02/13 11:49:48 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/45 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.45.0cc83296-abc7-4996-a595-0128be78a114.tmp
26/02/13 11:49:48 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.45.0cc83296-abc7-4996-a595-0128be78a114.tmp to file:/tmp/spark-checkpoint-enrichment/commits/45
26/02/13 11:49:48 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "fc5c6452-bd70-4c90-becf-772d3bebdb57",
  "name" : null,
  "timestamp" : "2026-02-13T11:49:47.646Z",
  "batchId" : 45,
  "numInputRows" : 111,
  "inputRowsPerSecond" : 9250.0,
  "processedRowsPerSecond" : 111.55778894472361,
  "durationMs" : {
    "addBatch" : 796,
    "commitOffsets" : 60,
    "getBatch" : 1,
    "latestOffset" : 2,
    "queryPlanning" : 63,
    "triggerExecution" : 995,
    "walCommit" : 73
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 4495,
        "1" : 4933,
        "0" : 6674
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 4532,
        "1" : 4970,
        "0" : 6711
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 4532,
        "1" : 4970,
        "0" : 6711
      }
    },
    "numInputRows" : 111,
    "inputRowsPerSecond" : 9250.0,
    "processedRowsPerSecond" : 111.55778894472361,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@7fe9b5de",
    "numOutputRows" : 28
  }
}
26/02/13 11:49:48 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/46 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.46.00d33f90-f386-4dbf-a803-1dd68a8de19d.tmp
26/02/13 11:49:48 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.46.00d33f90-f386-4dbf-a803-1dd68a8de19d.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/46
26/02/13 11:49:48 INFO MicroBatchExecution: Committed offsets for batch 46. Metadata OffsetSeqMetadata(0,1770983388643,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 11:49:48 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:49:48 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:49:48 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:49:48 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:49:48 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:49:48 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:49:48 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#41235 - origin_code.nullCount#41234) > 0)
26/02/13 11:49:48 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#41240 - destination_code.nullCount#41239) > 0)
26/02/13 11:49:48 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#41270 - callsign.nullCount#41269) > 0)
26/02/13 11:49:48 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:49:48 INFO DAGScheduler: Got job 101 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 11:49:48 INFO DAGScheduler: Final stage: ResultStage 155 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:49:48 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 154)
26/02/13 11:49:48 INFO DAGScheduler: Missing parents: List()
26/02/13 11:49:48 INFO DAGScheduler: Submitting ResultStage 155 (MapPartitionsRDD[549] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:49:48 INFO MemoryStore: Block broadcast_151 stored as values in memory (estimated size 99.3 KiB, free 433.8 MiB)
26/02/13 11:49:48 INFO MemoryStore: Block broadcast_151_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 433.7 MiB)
26/02/13 11:49:48 INFO BlockManagerInfo: Added broadcast_151_piece0 in memory on spark-master:40291 (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 11:49:48 INFO SparkContext: Created broadcast 151 from broadcast at DAGScheduler.scala:1585
26/02/13 11:49:48 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 155 (MapPartitionsRDD[549] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 11:49:48 INFO TaskSchedulerImpl: Adding task set 155.0 with 4 tasks resource profile 0
26/02/13 11:49:48 INFO TaskSetManager: Starting task 0.0 in stage 155.0 (TID 330) (172.18.0.15, executor 0, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:49:48 INFO TaskSetManager: Starting task 1.0 in stage 155.0 (TID 331) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:49:48 INFO BlockManagerInfo: Added broadcast_151_piece0 in memory on 172.18.0.15:34959 (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 11:49:48 INFO TaskSetManager: Starting task 2.0 in stage 155.0 (TID 332) (172.18.0.15, executor 0, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:49:48 INFO TaskSetManager: Finished task 0.0 in stage 155.0 (TID 330) in 21 ms on 172.18.0.15 (executor 0) (1/4)
26/02/13 11:49:48 INFO TaskSetManager: Starting task 3.0 in stage 155.0 (TID 333) (172.18.0.15, executor 0, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:49:48 INFO TaskSetManager: Finished task 1.0 in stage 155.0 (TID 331) in 23 ms on 172.18.0.15 (executor 0) (2/4)
26/02/13 11:49:48 INFO TaskSetManager: Finished task 3.0 in stage 155.0 (TID 333) in 13 ms on 172.18.0.15 (executor 0) (3/4)
26/02/13 11:49:48 INFO TaskSetManager: Finished task 2.0 in stage 155.0 (TID 332) in 18 ms on 172.18.0.15 (executor 0) (4/4)
26/02/13 11:49:48 INFO TaskSchedulerImpl: Removed TaskSet 155.0, whose tasks have all completed, from pool 
26/02/13 11:49:48 INFO DAGScheduler: ResultStage 155 (start at NativeMethodAccessorImpl.java:0) finished in 0.045 s
26/02/13 11:49:48 INFO DAGScheduler: Job 101 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:49:48 INFO TaskSchedulerImpl: Killing all running tasks in stage 155: Stage finished
26/02/13 11:49:48 INFO DAGScheduler: Job 101 finished: start at NativeMethodAccessorImpl.java:0, took 0.048040 s
26/02/13 11:49:48 INFO MemoryStore: Block broadcast_152_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.6 MiB)
26/02/13 11:49:48 INFO BlockManagerInfo: Added broadcast_152_piece0 in memory on spark-master:40291 (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 11:49:48 INFO SparkContext: Created broadcast 152 from start at NativeMethodAccessorImpl.java:0
26/02/13 11:49:48 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 46, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@4b1b87e]. The input RDD has 3 partitions.
26/02/13 11:49:48 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:49:48 INFO DAGScheduler: Got job 102 (start at NativeMethodAccessorImpl.java:0) with 3 output partitions
26/02/13 11:49:48 INFO DAGScheduler: Final stage: ResultStage 156 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:49:48 INFO DAGScheduler: Parents of final stage: List()
26/02/13 11:49:48 INFO DAGScheduler: Missing parents: List()
26/02/13 11:49:48 INFO DAGScheduler: Submitting ResultStage 156 (MapPartitionsRDD[555] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:49:48 INFO MemoryStore: Block broadcast_153 stored as values in memory (estimated size 49.9 KiB, free 433.6 MiB)
26/02/13 11:49:48 INFO MemoryStore: Block broadcast_153_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.6 MiB)
26/02/13 11:49:48 INFO BlockManagerInfo: Added broadcast_153_piece0 in memory on spark-master:40291 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 11:49:48 INFO SparkContext: Created broadcast 153 from broadcast at DAGScheduler.scala:1585
26/02/13 11:49:48 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 156 (MapPartitionsRDD[555] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
26/02/13 11:49:48 INFO TaskSchedulerImpl: Adding task set 156.0 with 3 tasks resource profile 0
26/02/13 11:49:48 INFO TaskSetManager: Starting task 0.0 in stage 156.0 (TID 334) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:49:48 INFO TaskSetManager: Starting task 1.0 in stage 156.0 (TID 335) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:49:48 INFO TaskSetManager: Starting task 2.0 in stage 156.0 (TID 336) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:49:48 INFO BlockManagerInfo: Added broadcast_153_piece0 in memory on 172.18.0.15:34959 (size: 19.5 KiB, free: 434.0 MiB)
26/02/13 11:49:48 INFO BlockManagerInfo: Added broadcast_153_piece0 in memory on 172.18.0.14:40939 (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 11:49:48 INFO BlockManagerInfo: Added broadcast_152_piece0 in memory on 172.18.0.15:34959 (size: 107.2 KiB, free: 433.9 MiB)
26/02/13 11:49:48 INFO BlockManagerInfo: Added broadcast_152_piece0 in memory on 172.18.0.14:40939 (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 11:49:48 INFO TaskSetManager: Finished task 1.0 in stage 156.0 (TID 335) in 86 ms on 172.18.0.15 (executor 0) (1/3)
26/02/13 11:49:48 INFO TaskSetManager: Finished task 0.0 in stage 156.0 (TID 334) in 95 ms on 172.18.0.14 (executor 1) (2/3)
26/02/13 11:49:48 INFO TaskSetManager: Finished task 2.0 in stage 156.0 (TID 336) in 96 ms on 172.18.0.14 (executor 1) (3/3)
26/02/13 11:49:48 INFO TaskSchedulerImpl: Removed TaskSet 156.0, whose tasks have all completed, from pool 
26/02/13 11:49:48 INFO DAGScheduler: ResultStage 156 (start at NativeMethodAccessorImpl.java:0) finished in 0.101 s
26/02/13 11:49:48 INFO DAGScheduler: Job 102 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:49:48 INFO TaskSchedulerImpl: Killing all running tasks in stage 156: Stage finished
26/02/13 11:49:48 INFO DAGScheduler: Job 102 finished: start at NativeMethodAccessorImpl.java:0, took 0.103314 s
26/02/13 11:49:48 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 46, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@4b1b87e] is committing.
26/02/13 11:49:48 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 46, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@4b1b87e] committed.
26/02/13 11:49:49 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/46 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.46.2b2c7fb3-6710-4a55-a7a0-d605e96dce1c.tmp
26/02/13 11:49:49 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.46.2b2c7fb3-6710-4a55-a7a0-d605e96dce1c.tmp to file:/tmp/spark-checkpoint-enrichment/commits/46
26/02/13 11:49:49 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "fc5c6452-bd70-4c90-becf-772d3bebdb57",
  "name" : null,
  "timestamp" : "2026-02-13T11:49:48.642Z",
  "batchId" : 46,
  "numInputRows" : 128,
  "inputRowsPerSecond" : 128.5140562248996,
  "processedRowsPerSecond" : 303.3175355450237,
  "durationMs" : {
    "addBatch" : 255,
    "commitOffsets" : 80,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 28,
    "triggerExecution" : 422,
    "walCommit" : 58
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 4532,
        "1" : 4970,
        "0" : 6711
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 4576,
        "1" : 5016,
        "0" : 6749
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 4576,
        "1" : 5016,
        "0" : 6749
      }
    },
    "numInputRows" : 128,
    "inputRowsPerSecond" : 128.5140562248996,
    "processedRowsPerSecond" : 303.3175355450237,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@7fe9b5de",
    "numOutputRows" : 29
  }
}
26/02/13 11:49:51 INFO BlockManagerInfo: Removed broadcast_150_piece0 on spark-master:40291 in memory (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 11:49:51 INFO BlockManagerInfo: Removed broadcast_150_piece0 on 172.18.0.14:40939 in memory (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 11:49:51 INFO BlockManagerInfo: Removed broadcast_150_piece0 on 172.18.0.15:34959 in memory (size: 19.5 KiB, free: 433.9 MiB)
26/02/13 11:49:51 INFO BlockManagerInfo: Removed broadcast_151_piece0 on spark-master:40291 in memory (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 11:49:51 INFO BlockManagerInfo: Removed broadcast_151_piece0 on 172.18.0.15:34959 in memory (size: 33.3 KiB, free: 433.9 MiB)
26/02/13 11:49:51 INFO BlockManagerInfo: Removed broadcast_148_piece0 on spark-master:40291 in memory (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 11:49:51 INFO BlockManagerInfo: Removed broadcast_148_piece0 on 172.18.0.15:34959 in memory (size: 33.3 KiB, free: 433.9 MiB)
26/02/13 11:49:51 INFO BlockManagerInfo: Removed broadcast_149_piece0 on 172.18.0.15:34959 in memory (size: 107.2 KiB, free: 434.0 MiB)
26/02/13 11:49:51 INFO BlockManagerInfo: Removed broadcast_149_piece0 on 172.18.0.14:40939 in memory (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 11:49:51 INFO BlockManagerInfo: Removed broadcast_149_piece0 on spark-master:40291 in memory (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 11:49:51 INFO BlockManagerInfo: Removed broadcast_153_piece0 on spark-master:40291 in memory (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 11:49:51 INFO BlockManagerInfo: Removed broadcast_153_piece0 on 172.18.0.15:34959 in memory (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 11:49:51 INFO BlockManagerInfo: Removed broadcast_153_piece0 on 172.18.0.14:40939 in memory (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 11:49:59 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 11:50:03 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/47 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.47.d4657ea0-6e83-4dea-a786-4740c110681a.tmp
26/02/13 11:50:03 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.47.d4657ea0-6e83-4dea-a786-4740c110681a.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/47
26/02/13 11:50:03 INFO MicroBatchExecution: Committed offsets for batch 47. Metadata OffsetSeqMetadata(0,1770983403485,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 11:50:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:50:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:50:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:50:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:50:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:50:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:50:03 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#42089 - origin_code.nullCount#42088) > 0)
26/02/13 11:50:03 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#42094 - destination_code.nullCount#42093) > 0)
26/02/13 11:50:03 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#42124 - callsign.nullCount#42123) > 0)
26/02/13 11:50:03 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:50:03 INFO DAGScheduler: Got job 103 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 11:50:03 INFO DAGScheduler: Final stage: ResultStage 158 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:50:03 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 157)
26/02/13 11:50:03 INFO DAGScheduler: Missing parents: List()
26/02/13 11:50:03 INFO DAGScheduler: Submitting ResultStage 158 (MapPartitionsRDD[560] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:50:03 INFO MemoryStore: Block broadcast_154 stored as values in memory (estimated size 99.3 KiB, free 434.0 MiB)
26/02/13 11:50:03 INFO MemoryStore: Block broadcast_154_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 433.9 MiB)
26/02/13 11:50:03 INFO BlockManagerInfo: Added broadcast_154_piece0 in memory on spark-master:40291 (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 11:50:03 INFO SparkContext: Created broadcast 154 from broadcast at DAGScheduler.scala:1585
26/02/13 11:50:03 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 158 (MapPartitionsRDD[560] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 11:50:03 INFO TaskSchedulerImpl: Adding task set 158.0 with 4 tasks resource profile 0
26/02/13 11:50:03 INFO TaskSetManager: Starting task 0.0 in stage 158.0 (TID 337) (172.18.0.15, executor 0, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:50:03 INFO TaskSetManager: Starting task 1.0 in stage 158.0 (TID 338) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:50:03 INFO BlockManagerInfo: Added broadcast_154_piece0 in memory on 172.18.0.15:34959 (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 11:50:03 INFO TaskSetManager: Starting task 2.0 in stage 158.0 (TID 339) (172.18.0.15, executor 0, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:50:03 INFO TaskSetManager: Finished task 1.0 in stage 158.0 (TID 338) in 28 ms on 172.18.0.15 (executor 0) (1/4)
26/02/13 11:50:03 INFO TaskSetManager: Starting task 3.0 in stage 158.0 (TID 340) (172.18.0.15, executor 0, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:50:03 INFO TaskSetManager: Finished task 0.0 in stage 158.0 (TID 337) in 31 ms on 172.18.0.15 (executor 0) (2/4)
26/02/13 11:50:03 INFO TaskSetManager: Finished task 2.0 in stage 158.0 (TID 339) in 30 ms on 172.18.0.15 (executor 0) (3/4)
26/02/13 11:50:03 INFO TaskSetManager: Finished task 3.0 in stage 158.0 (TID 340) in 31 ms on 172.18.0.15 (executor 0) (4/4)
26/02/13 11:50:03 INFO TaskSchedulerImpl: Removed TaskSet 158.0, whose tasks have all completed, from pool 
26/02/13 11:50:03 INFO DAGScheduler: ResultStage 158 (start at NativeMethodAccessorImpl.java:0) finished in 0.073 s
26/02/13 11:50:03 INFO DAGScheduler: Job 103 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:50:03 INFO TaskSchedulerImpl: Killing all running tasks in stage 158: Stage finished
26/02/13 11:50:03 INFO DAGScheduler: Job 103 finished: start at NativeMethodAccessorImpl.java:0, took 0.077516 s
26/02/13 11:50:03 INFO MemoryStore: Block broadcast_155_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.8 MiB)
26/02/13 11:50:03 INFO BlockManagerInfo: Added broadcast_155_piece0 in memory on spark-master:40291 (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 11:50:03 INFO SparkContext: Created broadcast 155 from start at NativeMethodAccessorImpl.java:0
26/02/13 11:50:03 INFO BlockManagerInfo: Removed broadcast_152_piece0 on spark-master:40291 in memory (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 11:50:03 INFO BlockManagerInfo: Removed broadcast_152_piece0 on 172.18.0.14:40939 in memory (size: 107.2 KiB, free: 434.4 MiB)
26/02/13 11:50:03 INFO BlockManagerInfo: Removed broadcast_152_piece0 on 172.18.0.15:34959 in memory (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 11:50:03 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 47, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@37a8e202]. The input RDD has 3 partitions.
26/02/13 11:50:03 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:50:03 INFO DAGScheduler: Got job 104 (start at NativeMethodAccessorImpl.java:0) with 3 output partitions
26/02/13 11:50:03 INFO DAGScheduler: Final stage: ResultStage 159 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:50:03 INFO DAGScheduler: Parents of final stage: List()
26/02/13 11:50:03 INFO DAGScheduler: Missing parents: List()
26/02/13 11:50:03 INFO DAGScheduler: Submitting ResultStage 159 (MapPartitionsRDD[566] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:50:03 INFO MemoryStore: Block broadcast_156 stored as values in memory (estimated size 49.9 KiB, free 433.9 MiB)
26/02/13 11:50:03 INFO MemoryStore: Block broadcast_156_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.9 MiB)
26/02/13 11:50:03 INFO BlockManagerInfo: Added broadcast_156_piece0 in memory on spark-master:40291 (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 11:50:03 INFO SparkContext: Created broadcast 156 from broadcast at DAGScheduler.scala:1585
26/02/13 11:50:03 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 159 (MapPartitionsRDD[566] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
26/02/13 11:50:03 INFO TaskSchedulerImpl: Adding task set 159.0 with 3 tasks resource profile 0
26/02/13 11:50:03 INFO TaskSetManager: Starting task 0.0 in stage 159.0 (TID 341) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:50:03 INFO TaskSetManager: Starting task 1.0 in stage 159.0 (TID 342) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:50:03 INFO TaskSetManager: Starting task 2.0 in stage 159.0 (TID 343) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:50:03 INFO BlockManagerInfo: Added broadcast_156_piece0 in memory on 172.18.0.15:34959 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 11:50:03 INFO BlockManagerInfo: Added broadcast_156_piece0 in memory on 172.18.0.14:40939 (size: 19.5 KiB, free: 434.4 MiB)
26/02/13 11:50:03 INFO BlockManagerInfo: Added broadcast_155_piece0 in memory on 172.18.0.15:34959 (size: 107.2 KiB, free: 434.0 MiB)
26/02/13 11:50:03 INFO BlockManagerInfo: Added broadcast_155_piece0 in memory on 172.18.0.14:40939 (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 11:50:04 INFO TaskSetManager: Finished task 1.0 in stage 159.0 (TID 342) in 574 ms on 172.18.0.15 (executor 0) (1/3)
26/02/13 11:50:04 INFO TaskSetManager: Finished task 0.0 in stage 159.0 (TID 341) in 578 ms on 172.18.0.14 (executor 1) (2/3)
26/02/13 11:50:04 INFO TaskSetManager: Finished task 2.0 in stage 159.0 (TID 343) in 580 ms on 172.18.0.14 (executor 1) (3/3)
26/02/13 11:50:04 INFO TaskSchedulerImpl: Removed TaskSet 159.0, whose tasks have all completed, from pool 
26/02/13 11:50:04 INFO DAGScheduler: ResultStage 159 (start at NativeMethodAccessorImpl.java:0) finished in 0.588 s
26/02/13 11:50:04 INFO DAGScheduler: Job 104 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:50:04 INFO TaskSchedulerImpl: Killing all running tasks in stage 159: Stage finished
26/02/13 11:50:04 INFO DAGScheduler: Job 104 finished: start at NativeMethodAccessorImpl.java:0, took 0.589989 s
26/02/13 11:50:04 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 47, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@37a8e202] is committing.
26/02/13 11:50:04 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 47, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@37a8e202] committed.
26/02/13 11:50:04 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/47 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.47.1687f3b8-8f5e-4b52-9c90-84c8ba83fe2d.tmp
26/02/13 11:50:04 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.47.1687f3b8-8f5e-4b52-9c90-84c8ba83fe2d.tmp to file:/tmp/spark-checkpoint-enrichment/commits/47
26/02/13 11:50:04 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "fc5c6452-bd70-4c90-becf-772d3bebdb57",
  "name" : null,
  "timestamp" : "2026-02-13T11:50:03.483Z",
  "batchId" : 47,
  "numInputRows" : 8,
  "inputRowsPerSecond" : 727.2727272727274,
  "processedRowsPerSecond" : 7.699711260827719,
  "durationMs" : {
    "addBatch" : 843,
    "commitOffsets" : 57,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 51,
    "triggerExecution" : 1039,
    "walCommit" : 85
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 4576,
        "1" : 5016,
        "0" : 6749
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 4577,
        "1" : 5021,
        "0" : 6751
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 4577,
        "1" : 5021,
        "0" : 6751
      }
    },
    "numInputRows" : 8,
    "inputRowsPerSecond" : 727.2727272727274,
    "processedRowsPerSecond" : 7.699711260827719,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@7fe9b5de",
    "numOutputRows" : 3
  }
}
26/02/13 11:50:04 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/48 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.48.ca8cc4bf-02dd-4dab-93c4-e8f77c8a81a7.tmp
26/02/13 11:50:04 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.48.ca8cc4bf-02dd-4dab-93c4-e8f77c8a81a7.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/48
26/02/13 11:50:04 INFO MicroBatchExecution: Committed offsets for batch 48. Metadata OffsetSeqMetadata(0,1770983404524,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 11:50:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:50:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:50:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:50:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:50:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:50:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:50:04 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#42943 - origin_code.nullCount#42942) > 0)
26/02/13 11:50:04 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#42948 - destination_code.nullCount#42947) > 0)
26/02/13 11:50:04 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#42978 - callsign.nullCount#42977) > 0)
26/02/13 11:50:04 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:50:04 INFO DAGScheduler: Got job 105 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 11:50:04 INFO DAGScheduler: Final stage: ResultStage 161 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:50:04 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 160)
26/02/13 11:50:04 INFO DAGScheduler: Missing parents: List()
26/02/13 11:50:04 INFO DAGScheduler: Submitting ResultStage 161 (MapPartitionsRDD[571] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:50:04 INFO MemoryStore: Block broadcast_157 stored as values in memory (estimated size 99.3 KiB, free 433.8 MiB)
26/02/13 11:50:04 INFO MemoryStore: Block broadcast_157_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 433.7 MiB)
26/02/13 11:50:04 INFO BlockManagerInfo: Added broadcast_157_piece0 in memory on spark-master:40291 (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 11:50:04 INFO SparkContext: Created broadcast 157 from broadcast at DAGScheduler.scala:1585
26/02/13 11:50:04 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 161 (MapPartitionsRDD[571] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 11:50:04 INFO TaskSchedulerImpl: Adding task set 161.0 with 4 tasks resource profile 0
26/02/13 11:50:04 INFO TaskSetManager: Starting task 0.0 in stage 161.0 (TID 344) (172.18.0.15, executor 0, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:50:04 INFO TaskSetManager: Starting task 1.0 in stage 161.0 (TID 345) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:50:04 INFO BlockManagerInfo: Added broadcast_157_piece0 in memory on 172.18.0.15:34959 (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 11:50:04 INFO TaskSetManager: Starting task 2.0 in stage 161.0 (TID 346) (172.18.0.15, executor 0, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:50:04 INFO TaskSetManager: Finished task 1.0 in stage 161.0 (TID 345) in 18 ms on 172.18.0.15 (executor 0) (1/4)
26/02/13 11:50:04 INFO TaskSetManager: Starting task 3.0 in stage 161.0 (TID 347) (172.18.0.15, executor 0, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:50:04 INFO TaskSetManager: Finished task 0.0 in stage 161.0 (TID 344) in 25 ms on 172.18.0.15 (executor 0) (2/4)
26/02/13 11:50:04 INFO TaskSetManager: Finished task 2.0 in stage 161.0 (TID 346) in 24 ms on 172.18.0.15 (executor 0) (3/4)
26/02/13 11:50:04 INFO TaskSetManager: Finished task 3.0 in stage 161.0 (TID 347) in 24 ms on 172.18.0.15 (executor 0) (4/4)
26/02/13 11:50:04 INFO TaskSchedulerImpl: Removed TaskSet 161.0, whose tasks have all completed, from pool 
26/02/13 11:50:04 INFO DAGScheduler: ResultStage 161 (start at NativeMethodAccessorImpl.java:0) finished in 0.055 s
26/02/13 11:50:04 INFO DAGScheduler: Job 105 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:50:04 INFO TaskSchedulerImpl: Killing all running tasks in stage 161: Stage finished
26/02/13 11:50:04 INFO DAGScheduler: Job 105 finished: start at NativeMethodAccessorImpl.java:0, took 0.057470 s
26/02/13 11:50:04 INFO MemoryStore: Block broadcast_158_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.6 MiB)
26/02/13 11:50:04 INFO BlockManagerInfo: Added broadcast_158_piece0 in memory on spark-master:40291 (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 11:50:04 INFO SparkContext: Created broadcast 158 from start at NativeMethodAccessorImpl.java:0
26/02/13 11:50:04 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 48, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@61fced5]. The input RDD has 3 partitions.
26/02/13 11:50:04 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:50:04 INFO DAGScheduler: Got job 106 (start at NativeMethodAccessorImpl.java:0) with 3 output partitions
26/02/13 11:50:04 INFO DAGScheduler: Final stage: ResultStage 162 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:50:04 INFO DAGScheduler: Parents of final stage: List()
26/02/13 11:50:04 INFO DAGScheduler: Missing parents: List()
26/02/13 11:50:04 INFO DAGScheduler: Submitting ResultStage 162 (MapPartitionsRDD[577] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:50:04 INFO MemoryStore: Block broadcast_159 stored as values in memory (estimated size 49.9 KiB, free 433.6 MiB)
26/02/13 11:50:04 INFO MemoryStore: Block broadcast_159_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.6 MiB)
26/02/13 11:50:04 INFO BlockManagerInfo: Added broadcast_159_piece0 in memory on spark-master:40291 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 11:50:04 INFO SparkContext: Created broadcast 159 from broadcast at DAGScheduler.scala:1585
26/02/13 11:50:04 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 162 (MapPartitionsRDD[577] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
26/02/13 11:50:04 INFO TaskSchedulerImpl: Adding task set 162.0 with 3 tasks resource profile 0
26/02/13 11:50:04 INFO TaskSetManager: Starting task 1.0 in stage 162.0 (TID 348) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:50:04 INFO TaskSetManager: Starting task 0.0 in stage 162.0 (TID 349) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:50:04 INFO TaskSetManager: Starting task 2.0 in stage 162.0 (TID 350) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:50:04 INFO BlockManagerInfo: Added broadcast_159_piece0 in memory on 172.18.0.15:34959 (size: 19.5 KiB, free: 434.0 MiB)
26/02/13 11:50:04 INFO BlockManagerInfo: Added broadcast_159_piece0 in memory on 172.18.0.14:40939 (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 11:50:04 INFO BlockManagerInfo: Added broadcast_158_piece0 in memory on 172.18.0.15:34959 (size: 107.2 KiB, free: 433.9 MiB)
26/02/13 11:50:04 INFO BlockManagerInfo: Added broadcast_158_piece0 in memory on 172.18.0.14:40939 (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 11:50:04 INFO TaskSetManager: Finished task 1.0 in stage 162.0 (TID 348) in 99 ms on 172.18.0.15 (executor 0) (1/3)
26/02/13 11:50:04 INFO TaskSetManager: Finished task 2.0 in stage 162.0 (TID 350) in 120 ms on 172.18.0.14 (executor 1) (2/3)
26/02/13 11:50:04 INFO TaskSetManager: Finished task 0.0 in stage 162.0 (TID 349) in 125 ms on 172.18.0.14 (executor 1) (3/3)
26/02/13 11:50:04 INFO TaskSchedulerImpl: Removed TaskSet 162.0, whose tasks have all completed, from pool 
26/02/13 11:50:04 INFO DAGScheduler: ResultStage 162 (start at NativeMethodAccessorImpl.java:0) finished in 0.131 s
26/02/13 11:50:04 INFO DAGScheduler: Job 106 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:50:04 INFO TaskSchedulerImpl: Killing all running tasks in stage 162: Stage finished
26/02/13 11:50:04 INFO DAGScheduler: Job 106 finished: start at NativeMethodAccessorImpl.java:0, took 0.133372 s
26/02/13 11:50:04 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 48, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@61fced5] is committing.
26/02/13 11:50:04 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 48, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@61fced5] committed.
26/02/13 11:50:04 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/48 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.48.f8bae496-51c4-4cb7-a65f-316176703de3.tmp
26/02/13 11:50:04 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.48.f8bae496-51c4-4cb7-a65f-316176703de3.tmp to file:/tmp/spark-checkpoint-enrichment/commits/48
26/02/13 11:50:04 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "fc5c6452-bd70-4c90-becf-772d3bebdb57",
  "name" : null,
  "timestamp" : "2026-02-13T11:50:04.523Z",
  "batchId" : 48,
  "numInputRows" : 230,
  "inputRowsPerSecond" : 221.15384615384616,
  "processedRowsPerSecond" : 504.38596491228066,
  "durationMs" : {
    "addBatch" : 286,
    "commitOffsets" : 66,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 35,
    "triggerExecution" : 456,
    "walCommit" : 66
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 4577,
        "1" : 5021,
        "0" : 6751
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 4656,
        "1" : 5101,
        "0" : 6822
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 4656,
        "1" : 5101,
        "0" : 6822
      }
    },
    "numInputRows" : 230,
    "inputRowsPerSecond" : 221.15384615384616,
    "processedRowsPerSecond" : 504.38596491228066,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@7fe9b5de",
    "numOutputRows" : 53
  }
}
26/02/13 11:50:07 INFO BlockManagerInfo: Removed broadcast_155_piece0 on spark-master:40291 in memory (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 11:50:07 INFO BlockManagerInfo: Removed broadcast_155_piece0 on 172.18.0.15:34959 in memory (size: 107.2 KiB, free: 434.0 MiB)
26/02/13 11:50:07 INFO BlockManagerInfo: Removed broadcast_155_piece0 on 172.18.0.14:40939 in memory (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 11:50:07 INFO BlockManagerInfo: Removed broadcast_154_piece0 on spark-master:40291 in memory (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 11:50:07 INFO BlockManagerInfo: Removed broadcast_154_piece0 on 172.18.0.15:34959 in memory (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 11:50:07 INFO BlockManagerInfo: Removed broadcast_157_piece0 on spark-master:40291 in memory (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 11:50:07 INFO BlockManagerInfo: Removed broadcast_157_piece0 on 172.18.0.15:34959 in memory (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 11:50:07 INFO BlockManagerInfo: Removed broadcast_159_piece0 on spark-master:40291 in memory (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 11:50:07 INFO BlockManagerInfo: Removed broadcast_159_piece0 on 172.18.0.14:40939 in memory (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 11:50:07 INFO BlockManagerInfo: Removed broadcast_159_piece0 on 172.18.0.15:34959 in memory (size: 19.5 KiB, free: 434.0 MiB)
26/02/13 11:50:07 INFO BlockManagerInfo: Removed broadcast_156_piece0 on spark-master:40291 in memory (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 11:50:07 INFO BlockManagerInfo: Removed broadcast_156_piece0 on 172.18.0.15:34959 in memory (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 11:50:07 INFO BlockManagerInfo: Removed broadcast_156_piece0 on 172.18.0.14:40939 in memory (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 11:50:14 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 11:50:19 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/49 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.49.c89cfcff-6694-4c7c-9c85-5478cd07f191.tmp
26/02/13 11:50:19 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.49.c89cfcff-6694-4c7c-9c85-5478cd07f191.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/49
26/02/13 11:50:19 INFO MicroBatchExecution: Committed offsets for batch 49. Metadata OffsetSeqMetadata(0,1770983419792,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 11:50:19 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:50:19 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:50:19 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:50:19 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:50:20 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:50:20 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:50:20 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#43797 - origin_code.nullCount#43796) > 0)
26/02/13 11:50:20 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#43802 - destination_code.nullCount#43801) > 0)
26/02/13 11:50:20 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#43832 - callsign.nullCount#43831) > 0)
26/02/13 11:50:20 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:50:20 INFO DAGScheduler: Got job 107 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 11:50:20 INFO DAGScheduler: Final stage: ResultStage 164 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:50:20 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 163)
26/02/13 11:50:20 INFO DAGScheduler: Missing parents: List()
26/02/13 11:50:20 INFO DAGScheduler: Submitting ResultStage 164 (MapPartitionsRDD[582] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:50:20 INFO MemoryStore: Block broadcast_160 stored as values in memory (estimated size 99.3 KiB, free 434.0 MiB)
26/02/13 11:50:20 INFO MemoryStore: Block broadcast_160_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 433.9 MiB)
26/02/13 11:50:20 INFO BlockManagerInfo: Added broadcast_160_piece0 in memory on spark-master:40291 (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 11:50:20 INFO SparkContext: Created broadcast 160 from broadcast at DAGScheduler.scala:1585
26/02/13 11:50:20 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 164 (MapPartitionsRDD[582] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 11:50:20 INFO TaskSchedulerImpl: Adding task set 164.0 with 4 tasks resource profile 0
26/02/13 11:50:20 INFO TaskSetManager: Starting task 0.0 in stage 164.0 (TID 351) (172.18.0.15, executor 0, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:50:20 INFO TaskSetManager: Starting task 1.0 in stage 164.0 (TID 352) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:50:20 INFO BlockManagerInfo: Added broadcast_160_piece0 in memory on 172.18.0.15:34959 (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 11:50:20 INFO TaskSetManager: Starting task 2.0 in stage 164.0 (TID 353) (172.18.0.15, executor 0, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:50:20 INFO TaskSetManager: Starting task 3.0 in stage 164.0 (TID 354) (172.18.0.15, executor 0, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:50:20 INFO TaskSetManager: Finished task 1.0 in stage 164.0 (TID 352) in 32 ms on 172.18.0.15 (executor 0) (1/4)
26/02/13 11:50:20 INFO TaskSetManager: Finished task 0.0 in stage 164.0 (TID 351) in 34 ms on 172.18.0.15 (executor 0) (2/4)
26/02/13 11:50:20 INFO TaskSetManager: Finished task 3.0 in stage 164.0 (TID 354) in 30 ms on 172.18.0.15 (executor 0) (3/4)
26/02/13 11:50:20 INFO BlockManagerInfo: Removed broadcast_158_piece0 on spark-master:40291 in memory (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 11:50:20 INFO TaskSetManager: Finished task 2.0 in stage 164.0 (TID 353) in 32 ms on 172.18.0.15 (executor 0) (4/4)
26/02/13 11:50:20 INFO TaskSchedulerImpl: Removed TaskSet 164.0, whose tasks have all completed, from pool 
26/02/13 11:50:20 INFO DAGScheduler: ResultStage 164 (start at NativeMethodAccessorImpl.java:0) finished in 0.077 s
26/02/13 11:50:20 INFO DAGScheduler: Job 107 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:50:20 INFO TaskSchedulerImpl: Killing all running tasks in stage 164: Stage finished
26/02/13 11:50:20 INFO DAGScheduler: Job 107 finished: start at NativeMethodAccessorImpl.java:0, took 0.081374 s
26/02/13 11:50:20 INFO BlockManagerInfo: Removed broadcast_158_piece0 on 172.18.0.15:34959 in memory (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 11:50:20 INFO BlockManagerInfo: Removed broadcast_158_piece0 on 172.18.0.14:40939 in memory (size: 107.2 KiB, free: 434.4 MiB)
26/02/13 11:50:20 INFO MemoryStore: Block broadcast_161_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.9 MiB)
26/02/13 11:50:20 INFO BlockManagerInfo: Added broadcast_161_piece0 in memory on spark-master:40291 (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 11:50:20 INFO SparkContext: Created broadcast 161 from start at NativeMethodAccessorImpl.java:0
26/02/13 11:50:20 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 49, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@1b4ce0c1]. The input RDD has 3 partitions.
26/02/13 11:50:20 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:50:20 INFO DAGScheduler: Got job 108 (start at NativeMethodAccessorImpl.java:0) with 3 output partitions
26/02/13 11:50:20 INFO DAGScheduler: Final stage: ResultStage 165 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:50:20 INFO DAGScheduler: Parents of final stage: List()
26/02/13 11:50:20 INFO DAGScheduler: Missing parents: List()
26/02/13 11:50:20 INFO DAGScheduler: Submitting ResultStage 165 (MapPartitionsRDD[588] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:50:20 INFO MemoryStore: Block broadcast_162 stored as values in memory (estimated size 49.9 KiB, free 433.9 MiB)
26/02/13 11:50:20 INFO MemoryStore: Block broadcast_162_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.9 MiB)
26/02/13 11:50:20 INFO BlockManagerInfo: Added broadcast_162_piece0 in memory on spark-master:40291 (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 11:50:20 INFO SparkContext: Created broadcast 162 from broadcast at DAGScheduler.scala:1585
26/02/13 11:50:20 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 165 (MapPartitionsRDD[588] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
26/02/13 11:50:20 INFO TaskSchedulerImpl: Adding task set 165.0 with 3 tasks resource profile 0
26/02/13 11:50:20 INFO TaskSetManager: Starting task 1.0 in stage 165.0 (TID 355) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:50:20 INFO TaskSetManager: Starting task 0.0 in stage 165.0 (TID 356) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:50:20 INFO TaskSetManager: Starting task 2.0 in stage 165.0 (TID 357) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:50:20 INFO BlockManagerInfo: Added broadcast_162_piece0 in memory on 172.18.0.14:40939 (size: 19.5 KiB, free: 434.4 MiB)
26/02/13 11:50:20 INFO BlockManagerInfo: Added broadcast_162_piece0 in memory on 172.18.0.15:34959 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 11:50:20 INFO BlockManagerInfo: Added broadcast_161_piece0 in memory on 172.18.0.14:40939 (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 11:50:20 INFO BlockManagerInfo: Added broadcast_161_piece0 in memory on 172.18.0.15:34959 (size: 107.2 KiB, free: 434.0 MiB)
26/02/13 11:50:20 INFO TaskSetManager: Finished task 1.0 in stage 165.0 (TID 355) in 578 ms on 172.18.0.15 (executor 0) (1/3)
26/02/13 11:50:20 INFO TaskSetManager: Finished task 2.0 in stage 165.0 (TID 357) in 583 ms on 172.18.0.14 (executor 1) (2/3)
26/02/13 11:50:20 INFO TaskSetManager: Finished task 0.0 in stage 165.0 (TID 356) in 587 ms on 172.18.0.14 (executor 1) (3/3)
26/02/13 11:50:20 INFO TaskSchedulerImpl: Removed TaskSet 165.0, whose tasks have all completed, from pool 
26/02/13 11:50:20 INFO DAGScheduler: ResultStage 165 (start at NativeMethodAccessorImpl.java:0) finished in 0.594 s
26/02/13 11:50:20 INFO DAGScheduler: Job 108 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:50:20 INFO TaskSchedulerImpl: Killing all running tasks in stage 165: Stage finished
26/02/13 11:50:20 INFO DAGScheduler: Job 108 finished: start at NativeMethodAccessorImpl.java:0, took 0.595699 s
26/02/13 11:50:20 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 49, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@1b4ce0c1] is committing.
26/02/13 11:50:20 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 49, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@1b4ce0c1] committed.
26/02/13 11:50:20 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/49 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.49.ee4a110c-782e-40ea-ad0c-5c1d2adb5106.tmp
26/02/13 11:50:20 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.49.ee4a110c-782e-40ea-ad0c-5c1d2adb5106.tmp to file:/tmp/spark-checkpoint-enrichment/commits/49
26/02/13 11:50:20 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "fc5c6452-bd70-4c90-becf-772d3bebdb57",
  "name" : null,
  "timestamp" : "2026-02-13T11:50:19.791Z",
  "batchId" : 49,
  "numInputRows" : 222,
  "inputRowsPerSecond" : 20181.818181818184,
  "processedRowsPerSecond" : 218.50393700787401,
  "durationMs" : {
    "addBatch" : 770,
    "commitOffsets" : 72,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 48,
    "triggerExecution" : 1016,
    "walCommit" : 124
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 4656,
        "1" : 5101,
        "0" : 6822
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 4730,
        "1" : 5175,
        "0" : 6896
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 4730,
        "1" : 5175,
        "0" : 6896
      }
    },
    "numInputRows" : 222,
    "inputRowsPerSecond" : 20181.818181818184,
    "processedRowsPerSecond" : 218.50393700787401,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@7fe9b5de",
    "numOutputRows" : 53
  }
}
26/02/13 11:50:20 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/50 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.50.87becfd8-0674-44d8-a901-e919ef745629.tmp
26/02/13 11:50:20 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.50.87becfd8-0674-44d8-a901-e919ef745629.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/50
26/02/13 11:50:20 INFO MicroBatchExecution: Committed offsets for batch 50. Metadata OffsetSeqMetadata(0,1770983420809,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 11:50:20 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:50:20 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:50:20 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:50:20 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:50:20 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:50:20 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:50:20 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#44651 - origin_code.nullCount#44650) > 0)
26/02/13 11:50:20 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#44656 - destination_code.nullCount#44655) > 0)
26/02/13 11:50:20 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#44686 - callsign.nullCount#44685) > 0)
26/02/13 11:50:21 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:50:21 INFO DAGScheduler: Got job 109 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 11:50:21 INFO DAGScheduler: Final stage: ResultStage 167 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:50:21 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 166)
26/02/13 11:50:21 INFO DAGScheduler: Missing parents: List()
26/02/13 11:50:21 INFO DAGScheduler: Submitting ResultStage 167 (MapPartitionsRDD[593] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:50:21 INFO MemoryStore: Block broadcast_163 stored as values in memory (estimated size 99.3 KiB, free 433.8 MiB)
26/02/13 11:50:21 INFO MemoryStore: Block broadcast_163_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 433.7 MiB)
26/02/13 11:50:21 INFO BlockManagerInfo: Added broadcast_163_piece0 in memory on spark-master:40291 (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 11:50:21 INFO SparkContext: Created broadcast 163 from broadcast at DAGScheduler.scala:1585
26/02/13 11:50:21 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 167 (MapPartitionsRDD[593] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 11:50:21 INFO TaskSchedulerImpl: Adding task set 167.0 with 4 tasks resource profile 0
26/02/13 11:50:21 INFO TaskSetManager: Starting task 0.0 in stage 167.0 (TID 358) (172.18.0.15, executor 0, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:50:21 INFO TaskSetManager: Starting task 1.0 in stage 167.0 (TID 359) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:50:21 INFO BlockManagerInfo: Added broadcast_163_piece0 in memory on 172.18.0.15:34959 (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 11:50:21 INFO TaskSetManager: Starting task 2.0 in stage 167.0 (TID 360) (172.18.0.15, executor 0, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:50:21 INFO TaskSetManager: Starting task 3.0 in stage 167.0 (TID 361) (172.18.0.15, executor 0, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:50:21 INFO TaskSetManager: Finished task 1.0 in stage 167.0 (TID 359) in 27 ms on 172.18.0.15 (executor 0) (1/4)
26/02/13 11:50:21 INFO TaskSetManager: Finished task 0.0 in stage 167.0 (TID 358) in 29 ms on 172.18.0.15 (executor 0) (2/4)
26/02/13 11:50:21 INFO TaskSetManager: Finished task 2.0 in stage 167.0 (TID 360) in 16 ms on 172.18.0.15 (executor 0) (3/4)
26/02/13 11:50:21 INFO TaskSetManager: Finished task 3.0 in stage 167.0 (TID 361) in 22 ms on 172.18.0.15 (executor 0) (4/4)
26/02/13 11:50:21 INFO TaskSchedulerImpl: Removed TaskSet 167.0, whose tasks have all completed, from pool 
26/02/13 11:50:21 INFO DAGScheduler: ResultStage 167 (start at NativeMethodAccessorImpl.java:0) finished in 0.056 s
26/02/13 11:50:21 INFO DAGScheduler: Job 109 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:50:21 INFO TaskSchedulerImpl: Killing all running tasks in stage 167: Stage finished
26/02/13 11:50:21 INFO DAGScheduler: Job 109 finished: start at NativeMethodAccessorImpl.java:0, took 0.058596 s
26/02/13 11:50:21 INFO MemoryStore: Block broadcast_164_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.6 MiB)
26/02/13 11:50:21 INFO BlockManagerInfo: Added broadcast_164_piece0 in memory on spark-master:40291 (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 11:50:21 INFO SparkContext: Created broadcast 164 from start at NativeMethodAccessorImpl.java:0
26/02/13 11:50:21 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 50, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@412529cf]. The input RDD has 2 partitions.
26/02/13 11:50:21 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:50:21 INFO DAGScheduler: Got job 110 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/02/13 11:50:21 INFO DAGScheduler: Final stage: ResultStage 168 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:50:21 INFO DAGScheduler: Parents of final stage: List()
26/02/13 11:50:21 INFO DAGScheduler: Missing parents: List()
26/02/13 11:50:21 INFO DAGScheduler: Submitting ResultStage 168 (MapPartitionsRDD[599] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:50:21 INFO MemoryStore: Block broadcast_165 stored as values in memory (estimated size 49.9 KiB, free 433.6 MiB)
26/02/13 11:50:21 INFO MemoryStore: Block broadcast_165_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.6 MiB)
26/02/13 11:50:21 INFO BlockManagerInfo: Added broadcast_165_piece0 in memory on spark-master:40291 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 11:50:21 INFO SparkContext: Created broadcast 165 from broadcast at DAGScheduler.scala:1585
26/02/13 11:50:21 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 168 (MapPartitionsRDD[599] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/02/13 11:50:21 INFO TaskSchedulerImpl: Adding task set 168.0 with 2 tasks resource profile 0
26/02/13 11:50:21 INFO TaskSetManager: Starting task 0.0 in stage 168.0 (TID 362) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:50:21 INFO TaskSetManager: Starting task 1.0 in stage 168.0 (TID 363) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:50:21 INFO BlockManagerInfo: Added broadcast_165_piece0 in memory on 172.18.0.15:34959 (size: 19.5 KiB, free: 434.0 MiB)
26/02/13 11:50:21 INFO BlockManagerInfo: Added broadcast_165_piece0 in memory on 172.18.0.14:40939 (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 11:50:21 INFO BlockManagerInfo: Added broadcast_164_piece0 in memory on 172.18.0.15:34959 (size: 107.2 KiB, free: 433.9 MiB)
26/02/13 11:50:21 INFO BlockManagerInfo: Added broadcast_164_piece0 in memory on 172.18.0.14:40939 (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 11:50:21 INFO TaskSetManager: Finished task 1.0 in stage 168.0 (TID 363) in 50 ms on 172.18.0.15 (executor 0) (1/2)
26/02/13 11:50:21 INFO TaskSetManager: Finished task 0.0 in stage 168.0 (TID 362) in 51 ms on 172.18.0.14 (executor 1) (2/2)
26/02/13 11:50:21 INFO TaskSchedulerImpl: Removed TaskSet 168.0, whose tasks have all completed, from pool 
26/02/13 11:50:21 INFO DAGScheduler: ResultStage 168 (start at NativeMethodAccessorImpl.java:0) finished in 0.058 s
26/02/13 11:50:21 INFO DAGScheduler: Job 110 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:50:21 INFO TaskSchedulerImpl: Killing all running tasks in stage 168: Stage finished
26/02/13 11:50:21 INFO DAGScheduler: Job 110 finished: start at NativeMethodAccessorImpl.java:0, took 0.060192 s
26/02/13 11:50:21 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 50, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@412529cf] is committing.
26/02/13 11:50:21 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 50, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@412529cf] committed.
26/02/13 11:50:21 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/50 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.50.5af698f0-4951-45e4-bf0b-860de690aea8.tmp
26/02/13 11:50:21 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.50.5af698f0-4951-45e4-bf0b-860de690aea8.tmp to file:/tmp/spark-checkpoint-enrichment/commits/50
26/02/13 11:50:21 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "fc5c6452-bd70-4c90-becf-772d3bebdb57",
  "name" : null,
  "timestamp" : "2026-02-13T11:50:20.808Z",
  "batchId" : 50,
  "numInputRows" : 17,
  "inputRowsPerSecond" : 16.71583087512291,
  "processedRowsPerSecond" : 42.82115869017632,
  "durationMs" : {
    "addBatch" : 223,
    "commitOffsets" : 69,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 35,
    "triggerExecution" : 397,
    "walCommit" : 68
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 4730,
        "1" : 5175,
        "0" : 6896
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 4736,
        "1" : 5186,
        "0" : 6896
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 4736,
        "1" : 5186,
        "0" : 6896
      }
    },
    "numInputRows" : 17,
    "inputRowsPerSecond" : 16.71583087512291,
    "processedRowsPerSecond" : 42.82115869017632,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@7fe9b5de",
    "numOutputRows" : 3
  }
}
26/02/13 11:50:23 INFO BlockManagerInfo: Removed broadcast_162_piece0 on spark-master:40291 in memory (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 11:50:23 INFO BlockManagerInfo: Removed broadcast_162_piece0 on 172.18.0.14:40939 in memory (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 11:50:23 INFO BlockManagerInfo: Removed broadcast_162_piece0 on 172.18.0.15:34959 in memory (size: 19.5 KiB, free: 433.9 MiB)
26/02/13 11:50:23 INFO BlockManagerInfo: Removed broadcast_163_piece0 on spark-master:40291 in memory (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 11:50:23 INFO BlockManagerInfo: Removed broadcast_163_piece0 on 172.18.0.15:34959 in memory (size: 33.3 KiB, free: 433.9 MiB)
26/02/13 11:50:23 INFO BlockManagerInfo: Removed broadcast_160_piece0 on spark-master:40291 in memory (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 11:50:23 INFO BlockManagerInfo: Removed broadcast_160_piece0 on 172.18.0.15:34959 in memory (size: 33.3 KiB, free: 433.9 MiB)
26/02/13 11:50:23 INFO BlockManagerInfo: Removed broadcast_161_piece0 on spark-master:40291 in memory (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 11:50:23 INFO BlockManagerInfo: Removed broadcast_161_piece0 on 172.18.0.15:34959 in memory (size: 107.2 KiB, free: 434.0 MiB)
26/02/13 11:50:23 INFO BlockManagerInfo: Removed broadcast_161_piece0 on 172.18.0.14:40939 in memory (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 11:50:23 INFO BlockManagerInfo: Removed broadcast_165_piece0 on spark-master:40291 in memory (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 11:50:23 INFO BlockManagerInfo: Removed broadcast_165_piece0 on 172.18.0.15:34959 in memory (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 11:50:23 INFO BlockManagerInfo: Removed broadcast_165_piece0 on 172.18.0.14:40939 in memory (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 11:50:31 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 11:50:36 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/51 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.51.472a996f-9542-4b69-9452-fc6fb7f1c12f.tmp
26/02/13 11:50:36 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.51.472a996f-9542-4b69-9452-fc6fb7f1c12f.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/51
26/02/13 11:50:36 INFO MicroBatchExecution: Committed offsets for batch 51. Metadata OffsetSeqMetadata(0,1770983436284,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 11:50:36 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:50:36 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:50:36 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:50:36 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:50:36 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:50:36 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:50:36 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#45505 - origin_code.nullCount#45504) > 0)
26/02/13 11:50:36 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#45510 - destination_code.nullCount#45509) > 0)
26/02/13 11:50:36 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#45540 - callsign.nullCount#45539) > 0)
26/02/13 11:50:36 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:50:36 INFO DAGScheduler: Got job 111 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 11:50:36 INFO DAGScheduler: Final stage: ResultStage 170 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:50:36 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 169)
26/02/13 11:50:36 INFO DAGScheduler: Missing parents: List()
26/02/13 11:50:36 INFO DAGScheduler: Submitting ResultStage 170 (MapPartitionsRDD[604] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:50:36 INFO MemoryStore: Block broadcast_166 stored as values in memory (estimated size 99.3 KiB, free 434.0 MiB)
26/02/13 11:50:36 INFO MemoryStore: Block broadcast_166_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 433.9 MiB)
26/02/13 11:50:36 INFO BlockManagerInfo: Added broadcast_166_piece0 in memory on spark-master:40291 (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 11:50:36 INFO SparkContext: Created broadcast 166 from broadcast at DAGScheduler.scala:1585
26/02/13 11:50:36 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 170 (MapPartitionsRDD[604] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 11:50:36 INFO TaskSchedulerImpl: Adding task set 170.0 with 4 tasks resource profile 0
26/02/13 11:50:36 INFO TaskSetManager: Starting task 0.0 in stage 170.0 (TID 364) (172.18.0.15, executor 0, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:50:36 INFO TaskSetManager: Starting task 1.0 in stage 170.0 (TID 365) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:50:36 INFO BlockManagerInfo: Added broadcast_166_piece0 in memory on 172.18.0.15:34959 (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 11:50:36 INFO TaskSetManager: Starting task 2.0 in stage 170.0 (TID 366) (172.18.0.15, executor 0, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:50:36 INFO TaskSetManager: Finished task 0.0 in stage 170.0 (TID 364) in 22 ms on 172.18.0.15 (executor 0) (1/4)
26/02/13 11:50:36 INFO TaskSetManager: Starting task 3.0 in stage 170.0 (TID 367) (172.18.0.15, executor 0, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:50:36 INFO TaskSetManager: Finished task 1.0 in stage 170.0 (TID 365) in 26 ms on 172.18.0.15 (executor 0) (2/4)
26/02/13 11:50:36 INFO TaskSetManager: Finished task 2.0 in stage 170.0 (TID 366) in 16 ms on 172.18.0.15 (executor 0) (3/4)
26/02/13 11:50:36 INFO TaskSetManager: Finished task 3.0 in stage 170.0 (TID 367) in 17 ms on 172.18.0.15 (executor 0) (4/4)
26/02/13 11:50:36 INFO TaskSchedulerImpl: Removed TaskSet 170.0, whose tasks have all completed, from pool 
26/02/13 11:50:36 INFO DAGScheduler: ResultStage 170 (start at NativeMethodAccessorImpl.java:0) finished in 0.054 s
26/02/13 11:50:36 INFO DAGScheduler: Job 111 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:50:36 INFO TaskSchedulerImpl: Killing all running tasks in stage 170: Stage finished
26/02/13 11:50:36 INFO DAGScheduler: Job 111 finished: start at NativeMethodAccessorImpl.java:0, took 0.057691 s
26/02/13 11:50:36 INFO MemoryStore: Block broadcast_167_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.8 MiB)
26/02/13 11:50:36 INFO BlockManagerInfo: Added broadcast_167_piece0 in memory on spark-master:40291 (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 11:50:36 INFO SparkContext: Created broadcast 167 from start at NativeMethodAccessorImpl.java:0
26/02/13 11:50:36 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 51, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@76565bf6]. The input RDD has 3 partitions.
26/02/13 11:50:36 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:50:36 INFO DAGScheduler: Got job 112 (start at NativeMethodAccessorImpl.java:0) with 3 output partitions
26/02/13 11:50:36 INFO DAGScheduler: Final stage: ResultStage 171 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:50:36 INFO DAGScheduler: Parents of final stage: List()
26/02/13 11:50:36 INFO DAGScheduler: Missing parents: List()
26/02/13 11:50:36 INFO DAGScheduler: Submitting ResultStage 171 (MapPartitionsRDD[610] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:50:36 INFO BlockManagerInfo: Removed broadcast_164_piece0 on spark-master:40291 in memory (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 11:50:36 INFO BlockManagerInfo: Removed broadcast_164_piece0 on 172.18.0.15:34959 in memory (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 11:50:36 INFO MemoryStore: Block broadcast_168 stored as values in memory (estimated size 49.9 KiB, free 433.9 MiB)
26/02/13 11:50:36 INFO BlockManagerInfo: Removed broadcast_164_piece0 on 172.18.0.14:40939 in memory (size: 107.2 KiB, free: 434.4 MiB)
26/02/13 11:50:36 INFO MemoryStore: Block broadcast_168_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.9 MiB)
26/02/13 11:50:36 INFO BlockManagerInfo: Added broadcast_168_piece0 in memory on spark-master:40291 (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 11:50:36 INFO SparkContext: Created broadcast 168 from broadcast at DAGScheduler.scala:1585
26/02/13 11:50:36 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 171 (MapPartitionsRDD[610] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
26/02/13 11:50:36 INFO TaskSchedulerImpl: Adding task set 171.0 with 3 tasks resource profile 0
26/02/13 11:50:36 INFO TaskSetManager: Starting task 1.0 in stage 171.0 (TID 368) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:50:36 INFO TaskSetManager: Starting task 0.0 in stage 171.0 (TID 369) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:50:36 INFO TaskSetManager: Starting task 2.0 in stage 171.0 (TID 370) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:50:36 INFO BlockManagerInfo: Added broadcast_168_piece0 in memory on 172.18.0.15:34959 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 11:50:36 INFO BlockManagerInfo: Added broadcast_168_piece0 in memory on 172.18.0.14:40939 (size: 19.5 KiB, free: 434.4 MiB)
26/02/13 11:50:36 INFO BlockManagerInfo: Added broadcast_167_piece0 in memory on 172.18.0.15:34959 (size: 107.2 KiB, free: 434.0 MiB)
26/02/13 11:50:36 INFO BlockManagerInfo: Added broadcast_167_piece0 in memory on 172.18.0.14:40939 (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 11:50:37 INFO TaskSetManager: Finished task 2.0 in stage 171.0 (TID 370) in 573 ms on 172.18.0.14 (executor 1) (1/3)
26/02/13 11:50:37 INFO TaskSetManager: Finished task 1.0 in stage 171.0 (TID 368) in 578 ms on 172.18.0.15 (executor 0) (2/3)
26/02/13 11:50:37 INFO TaskSetManager: Finished task 0.0 in stage 171.0 (TID 369) in 579 ms on 172.18.0.14 (executor 1) (3/3)
26/02/13 11:50:37 INFO TaskSchedulerImpl: Removed TaskSet 171.0, whose tasks have all completed, from pool 
26/02/13 11:50:37 INFO DAGScheduler: ResultStage 171 (start at NativeMethodAccessorImpl.java:0) finished in 0.585 s
26/02/13 11:50:37 INFO DAGScheduler: Job 112 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:50:37 INFO TaskSchedulerImpl: Killing all running tasks in stage 171: Stage finished
26/02/13 11:50:37 INFO DAGScheduler: Job 112 finished: start at NativeMethodAccessorImpl.java:0, took 0.594821 s
26/02/13 11:50:37 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 51, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@76565bf6] is committing.
26/02/13 11:50:37 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 51, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@76565bf6] committed.
26/02/13 11:50:37 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/51 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.51.311530e1-e148-452d-b787-02cdab14b6e5.tmp
26/02/13 11:50:37 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.51.311530e1-e148-452d-b787-02cdab14b6e5.tmp to file:/tmp/spark-checkpoint-enrichment/commits/51
26/02/13 11:50:37 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "fc5c6452-bd70-4c90-becf-772d3bebdb57",
  "name" : null,
  "timestamp" : "2026-02-13T11:50:36.282Z",
  "batchId" : 51,
  "numInputRows" : 87,
  "inputRowsPerSecond" : 7250.0,
  "processedRowsPerSecond" : 89.87603305785125,
  "durationMs" : {
    "addBatch" : 754,
    "commitOffsets" : 57,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 50,
    "triggerExecution" : 968,
    "walCommit" : 104
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 4736,
        "1" : 5186,
        "0" : 6896
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 4765,
        "1" : 5218,
        "0" : 6922
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 4765,
        "1" : 5218,
        "0" : 6922
      }
    },
    "numInputRows" : 87,
    "inputRowsPerSecond" : 7250.0,
    "processedRowsPerSecond" : 89.87603305785125,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@7fe9b5de",
    "numOutputRows" : 23
  }
}
26/02/13 11:50:37 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/52 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.52.a60c7266-9088-42c8-969b-ab03ab1f69a9.tmp
26/02/13 11:50:37 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.52.a60c7266-9088-42c8-969b-ab03ab1f69a9.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/52
26/02/13 11:50:37 INFO MicroBatchExecution: Committed offsets for batch 52. Metadata OffsetSeqMetadata(0,1770983437253,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 11:50:37 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:50:37 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:50:37 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:50:37 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:50:37 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:50:37 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:50:37 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#46359 - origin_code.nullCount#46358) > 0)
26/02/13 11:50:37 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#46364 - destination_code.nullCount#46363) > 0)
26/02/13 11:50:37 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#46394 - callsign.nullCount#46393) > 0)
26/02/13 11:50:37 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:50:37 INFO DAGScheduler: Got job 113 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 11:50:37 INFO DAGScheduler: Final stage: ResultStage 173 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:50:37 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 172)
26/02/13 11:50:37 INFO DAGScheduler: Missing parents: List()
26/02/13 11:50:37 INFO DAGScheduler: Submitting ResultStage 173 (MapPartitionsRDD[615] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:50:37 INFO MemoryStore: Block broadcast_169 stored as values in memory (estimated size 99.3 KiB, free 433.8 MiB)
26/02/13 11:50:37 INFO MemoryStore: Block broadcast_169_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 433.7 MiB)
26/02/13 11:50:37 INFO BlockManagerInfo: Added broadcast_169_piece0 in memory on spark-master:40291 (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 11:50:37 INFO SparkContext: Created broadcast 169 from broadcast at DAGScheduler.scala:1585
26/02/13 11:50:37 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 173 (MapPartitionsRDD[615] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 11:50:37 INFO TaskSchedulerImpl: Adding task set 173.0 with 4 tasks resource profile 0
26/02/13 11:50:37 INFO TaskSetManager: Starting task 0.0 in stage 173.0 (TID 371) (172.18.0.15, executor 0, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:50:37 INFO TaskSetManager: Starting task 1.0 in stage 173.0 (TID 372) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:50:37 INFO BlockManagerInfo: Added broadcast_169_piece0 in memory on 172.18.0.15:34959 (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 11:50:37 INFO TaskSetManager: Starting task 2.0 in stage 173.0 (TID 373) (172.18.0.15, executor 0, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:50:37 INFO TaskSetManager: Starting task 3.0 in stage 173.0 (TID 374) (172.18.0.15, executor 0, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:50:37 INFO TaskSetManager: Finished task 0.0 in stage 173.0 (TID 371) in 21 ms on 172.18.0.15 (executor 0) (1/4)
26/02/13 11:50:37 INFO TaskSetManager: Finished task 1.0 in stage 173.0 (TID 372) in 22 ms on 172.18.0.15 (executor 0) (2/4)
26/02/13 11:50:37 INFO TaskSetManager: Finished task 3.0 in stage 173.0 (TID 374) in 11 ms on 172.18.0.15 (executor 0) (3/4)
26/02/13 11:50:37 INFO TaskSetManager: Finished task 2.0 in stage 173.0 (TID 373) in 13 ms on 172.18.0.15 (executor 0) (4/4)
26/02/13 11:50:37 INFO TaskSchedulerImpl: Removed TaskSet 173.0, whose tasks have all completed, from pool 
26/02/13 11:50:37 INFO DAGScheduler: ResultStage 173 (start at NativeMethodAccessorImpl.java:0) finished in 0.040 s
26/02/13 11:50:37 INFO DAGScheduler: Job 113 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:50:37 INFO TaskSchedulerImpl: Killing all running tasks in stage 173: Stage finished
26/02/13 11:50:37 INFO DAGScheduler: Job 113 finished: start at NativeMethodAccessorImpl.java:0, took 0.042555 s
26/02/13 11:50:37 INFO MemoryStore: Block broadcast_170_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.6 MiB)
26/02/13 11:50:37 INFO BlockManagerInfo: Added broadcast_170_piece0 in memory on spark-master:40291 (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 11:50:37 INFO SparkContext: Created broadcast 170 from start at NativeMethodAccessorImpl.java:0
26/02/13 11:50:37 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 52, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@4e32ed5a]. The input RDD has 3 partitions.
26/02/13 11:50:37 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:50:37 INFO DAGScheduler: Got job 114 (start at NativeMethodAccessorImpl.java:0) with 3 output partitions
26/02/13 11:50:37 INFO DAGScheduler: Final stage: ResultStage 174 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:50:37 INFO DAGScheduler: Parents of final stage: List()
26/02/13 11:50:37 INFO DAGScheduler: Missing parents: List()
26/02/13 11:50:37 INFO DAGScheduler: Submitting ResultStage 174 (MapPartitionsRDD[621] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:50:37 INFO MemoryStore: Block broadcast_171 stored as values in memory (estimated size 49.9 KiB, free 433.6 MiB)
26/02/13 11:50:37 INFO MemoryStore: Block broadcast_171_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.6 MiB)
26/02/13 11:50:37 INFO BlockManagerInfo: Added broadcast_171_piece0 in memory on spark-master:40291 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 11:50:37 INFO SparkContext: Created broadcast 171 from broadcast at DAGScheduler.scala:1585
26/02/13 11:50:37 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 174 (MapPartitionsRDD[621] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
26/02/13 11:50:37 INFO TaskSchedulerImpl: Adding task set 174.0 with 3 tasks resource profile 0
26/02/13 11:50:37 INFO TaskSetManager: Starting task 0.0 in stage 174.0 (TID 375) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:50:37 INFO TaskSetManager: Starting task 1.0 in stage 174.0 (TID 376) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:50:37 INFO TaskSetManager: Starting task 2.0 in stage 174.0 (TID 377) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:50:37 INFO BlockManagerInfo: Added broadcast_171_piece0 in memory on 172.18.0.15:34959 (size: 19.5 KiB, free: 434.0 MiB)
26/02/13 11:50:37 INFO BlockManagerInfo: Added broadcast_171_piece0 in memory on 172.18.0.14:40939 (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 11:50:37 INFO BlockManagerInfo: Added broadcast_170_piece0 in memory on 172.18.0.15:34959 (size: 107.2 KiB, free: 433.9 MiB)
26/02/13 11:50:37 INFO BlockManagerInfo: Added broadcast_170_piece0 in memory on 172.18.0.14:40939 (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 11:50:37 INFO TaskSetManager: Finished task 1.0 in stage 174.0 (TID 376) in 78 ms on 172.18.0.15 (executor 0) (1/3)
26/02/13 11:50:37 INFO TaskSetManager: Finished task 0.0 in stage 174.0 (TID 375) in 78 ms on 172.18.0.14 (executor 1) (2/3)
26/02/13 11:50:37 INFO TaskSetManager: Finished task 2.0 in stage 174.0 (TID 377) in 79 ms on 172.18.0.14 (executor 1) (3/3)
26/02/13 11:50:37 INFO TaskSchedulerImpl: Removed TaskSet 174.0, whose tasks have all completed, from pool 
26/02/13 11:50:37 INFO DAGScheduler: ResultStage 174 (start at NativeMethodAccessorImpl.java:0) finished in 0.087 s
26/02/13 11:50:37 INFO DAGScheduler: Job 114 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:50:37 INFO TaskSchedulerImpl: Killing all running tasks in stage 174: Stage finished
26/02/13 11:50:37 INFO DAGScheduler: Job 114 finished: start at NativeMethodAccessorImpl.java:0, took 0.088460 s
26/02/13 11:50:37 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 52, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@4e32ed5a] is committing.
26/02/13 11:50:37 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 52, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@4e32ed5a] committed.
26/02/13 11:50:37 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/52 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.52.bce845e9-773c-4496-8679-36bbdd03bf50.tmp
26/02/13 11:50:37 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.52.bce845e9-773c-4496-8679-36bbdd03bf50.tmp to file:/tmp/spark-checkpoint-enrichment/commits/52
26/02/13 11:50:37 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "fc5c6452-bd70-4c90-becf-772d3bebdb57",
  "name" : null,
  "timestamp" : "2026-02-13T11:50:37.251Z",
  "batchId" : 52,
  "numInputRows" : 154,
  "inputRowsPerSecond" : 158.92672858617132,
  "processedRowsPerSecond" : 412.8686327077748,
  "durationMs" : {
    "addBatch" : 221,
    "commitOffsets" : 65,
    "getBatch" : 0,
    "latestOffset" : 2,
    "queryPlanning" : 32,
    "triggerExecution" : 373,
    "walCommit" : 53
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 4765,
        "1" : 5218,
        "0" : 6922
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 4816,
        "1" : 5271,
        "0" : 6972
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 4816,
        "1" : 5271,
        "0" : 6972
      }
    },
    "numInputRows" : 154,
    "inputRowsPerSecond" : 158.92672858617132,
    "processedRowsPerSecond" : 412.8686327077748,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@7fe9b5de",
    "numOutputRows" : 32
  }
}
26/02/13 11:50:39 INFO BlockManagerInfo: Removed broadcast_168_piece0 on spark-master:40291 in memory (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 11:50:39 INFO BlockManagerInfo: Removed broadcast_168_piece0 on 172.18.0.15:34959 in memory (size: 19.5 KiB, free: 433.9 MiB)
26/02/13 11:50:39 INFO BlockManagerInfo: Removed broadcast_168_piece0 on 172.18.0.14:40939 in memory (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 11:50:39 INFO BlockManagerInfo: Removed broadcast_169_piece0 on spark-master:40291 in memory (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 11:50:39 INFO BlockManagerInfo: Removed broadcast_169_piece0 on 172.18.0.15:34959 in memory (size: 33.3 KiB, free: 433.9 MiB)
26/02/13 11:50:39 INFO BlockManagerInfo: Removed broadcast_167_piece0 on spark-master:40291 in memory (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 11:50:39 INFO BlockManagerInfo: Removed broadcast_167_piece0 on 172.18.0.15:34959 in memory (size: 107.2 KiB, free: 434.0 MiB)
26/02/13 11:50:39 INFO BlockManagerInfo: Removed broadcast_167_piece0 on 172.18.0.14:40939 in memory (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 11:50:39 INFO BlockManagerInfo: Removed broadcast_166_piece0 on spark-master:40291 in memory (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 11:50:39 INFO BlockManagerInfo: Removed broadcast_166_piece0 on 172.18.0.15:34959 in memory (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 11:50:39 INFO BlockManagerInfo: Removed broadcast_171_piece0 on spark-master:40291 in memory (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 11:50:39 INFO BlockManagerInfo: Removed broadcast_171_piece0 on 172.18.0.14:40939 in memory (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 11:50:39 INFO BlockManagerInfo: Removed broadcast_171_piece0 on 172.18.0.15:34959 in memory (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 11:50:47 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 11:50:52 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/53 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.53.ae53f394-6d1a-4215-8748-530d67eb6738.tmp
26/02/13 11:50:52 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.53.ae53f394-6d1a-4215-8748-530d67eb6738.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/53
26/02/13 11:50:52 INFO MicroBatchExecution: Committed offsets for batch 53. Metadata OffsetSeqMetadata(0,1770983452855,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 11:50:53 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:50:53 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:50:53 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:50:53 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:50:53 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:50:53 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:50:53 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#47213 - origin_code.nullCount#47212) > 0)
26/02/13 11:50:53 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#47218 - destination_code.nullCount#47217) > 0)
26/02/13 11:50:53 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#47248 - callsign.nullCount#47247) > 0)
26/02/13 11:50:53 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:50:53 INFO DAGScheduler: Got job 115 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 11:50:53 INFO DAGScheduler: Final stage: ResultStage 176 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:50:53 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 175)
26/02/13 11:50:53 INFO DAGScheduler: Missing parents: List()
26/02/13 11:50:53 INFO DAGScheduler: Submitting ResultStage 176 (MapPartitionsRDD[626] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:50:53 INFO MemoryStore: Block broadcast_172 stored as values in memory (estimated size 99.3 KiB, free 434.0 MiB)
26/02/13 11:50:53 INFO MemoryStore: Block broadcast_172_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 433.9 MiB)
26/02/13 11:50:53 INFO BlockManagerInfo: Added broadcast_172_piece0 in memory on spark-master:40291 (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 11:50:53 INFO SparkContext: Created broadcast 172 from broadcast at DAGScheduler.scala:1585
26/02/13 11:50:53 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 176 (MapPartitionsRDD[626] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 11:50:53 INFO TaskSchedulerImpl: Adding task set 176.0 with 4 tasks resource profile 0
26/02/13 11:50:53 INFO TaskSetManager: Starting task 0.0 in stage 176.0 (TID 378) (172.18.0.15, executor 0, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:50:53 INFO TaskSetManager: Starting task 1.0 in stage 176.0 (TID 379) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:50:53 INFO BlockManagerInfo: Added broadcast_172_piece0 in memory on 172.18.0.15:34959 (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 11:50:53 INFO TaskSetManager: Starting task 2.0 in stage 176.0 (TID 380) (172.18.0.15, executor 0, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:50:53 INFO TaskSetManager: Finished task 1.0 in stage 176.0 (TID 379) in 18 ms on 172.18.0.15 (executor 0) (1/4)
26/02/13 11:50:53 INFO TaskSetManager: Starting task 3.0 in stage 176.0 (TID 381) (172.18.0.15, executor 0, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:50:53 INFO TaskSetManager: Finished task 0.0 in stage 176.0 (TID 378) in 22 ms on 172.18.0.15 (executor 0) (2/4)
26/02/13 11:50:53 INFO TaskSetManager: Finished task 2.0 in stage 176.0 (TID 380) in 16 ms on 172.18.0.15 (executor 0) (3/4)
26/02/13 11:50:53 INFO TaskSetManager: Finished task 3.0 in stage 176.0 (TID 381) in 22 ms on 172.18.0.15 (executor 0) (4/4)
26/02/13 11:50:53 INFO TaskSchedulerImpl: Removed TaskSet 176.0, whose tasks have all completed, from pool 
26/02/13 11:50:53 INFO DAGScheduler: ResultStage 176 (start at NativeMethodAccessorImpl.java:0) finished in 0.054 s
26/02/13 11:50:53 INFO DAGScheduler: Job 115 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:50:53 INFO TaskSchedulerImpl: Killing all running tasks in stage 176: Stage finished
26/02/13 11:50:53 INFO DAGScheduler: Job 115 finished: start at NativeMethodAccessorImpl.java:0, took 0.056686 s
26/02/13 11:50:53 INFO MemoryStore: Block broadcast_173_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.8 MiB)
26/02/13 11:50:53 INFO BlockManagerInfo: Added broadcast_173_piece0 in memory on spark-master:40291 (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 11:50:53 INFO SparkContext: Created broadcast 173 from start at NativeMethodAccessorImpl.java:0
26/02/13 11:50:53 INFO BlockManagerInfo: Removed broadcast_170_piece0 on spark-master:40291 in memory (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 11:50:53 INFO BlockManagerInfo: Removed broadcast_170_piece0 on 172.18.0.14:40939 in memory (size: 107.2 KiB, free: 434.4 MiB)
26/02/13 11:50:53 INFO BlockManagerInfo: Removed broadcast_170_piece0 on 172.18.0.15:34959 in memory (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 11:50:53 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 53, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@5b30cd30]. The input RDD has 3 partitions.
26/02/13 11:50:53 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:50:53 INFO DAGScheduler: Got job 116 (start at NativeMethodAccessorImpl.java:0) with 3 output partitions
26/02/13 11:50:53 INFO DAGScheduler: Final stage: ResultStage 177 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:50:53 INFO DAGScheduler: Parents of final stage: List()
26/02/13 11:50:53 INFO DAGScheduler: Missing parents: List()
26/02/13 11:50:53 INFO DAGScheduler: Submitting ResultStage 177 (MapPartitionsRDD[632] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:50:53 INFO MemoryStore: Block broadcast_174 stored as values in memory (estimated size 49.9 KiB, free 433.9 MiB)
26/02/13 11:50:53 INFO MemoryStore: Block broadcast_174_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.9 MiB)
26/02/13 11:50:53 INFO BlockManagerInfo: Added broadcast_174_piece0 in memory on spark-master:40291 (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 11:50:53 INFO SparkContext: Created broadcast 174 from broadcast at DAGScheduler.scala:1585
26/02/13 11:50:53 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 177 (MapPartitionsRDD[632] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
26/02/13 11:50:53 INFO TaskSchedulerImpl: Adding task set 177.0 with 3 tasks resource profile 0
26/02/13 11:50:53 INFO TaskSetManager: Starting task 1.0 in stage 177.0 (TID 382) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:50:53 INFO TaskSetManager: Starting task 0.0 in stage 177.0 (TID 383) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:50:53 INFO TaskSetManager: Starting task 2.0 in stage 177.0 (TID 384) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:50:53 INFO BlockManagerInfo: Added broadcast_174_piece0 in memory on 172.18.0.15:34959 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 11:50:53 INFO BlockManagerInfo: Added broadcast_174_piece0 in memory on 172.18.0.14:40939 (size: 19.5 KiB, free: 434.4 MiB)
26/02/13 11:50:53 INFO BlockManagerInfo: Added broadcast_173_piece0 in memory on 172.18.0.15:34959 (size: 107.2 KiB, free: 434.0 MiB)
26/02/13 11:50:53 INFO BlockManagerInfo: Added broadcast_173_piece0 in memory on 172.18.0.14:40939 (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 11:50:53 INFO TaskSetManager: Finished task 1.0 in stage 177.0 (TID 382) in 579 ms on 172.18.0.15 (executor 0) (1/3)
26/02/13 11:50:53 INFO TaskSetManager: Finished task 0.0 in stage 177.0 (TID 383) in 598 ms on 172.18.0.14 (executor 1) (2/3)
26/02/13 11:50:53 INFO TaskSetManager: Finished task 2.0 in stage 177.0 (TID 384) in 597 ms on 172.18.0.14 (executor 1) (3/3)
26/02/13 11:50:53 INFO DAGScheduler: ResultStage 177 (start at NativeMethodAccessorImpl.java:0) finished in 0.606 s
26/02/13 11:50:53 INFO TaskSchedulerImpl: Removed TaskSet 177.0, whose tasks have all completed, from pool 
26/02/13 11:50:53 INFO DAGScheduler: Job 116 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:50:53 INFO TaskSchedulerImpl: Killing all running tasks in stage 177: Stage finished
26/02/13 11:50:53 INFO DAGScheduler: Job 116 finished: start at NativeMethodAccessorImpl.java:0, took 0.609472 s
26/02/13 11:50:53 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 53, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@5b30cd30] is committing.
26/02/13 11:50:53 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 53, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@5b30cd30] committed.
26/02/13 11:50:53 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/53 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.53.b54e4f67-8e44-4dc8-917e-ce77b9e3fc39.tmp
26/02/13 11:50:53 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.53.b54e4f67-8e44-4dc8-917e-ce77b9e3fc39.tmp to file:/tmp/spark-checkpoint-enrichment/commits/53
26/02/13 11:50:53 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "fc5c6452-bd70-4c90-becf-772d3bebdb57",
  "name" : null,
  "timestamp" : "2026-02-13T11:50:52.854Z",
  "batchId" : 53,
  "numInputRows" : 107,
  "inputRowsPerSecond" : 9727.272727272728,
  "processedRowsPerSecond" : 106.36182902584493,
  "durationMs" : {
    "addBatch" : 777,
    "commitOffsets" : 59,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 51,
    "triggerExecution" : 1006,
    "walCommit" : 117
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 4816,
        "1" : 5271,
        "0" : 6972
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 4849,
        "1" : 5308,
        "0" : 7009
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 4849,
        "1" : 5308,
        "0" : 7009
      }
    },
    "numInputRows" : 107,
    "inputRowsPerSecond" : 9727.272727272728,
    "processedRowsPerSecond" : 106.36182902584493,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@7fe9b5de",
    "numOutputRows" : 27
  }
}
26/02/13 11:50:53 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/54 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.54.57ec7e68-5935-46b1-a97e-9692bc1fa603.tmp
26/02/13 11:50:53 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.54.57ec7e68-5935-46b1-a97e-9692bc1fa603.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/54
26/02/13 11:50:53 INFO MicroBatchExecution: Committed offsets for batch 54. Metadata OffsetSeqMetadata(0,1770983453862,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 11:50:53 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:50:53 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:50:53 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:50:53 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:50:54 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:50:54 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:50:54 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#48067 - origin_code.nullCount#48066) > 0)
26/02/13 11:50:54 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#48072 - destination_code.nullCount#48071) > 0)
26/02/13 11:50:54 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#48102 - callsign.nullCount#48101) > 0)
26/02/13 11:50:54 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:50:54 INFO DAGScheduler: Got job 117 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 11:50:54 INFO DAGScheduler: Final stage: ResultStage 179 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:50:54 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 178)
26/02/13 11:50:54 INFO DAGScheduler: Missing parents: List()
26/02/13 11:50:54 INFO DAGScheduler: Submitting ResultStage 179 (MapPartitionsRDD[637] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:50:54 INFO MemoryStore: Block broadcast_175 stored as values in memory (estimated size 99.3 KiB, free 433.8 MiB)
26/02/13 11:50:54 INFO MemoryStore: Block broadcast_175_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 433.7 MiB)
26/02/13 11:50:54 INFO BlockManagerInfo: Added broadcast_175_piece0 in memory on spark-master:40291 (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 11:50:54 INFO SparkContext: Created broadcast 175 from broadcast at DAGScheduler.scala:1585
26/02/13 11:50:54 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 179 (MapPartitionsRDD[637] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 11:50:54 INFO TaskSchedulerImpl: Adding task set 179.0 with 4 tasks resource profile 0
26/02/13 11:50:54 INFO TaskSetManager: Starting task 0.0 in stage 179.0 (TID 385) (172.18.0.15, executor 0, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:50:54 INFO TaskSetManager: Starting task 1.0 in stage 179.0 (TID 386) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:50:54 INFO BlockManagerInfo: Added broadcast_175_piece0 in memory on 172.18.0.15:34959 (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 11:50:54 INFO TaskSetManager: Starting task 2.0 in stage 179.0 (TID 387) (172.18.0.15, executor 0, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:50:54 INFO TaskSetManager: Finished task 1.0 in stage 179.0 (TID 386) in 15 ms on 172.18.0.15 (executor 0) (1/4)
26/02/13 11:50:54 INFO TaskSetManager: Starting task 3.0 in stage 179.0 (TID 388) (172.18.0.15, executor 0, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:50:54 INFO TaskSetManager: Finished task 0.0 in stage 179.0 (TID 385) in 22 ms on 172.18.0.15 (executor 0) (2/4)
26/02/13 11:50:54 INFO TaskSetManager: Finished task 2.0 in stage 179.0 (TID 387) in 17 ms on 172.18.0.15 (executor 0) (3/4)
26/02/13 11:50:54 INFO TaskSetManager: Finished task 3.0 in stage 179.0 (TID 388) in 17 ms on 172.18.0.15 (executor 0) (4/4)
26/02/13 11:50:54 INFO TaskSchedulerImpl: Removed TaskSet 179.0, whose tasks have all completed, from pool 
26/02/13 11:50:54 INFO DAGScheduler: ResultStage 179 (start at NativeMethodAccessorImpl.java:0) finished in 0.046 s
26/02/13 11:50:54 INFO DAGScheduler: Job 117 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:50:54 INFO TaskSchedulerImpl: Killing all running tasks in stage 179: Stage finished
26/02/13 11:50:54 INFO DAGScheduler: Job 117 finished: start at NativeMethodAccessorImpl.java:0, took 0.049001 s
26/02/13 11:50:54 INFO MemoryStore: Block broadcast_176_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.6 MiB)
26/02/13 11:50:54 INFO BlockManagerInfo: Added broadcast_176_piece0 in memory on spark-master:40291 (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 11:50:54 INFO SparkContext: Created broadcast 176 from start at NativeMethodAccessorImpl.java:0
26/02/13 11:50:54 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 54, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@67300402]. The input RDD has 3 partitions.
26/02/13 11:50:54 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:50:54 INFO DAGScheduler: Got job 118 (start at NativeMethodAccessorImpl.java:0) with 3 output partitions
26/02/13 11:50:54 INFO DAGScheduler: Final stage: ResultStage 180 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:50:54 INFO DAGScheduler: Parents of final stage: List()
26/02/13 11:50:54 INFO DAGScheduler: Missing parents: List()
26/02/13 11:50:54 INFO DAGScheduler: Submitting ResultStage 180 (MapPartitionsRDD[643] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:50:54 INFO MemoryStore: Block broadcast_177 stored as values in memory (estimated size 49.9 KiB, free 433.6 MiB)
26/02/13 11:50:54 INFO MemoryStore: Block broadcast_177_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.6 MiB)
26/02/13 11:50:54 INFO BlockManagerInfo: Added broadcast_177_piece0 in memory on spark-master:40291 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 11:50:54 INFO SparkContext: Created broadcast 177 from broadcast at DAGScheduler.scala:1585
26/02/13 11:50:54 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 180 (MapPartitionsRDD[643] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
26/02/13 11:50:54 INFO TaskSchedulerImpl: Adding task set 180.0 with 3 tasks resource profile 0
26/02/13 11:50:54 INFO TaskSetManager: Starting task 1.0 in stage 180.0 (TID 389) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:50:54 INFO TaskSetManager: Starting task 0.0 in stage 180.0 (TID 390) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:50:54 INFO TaskSetManager: Starting task 2.0 in stage 180.0 (TID 391) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:50:54 INFO BlockManagerInfo: Added broadcast_177_piece0 in memory on 172.18.0.15:34959 (size: 19.5 KiB, free: 434.0 MiB)
26/02/13 11:50:54 INFO BlockManagerInfo: Added broadcast_177_piece0 in memory on 172.18.0.14:40939 (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 11:50:54 INFO BlockManagerInfo: Added broadcast_176_piece0 in memory on 172.18.0.15:34959 (size: 107.2 KiB, free: 433.9 MiB)
26/02/13 11:50:54 INFO BlockManagerInfo: Added broadcast_176_piece0 in memory on 172.18.0.14:40939 (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 11:50:54 INFO TaskSetManager: Finished task 1.0 in stage 180.0 (TID 389) in 43 ms on 172.18.0.15 (executor 0) (1/3)
26/02/13 11:50:54 INFO TaskSetManager: Finished task 2.0 in stage 180.0 (TID 391) in 61 ms on 172.18.0.14 (executor 1) (2/3)
26/02/13 11:50:54 INFO TaskSetManager: Finished task 0.0 in stage 180.0 (TID 390) in 62 ms on 172.18.0.14 (executor 1) (3/3)
26/02/13 11:50:54 INFO TaskSchedulerImpl: Removed TaskSet 180.0, whose tasks have all completed, from pool 
26/02/13 11:50:54 INFO DAGScheduler: ResultStage 180 (start at NativeMethodAccessorImpl.java:0) finished in 0.068 s
26/02/13 11:50:54 INFO DAGScheduler: Job 118 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:50:54 INFO TaskSchedulerImpl: Killing all running tasks in stage 180: Stage finished
26/02/13 11:50:54 INFO DAGScheduler: Job 118 finished: start at NativeMethodAccessorImpl.java:0, took 0.070381 s
26/02/13 11:50:54 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 54, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@67300402] is committing.
26/02/13 11:50:54 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 54, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@67300402] committed.
26/02/13 11:50:54 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/54 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.54.ff00e075-a68c-44c0-b8d0-8d5268e12052.tmp
26/02/13 11:50:54 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.54.ff00e075-a68c-44c0-b8d0-8d5268e12052.tmp to file:/tmp/spark-checkpoint-enrichment/commits/54
26/02/13 11:50:54 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "fc5c6452-bd70-4c90-becf-772d3bebdb57",
  "name" : null,
  "timestamp" : "2026-02-13T11:50:53.861Z",
  "batchId" : 54,
  "numInputRows" : 134,
  "inputRowsPerSecond" : 133.06852035749753,
  "processedRowsPerSecond" : 363.14363143631437,
  "durationMs" : {
    "addBatch" : 214,
    "commitOffsets" : 64,
    "getBatch" : 1,
    "latestOffset" : 1,
    "queryPlanning" : 30,
    "triggerExecution" : 369,
    "walCommit" : 58
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 4849,
        "1" : 5308,
        "0" : 7009
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 4897,
        "1" : 5356,
        "0" : 7047
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 4897,
        "1" : 5356,
        "0" : 7047
      }
    },
    "numInputRows" : 134,
    "inputRowsPerSecond" : 133.06852035749753,
    "processedRowsPerSecond" : 363.14363143631437,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@7fe9b5de",
    "numOutputRows" : 28
  }
}
26/02/13 11:50:55 INFO BlockManagerInfo: Removed broadcast_175_piece0 on spark-master:40291 in memory (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 11:50:55 INFO BlockManagerInfo: Removed broadcast_175_piece0 on 172.18.0.15:34959 in memory (size: 33.3 KiB, free: 433.9 MiB)
26/02/13 11:50:55 INFO BlockManagerInfo: Removed broadcast_172_piece0 on spark-master:40291 in memory (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 11:50:55 INFO BlockManagerInfo: Removed broadcast_172_piece0 on 172.18.0.15:34959 in memory (size: 33.3 KiB, free: 433.9 MiB)
26/02/13 11:50:55 INFO BlockManagerInfo: Removed broadcast_174_piece0 on spark-master:40291 in memory (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 11:50:55 INFO BlockManagerInfo: Removed broadcast_174_piece0 on 172.18.0.14:40939 in memory (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 11:50:55 INFO BlockManagerInfo: Removed broadcast_174_piece0 on 172.18.0.15:34959 in memory (size: 19.5 KiB, free: 433.9 MiB)
26/02/13 11:50:55 INFO BlockManagerInfo: Removed broadcast_173_piece0 on spark-master:40291 in memory (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 11:50:55 INFO BlockManagerInfo: Removed broadcast_173_piece0 on 172.18.0.14:40939 in memory (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 11:50:55 INFO BlockManagerInfo: Removed broadcast_173_piece0 on 172.18.0.15:34959 in memory (size: 107.2 KiB, free: 434.0 MiB)
26/02/13 11:50:55 INFO BlockManagerInfo: Removed broadcast_177_piece0 on spark-master:40291 in memory (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 11:50:55 INFO BlockManagerInfo: Removed broadcast_177_piece0 on 172.18.0.14:40939 in memory (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 11:50:55 INFO BlockManagerInfo: Removed broadcast_177_piece0 on 172.18.0.15:34959 in memory (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 11:51:04 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 11:51:09 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/55 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.55.f16cd917-6618-4c9b-b41c-a46fe722c6ad.tmp
26/02/13 11:51:09 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.55.f16cd917-6618-4c9b-b41c-a46fe722c6ad.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/55
26/02/13 11:51:09 INFO MicroBatchExecution: Committed offsets for batch 55. Metadata OffsetSeqMetadata(0,1770983469569,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 11:51:09 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:51:09 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:51:09 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:51:09 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:51:09 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:51:09 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:51:09 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#48921 - origin_code.nullCount#48920) > 0)
26/02/13 11:51:09 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#48926 - destination_code.nullCount#48925) > 0)
26/02/13 11:51:09 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#48956 - callsign.nullCount#48955) > 0)
26/02/13 11:51:09 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:51:09 INFO DAGScheduler: Got job 119 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 11:51:09 INFO DAGScheduler: Final stage: ResultStage 182 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:51:09 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 181)
26/02/13 11:51:09 INFO DAGScheduler: Missing parents: List()
26/02/13 11:51:09 INFO DAGScheduler: Submitting ResultStage 182 (MapPartitionsRDD[648] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:51:09 INFO MemoryStore: Block broadcast_178 stored as values in memory (estimated size 99.3 KiB, free 434.0 MiB)
26/02/13 11:51:09 INFO MemoryStore: Block broadcast_178_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 433.9 MiB)
26/02/13 11:51:09 INFO BlockManagerInfo: Added broadcast_178_piece0 in memory on spark-master:40291 (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 11:51:09 INFO SparkContext: Created broadcast 178 from broadcast at DAGScheduler.scala:1585
26/02/13 11:51:09 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 182 (MapPartitionsRDD[648] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 11:51:09 INFO TaskSchedulerImpl: Adding task set 182.0 with 4 tasks resource profile 0
26/02/13 11:51:09 INFO TaskSetManager: Starting task 0.0 in stage 182.0 (TID 392) (172.18.0.15, executor 0, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:51:09 INFO TaskSetManager: Starting task 1.0 in stage 182.0 (TID 393) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:51:09 INFO BlockManagerInfo: Added broadcast_178_piece0 in memory on 172.18.0.15:34959 (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 11:51:09 INFO TaskSetManager: Starting task 2.0 in stage 182.0 (TID 394) (172.18.0.15, executor 0, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:51:09 INFO TaskSetManager: Finished task 1.0 in stage 182.0 (TID 393) in 25 ms on 172.18.0.15 (executor 0) (1/4)
26/02/13 11:51:09 INFO TaskSetManager: Starting task 3.0 in stage 182.0 (TID 395) (172.18.0.15, executor 0, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:51:09 INFO TaskSetManager: Finished task 0.0 in stage 182.0 (TID 392) in 29 ms on 172.18.0.15 (executor 0) (2/4)
26/02/13 11:51:09 INFO TaskSetManager: Finished task 2.0 in stage 182.0 (TID 394) in 17 ms on 172.18.0.15 (executor 0) (3/4)
26/02/13 11:51:09 INFO TaskSetManager: Finished task 3.0 in stage 182.0 (TID 395) in 16 ms on 172.18.0.15 (executor 0) (4/4)
26/02/13 11:51:09 INFO TaskSchedulerImpl: Removed TaskSet 182.0, whose tasks have all completed, from pool 
26/02/13 11:51:09 INFO DAGScheduler: ResultStage 182 (start at NativeMethodAccessorImpl.java:0) finished in 0.056 s
26/02/13 11:51:09 INFO DAGScheduler: Job 119 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:51:09 INFO TaskSchedulerImpl: Killing all running tasks in stage 182: Stage finished
26/02/13 11:51:09 INFO DAGScheduler: Job 119 finished: start at NativeMethodAccessorImpl.java:0, took 0.058176 s
26/02/13 11:51:09 INFO MemoryStore: Block broadcast_179_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.8 MiB)
26/02/13 11:51:09 INFO BlockManagerInfo: Added broadcast_179_piece0 in memory on spark-master:40291 (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 11:51:09 INFO SparkContext: Created broadcast 179 from start at NativeMethodAccessorImpl.java:0
26/02/13 11:51:09 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 55, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@5e131671]. The input RDD has 3 partitions.
26/02/13 11:51:09 INFO BlockManagerInfo: Removed broadcast_176_piece0 on spark-master:40291 in memory (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 11:51:09 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:51:09 INFO DAGScheduler: Got job 120 (start at NativeMethodAccessorImpl.java:0) with 3 output partitions
26/02/13 11:51:09 INFO DAGScheduler: Final stage: ResultStage 183 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:51:09 INFO DAGScheduler: Parents of final stage: List()
26/02/13 11:51:09 INFO DAGScheduler: Missing parents: List()
26/02/13 11:51:09 INFO DAGScheduler: Submitting ResultStage 183 (MapPartitionsRDD[654] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:51:09 INFO BlockManagerInfo: Removed broadcast_176_piece0 on 172.18.0.14:40939 in memory (size: 107.2 KiB, free: 434.4 MiB)
26/02/13 11:51:09 INFO BlockManagerInfo: Removed broadcast_176_piece0 on 172.18.0.15:34959 in memory (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 11:51:09 INFO MemoryStore: Block broadcast_180 stored as values in memory (estimated size 49.9 KiB, free 433.9 MiB)
26/02/13 11:51:09 INFO MemoryStore: Block broadcast_180_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.9 MiB)
26/02/13 11:51:09 INFO BlockManagerInfo: Added broadcast_180_piece0 in memory on spark-master:40291 (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 11:51:09 INFO SparkContext: Created broadcast 180 from broadcast at DAGScheduler.scala:1585
26/02/13 11:51:09 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 183 (MapPartitionsRDD[654] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
26/02/13 11:51:09 INFO TaskSchedulerImpl: Adding task set 183.0 with 3 tasks resource profile 0
26/02/13 11:51:09 INFO TaskSetManager: Starting task 1.0 in stage 183.0 (TID 396) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:51:09 INFO TaskSetManager: Starting task 0.0 in stage 183.0 (TID 397) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:51:09 INFO TaskSetManager: Starting task 2.0 in stage 183.0 (TID 398) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:51:09 INFO BlockManagerInfo: Added broadcast_180_piece0 in memory on 172.18.0.15:34959 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 11:51:09 INFO BlockManagerInfo: Added broadcast_180_piece0 in memory on 172.18.0.14:40939 (size: 19.5 KiB, free: 434.4 MiB)
26/02/13 11:51:09 INFO BlockManagerInfo: Added broadcast_179_piece0 in memory on 172.18.0.15:34959 (size: 107.2 KiB, free: 434.0 MiB)
26/02/13 11:51:09 INFO BlockManagerInfo: Added broadcast_179_piece0 in memory on 172.18.0.14:40939 (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 11:51:10 INFO TaskSetManager: Finished task 2.0 in stage 183.0 (TID 398) in 571 ms on 172.18.0.14 (executor 1) (1/3)
26/02/13 11:51:10 INFO TaskSetManager: Finished task 0.0 in stage 183.0 (TID 397) in 575 ms on 172.18.0.14 (executor 1) (2/3)
26/02/13 11:51:10 INFO TaskSetManager: Finished task 1.0 in stage 183.0 (TID 396) in 582 ms on 172.18.0.15 (executor 0) (3/3)
26/02/13 11:51:10 INFO TaskSchedulerImpl: Removed TaskSet 183.0, whose tasks have all completed, from pool 
26/02/13 11:51:10 INFO DAGScheduler: ResultStage 183 (start at NativeMethodAccessorImpl.java:0) finished in 0.591 s
26/02/13 11:51:10 INFO DAGScheduler: Job 120 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:51:10 INFO TaskSchedulerImpl: Killing all running tasks in stage 183: Stage finished
26/02/13 11:51:10 INFO DAGScheduler: Job 120 finished: start at NativeMethodAccessorImpl.java:0, took 0.592416 s
26/02/13 11:51:10 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 55, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@5e131671] is committing.
26/02/13 11:51:10 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 55, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@5e131671] committed.
26/02/13 11:51:10 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/55 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.55.081e07d8-e45e-4385-950a-82e434e925a8.tmp
26/02/13 11:51:10 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.55.081e07d8-e45e-4385-950a-82e434e925a8.tmp to file:/tmp/spark-checkpoint-enrichment/commits/55
26/02/13 11:51:10 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "fc5c6452-bd70-4c90-becf-772d3bebdb57",
  "name" : null,
  "timestamp" : "2026-02-13T11:51:09.567Z",
  "batchId" : 55,
  "numInputRows" : 81,
  "inputRowsPerSecond" : 6750.0,
  "processedRowsPerSecond" : 86.90987124463518,
  "durationMs" : {
    "addBatch" : 753,
    "commitOffsets" : 72,
    "getBatch" : 0,
    "latestOffset" : 2,
    "queryPlanning" : 28,
    "triggerExecution" : 932,
    "walCommit" : 77
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 4897,
        "1" : 5356,
        "0" : 7047
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 4924,
        "1" : 5388,
        "0" : 7069
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 4924,
        "1" : 5388,
        "0" : 7069
      }
    },
    "numInputRows" : 81,
    "inputRowsPerSecond" : 6750.0,
    "processedRowsPerSecond" : 86.90987124463518,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@7fe9b5de",
    "numOutputRows" : 23
  }
}
26/02/13 11:51:10 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/56 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.56.bd83fe37-b58a-4b5f-ac62-52613716d4f6.tmp
26/02/13 11:51:10 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.56.bd83fe37-b58a-4b5f-ac62-52613716d4f6.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/56
26/02/13 11:51:10 INFO MicroBatchExecution: Committed offsets for batch 56. Metadata OffsetSeqMetadata(0,1770983470501,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 11:51:10 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:51:10 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:51:10 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:51:10 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:51:10 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:51:10 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:51:10 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#49775 - origin_code.nullCount#49774) > 0)
26/02/13 11:51:10 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#49780 - destination_code.nullCount#49779) > 0)
26/02/13 11:51:10 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#49810 - callsign.nullCount#49809) > 0)
26/02/13 11:51:10 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:51:10 INFO DAGScheduler: Got job 121 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 11:51:10 INFO DAGScheduler: Final stage: ResultStage 185 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:51:10 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 184)
26/02/13 11:51:10 INFO DAGScheduler: Missing parents: List()
26/02/13 11:51:10 INFO DAGScheduler: Submitting ResultStage 185 (MapPartitionsRDD[659] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:51:10 INFO MemoryStore: Block broadcast_181 stored as values in memory (estimated size 99.3 KiB, free 433.8 MiB)
26/02/13 11:51:10 INFO MemoryStore: Block broadcast_181_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 433.7 MiB)
26/02/13 11:51:10 INFO BlockManagerInfo: Added broadcast_181_piece0 in memory on spark-master:40291 (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 11:51:10 INFO SparkContext: Created broadcast 181 from broadcast at DAGScheduler.scala:1585
26/02/13 11:51:10 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 185 (MapPartitionsRDD[659] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 11:51:10 INFO TaskSchedulerImpl: Adding task set 185.0 with 4 tasks resource profile 0
26/02/13 11:51:10 INFO TaskSetManager: Starting task 0.0 in stage 185.0 (TID 399) (172.18.0.15, executor 0, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:51:10 INFO TaskSetManager: Starting task 1.0 in stage 185.0 (TID 400) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:51:10 INFO BlockManagerInfo: Added broadcast_181_piece0 in memory on 172.18.0.15:34959 (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 11:51:10 INFO TaskSetManager: Starting task 2.0 in stage 185.0 (TID 401) (172.18.0.15, executor 0, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:51:10 INFO TaskSetManager: Finished task 0.0 in stage 185.0 (TID 399) in 18 ms on 172.18.0.15 (executor 0) (1/4)
26/02/13 11:51:10 INFO TaskSetManager: Starting task 3.0 in stage 185.0 (TID 402) (172.18.0.15, executor 0, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:51:10 INFO TaskSetManager: Finished task 1.0 in stage 185.0 (TID 400) in 21 ms on 172.18.0.15 (executor 0) (2/4)
26/02/13 11:51:10 INFO TaskSetManager: Finished task 2.0 in stage 185.0 (TID 401) in 13 ms on 172.18.0.15 (executor 0) (3/4)
26/02/13 11:51:10 INFO TaskSetManager: Finished task 3.0 in stage 185.0 (TID 402) in 11 ms on 172.18.0.15 (executor 0) (4/4)
26/02/13 11:51:10 INFO TaskSchedulerImpl: Removed TaskSet 185.0, whose tasks have all completed, from pool 
26/02/13 11:51:10 INFO DAGScheduler: ResultStage 185 (start at NativeMethodAccessorImpl.java:0) finished in 0.039 s
26/02/13 11:51:10 INFO DAGScheduler: Job 121 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:51:10 INFO TaskSchedulerImpl: Killing all running tasks in stage 185: Stage finished
26/02/13 11:51:10 INFO DAGScheduler: Job 121 finished: start at NativeMethodAccessorImpl.java:0, took 0.040467 s
26/02/13 11:51:10 INFO MemoryStore: Block broadcast_182_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.6 MiB)
26/02/13 11:51:10 INFO BlockManagerInfo: Added broadcast_182_piece0 in memory on spark-master:40291 (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 11:51:10 INFO SparkContext: Created broadcast 182 from start at NativeMethodAccessorImpl.java:0
26/02/13 11:51:10 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 56, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@234ede44]. The input RDD has 3 partitions.
26/02/13 11:51:10 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:51:10 INFO DAGScheduler: Got job 122 (start at NativeMethodAccessorImpl.java:0) with 3 output partitions
26/02/13 11:51:10 INFO DAGScheduler: Final stage: ResultStage 186 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:51:10 INFO DAGScheduler: Parents of final stage: List()
26/02/13 11:51:10 INFO DAGScheduler: Missing parents: List()
26/02/13 11:51:10 INFO DAGScheduler: Submitting ResultStage 186 (MapPartitionsRDD[665] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:51:10 INFO MemoryStore: Block broadcast_183 stored as values in memory (estimated size 49.9 KiB, free 433.6 MiB)
26/02/13 11:51:10 INFO MemoryStore: Block broadcast_183_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.6 MiB)
26/02/13 11:51:10 INFO BlockManagerInfo: Added broadcast_183_piece0 in memory on spark-master:40291 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 11:51:10 INFO SparkContext: Created broadcast 183 from broadcast at DAGScheduler.scala:1585
26/02/13 11:51:10 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 186 (MapPartitionsRDD[665] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
26/02/13 11:51:10 INFO TaskSchedulerImpl: Adding task set 186.0 with 3 tasks resource profile 0
26/02/13 11:51:10 INFO TaskSetManager: Starting task 0.0 in stage 186.0 (TID 403) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:51:10 INFO TaskSetManager: Starting task 1.0 in stage 186.0 (TID 404) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:51:10 INFO TaskSetManager: Starting task 2.0 in stage 186.0 (TID 405) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:51:10 INFO BlockManagerInfo: Added broadcast_183_piece0 in memory on 172.18.0.15:34959 (size: 19.5 KiB, free: 434.0 MiB)
26/02/13 11:51:10 INFO BlockManagerInfo: Added broadcast_183_piece0 in memory on 172.18.0.14:40939 (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 11:51:10 INFO BlockManagerInfo: Added broadcast_182_piece0 in memory on 172.18.0.15:34959 (size: 107.2 KiB, free: 433.9 MiB)
26/02/13 11:51:10 INFO BlockManagerInfo: Added broadcast_182_piece0 in memory on 172.18.0.14:40939 (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 11:51:10 INFO TaskSetManager: Finished task 0.0 in stage 186.0 (TID 403) in 71 ms on 172.18.0.14 (executor 1) (1/3)
26/02/13 11:51:10 INFO TaskSetManager: Finished task 2.0 in stage 186.0 (TID 405) in 73 ms on 172.18.0.14 (executor 1) (2/3)
26/02/13 11:51:10 INFO TaskSetManager: Finished task 1.0 in stage 186.0 (TID 404) in 74 ms on 172.18.0.15 (executor 0) (3/3)
26/02/13 11:51:10 INFO TaskSchedulerImpl: Removed TaskSet 186.0, whose tasks have all completed, from pool 
26/02/13 11:51:10 INFO DAGScheduler: ResultStage 186 (start at NativeMethodAccessorImpl.java:0) finished in 0.080 s
26/02/13 11:51:10 INFO DAGScheduler: Job 122 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:51:10 INFO TaskSchedulerImpl: Killing all running tasks in stage 186: Stage finished
26/02/13 11:51:10 INFO DAGScheduler: Job 122 finished: start at NativeMethodAccessorImpl.java:0, took 0.081760 s
26/02/13 11:51:10 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 56, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@234ede44] is committing.
26/02/13 11:51:10 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 56, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@234ede44] committed.
26/02/13 11:51:10 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/56 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.56.fa11fab7-f551-4113-ada7-5a92855d16e9.tmp
26/02/13 11:51:10 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.56.fa11fab7-f551-4113-ada7-5a92855d16e9.tmp to file:/tmp/spark-checkpoint-enrichment/commits/56
26/02/13 11:51:10 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "fc5c6452-bd70-4c90-becf-772d3bebdb57",
  "name" : null,
  "timestamp" : "2026-02-13T11:51:10.500Z",
  "batchId" : 56,
  "numInputRows" : 162,
  "inputRowsPerSecond" : 173.63344051446944,
  "processedRowsPerSecond" : 379.3911007025761,
  "durationMs" : {
    "addBatch" : 229,
    "commitOffsets" : 97,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 35,
    "triggerExecution" : 427,
    "walCommit" : 64
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 4924,
        "1" : 5388,
        "0" : 7069
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 4978,
        "1" : 5443,
        "0" : 7122
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 4978,
        "1" : 5443,
        "0" : 7122
      }
    },
    "numInputRows" : 162,
    "inputRowsPerSecond" : 173.63344051446944,
    "processedRowsPerSecond" : 379.3911007025761,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@7fe9b5de",
    "numOutputRows" : 32
  }
}
26/02/13 11:51:12 INFO BlockManagerInfo: Removed broadcast_183_piece0 on spark-master:40291 in memory (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 11:51:12 INFO BlockManagerInfo: Removed broadcast_183_piece0 on 172.18.0.14:40939 in memory (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 11:51:12 INFO BlockManagerInfo: Removed broadcast_183_piece0 on 172.18.0.15:34959 in memory (size: 19.5 KiB, free: 433.9 MiB)
26/02/13 11:51:12 INFO BlockManagerInfo: Removed broadcast_180_piece0 on spark-master:40291 in memory (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 11:51:12 INFO BlockManagerInfo: Removed broadcast_180_piece0 on 172.18.0.14:40939 in memory (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 11:51:12 INFO BlockManagerInfo: Removed broadcast_180_piece0 on 172.18.0.15:34959 in memory (size: 19.5 KiB, free: 433.9 MiB)
26/02/13 11:51:12 INFO BlockManagerInfo: Removed broadcast_179_piece0 on spark-master:40291 in memory (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 11:51:12 INFO BlockManagerInfo: Removed broadcast_179_piece0 on 172.18.0.14:40939 in memory (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 11:51:12 INFO BlockManagerInfo: Removed broadcast_179_piece0 on 172.18.0.15:34959 in memory (size: 107.2 KiB, free: 434.0 MiB)
26/02/13 11:51:12 INFO BlockManagerInfo: Removed broadcast_181_piece0 on spark-master:40291 in memory (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 11:51:12 INFO BlockManagerInfo: Removed broadcast_181_piece0 on 172.18.0.15:34959 in memory (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 11:51:12 INFO BlockManagerInfo: Removed broadcast_178_piece0 on spark-master:40291 in memory (size: 33.3 KiB, free: 434.3 MiB)
26/02/13 11:51:12 INFO BlockManagerInfo: Removed broadcast_178_piece0 on 172.18.0.15:34959 in memory (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 11:51:20 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 11:51:26 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/57 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.57.b5dca304-1220-494d-8c28-8b727db500fa.tmp
26/02/13 11:51:26 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.57.b5dca304-1220-494d-8c28-8b727db500fa.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/57
26/02/13 11:51:26 INFO MicroBatchExecution: Committed offsets for batch 57. Metadata OffsetSeqMetadata(0,1770983486323,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 11:51:26 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:51:26 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:51:26 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:51:26 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:51:26 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:51:26 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:51:26 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#50629 - origin_code.nullCount#50628) > 0)
26/02/13 11:51:26 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#50634 - destination_code.nullCount#50633) > 0)
26/02/13 11:51:26 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#50664 - callsign.nullCount#50663) > 0)
26/02/13 11:51:26 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:51:26 INFO DAGScheduler: Got job 123 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 11:51:26 INFO DAGScheduler: Final stage: ResultStage 188 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:51:26 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 187)
26/02/13 11:51:26 INFO DAGScheduler: Missing parents: List()
26/02/13 11:51:26 INFO DAGScheduler: Submitting ResultStage 188 (MapPartitionsRDD[670] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:51:26 INFO MemoryStore: Block broadcast_184 stored as values in memory (estimated size 99.3 KiB, free 434.0 MiB)
26/02/13 11:51:26 INFO MemoryStore: Block broadcast_184_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 433.9 MiB)
26/02/13 11:51:26 INFO BlockManagerInfo: Added broadcast_184_piece0 in memory on spark-master:40291 (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 11:51:26 INFO SparkContext: Created broadcast 184 from broadcast at DAGScheduler.scala:1585
26/02/13 11:51:26 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 188 (MapPartitionsRDD[670] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 11:51:26 INFO TaskSchedulerImpl: Adding task set 188.0 with 4 tasks resource profile 0
26/02/13 11:51:26 INFO TaskSetManager: Starting task 0.0 in stage 188.0 (TID 406) (172.18.0.15, executor 0, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:51:26 INFO TaskSetManager: Starting task 1.0 in stage 188.0 (TID 407) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:51:26 INFO BlockManagerInfo: Added broadcast_184_piece0 in memory on 172.18.0.15:34959 (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 11:51:26 INFO TaskSetManager: Starting task 2.0 in stage 188.0 (TID 408) (172.18.0.15, executor 0, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:51:26 INFO TaskSetManager: Finished task 1.0 in stage 188.0 (TID 407) in 34 ms on 172.18.0.15 (executor 0) (1/4)
26/02/13 11:51:26 INFO TaskSetManager: Starting task 3.0 in stage 188.0 (TID 409) (172.18.0.15, executor 0, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:51:26 INFO TaskSetManager: Finished task 0.0 in stage 188.0 (TID 406) in 38 ms on 172.18.0.15 (executor 0) (2/4)
26/02/13 11:51:26 INFO TaskSetManager: Finished task 3.0 in stage 188.0 (TID 409) in 13 ms on 172.18.0.15 (executor 0) (3/4)
26/02/13 11:51:26 INFO TaskSetManager: Finished task 2.0 in stage 188.0 (TID 408) in 17 ms on 172.18.0.15 (executor 0) (4/4)
26/02/13 11:51:26 INFO TaskSchedulerImpl: Removed TaskSet 188.0, whose tasks have all completed, from pool 
26/02/13 11:51:26 INFO DAGScheduler: ResultStage 188 (start at NativeMethodAccessorImpl.java:0) finished in 0.060 s
26/02/13 11:51:26 INFO DAGScheduler: Job 123 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:51:26 INFO TaskSchedulerImpl: Killing all running tasks in stage 188: Stage finished
26/02/13 11:51:26 INFO DAGScheduler: Job 123 finished: start at NativeMethodAccessorImpl.java:0, took 0.061787 s
26/02/13 11:51:26 INFO MemoryStore: Block broadcast_185_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.8 MiB)
26/02/13 11:51:26 INFO BlockManagerInfo: Added broadcast_185_piece0 in memory on spark-master:40291 (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 11:51:26 INFO SparkContext: Created broadcast 185 from start at NativeMethodAccessorImpl.java:0
26/02/13 11:51:26 INFO BlockManagerInfo: Removed broadcast_182_piece0 on spark-master:40291 in memory (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 11:51:26 INFO BlockManagerInfo: Removed broadcast_182_piece0 on 172.18.0.15:34959 in memory (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 11:51:26 INFO BlockManagerInfo: Removed broadcast_182_piece0 on 172.18.0.14:40939 in memory (size: 107.2 KiB, free: 434.4 MiB)
26/02/13 11:51:26 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 57, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@40fd2477]. The input RDD has 3 partitions.
26/02/13 11:51:26 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:51:26 INFO DAGScheduler: Got job 124 (start at NativeMethodAccessorImpl.java:0) with 3 output partitions
26/02/13 11:51:26 INFO DAGScheduler: Final stage: ResultStage 189 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:51:26 INFO DAGScheduler: Parents of final stage: List()
26/02/13 11:51:26 INFO DAGScheduler: Missing parents: List()
26/02/13 11:51:26 INFO DAGScheduler: Submitting ResultStage 189 (MapPartitionsRDD[676] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:51:26 INFO MemoryStore: Block broadcast_186 stored as values in memory (estimated size 49.9 KiB, free 433.9 MiB)
26/02/13 11:51:26 INFO MemoryStore: Block broadcast_186_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.9 MiB)
26/02/13 11:51:26 INFO BlockManagerInfo: Added broadcast_186_piece0 in memory on spark-master:40291 (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 11:51:26 INFO SparkContext: Created broadcast 186 from broadcast at DAGScheduler.scala:1585
26/02/13 11:51:26 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 189 (MapPartitionsRDD[676] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
26/02/13 11:51:26 INFO TaskSchedulerImpl: Adding task set 189.0 with 3 tasks resource profile 0
26/02/13 11:51:26 INFO TaskSetManager: Starting task 1.0 in stage 189.0 (TID 410) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:51:26 INFO TaskSetManager: Starting task 0.0 in stage 189.0 (TID 411) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:51:26 INFO TaskSetManager: Starting task 2.0 in stage 189.0 (TID 412) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:51:26 INFO BlockManagerInfo: Added broadcast_186_piece0 in memory on 172.18.0.15:34959 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 11:51:26 INFO BlockManagerInfo: Added broadcast_186_piece0 in memory on 172.18.0.14:40939 (size: 19.5 KiB, free: 434.4 MiB)
26/02/13 11:51:26 INFO BlockManagerInfo: Added broadcast_185_piece0 in memory on 172.18.0.15:34959 (size: 107.2 KiB, free: 434.0 MiB)
26/02/13 11:51:26 INFO BlockManagerInfo: Added broadcast_185_piece0 in memory on 172.18.0.14:40939 (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 11:51:27 INFO TaskSetManager: Finished task 1.0 in stage 189.0 (TID 410) in 568 ms on 172.18.0.15 (executor 0) (1/3)
26/02/13 11:51:27 INFO TaskSetManager: Finished task 0.0 in stage 189.0 (TID 411) in 578 ms on 172.18.0.14 (executor 1) (2/3)
26/02/13 11:51:27 INFO TaskSetManager: Finished task 2.0 in stage 189.0 (TID 412) in 579 ms on 172.18.0.14 (executor 1) (3/3)
26/02/13 11:51:27 INFO TaskSchedulerImpl: Removed TaskSet 189.0, whose tasks have all completed, from pool 
26/02/13 11:51:27 INFO DAGScheduler: ResultStage 189 (start at NativeMethodAccessorImpl.java:0) finished in 0.585 s
26/02/13 11:51:27 INFO DAGScheduler: Job 124 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:51:27 INFO TaskSchedulerImpl: Killing all running tasks in stage 189: Stage finished
26/02/13 11:51:27 INFO DAGScheduler: Job 124 finished: start at NativeMethodAccessorImpl.java:0, took 0.588356 s
26/02/13 11:51:27 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 57, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@40fd2477] is committing.
26/02/13 11:51:27 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 57, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@40fd2477] committed.
26/02/13 11:51:27 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/57 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.57.e9bec86a-1ef0-4e93-936d-cc0702361220.tmp
26/02/13 11:51:27 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.57.e9bec86a-1ef0-4e93-936d-cc0702361220.tmp to file:/tmp/spark-checkpoint-enrichment/commits/57
26/02/13 11:51:27 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "fc5c6452-bd70-4c90-becf-772d3bebdb57",
  "name" : null,
  "timestamp" : "2026-02-13T11:51:26.321Z",
  "batchId" : 57,
  "numInputRows" : 8,
  "inputRowsPerSecond" : 666.6666666666666,
  "processedRowsPerSecond" : 8.447729672650476,
  "durationMs" : {
    "addBatch" : 753,
    "commitOffsets" : 67,
    "getBatch" : 1,
    "latestOffset" : 2,
    "queryPlanning" : 30,
    "triggerExecution" : 947,
    "walCommit" : 94
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 4978,
        "1" : 5443,
        "0" : 7122
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 4979,
        "1" : 5448,
        "0" : 7124
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 4979,
        "1" : 5448,
        "0" : 7124
      }
    },
    "numInputRows" : 8,
    "inputRowsPerSecond" : 666.6666666666666,
    "processedRowsPerSecond" : 8.447729672650476,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@7fe9b5de",
    "numOutputRows" : 3
  }
}
26/02/13 11:51:27 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/58 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.58.01e2aa0d-d5cb-4392-a3c5-6b77351e2eef.tmp
26/02/13 11:51:27 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.58.01e2aa0d-d5cb-4392-a3c5-6b77351e2eef.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/58
26/02/13 11:51:27 INFO MicroBatchExecution: Committed offsets for batch 58. Metadata OffsetSeqMetadata(0,1770983487270,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 11:51:27 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:51:27 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:51:27 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:51:27 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:51:27 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:51:27 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:51:27 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#51483 - origin_code.nullCount#51482) > 0)
26/02/13 11:51:27 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#51488 - destination_code.nullCount#51487) > 0)
26/02/13 11:51:27 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#51518 - callsign.nullCount#51517) > 0)
26/02/13 11:51:27 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:51:27 INFO DAGScheduler: Got job 125 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 11:51:27 INFO DAGScheduler: Final stage: ResultStage 191 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:51:27 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 190)
26/02/13 11:51:27 INFO DAGScheduler: Missing parents: List()
26/02/13 11:51:27 INFO DAGScheduler: Submitting ResultStage 191 (MapPartitionsRDD[681] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:51:27 INFO MemoryStore: Block broadcast_187 stored as values in memory (estimated size 99.3 KiB, free 433.8 MiB)
26/02/13 11:51:27 INFO MemoryStore: Block broadcast_187_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 433.7 MiB)
26/02/13 11:51:27 INFO BlockManagerInfo: Added broadcast_187_piece0 in memory on spark-master:40291 (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 11:51:27 INFO SparkContext: Created broadcast 187 from broadcast at DAGScheduler.scala:1585
26/02/13 11:51:27 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 191 (MapPartitionsRDD[681] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 11:51:27 INFO TaskSchedulerImpl: Adding task set 191.0 with 4 tasks resource profile 0
26/02/13 11:51:27 INFO TaskSetManager: Starting task 0.0 in stage 191.0 (TID 413) (172.18.0.15, executor 0, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:51:27 INFO TaskSetManager: Starting task 1.0 in stage 191.0 (TID 414) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:51:27 INFO BlockManagerInfo: Added broadcast_187_piece0 in memory on 172.18.0.15:34959 (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 11:51:27 INFO TaskSetManager: Starting task 2.0 in stage 191.0 (TID 415) (172.18.0.15, executor 0, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:51:27 INFO TaskSetManager: Finished task 1.0 in stage 191.0 (TID 414) in 33 ms on 172.18.0.15 (executor 0) (1/4)
26/02/13 11:51:27 INFO TaskSetManager: Starting task 3.0 in stage 191.0 (TID 416) (172.18.0.15, executor 0, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:51:27 INFO TaskSetManager: Finished task 0.0 in stage 191.0 (TID 413) in 39 ms on 172.18.0.15 (executor 0) (2/4)
26/02/13 11:51:27 INFO TaskSetManager: Finished task 2.0 in stage 191.0 (TID 415) in 18 ms on 172.18.0.15 (executor 0) (3/4)
26/02/13 11:51:27 INFO TaskSetManager: Finished task 3.0 in stage 191.0 (TID 416) in 16 ms on 172.18.0.15 (executor 0) (4/4)
26/02/13 11:51:27 INFO TaskSchedulerImpl: Removed TaskSet 191.0, whose tasks have all completed, from pool 
26/02/13 11:51:27 INFO DAGScheduler: ResultStage 191 (start at NativeMethodAccessorImpl.java:0) finished in 0.064 s
26/02/13 11:51:27 INFO DAGScheduler: Job 125 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:51:27 INFO TaskSchedulerImpl: Killing all running tasks in stage 191: Stage finished
26/02/13 11:51:27 INFO DAGScheduler: Job 125 finished: start at NativeMethodAccessorImpl.java:0, took 0.068196 s
26/02/13 11:51:27 INFO MemoryStore: Block broadcast_188_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.6 MiB)
26/02/13 11:51:27 INFO BlockManagerInfo: Added broadcast_188_piece0 in memory on spark-master:40291 (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 11:51:27 INFO SparkContext: Created broadcast 188 from start at NativeMethodAccessorImpl.java:0
26/02/13 11:51:27 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 58, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@2d9a4a6f]. The input RDD has 3 partitions.
26/02/13 11:51:27 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:51:27 INFO DAGScheduler: Got job 126 (start at NativeMethodAccessorImpl.java:0) with 3 output partitions
26/02/13 11:51:27 INFO DAGScheduler: Final stage: ResultStage 192 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:51:27 INFO DAGScheduler: Parents of final stage: List()
26/02/13 11:51:27 INFO DAGScheduler: Missing parents: List()
26/02/13 11:51:27 INFO DAGScheduler: Submitting ResultStage 192 (MapPartitionsRDD[687] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:51:27 INFO MemoryStore: Block broadcast_189 stored as values in memory (estimated size 49.9 KiB, free 433.6 MiB)
26/02/13 11:51:27 INFO MemoryStore: Block broadcast_189_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.6 MiB)
26/02/13 11:51:27 INFO BlockManagerInfo: Added broadcast_189_piece0 in memory on spark-master:40291 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 11:51:27 INFO SparkContext: Created broadcast 189 from broadcast at DAGScheduler.scala:1585
26/02/13 11:51:27 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 192 (MapPartitionsRDD[687] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
26/02/13 11:51:27 INFO TaskSchedulerImpl: Adding task set 192.0 with 3 tasks resource profile 0
26/02/13 11:51:27 INFO TaskSetManager: Starting task 1.0 in stage 192.0 (TID 417) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:51:27 INFO TaskSetManager: Starting task 0.0 in stage 192.0 (TID 418) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:51:27 INFO TaskSetManager: Starting task 2.0 in stage 192.0 (TID 419) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:51:27 INFO BlockManagerInfo: Added broadcast_189_piece0 in memory on 172.18.0.15:34959 (size: 19.5 KiB, free: 434.0 MiB)
26/02/13 11:51:27 INFO BlockManagerInfo: Added broadcast_189_piece0 in memory on 172.18.0.14:40939 (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 11:51:27 INFO BlockManagerInfo: Added broadcast_188_piece0 in memory on 172.18.0.14:40939 (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 11:51:27 INFO BlockManagerInfo: Added broadcast_188_piece0 in memory on 172.18.0.15:34959 (size: 107.2 KiB, free: 433.9 MiB)
26/02/13 11:51:27 INFO TaskSetManager: Finished task 1.0 in stage 192.0 (TID 417) in 83 ms on 172.18.0.15 (executor 0) (1/3)
26/02/13 11:51:27 INFO TaskSetManager: Finished task 0.0 in stage 192.0 (TID 418) in 93 ms on 172.18.0.14 (executor 1) (2/3)
26/02/13 11:51:27 INFO TaskSetManager: Finished task 2.0 in stage 192.0 (TID 419) in 95 ms on 172.18.0.14 (executor 1) (3/3)
26/02/13 11:51:27 INFO TaskSchedulerImpl: Removed TaskSet 192.0, whose tasks have all completed, from pool 
26/02/13 11:51:27 INFO DAGScheduler: ResultStage 192 (start at NativeMethodAccessorImpl.java:0) finished in 0.102 s
26/02/13 11:51:27 INFO DAGScheduler: Job 126 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:51:27 INFO TaskSchedulerImpl: Killing all running tasks in stage 192: Stage finished
26/02/13 11:51:27 INFO DAGScheduler: Job 126 finished: start at NativeMethodAccessorImpl.java:0, took 0.103884 s
26/02/13 11:51:27 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 58, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@2d9a4a6f] is committing.
26/02/13 11:51:27 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 58, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@2d9a4a6f] committed.
26/02/13 11:51:27 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/58 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.58.6c758559-d9e3-4e89-8083-eda37669c58d.tmp
26/02/13 11:51:27 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.58.6c758559-d9e3-4e89-8083-eda37669c58d.tmp to file:/tmp/spark-checkpoint-enrichment/commits/58
26/02/13 11:51:27 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "fc5c6452-bd70-4c90-becf-772d3bebdb57",
  "name" : null,
  "timestamp" : "2026-02-13T11:51:27.269Z",
  "batchId" : 58,
  "numInputRows" : 236,
  "inputRowsPerSecond" : 248.94514767932492,
  "processedRowsPerSecond" : 536.3636363636364,
  "durationMs" : {
    "addBatch" : 263,
    "commitOffsets" : 59,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 47,
    "triggerExecution" : 440,
    "walCommit" : 70
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 4979,
        "1" : 5448,
        "0" : 7124
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 5058,
        "1" : 5531,
        "0" : 7198
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 5058,
        "1" : 5531,
        "0" : 7198
      }
    },
    "numInputRows" : 236,
    "inputRowsPerSecond" : 248.94514767932492,
    "processedRowsPerSecond" : 536.3636363636364,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@7fe9b5de",
    "numOutputRows" : 52
  }
}
26/02/13 11:51:29 INFO BlockManagerInfo: Removed broadcast_186_piece0 on spark-master:40291 in memory (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 11:51:29 INFO BlockManagerInfo: Removed broadcast_186_piece0 on 172.18.0.15:34959 in memory (size: 19.5 KiB, free: 433.9 MiB)
26/02/13 11:51:29 INFO BlockManagerInfo: Removed broadcast_186_piece0 on 172.18.0.14:40939 in memory (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 11:51:29 INFO BlockManagerInfo: Removed broadcast_187_piece0 on spark-master:40291 in memory (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 11:51:29 INFO BlockManagerInfo: Removed broadcast_187_piece0 on 172.18.0.15:34959 in memory (size: 33.3 KiB, free: 433.9 MiB)
26/02/13 11:51:29 INFO BlockManagerInfo: Removed broadcast_189_piece0 on spark-master:40291 in memory (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 11:51:29 INFO BlockManagerInfo: Removed broadcast_189_piece0 on 172.18.0.15:34959 in memory (size: 19.5 KiB, free: 433.9 MiB)
26/02/13 11:51:29 INFO BlockManagerInfo: Removed broadcast_189_piece0 on 172.18.0.14:40939 in memory (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 11:51:29 INFO BlockManagerInfo: Removed broadcast_184_piece0 on spark-master:40291 in memory (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 11:51:29 INFO BlockManagerInfo: Removed broadcast_184_piece0 on 172.18.0.15:34959 in memory (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 11:51:29 INFO BlockManagerInfo: Removed broadcast_185_piece0 on spark-master:40291 in memory (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 11:51:29 INFO BlockManagerInfo: Removed broadcast_185_piece0 on 172.18.0.15:34959 in memory (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 11:51:29 INFO BlockManagerInfo: Removed broadcast_185_piece0 on 172.18.0.14:40939 in memory (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 11:51:37 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 11:51:42 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/59 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.59.3410bae1-262e-4467-8c11-11976615556d.tmp
26/02/13 11:51:42 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.59.3410bae1-262e-4467-8c11-11976615556d.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/59
26/02/13 11:51:42 INFO MicroBatchExecution: Committed offsets for batch 59. Metadata OffsetSeqMetadata(0,1770983502659,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 11:51:42 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:51:42 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:51:42 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:51:42 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:51:42 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:51:42 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:51:42 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#52337 - origin_code.nullCount#52336) > 0)
26/02/13 11:51:42 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#52342 - destination_code.nullCount#52341) > 0)
26/02/13 11:51:42 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#52372 - callsign.nullCount#52371) > 0)
26/02/13 11:51:42 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:51:42 INFO DAGScheduler: Got job 127 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 11:51:42 INFO DAGScheduler: Final stage: ResultStage 194 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:51:42 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 193)
26/02/13 11:51:42 INFO DAGScheduler: Missing parents: List()
26/02/13 11:51:42 INFO DAGScheduler: Submitting ResultStage 194 (MapPartitionsRDD[692] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:51:42 INFO MemoryStore: Block broadcast_190 stored as values in memory (estimated size 99.3 KiB, free 434.0 MiB)
26/02/13 11:51:42 INFO MemoryStore: Block broadcast_190_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 433.9 MiB)
26/02/13 11:51:42 INFO BlockManagerInfo: Added broadcast_190_piece0 in memory on spark-master:40291 (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 11:51:42 INFO SparkContext: Created broadcast 190 from broadcast at DAGScheduler.scala:1585
26/02/13 11:51:42 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 194 (MapPartitionsRDD[692] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 11:51:42 INFO TaskSchedulerImpl: Adding task set 194.0 with 4 tasks resource profile 0
26/02/13 11:51:42 INFO TaskSetManager: Starting task 0.0 in stage 194.0 (TID 420) (172.18.0.15, executor 0, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:51:42 INFO TaskSetManager: Starting task 1.0 in stage 194.0 (TID 421) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:51:42 INFO BlockManagerInfo: Added broadcast_190_piece0 in memory on 172.18.0.15:34959 (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 11:51:42 INFO TaskSetManager: Starting task 2.0 in stage 194.0 (TID 422) (172.18.0.15, executor 0, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:51:42 INFO TaskSetManager: Finished task 1.0 in stage 194.0 (TID 421) in 24 ms on 172.18.0.15 (executor 0) (1/4)
26/02/13 11:51:42 INFO TaskSetManager: Starting task 3.0 in stage 194.0 (TID 423) (172.18.0.15, executor 0, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:51:42 INFO TaskSetManager: Finished task 0.0 in stage 194.0 (TID 420) in 31 ms on 172.18.0.15 (executor 0) (2/4)
26/02/13 11:51:42 INFO TaskSetManager: Finished task 2.0 in stage 194.0 (TID 422) in 16 ms on 172.18.0.15 (executor 0) (3/4)
26/02/13 11:51:42 INFO TaskSetManager: Finished task 3.0 in stage 194.0 (TID 423) in 15 ms on 172.18.0.15 (executor 0) (4/4)
26/02/13 11:51:42 INFO TaskSchedulerImpl: Removed TaskSet 194.0, whose tasks have all completed, from pool 
26/02/13 11:51:42 INFO DAGScheduler: ResultStage 194 (start at NativeMethodAccessorImpl.java:0) finished in 0.059 s
26/02/13 11:51:42 INFO DAGScheduler: Job 127 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:51:42 INFO TaskSchedulerImpl: Killing all running tasks in stage 194: Stage finished
26/02/13 11:51:42 INFO DAGScheduler: Job 127 finished: start at NativeMethodAccessorImpl.java:0, took 0.061576 s
26/02/13 11:51:42 INFO MemoryStore: Block broadcast_191_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.8 MiB)
26/02/13 11:51:42 INFO BlockManagerInfo: Added broadcast_191_piece0 in memory on spark-master:40291 (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 11:51:42 INFO SparkContext: Created broadcast 191 from start at NativeMethodAccessorImpl.java:0
26/02/13 11:51:42 INFO BlockManagerInfo: Removed broadcast_188_piece0 on 172.18.0.15:34959 in memory (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 11:51:42 INFO BlockManagerInfo: Removed broadcast_188_piece0 on spark-master:40291 in memory (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 11:51:42 INFO BlockManagerInfo: Removed broadcast_188_piece0 on 172.18.0.14:40939 in memory (size: 107.2 KiB, free: 434.4 MiB)
26/02/13 11:51:42 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 59, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@1542b49b]. The input RDD has 3 partitions.
26/02/13 11:51:42 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:51:42 INFO DAGScheduler: Got job 128 (start at NativeMethodAccessorImpl.java:0) with 3 output partitions
26/02/13 11:51:42 INFO DAGScheduler: Final stage: ResultStage 195 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:51:42 INFO DAGScheduler: Parents of final stage: List()
26/02/13 11:51:42 INFO DAGScheduler: Missing parents: List()
26/02/13 11:51:42 INFO DAGScheduler: Submitting ResultStage 195 (MapPartitionsRDD[698] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:51:42 INFO MemoryStore: Block broadcast_192 stored as values in memory (estimated size 49.9 KiB, free 433.9 MiB)
26/02/13 11:51:42 INFO MemoryStore: Block broadcast_192_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.9 MiB)
26/02/13 11:51:42 INFO BlockManagerInfo: Added broadcast_192_piece0 in memory on spark-master:40291 (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 11:51:42 INFO SparkContext: Created broadcast 192 from broadcast at DAGScheduler.scala:1585
26/02/13 11:51:42 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 195 (MapPartitionsRDD[698] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
26/02/13 11:51:42 INFO TaskSchedulerImpl: Adding task set 195.0 with 3 tasks resource profile 0
26/02/13 11:51:42 INFO TaskSetManager: Starting task 1.0 in stage 195.0 (TID 424) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:51:42 INFO TaskSetManager: Starting task 0.0 in stage 195.0 (TID 425) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:51:42 INFO TaskSetManager: Starting task 2.0 in stage 195.0 (TID 426) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:51:42 INFO BlockManagerInfo: Added broadcast_192_piece0 in memory on 172.18.0.15:34959 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 11:51:42 INFO BlockManagerInfo: Added broadcast_192_piece0 in memory on 172.18.0.14:40939 (size: 19.5 KiB, free: 434.4 MiB)
26/02/13 11:51:42 INFO BlockManagerInfo: Added broadcast_191_piece0 in memory on 172.18.0.15:34959 (size: 107.2 KiB, free: 434.0 MiB)
26/02/13 11:51:43 INFO BlockManagerInfo: Added broadcast_191_piece0 in memory on 172.18.0.14:40939 (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 11:51:43 INFO TaskSetManager: Finished task 1.0 in stage 195.0 (TID 424) in 573 ms on 172.18.0.15 (executor 0) (1/3)
26/02/13 11:51:43 INFO TaskSetManager: Finished task 2.0 in stage 195.0 (TID 426) in 616 ms on 172.18.0.14 (executor 1) (2/3)
26/02/13 11:51:43 INFO TaskSetManager: Finished task 0.0 in stage 195.0 (TID 425) in 627 ms on 172.18.0.14 (executor 1) (3/3)
26/02/13 11:51:43 INFO TaskSchedulerImpl: Removed TaskSet 195.0, whose tasks have all completed, from pool 
26/02/13 11:51:43 INFO DAGScheduler: ResultStage 195 (start at NativeMethodAccessorImpl.java:0) finished in 0.633 s
26/02/13 11:51:43 INFO DAGScheduler: Job 128 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:51:43 INFO TaskSchedulerImpl: Killing all running tasks in stage 195: Stage finished
26/02/13 11:51:43 INFO DAGScheduler: Job 128 finished: start at NativeMethodAccessorImpl.java:0, took 0.636174 s
26/02/13 11:51:43 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 59, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@1542b49b] is committing.
26/02/13 11:51:43 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 59, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@1542b49b] committed.
26/02/13 11:51:43 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/59 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.59.63f9228f-0ab6-4b1e-b3b5-bab544d6bdc3.tmp
26/02/13 11:51:43 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.59.63f9228f-0ab6-4b1e-b3b5-bab544d6bdc3.tmp to file:/tmp/spark-checkpoint-enrichment/commits/59
26/02/13 11:51:43 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "fc5c6452-bd70-4c90-becf-772d3bebdb57",
  "name" : null,
  "timestamp" : "2026-02-13T11:51:42.658Z",
  "batchId" : 59,
  "numInputRows" : 106,
  "inputRowsPerSecond" : 8833.333333333334,
  "processedRowsPerSecond" : 105.36779324055667,
  "durationMs" : {
    "addBatch" : 826,
    "commitOffsets" : 74,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 39,
    "triggerExecution" : 1006,
    "walCommit" : 66
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 5058,
        "1" : 5531,
        "0" : 7198
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 5090,
        "1" : 5572,
        "0" : 7231
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 5090,
        "1" : 5572,
        "0" : 7231
      }
    },
    "numInputRows" : 106,
    "inputRowsPerSecond" : 8833.333333333334,
    "processedRowsPerSecond" : 105.36779324055667,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@7fe9b5de",
    "numOutputRows" : 25
  }
}
26/02/13 11:51:43 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/60 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.60.bff78e58-2056-4aa7-bf8c-1c776973c1c6.tmp
26/02/13 11:51:43 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.60.bff78e58-2056-4aa7-bf8c-1c776973c1c6.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/60
26/02/13 11:51:43 INFO MicroBatchExecution: Committed offsets for batch 60. Metadata OffsetSeqMetadata(0,1770983503667,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 11:51:43 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:51:43 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:51:43 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:51:43 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:51:43 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:51:43 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:51:43 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#53191 - origin_code.nullCount#53190) > 0)
26/02/13 11:51:43 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#53196 - destination_code.nullCount#53195) > 0)
26/02/13 11:51:43 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#53226 - callsign.nullCount#53225) > 0)
26/02/13 11:51:43 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:51:43 INFO DAGScheduler: Got job 129 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 11:51:43 INFO DAGScheduler: Final stage: ResultStage 197 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:51:43 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 196)
26/02/13 11:51:43 INFO DAGScheduler: Missing parents: List()
26/02/13 11:51:43 INFO DAGScheduler: Submitting ResultStage 197 (MapPartitionsRDD[703] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:51:43 INFO MemoryStore: Block broadcast_193 stored as values in memory (estimated size 99.3 KiB, free 433.8 MiB)
26/02/13 11:51:43 INFO MemoryStore: Block broadcast_193_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 433.7 MiB)
26/02/13 11:51:43 INFO BlockManagerInfo: Added broadcast_193_piece0 in memory on spark-master:40291 (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 11:51:43 INFO SparkContext: Created broadcast 193 from broadcast at DAGScheduler.scala:1585
26/02/13 11:51:43 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 197 (MapPartitionsRDD[703] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 11:51:43 INFO TaskSchedulerImpl: Adding task set 197.0 with 4 tasks resource profile 0
26/02/13 11:51:43 INFO TaskSetManager: Starting task 0.0 in stage 197.0 (TID 427) (172.18.0.15, executor 0, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:51:43 INFO TaskSetManager: Starting task 1.0 in stage 197.0 (TID 428) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:51:43 INFO BlockManagerInfo: Added broadcast_193_piece0 in memory on 172.18.0.15:34959 (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 11:51:43 INFO TaskSetManager: Starting task 2.0 in stage 197.0 (TID 429) (172.18.0.15, executor 0, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:51:43 INFO TaskSetManager: Finished task 0.0 in stage 197.0 (TID 427) in 52 ms on 172.18.0.15 (executor 0) (1/4)
26/02/13 11:51:43 INFO TaskSetManager: Starting task 3.0 in stage 197.0 (TID 430) (172.18.0.15, executor 0, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:51:43 INFO TaskSetManager: Finished task 1.0 in stage 197.0 (TID 428) in 58 ms on 172.18.0.15 (executor 0) (2/4)
26/02/13 11:51:43 INFO TaskSetManager: Finished task 2.0 in stage 197.0 (TID 429) in 31 ms on 172.18.0.15 (executor 0) (3/4)
26/02/13 11:51:44 INFO TaskSetManager: Finished task 3.0 in stage 197.0 (TID 430) in 34 ms on 172.18.0.15 (executor 0) (4/4)
26/02/13 11:51:44 INFO TaskSchedulerImpl: Removed TaskSet 197.0, whose tasks have all completed, from pool 
26/02/13 11:51:44 INFO DAGScheduler: ResultStage 197 (start at NativeMethodAccessorImpl.java:0) finished in 0.104 s
26/02/13 11:51:44 INFO DAGScheduler: Job 129 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:51:44 INFO TaskSchedulerImpl: Killing all running tasks in stage 197: Stage finished
26/02/13 11:51:44 INFO DAGScheduler: Job 129 finished: start at NativeMethodAccessorImpl.java:0, took 0.110536 s
26/02/13 11:51:44 INFO MemoryStore: Block broadcast_194_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.6 MiB)
26/02/13 11:51:44 INFO BlockManagerInfo: Added broadcast_194_piece0 in memory on spark-master:40291 (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 11:51:44 INFO SparkContext: Created broadcast 194 from start at NativeMethodAccessorImpl.java:0
26/02/13 11:51:44 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 60, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@4d92dc43]. The input RDD has 3 partitions.
26/02/13 11:51:44 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:51:44 INFO DAGScheduler: Got job 130 (start at NativeMethodAccessorImpl.java:0) with 3 output partitions
26/02/13 11:51:44 INFO DAGScheduler: Final stage: ResultStage 198 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:51:44 INFO DAGScheduler: Parents of final stage: List()
26/02/13 11:51:44 INFO DAGScheduler: Missing parents: List()
26/02/13 11:51:44 INFO DAGScheduler: Submitting ResultStage 198 (MapPartitionsRDD[709] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:51:44 INFO MemoryStore: Block broadcast_195 stored as values in memory (estimated size 49.9 KiB, free 433.6 MiB)
26/02/13 11:51:44 INFO MemoryStore: Block broadcast_195_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.6 MiB)
26/02/13 11:51:44 INFO BlockManagerInfo: Added broadcast_195_piece0 in memory on spark-master:40291 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 11:51:44 INFO SparkContext: Created broadcast 195 from broadcast at DAGScheduler.scala:1585
26/02/13 11:51:44 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 198 (MapPartitionsRDD[709] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
26/02/13 11:51:44 INFO TaskSchedulerImpl: Adding task set 198.0 with 3 tasks resource profile 0
26/02/13 11:51:44 INFO TaskSetManager: Starting task 0.0 in stage 198.0 (TID 431) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:51:44 INFO TaskSetManager: Starting task 1.0 in stage 198.0 (TID 432) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:51:44 INFO TaskSetManager: Starting task 2.0 in stage 198.0 (TID 433) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:51:44 INFO BlockManagerInfo: Added broadcast_195_piece0 in memory on 172.18.0.15:34959 (size: 19.5 KiB, free: 434.0 MiB)
26/02/13 11:51:44 INFO BlockManagerInfo: Added broadcast_195_piece0 in memory on 172.18.0.14:40939 (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 11:51:44 INFO BlockManagerInfo: Added broadcast_194_piece0 in memory on 172.18.0.15:34959 (size: 107.2 KiB, free: 433.9 MiB)
26/02/13 11:51:44 INFO BlockManagerInfo: Added broadcast_194_piece0 in memory on 172.18.0.14:40939 (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 11:51:44 INFO TaskSetManager: Finished task 1.0 in stage 198.0 (TID 432) in 132 ms on 172.18.0.15 (executor 0) (1/3)
26/02/13 11:51:44 INFO TaskSetManager: Finished task 0.0 in stage 198.0 (TID 431) in 197 ms on 172.18.0.14 (executor 1) (2/3)
26/02/13 11:51:44 INFO TaskSetManager: Finished task 2.0 in stage 198.0 (TID 433) in 197 ms on 172.18.0.14 (executor 1) (3/3)
26/02/13 11:51:44 INFO TaskSchedulerImpl: Removed TaskSet 198.0, whose tasks have all completed, from pool 
26/02/13 11:51:44 INFO DAGScheduler: ResultStage 198 (start at NativeMethodAccessorImpl.java:0) finished in 0.211 s
26/02/13 11:51:44 INFO DAGScheduler: Job 130 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:51:44 INFO TaskSchedulerImpl: Killing all running tasks in stage 198: Stage finished
26/02/13 11:51:44 INFO DAGScheduler: Job 130 finished: start at NativeMethodAccessorImpl.java:0, took 0.214178 s
26/02/13 11:51:44 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 60, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@4d92dc43] is committing.
26/02/13 11:51:44 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 60, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@4d92dc43] committed.
26/02/13 11:51:44 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/60 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.60.dd0fd98f-cba2-46f9-8496-cb6a552417dc.tmp
26/02/13 11:51:44 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.60.dd0fd98f-cba2-46f9-8496-cb6a552417dc.tmp to file:/tmp/spark-checkpoint-enrichment/commits/60
26/02/13 11:51:44 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "fc5c6452-bd70-4c90-becf-772d3bebdb57",
  "name" : null,
  "timestamp" : "2026-02-13T11:51:43.665Z",
  "batchId" : 60,
  "numInputRows" : 141,
  "inputRowsPerSecond" : 140.0198609731877,
  "processedRowsPerSecond" : 209.82142857142856,
  "durationMs" : {
    "addBatch" : 494,
    "commitOffsets" : 72,
    "getBatch" : 0,
    "latestOffset" : 2,
    "queryPlanning" : 36,
    "triggerExecution" : 672,
    "walCommit" : 68
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 5090,
        "1" : 5572,
        "0" : 7231
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 5139,
        "1" : 5620,
        "0" : 7275
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 5139,
        "1" : 5620,
        "0" : 7275
      }
    },
    "numInputRows" : 141,
    "inputRowsPerSecond" : 140.0198609731877,
    "processedRowsPerSecond" : 209.82142857142856,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@7fe9b5de",
    "numOutputRows" : 29
  }
}
26/02/13 11:51:45 INFO BlockManagerInfo: Removed broadcast_190_piece0 on spark-master:40291 in memory (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 11:51:45 INFO BlockManagerInfo: Removed broadcast_190_piece0 on 172.18.0.15:34959 in memory (size: 33.3 KiB, free: 433.9 MiB)
26/02/13 11:51:45 INFO BlockManagerInfo: Removed broadcast_195_piece0 on spark-master:40291 in memory (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 11:51:45 INFO BlockManagerInfo: Removed broadcast_195_piece0 on 172.18.0.15:34959 in memory (size: 19.5 KiB, free: 433.9 MiB)
26/02/13 11:51:45 INFO BlockManagerInfo: Removed broadcast_195_piece0 on 172.18.0.14:40939 in memory (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 11:51:45 INFO BlockManagerInfo: Removed broadcast_191_piece0 on spark-master:40291 in memory (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 11:51:45 INFO BlockManagerInfo: Removed broadcast_191_piece0 on 172.18.0.15:34959 in memory (size: 107.2 KiB, free: 434.0 MiB)
26/02/13 11:51:45 INFO BlockManagerInfo: Removed broadcast_191_piece0 on 172.18.0.14:40939 in memory (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 11:51:45 INFO BlockManagerInfo: Removed broadcast_192_piece0 on spark-master:40291 in memory (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 11:51:45 INFO BlockManagerInfo: Removed broadcast_192_piece0 on 172.18.0.15:34959 in memory (size: 19.5 KiB, free: 434.0 MiB)
26/02/13 11:51:45 INFO BlockManagerInfo: Removed broadcast_192_piece0 on 172.18.0.14:40939 in memory (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 11:51:45 INFO BlockManagerInfo: Removed broadcast_193_piece0 on spark-master:40291 in memory (size: 33.3 KiB, free: 434.3 MiB)
26/02/13 11:51:45 INFO BlockManagerInfo: Removed broadcast_193_piece0 on 172.18.0.15:34959 in memory (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 11:51:54 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 11:51:59 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/61 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.61.f292dfc5-b3c9-4b89-9221-3507e4320f90.tmp
26/02/13 11:51:59 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.61.f292dfc5-b3c9-4b89-9221-3507e4320f90.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/61
26/02/13 11:51:59 INFO MicroBatchExecution: Committed offsets for batch 61. Metadata OffsetSeqMetadata(0,1770983519140,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 11:51:59 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:51:59 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:51:59 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:51:59 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:51:59 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:51:59 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:51:59 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#54045 - origin_code.nullCount#54044) > 0)
26/02/13 11:51:59 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#54050 - destination_code.nullCount#54049) > 0)
26/02/13 11:51:59 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#54080 - callsign.nullCount#54079) > 0)
26/02/13 11:51:59 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:51:59 INFO DAGScheduler: Got job 131 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 11:51:59 INFO DAGScheduler: Final stage: ResultStage 200 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:51:59 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 199)
26/02/13 11:51:59 INFO DAGScheduler: Missing parents: List()
26/02/13 11:51:59 INFO DAGScheduler: Submitting ResultStage 200 (MapPartitionsRDD[714] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:51:59 INFO MemoryStore: Block broadcast_196 stored as values in memory (estimated size 99.3 KiB, free 434.0 MiB)
26/02/13 11:51:59 INFO MemoryStore: Block broadcast_196_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 433.9 MiB)
26/02/13 11:51:59 INFO BlockManagerInfo: Added broadcast_196_piece0 in memory on spark-master:40291 (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 11:51:59 INFO SparkContext: Created broadcast 196 from broadcast at DAGScheduler.scala:1585
26/02/13 11:51:59 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 200 (MapPartitionsRDD[714] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 11:51:59 INFO TaskSchedulerImpl: Adding task set 200.0 with 4 tasks resource profile 0
26/02/13 11:51:59 INFO TaskSetManager: Starting task 0.0 in stage 200.0 (TID 434) (172.18.0.15, executor 0, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:51:59 INFO TaskSetManager: Starting task 1.0 in stage 200.0 (TID 435) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:51:59 INFO BlockManagerInfo: Added broadcast_196_piece0 in memory on 172.18.0.15:34959 (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 11:51:59 INFO TaskSetManager: Starting task 2.0 in stage 200.0 (TID 436) (172.18.0.15, executor 0, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:51:59 INFO TaskSetManager: Finished task 1.0 in stage 200.0 (TID 435) in 18 ms on 172.18.0.15 (executor 0) (1/4)
26/02/13 11:51:59 INFO TaskSetManager: Starting task 3.0 in stage 200.0 (TID 437) (172.18.0.15, executor 0, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:51:59 INFO TaskSetManager: Finished task 0.0 in stage 200.0 (TID 434) in 19 ms on 172.18.0.15 (executor 0) (2/4)
26/02/13 11:51:59 INFO TaskSetManager: Finished task 2.0 in stage 200.0 (TID 436) in 13 ms on 172.18.0.15 (executor 0) (3/4)
26/02/13 11:51:59 INFO TaskSetManager: Finished task 3.0 in stage 200.0 (TID 437) in 18 ms on 172.18.0.15 (executor 0) (4/4)
26/02/13 11:51:59 INFO TaskSchedulerImpl: Removed TaskSet 200.0, whose tasks have all completed, from pool 
26/02/13 11:51:59 INFO DAGScheduler: ResultStage 200 (start at NativeMethodAccessorImpl.java:0) finished in 0.044 s
26/02/13 11:51:59 INFO DAGScheduler: Job 131 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:51:59 INFO TaskSchedulerImpl: Killing all running tasks in stage 200: Stage finished
26/02/13 11:51:59 INFO DAGScheduler: Job 131 finished: start at NativeMethodAccessorImpl.java:0, took 0.046068 s
26/02/13 11:51:59 INFO MemoryStore: Block broadcast_197_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.8 MiB)
26/02/13 11:51:59 INFO BlockManagerInfo: Added broadcast_197_piece0 in memory on spark-master:40291 (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 11:51:59 INFO SparkContext: Created broadcast 197 from start at NativeMethodAccessorImpl.java:0
26/02/13 11:51:59 INFO BlockManagerInfo: Removed broadcast_194_piece0 on 172.18.0.15:34959 in memory (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 11:51:59 INFO BlockManagerInfo: Removed broadcast_194_piece0 on spark-master:40291 in memory (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 11:51:59 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 61, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@15811a92]. The input RDD has 3 partitions.
26/02/13 11:51:59 INFO BlockManagerInfo: Removed broadcast_194_piece0 on 172.18.0.14:40939 in memory (size: 107.2 KiB, free: 434.4 MiB)
26/02/13 11:51:59 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:51:59 INFO DAGScheduler: Got job 132 (start at NativeMethodAccessorImpl.java:0) with 3 output partitions
26/02/13 11:51:59 INFO DAGScheduler: Final stage: ResultStage 201 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:51:59 INFO DAGScheduler: Parents of final stage: List()
26/02/13 11:51:59 INFO DAGScheduler: Missing parents: List()
26/02/13 11:51:59 INFO DAGScheduler: Submitting ResultStage 201 (MapPartitionsRDD[720] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:51:59 INFO MemoryStore: Block broadcast_198 stored as values in memory (estimated size 49.9 KiB, free 433.9 MiB)
26/02/13 11:51:59 INFO MemoryStore: Block broadcast_198_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.9 MiB)
26/02/13 11:51:59 INFO BlockManagerInfo: Added broadcast_198_piece0 in memory on spark-master:40291 (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 11:51:59 INFO SparkContext: Created broadcast 198 from broadcast at DAGScheduler.scala:1585
26/02/13 11:51:59 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 201 (MapPartitionsRDD[720] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
26/02/13 11:51:59 INFO TaskSchedulerImpl: Adding task set 201.0 with 3 tasks resource profile 0
26/02/13 11:51:59 INFO TaskSetManager: Starting task 0.0 in stage 201.0 (TID 438) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:51:59 INFO TaskSetManager: Starting task 1.0 in stage 201.0 (TID 439) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:51:59 INFO TaskSetManager: Starting task 2.0 in stage 201.0 (TID 440) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:51:59 INFO BlockManagerInfo: Added broadcast_198_piece0 in memory on 172.18.0.15:34959 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 11:51:59 INFO BlockManagerInfo: Added broadcast_198_piece0 in memory on 172.18.0.14:40939 (size: 19.5 KiB, free: 434.4 MiB)
26/02/13 11:51:59 INFO BlockManagerInfo: Added broadcast_197_piece0 in memory on 172.18.0.15:34959 (size: 107.2 KiB, free: 434.0 MiB)
26/02/13 11:51:59 INFO BlockManagerInfo: Added broadcast_197_piece0 in memory on 172.18.0.14:40939 (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 11:51:59 INFO TaskSetManager: Finished task 1.0 in stage 201.0 (TID 439) in 557 ms on 172.18.0.15 (executor 0) (1/3)
26/02/13 11:52:00 INFO TaskSetManager: Finished task 2.0 in stage 201.0 (TID 440) in 581 ms on 172.18.0.14 (executor 1) (2/3)
26/02/13 11:52:00 INFO TaskSetManager: Finished task 0.0 in stage 201.0 (TID 438) in 584 ms on 172.18.0.14 (executor 1) (3/3)
26/02/13 11:52:00 INFO TaskSchedulerImpl: Removed TaskSet 201.0, whose tasks have all completed, from pool 
26/02/13 11:52:00 INFO DAGScheduler: ResultStage 201 (start at NativeMethodAccessorImpl.java:0) finished in 0.590 s
26/02/13 11:52:00 INFO DAGScheduler: Job 132 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:52:00 INFO TaskSchedulerImpl: Killing all running tasks in stage 201: Stage finished
26/02/13 11:52:00 INFO DAGScheduler: Job 132 finished: start at NativeMethodAccessorImpl.java:0, took 0.592242 s
26/02/13 11:52:00 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 61, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@15811a92] is committing.
26/02/13 11:52:00 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 61, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@15811a92] committed.
26/02/13 11:52:00 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/61 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.61.d88b9eae-bacd-4390-8bf3-cd0df64cfdff.tmp
26/02/13 11:52:00 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.61.d88b9eae-bacd-4390-8bf3-cd0df64cfdff.tmp to file:/tmp/spark-checkpoint-enrichment/commits/61
26/02/13 11:52:00 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "fc5c6452-bd70-4c90-becf-772d3bebdb57",
  "name" : null,
  "timestamp" : "2026-02-13T11:51:59.138Z",
  "batchId" : 61,
  "numInputRows" : 93,
  "inputRowsPerSecond" : 8454.545454545456,
  "processedRowsPerSecond" : 99.2529348986126,
  "durationMs" : {
    "addBatch" : 756,
    "commitOffsets" : 56,
    "getBatch" : 1,
    "latestOffset" : 1,
    "queryPlanning" : 53,
    "triggerExecution" : 937,
    "walCommit" : 69
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 5139,
        "1" : 5620,
        "0" : 7275
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 5170,
        "1" : 5654,
        "0" : 7303
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 5170,
        "1" : 5654,
        "0" : 7303
      }
    },
    "numInputRows" : 93,
    "inputRowsPerSecond" : 8454.545454545456,
    "processedRowsPerSecond" : 99.2529348986126,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@7fe9b5de",
    "numOutputRows" : 23
  }
}
26/02/13 11:52:00 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/62 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.62.f01794a3-7897-42f5-9eb8-79e174baab93.tmp
26/02/13 11:52:00 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.62.f01794a3-7897-42f5-9eb8-79e174baab93.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/62
26/02/13 11:52:00 INFO MicroBatchExecution: Committed offsets for batch 62. Metadata OffsetSeqMetadata(0,1770983520077,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 11:52:00 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:52:00 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:52:00 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:52:00 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:52:00 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:52:00 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:52:00 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#54899 - origin_code.nullCount#54898) > 0)
26/02/13 11:52:00 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#54904 - destination_code.nullCount#54903) > 0)
26/02/13 11:52:00 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#54934 - callsign.nullCount#54933) > 0)
26/02/13 11:52:00 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:52:00 INFO DAGScheduler: Got job 133 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 11:52:00 INFO DAGScheduler: Final stage: ResultStage 203 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:52:00 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 202)
26/02/13 11:52:00 INFO DAGScheduler: Missing parents: List()
26/02/13 11:52:00 INFO DAGScheduler: Submitting ResultStage 203 (MapPartitionsRDD[725] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:52:00 INFO MemoryStore: Block broadcast_199 stored as values in memory (estimated size 99.3 KiB, free 433.8 MiB)
26/02/13 11:52:00 INFO MemoryStore: Block broadcast_199_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 433.7 MiB)
26/02/13 11:52:00 INFO BlockManagerInfo: Added broadcast_199_piece0 in memory on spark-master:40291 (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 11:52:00 INFO SparkContext: Created broadcast 199 from broadcast at DAGScheduler.scala:1585
26/02/13 11:52:00 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 203 (MapPartitionsRDD[725] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 11:52:00 INFO TaskSchedulerImpl: Adding task set 203.0 with 4 tasks resource profile 0
26/02/13 11:52:00 INFO TaskSetManager: Starting task 0.0 in stage 203.0 (TID 441) (172.18.0.15, executor 0, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:52:00 INFO TaskSetManager: Starting task 1.0 in stage 203.0 (TID 442) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:52:00 INFO BlockManagerInfo: Added broadcast_199_piece0 in memory on 172.18.0.15:34959 (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 11:52:00 INFO TaskSetManager: Starting task 2.0 in stage 203.0 (TID 443) (172.18.0.15, executor 0, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:52:00 INFO TaskSetManager: Finished task 1.0 in stage 203.0 (TID 442) in 14 ms on 172.18.0.15 (executor 0) (1/4)
26/02/13 11:52:00 INFO TaskSetManager: Starting task 3.0 in stage 203.0 (TID 444) (172.18.0.15, executor 0, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:52:00 INFO TaskSetManager: Finished task 2.0 in stage 203.0 (TID 443) in 9 ms on 172.18.0.15 (executor 0) (2/4)
26/02/13 11:52:00 INFO TaskSetManager: Finished task 0.0 in stage 203.0 (TID 441) in 24 ms on 172.18.0.15 (executor 0) (3/4)
26/02/13 11:52:00 INFO TaskSetManager: Finished task 3.0 in stage 203.0 (TID 444) in 17 ms on 172.18.0.15 (executor 0) (4/4)
26/02/13 11:52:00 INFO TaskSchedulerImpl: Removed TaskSet 203.0, whose tasks have all completed, from pool 
26/02/13 11:52:00 INFO DAGScheduler: ResultStage 203 (start at NativeMethodAccessorImpl.java:0) finished in 0.046 s
26/02/13 11:52:00 INFO DAGScheduler: Job 133 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:52:00 INFO TaskSchedulerImpl: Killing all running tasks in stage 203: Stage finished
26/02/13 11:52:00 INFO DAGScheduler: Job 133 finished: start at NativeMethodAccessorImpl.java:0, took 0.048059 s
26/02/13 11:52:00 INFO MemoryStore: Block broadcast_200_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.6 MiB)
26/02/13 11:52:00 INFO BlockManagerInfo: Added broadcast_200_piece0 in memory on spark-master:40291 (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 11:52:00 INFO SparkContext: Created broadcast 200 from start at NativeMethodAccessorImpl.java:0
26/02/13 11:52:00 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 62, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@1cafe008]. The input RDD has 3 partitions.
26/02/13 11:52:00 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:52:00 INFO DAGScheduler: Got job 134 (start at NativeMethodAccessorImpl.java:0) with 3 output partitions
26/02/13 11:52:00 INFO DAGScheduler: Final stage: ResultStage 204 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:52:00 INFO DAGScheduler: Parents of final stage: List()
26/02/13 11:52:00 INFO DAGScheduler: Missing parents: List()
26/02/13 11:52:00 INFO DAGScheduler: Submitting ResultStage 204 (MapPartitionsRDD[731] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:52:00 INFO MemoryStore: Block broadcast_201 stored as values in memory (estimated size 49.9 KiB, free 433.6 MiB)
26/02/13 11:52:00 INFO MemoryStore: Block broadcast_201_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.6 MiB)
26/02/13 11:52:00 INFO BlockManagerInfo: Added broadcast_201_piece0 in memory on spark-master:40291 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 11:52:00 INFO SparkContext: Created broadcast 201 from broadcast at DAGScheduler.scala:1585
26/02/13 11:52:00 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 204 (MapPartitionsRDD[731] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
26/02/13 11:52:00 INFO TaskSchedulerImpl: Adding task set 204.0 with 3 tasks resource profile 0
26/02/13 11:52:00 INFO TaskSetManager: Starting task 0.0 in stage 204.0 (TID 445) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:52:00 INFO TaskSetManager: Starting task 1.0 in stage 204.0 (TID 446) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:52:00 INFO TaskSetManager: Starting task 2.0 in stage 204.0 (TID 447) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:52:00 INFO BlockManagerInfo: Added broadcast_201_piece0 in memory on 172.18.0.14:40939 (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 11:52:00 INFO BlockManagerInfo: Added broadcast_201_piece0 in memory on 172.18.0.15:34959 (size: 19.5 KiB, free: 434.0 MiB)
26/02/13 11:52:00 INFO BlockManagerInfo: Added broadcast_200_piece0 in memory on 172.18.0.15:34959 (size: 107.2 KiB, free: 433.9 MiB)
26/02/13 11:52:00 INFO BlockManagerInfo: Added broadcast_200_piece0 in memory on 172.18.0.14:40939 (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 11:52:00 INFO TaskSetManager: Finished task 1.0 in stage 204.0 (TID 446) in 53 ms on 172.18.0.15 (executor 0) (1/3)
26/02/13 11:52:00 INFO TaskSetManager: Finished task 2.0 in stage 204.0 (TID 447) in 63 ms on 172.18.0.14 (executor 1) (2/3)
26/02/13 11:52:00 INFO TaskSetManager: Finished task 0.0 in stage 204.0 (TID 445) in 71 ms on 172.18.0.14 (executor 1) (3/3)
26/02/13 11:52:00 INFO TaskSchedulerImpl: Removed TaskSet 204.0, whose tasks have all completed, from pool 
26/02/13 11:52:00 INFO DAGScheduler: ResultStage 204 (start at NativeMethodAccessorImpl.java:0) finished in 0.077 s
26/02/13 11:52:00 INFO DAGScheduler: Job 134 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:52:00 INFO TaskSchedulerImpl: Killing all running tasks in stage 204: Stage finished
26/02/13 11:52:00 INFO DAGScheduler: Job 134 finished: start at NativeMethodAccessorImpl.java:0, took 0.078908 s
26/02/13 11:52:00 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 62, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@1cafe008] is committing.
26/02/13 11:52:00 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 62, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@1cafe008] committed.
26/02/13 11:52:00 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/62 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.62.4be833a0-4c6b-425a-b10e-3de905b2c78e.tmp
26/02/13 11:52:00 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.62.4be833a0-4c6b-425a-b10e-3de905b2c78e.tmp to file:/tmp/spark-checkpoint-enrichment/commits/62
26/02/13 11:52:00 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "fc5c6452-bd70-4c90-becf-772d3bebdb57",
  "name" : null,
  "timestamp" : "2026-02-13T11:52:00.076Z",
  "batchId" : 62,
  "numInputRows" : 154,
  "inputRowsPerSecond" : 164.17910447761196,
  "processedRowsPerSecond" : 426.5927977839335,
  "durationMs" : {
    "addBatch" : 203,
    "commitOffsets" : 58,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 43,
    "triggerExecution" : 361,
    "walCommit" : 56
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 5170,
        "1" : 5654,
        "0" : 7303
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 5219,
        "1" : 5709,
        "0" : 7353
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 5219,
        "1" : 5709,
        "0" : 7353
      }
    },
    "numInputRows" : 154,
    "inputRowsPerSecond" : 164.17910447761196,
    "processedRowsPerSecond" : 426.5927977839335,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@7fe9b5de",
    "numOutputRows" : 31
  }
}
26/02/13 11:52:01 INFO BlockManagerInfo: Removed broadcast_199_piece0 on spark-master:40291 in memory (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 11:52:01 INFO BlockManagerInfo: Removed broadcast_199_piece0 on 172.18.0.15:34959 in memory (size: 33.3 KiB, free: 433.9 MiB)
26/02/13 11:52:01 INFO BlockManagerInfo: Removed broadcast_197_piece0 on spark-master:40291 in memory (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 11:52:01 INFO BlockManagerInfo: Removed broadcast_197_piece0 on 172.18.0.15:34959 in memory (size: 107.2 KiB, free: 434.0 MiB)
26/02/13 11:52:01 INFO BlockManagerInfo: Removed broadcast_197_piece0 on 172.18.0.14:40939 in memory (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 11:52:01 INFO BlockManagerInfo: Removed broadcast_201_piece0 on spark-master:40291 in memory (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 11:52:01 INFO BlockManagerInfo: Removed broadcast_201_piece0 on 172.18.0.14:40939 in memory (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 11:52:01 INFO BlockManagerInfo: Removed broadcast_201_piece0 on 172.18.0.15:34959 in memory (size: 19.5 KiB, free: 434.0 MiB)
26/02/13 11:52:01 INFO BlockManagerInfo: Removed broadcast_198_piece0 on spark-master:40291 in memory (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 11:52:01 INFO BlockManagerInfo: Removed broadcast_198_piece0 on 172.18.0.15:34959 in memory (size: 19.5 KiB, free: 434.0 MiB)
26/02/13 11:52:01 INFO BlockManagerInfo: Removed broadcast_198_piece0 on 172.18.0.14:40939 in memory (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 11:52:01 INFO BlockManagerInfo: Removed broadcast_196_piece0 on spark-master:40291 in memory (size: 33.3 KiB, free: 434.3 MiB)
26/02/13 11:52:01 INFO BlockManagerInfo: Removed broadcast_196_piece0 on 172.18.0.15:34959 in memory (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 11:52:10 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 11:52:15 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/63 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.63.0981d4df-d923-4668-973a-7b4dee3998fb.tmp
26/02/13 11:52:15 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.63.0981d4df-d923-4668-973a-7b4dee3998fb.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/63
26/02/13 11:52:15 INFO MicroBatchExecution: Committed offsets for batch 63. Metadata OffsetSeqMetadata(0,1770983535289,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 11:52:15 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:52:15 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:52:15 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:52:15 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:52:15 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:52:15 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:52:15 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#55753 - origin_code.nullCount#55752) > 0)
26/02/13 11:52:15 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#55758 - destination_code.nullCount#55757) > 0)
26/02/13 11:52:15 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#55788 - callsign.nullCount#55787) > 0)
26/02/13 11:52:15 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:52:15 INFO DAGScheduler: Got job 135 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 11:52:15 INFO DAGScheduler: Final stage: ResultStage 206 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:52:15 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 205)
26/02/13 11:52:15 INFO DAGScheduler: Missing parents: List()
26/02/13 11:52:15 INFO DAGScheduler: Submitting ResultStage 206 (MapPartitionsRDD[736] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:52:15 INFO MemoryStore: Block broadcast_202 stored as values in memory (estimated size 99.3 KiB, free 434.0 MiB)
26/02/13 11:52:15 INFO MemoryStore: Block broadcast_202_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 433.9 MiB)
26/02/13 11:52:15 INFO BlockManagerInfo: Added broadcast_202_piece0 in memory on spark-master:40291 (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 11:52:15 INFO SparkContext: Created broadcast 202 from broadcast at DAGScheduler.scala:1585
26/02/13 11:52:15 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 206 (MapPartitionsRDD[736] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 11:52:15 INFO TaskSchedulerImpl: Adding task set 206.0 with 4 tasks resource profile 0
26/02/13 11:52:15 INFO TaskSetManager: Starting task 0.0 in stage 206.0 (TID 448) (172.18.0.15, executor 0, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:52:15 INFO TaskSetManager: Starting task 1.0 in stage 206.0 (TID 449) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:52:15 INFO BlockManagerInfo: Added broadcast_202_piece0 in memory on 172.18.0.15:34959 (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 11:52:15 INFO TaskSetManager: Starting task 2.0 in stage 206.0 (TID 450) (172.18.0.15, executor 0, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:52:15 INFO TaskSetManager: Finished task 0.0 in stage 206.0 (TID 448) in 25 ms on 172.18.0.15 (executor 0) (1/4)
26/02/13 11:52:15 INFO TaskSetManager: Starting task 3.0 in stage 206.0 (TID 451) (172.18.0.15, executor 0, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:52:15 INFO TaskSetManager: Finished task 1.0 in stage 206.0 (TID 449) in 29 ms on 172.18.0.15 (executor 0) (2/4)
26/02/13 11:52:15 INFO TaskSetManager: Finished task 2.0 in stage 206.0 (TID 450) in 14 ms on 172.18.0.15 (executor 0) (3/4)
26/02/13 11:52:15 INFO TaskSetManager: Finished task 3.0 in stage 206.0 (TID 451) in 26 ms on 172.18.0.15 (executor 0) (4/4)
26/02/13 11:52:15 INFO BlockManagerInfo: Removed broadcast_200_piece0 on spark-master:40291 in memory (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 11:52:15 INFO DAGScheduler: ResultStage 206 (start at NativeMethodAccessorImpl.java:0) finished in 0.062 s
26/02/13 11:52:15 INFO TaskSchedulerImpl: Removed TaskSet 206.0, whose tasks have all completed, from pool 
26/02/13 11:52:15 INFO DAGScheduler: Job 135 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:52:15 INFO TaskSchedulerImpl: Killing all running tasks in stage 206: Stage finished
26/02/13 11:52:15 INFO DAGScheduler: Job 135 finished: start at NativeMethodAccessorImpl.java:0, took 0.065329 s
26/02/13 11:52:15 INFO BlockManagerInfo: Removed broadcast_200_piece0 on 172.18.0.15:34959 in memory (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 11:52:15 INFO BlockManagerInfo: Removed broadcast_200_piece0 on 172.18.0.14:40939 in memory (size: 107.2 KiB, free: 434.4 MiB)
26/02/13 11:52:15 INFO MemoryStore: Block broadcast_203_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.9 MiB)
26/02/13 11:52:15 INFO BlockManagerInfo: Added broadcast_203_piece0 in memory on spark-master:40291 (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 11:52:15 INFO SparkContext: Created broadcast 203 from start at NativeMethodAccessorImpl.java:0
26/02/13 11:52:15 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 63, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@558da57a]. The input RDD has 3 partitions.
26/02/13 11:52:15 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:52:15 INFO DAGScheduler: Got job 136 (start at NativeMethodAccessorImpl.java:0) with 3 output partitions
26/02/13 11:52:15 INFO DAGScheduler: Final stage: ResultStage 207 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:52:15 INFO DAGScheduler: Parents of final stage: List()
26/02/13 11:52:15 INFO DAGScheduler: Missing parents: List()
26/02/13 11:52:15 INFO DAGScheduler: Submitting ResultStage 207 (MapPartitionsRDD[742] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:52:15 INFO MemoryStore: Block broadcast_204 stored as values in memory (estimated size 49.9 KiB, free 433.9 MiB)
26/02/13 11:52:15 INFO MemoryStore: Block broadcast_204_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.9 MiB)
26/02/13 11:52:15 INFO BlockManagerInfo: Added broadcast_204_piece0 in memory on spark-master:40291 (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 11:52:15 INFO SparkContext: Created broadcast 204 from broadcast at DAGScheduler.scala:1585
26/02/13 11:52:15 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 207 (MapPartitionsRDD[742] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
26/02/13 11:52:15 INFO TaskSchedulerImpl: Adding task set 207.0 with 3 tasks resource profile 0
26/02/13 11:52:15 INFO TaskSetManager: Starting task 1.0 in stage 207.0 (TID 452) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:52:15 INFO TaskSetManager: Starting task 0.0 in stage 207.0 (TID 453) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:52:15 INFO TaskSetManager: Starting task 2.0 in stage 207.0 (TID 454) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:52:15 INFO BlockManagerInfo: Added broadcast_204_piece0 in memory on 172.18.0.15:34959 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 11:52:15 INFO BlockManagerInfo: Added broadcast_204_piece0 in memory on 172.18.0.14:40939 (size: 19.5 KiB, free: 434.4 MiB)
26/02/13 11:52:15 INFO BlockManagerInfo: Added broadcast_203_piece0 in memory on 172.18.0.15:34959 (size: 107.2 KiB, free: 434.0 MiB)
26/02/13 11:52:15 INFO BlockManagerInfo: Added broadcast_203_piece0 in memory on 172.18.0.14:40939 (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 11:52:16 INFO TaskSetManager: Finished task 0.0 in stage 207.0 (TID 453) in 565 ms on 172.18.0.14 (executor 1) (1/3)
26/02/13 11:52:16 INFO TaskSetManager: Finished task 2.0 in stage 207.0 (TID 454) in 568 ms on 172.18.0.14 (executor 1) (2/3)
26/02/13 11:52:16 INFO TaskSetManager: Finished task 1.0 in stage 207.0 (TID 452) in 573 ms on 172.18.0.15 (executor 0) (3/3)
26/02/13 11:52:16 INFO TaskSchedulerImpl: Removed TaskSet 207.0, whose tasks have all completed, from pool 
26/02/13 11:52:16 INFO DAGScheduler: ResultStage 207 (start at NativeMethodAccessorImpl.java:0) finished in 0.578 s
26/02/13 11:52:16 INFO DAGScheduler: Job 136 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:52:16 INFO TaskSchedulerImpl: Killing all running tasks in stage 207: Stage finished
26/02/13 11:52:16 INFO DAGScheduler: Job 136 finished: start at NativeMethodAccessorImpl.java:0, took 0.580253 s
26/02/13 11:52:16 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 63, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@558da57a] is committing.
26/02/13 11:52:16 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 63, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@558da57a] committed.
26/02/13 11:52:16 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/63 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.63.6906536d-1280-421c-898d-ded02bef2fef.tmp
26/02/13 11:52:16 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.63.6906536d-1280-421c-898d-ded02bef2fef.tmp to file:/tmp/spark-checkpoint-enrichment/commits/63
26/02/13 11:52:16 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "fc5c6452-bd70-4c90-becf-772d3bebdb57",
  "name" : null,
  "timestamp" : "2026-02-13T11:52:15.287Z",
  "batchId" : 63,
  "numInputRows" : 46,
  "inputRowsPerSecond" : 3833.333333333333,
  "processedRowsPerSecond" : 50.108932461873636,
  "durationMs" : {
    "addBatch" : 738,
    "commitOffsets" : 59,
    "getBatch" : 0,
    "latestOffset" : 2,
    "queryPlanning" : 40,
    "triggerExecution" : 918,
    "walCommit" : 79
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 5219,
        "1" : 5709,
        "0" : 7353
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 5233,
        "1" : 5730,
        "0" : 7364
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 5233,
        "1" : 5730,
        "0" : 7364
      }
    },
    "numInputRows" : 46,
    "inputRowsPerSecond" : 3833.333333333333,
    "processedRowsPerSecond" : 50.108932461873636,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@7fe9b5de",
    "numOutputRows" : 13
  }
}
26/02/13 11:52:16 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/64 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.64.4dc8411c-8e2c-4ab1-8d74-90537da49496.tmp
26/02/13 11:52:16 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.64.4dc8411c-8e2c-4ab1-8d74-90537da49496.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/64
26/02/13 11:52:16 INFO MicroBatchExecution: Committed offsets for batch 64. Metadata OffsetSeqMetadata(0,1770983536207,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 11:52:16 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:52:16 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:52:16 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:52:16 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:52:16 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:52:16 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:52:16 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#56607 - origin_code.nullCount#56606) > 0)
26/02/13 11:52:16 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#56612 - destination_code.nullCount#56611) > 0)
26/02/13 11:52:16 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#56642 - callsign.nullCount#56641) > 0)
26/02/13 11:52:16 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:52:16 INFO DAGScheduler: Got job 137 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 11:52:16 INFO DAGScheduler: Final stage: ResultStage 209 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:52:16 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 208)
26/02/13 11:52:16 INFO DAGScheduler: Missing parents: List()
26/02/13 11:52:16 INFO DAGScheduler: Submitting ResultStage 209 (MapPartitionsRDD[747] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:52:16 INFO MemoryStore: Block broadcast_205 stored as values in memory (estimated size 99.3 KiB, free 433.8 MiB)
26/02/13 11:52:16 INFO MemoryStore: Block broadcast_205_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 433.7 MiB)
26/02/13 11:52:16 INFO BlockManagerInfo: Added broadcast_205_piece0 in memory on spark-master:40291 (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 11:52:16 INFO SparkContext: Created broadcast 205 from broadcast at DAGScheduler.scala:1585
26/02/13 11:52:16 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 209 (MapPartitionsRDD[747] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 11:52:16 INFO TaskSchedulerImpl: Adding task set 209.0 with 4 tasks resource profile 0
26/02/13 11:52:16 INFO TaskSetManager: Starting task 0.0 in stage 209.0 (TID 455) (172.18.0.15, executor 0, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:52:16 INFO TaskSetManager: Starting task 1.0 in stage 209.0 (TID 456) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:52:16 INFO BlockManagerInfo: Added broadcast_205_piece0 in memory on 172.18.0.15:34959 (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 11:52:16 INFO TaskSetManager: Starting task 2.0 in stage 209.0 (TID 457) (172.18.0.15, executor 0, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:52:16 INFO TaskSetManager: Finished task 1.0 in stage 209.0 (TID 456) in 26 ms on 172.18.0.15 (executor 0) (1/4)
26/02/13 11:52:16 INFO TaskSetManager: Starting task 3.0 in stage 209.0 (TID 458) (172.18.0.15, executor 0, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:52:16 INFO TaskSetManager: Finished task 0.0 in stage 209.0 (TID 455) in 27 ms on 172.18.0.15 (executor 0) (2/4)
26/02/13 11:52:16 INFO TaskSetManager: Finished task 2.0 in stage 209.0 (TID 457) in 11 ms on 172.18.0.15 (executor 0) (3/4)
26/02/13 11:52:16 INFO TaskSetManager: Finished task 3.0 in stage 209.0 (TID 458) in 18 ms on 172.18.0.15 (executor 0) (4/4)
26/02/13 11:52:16 INFO TaskSchedulerImpl: Removed TaskSet 209.0, whose tasks have all completed, from pool 
26/02/13 11:52:16 INFO DAGScheduler: ResultStage 209 (start at NativeMethodAccessorImpl.java:0) finished in 0.054 s
26/02/13 11:52:16 INFO DAGScheduler: Job 137 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:52:16 INFO TaskSchedulerImpl: Killing all running tasks in stage 209: Stage finished
26/02/13 11:52:16 INFO DAGScheduler: Job 137 finished: start at NativeMethodAccessorImpl.java:0, took 0.056274 s
26/02/13 11:52:16 INFO MemoryStore: Block broadcast_206_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.6 MiB)
26/02/13 11:52:16 INFO BlockManagerInfo: Added broadcast_206_piece0 in memory on spark-master:40291 (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 11:52:16 INFO SparkContext: Created broadcast 206 from start at NativeMethodAccessorImpl.java:0
26/02/13 11:52:16 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 64, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@60afa202]. The input RDD has 3 partitions.
26/02/13 11:52:16 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:52:16 INFO DAGScheduler: Got job 138 (start at NativeMethodAccessorImpl.java:0) with 3 output partitions
26/02/13 11:52:16 INFO DAGScheduler: Final stage: ResultStage 210 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:52:16 INFO DAGScheduler: Parents of final stage: List()
26/02/13 11:52:16 INFO DAGScheduler: Missing parents: List()
26/02/13 11:52:16 INFO DAGScheduler: Submitting ResultStage 210 (MapPartitionsRDD[753] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:52:16 INFO MemoryStore: Block broadcast_207 stored as values in memory (estimated size 49.9 KiB, free 433.6 MiB)
26/02/13 11:52:16 INFO MemoryStore: Block broadcast_207_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.6 MiB)
26/02/13 11:52:16 INFO BlockManagerInfo: Added broadcast_207_piece0 in memory on spark-master:40291 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 11:52:16 INFO SparkContext: Created broadcast 207 from broadcast at DAGScheduler.scala:1585
26/02/13 11:52:16 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 210 (MapPartitionsRDD[753] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
26/02/13 11:52:16 INFO TaskSchedulerImpl: Adding task set 210.0 with 3 tasks resource profile 0
26/02/13 11:52:16 INFO TaskSetManager: Starting task 0.0 in stage 210.0 (TID 459) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:52:16 INFO TaskSetManager: Starting task 1.0 in stage 210.0 (TID 460) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:52:16 INFO TaskSetManager: Starting task 2.0 in stage 210.0 (TID 461) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:52:16 INFO BlockManagerInfo: Added broadcast_207_piece0 in memory on 172.18.0.14:40939 (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 11:52:16 INFO BlockManagerInfo: Added broadcast_207_piece0 in memory on 172.18.0.15:34959 (size: 19.5 KiB, free: 434.0 MiB)
26/02/13 11:52:16 INFO BlockManagerInfo: Added broadcast_206_piece0 in memory on 172.18.0.14:40939 (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 11:52:16 INFO BlockManagerInfo: Added broadcast_206_piece0 in memory on 172.18.0.15:34959 (size: 107.2 KiB, free: 433.9 MiB)
26/02/13 11:52:16 INFO TaskSetManager: Finished task 0.0 in stage 210.0 (TID 459) in 72 ms on 172.18.0.14 (executor 1) (1/3)
26/02/13 11:52:16 INFO TaskSetManager: Finished task 2.0 in stage 210.0 (TID 461) in 77 ms on 172.18.0.14 (executor 1) (2/3)
26/02/13 11:52:16 INFO TaskSetManager: Finished task 1.0 in stage 210.0 (TID 460) in 78 ms on 172.18.0.15 (executor 0) (3/3)
26/02/13 11:52:16 INFO TaskSchedulerImpl: Removed TaskSet 210.0, whose tasks have all completed, from pool 
26/02/13 11:52:16 INFO DAGScheduler: ResultStage 210 (start at NativeMethodAccessorImpl.java:0) finished in 0.083 s
26/02/13 11:52:16 INFO DAGScheduler: Job 138 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:52:16 INFO TaskSchedulerImpl: Killing all running tasks in stage 210: Stage finished
26/02/13 11:52:16 INFO DAGScheduler: Job 138 finished: start at NativeMethodAccessorImpl.java:0, took 0.085227 s
26/02/13 11:52:16 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 64, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@60afa202] is committing.
26/02/13 11:52:16 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 64, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@60afa202] committed.
26/02/13 11:52:16 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/64 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.64.ef56983a-6ab9-488f-b24b-691738e09f06.tmp
26/02/13 11:52:16 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.64.ef56983a-6ab9-488f-b24b-691738e09f06.tmp to file:/tmp/spark-checkpoint-enrichment/commits/64
26/02/13 11:52:16 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "fc5c6452-bd70-4c90-becf-772d3bebdb57",
  "name" : null,
  "timestamp" : "2026-02-13T11:52:16.206Z",
  "batchId" : 64,
  "numInputRows" : 201,
  "inputRowsPerSecond" : 218.71599564744287,
  "processedRowsPerSecond" : 541.77897574124,
  "durationMs" : {
    "addBatch" : 228,
    "commitOffsets" : 55,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 26,
    "triggerExecution" : 371,
    "walCommit" : 59
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 5233,
        "1" : 5730,
        "0" : 7364
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 5298,
        "1" : 5799,
        "0" : 7431
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 5298,
        "1" : 5799,
        "0" : 7431
      }
    },
    "numInputRows" : 201,
    "inputRowsPerSecond" : 218.71599564744287,
    "processedRowsPerSecond" : 541.77897574124,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@7fe9b5de",
    "numOutputRows" : 41
  }
}
26/02/13 11:52:17 INFO BlockManagerInfo: Removed broadcast_207_piece0 on spark-master:40291 in memory (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 11:52:17 INFO BlockManagerInfo: Removed broadcast_207_piece0 on 172.18.0.15:34959 in memory (size: 19.5 KiB, free: 433.9 MiB)
26/02/13 11:52:17 INFO BlockManagerInfo: Removed broadcast_207_piece0 on 172.18.0.14:40939 in memory (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 11:52:17 INFO BlockManagerInfo: Removed broadcast_204_piece0 on spark-master:40291 in memory (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 11:52:17 INFO BlockManagerInfo: Removed broadcast_204_piece0 on 172.18.0.15:34959 in memory (size: 19.5 KiB, free: 433.9 MiB)
26/02/13 11:52:17 INFO BlockManagerInfo: Removed broadcast_204_piece0 on 172.18.0.14:40939 in memory (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 11:52:17 INFO BlockManagerInfo: Removed broadcast_203_piece0 on spark-master:40291 in memory (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 11:52:17 INFO BlockManagerInfo: Removed broadcast_203_piece0 on 172.18.0.14:40939 in memory (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 11:52:17 INFO BlockManagerInfo: Removed broadcast_203_piece0 on 172.18.0.15:34959 in memory (size: 107.2 KiB, free: 434.0 MiB)
26/02/13 11:52:17 INFO BlockManagerInfo: Removed broadcast_205_piece0 on spark-master:40291 in memory (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 11:52:17 INFO BlockManagerInfo: Removed broadcast_205_piece0 on 172.18.0.15:34959 in memory (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 11:52:17 INFO BlockManagerInfo: Removed broadcast_202_piece0 on spark-master:40291 in memory (size: 33.3 KiB, free: 434.3 MiB)
26/02/13 11:52:17 INFO BlockManagerInfo: Removed broadcast_202_piece0 on 172.18.0.15:34959 in memory (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 11:52:26 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 11:52:35 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/65 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.65.1429c444-443f-48ed-8b59-f3dce174c4b4.tmp
26/02/13 11:52:35 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.65.1429c444-443f-48ed-8b59-f3dce174c4b4.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/65
26/02/13 11:52:35 INFO MicroBatchExecution: Committed offsets for batch 65. Metadata OffsetSeqMetadata(0,1770983555439,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 11:52:35 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:52:35 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:52:35 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:52:35 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:52:35 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:52:35 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:52:35 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#57461 - origin_code.nullCount#57460) > 0)
26/02/13 11:52:35 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#57466 - destination_code.nullCount#57465) > 0)
26/02/13 11:52:35 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#57496 - callsign.nullCount#57495) > 0)
26/02/13 11:52:35 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:52:35 INFO DAGScheduler: Got job 139 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 11:52:35 INFO DAGScheduler: Final stage: ResultStage 212 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:52:35 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 211)
26/02/13 11:52:35 INFO DAGScheduler: Missing parents: List()
26/02/13 11:52:35 INFO DAGScheduler: Submitting ResultStage 212 (MapPartitionsRDD[758] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:52:35 INFO MemoryStore: Block broadcast_208 stored as values in memory (estimated size 99.3 KiB, free 434.0 MiB)
26/02/13 11:52:35 INFO MemoryStore: Block broadcast_208_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 433.9 MiB)
26/02/13 11:52:35 INFO BlockManagerInfo: Added broadcast_208_piece0 in memory on spark-master:40291 (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 11:52:35 INFO SparkContext: Created broadcast 208 from broadcast at DAGScheduler.scala:1585
26/02/13 11:52:35 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 212 (MapPartitionsRDD[758] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 11:52:35 INFO TaskSchedulerImpl: Adding task set 212.0 with 4 tasks resource profile 0
26/02/13 11:52:35 INFO TaskSetManager: Starting task 0.0 in stage 212.0 (TID 462) (172.18.0.15, executor 0, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:52:35 INFO TaskSetManager: Starting task 1.0 in stage 212.0 (TID 463) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:52:35 INFO BlockManagerInfo: Added broadcast_208_piece0 in memory on 172.18.0.15:34959 (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 11:52:35 INFO TaskSetManager: Starting task 2.0 in stage 212.0 (TID 464) (172.18.0.15, executor 0, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:52:35 INFO TaskSetManager: Starting task 3.0 in stage 212.0 (TID 465) (172.18.0.15, executor 0, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:52:35 INFO TaskSetManager: Finished task 0.0 in stage 212.0 (TID 462) in 20 ms on 172.18.0.15 (executor 0) (1/4)
26/02/13 11:52:35 INFO TaskSetManager: Finished task 1.0 in stage 212.0 (TID 463) in 20 ms on 172.18.0.15 (executor 0) (2/4)
26/02/13 11:52:35 INFO TaskSetManager: Finished task 3.0 in stage 212.0 (TID 465) in 15 ms on 172.18.0.15 (executor 0) (3/4)
26/02/13 11:52:35 INFO TaskSetManager: Finished task 2.0 in stage 212.0 (TID 464) in 15 ms on 172.18.0.15 (executor 0) (4/4)
26/02/13 11:52:35 INFO TaskSchedulerImpl: Removed TaskSet 212.0, whose tasks have all completed, from pool 
26/02/13 11:52:35 INFO DAGScheduler: ResultStage 212 (start at NativeMethodAccessorImpl.java:0) finished in 0.048 s
26/02/13 11:52:35 INFO DAGScheduler: Job 139 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:52:35 INFO TaskSchedulerImpl: Killing all running tasks in stage 212: Stage finished
26/02/13 11:52:35 INFO DAGScheduler: Job 139 finished: start at NativeMethodAccessorImpl.java:0, took 0.049565 s
26/02/13 11:52:35 INFO MemoryStore: Block broadcast_209_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.8 MiB)
26/02/13 11:52:35 INFO BlockManagerInfo: Added broadcast_209_piece0 in memory on spark-master:40291 (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 11:52:35 INFO SparkContext: Created broadcast 209 from start at NativeMethodAccessorImpl.java:0
26/02/13 11:52:35 INFO BlockManagerInfo: Removed broadcast_206_piece0 on spark-master:40291 in memory (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 11:52:35 INFO BlockManagerInfo: Removed broadcast_206_piece0 on 172.18.0.14:40939 in memory (size: 107.2 KiB, free: 434.4 MiB)
26/02/13 11:52:35 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 65, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@761a5523]. The input RDD has 3 partitions.
26/02/13 11:52:35 INFO BlockManagerInfo: Removed broadcast_206_piece0 on 172.18.0.15:34959 in memory (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 11:52:35 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:52:35 INFO DAGScheduler: Got job 140 (start at NativeMethodAccessorImpl.java:0) with 3 output partitions
26/02/13 11:52:35 INFO DAGScheduler: Final stage: ResultStage 213 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:52:35 INFO DAGScheduler: Parents of final stage: List()
26/02/13 11:52:35 INFO DAGScheduler: Missing parents: List()
26/02/13 11:52:35 INFO DAGScheduler: Submitting ResultStage 213 (MapPartitionsRDD[764] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:52:35 INFO MemoryStore: Block broadcast_210 stored as values in memory (estimated size 49.9 KiB, free 433.9 MiB)
26/02/13 11:52:35 INFO MemoryStore: Block broadcast_210_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.9 MiB)
26/02/13 11:52:35 INFO BlockManagerInfo: Added broadcast_210_piece0 in memory on spark-master:40291 (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 11:52:35 INFO SparkContext: Created broadcast 210 from broadcast at DAGScheduler.scala:1585
26/02/13 11:52:35 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 213 (MapPartitionsRDD[764] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
26/02/13 11:52:35 INFO TaskSchedulerImpl: Adding task set 213.0 with 3 tasks resource profile 0
26/02/13 11:52:35 INFO TaskSetManager: Starting task 0.0 in stage 213.0 (TID 466) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:52:35 INFO TaskSetManager: Starting task 1.0 in stage 213.0 (TID 467) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:52:35 INFO TaskSetManager: Starting task 2.0 in stage 213.0 (TID 468) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:52:35 INFO BlockManagerInfo: Added broadcast_210_piece0 in memory on 172.18.0.15:34959 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 11:52:35 INFO BlockManagerInfo: Added broadcast_210_piece0 in memory on 172.18.0.14:40939 (size: 19.5 KiB, free: 434.4 MiB)
26/02/13 11:52:35 INFO BlockManagerInfo: Added broadcast_209_piece0 in memory on 172.18.0.15:34959 (size: 107.2 KiB, free: 434.0 MiB)
26/02/13 11:52:35 INFO BlockManagerInfo: Added broadcast_209_piece0 in memory on 172.18.0.14:40939 (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 11:52:36 INFO TaskSetManager: Finished task 2.0 in stage 213.0 (TID 468) in 567 ms on 172.18.0.14 (executor 1) (1/3)
26/02/13 11:52:36 INFO TaskSetManager: Finished task 1.0 in stage 213.0 (TID 467) in 568 ms on 172.18.0.15 (executor 0) (2/3)
26/02/13 11:52:36 INFO TaskSetManager: Finished task 0.0 in stage 213.0 (TID 466) in 568 ms on 172.18.0.14 (executor 1) (3/3)
26/02/13 11:52:36 INFO TaskSchedulerImpl: Removed TaskSet 213.0, whose tasks have all completed, from pool 
26/02/13 11:52:36 INFO DAGScheduler: ResultStage 213 (start at NativeMethodAccessorImpl.java:0) finished in 0.573 s
26/02/13 11:52:36 INFO DAGScheduler: Job 140 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:52:36 INFO TaskSchedulerImpl: Killing all running tasks in stage 213: Stage finished
26/02/13 11:52:36 INFO DAGScheduler: Job 140 finished: start at NativeMethodAccessorImpl.java:0, took 0.574381 s
26/02/13 11:52:36 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 65, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@761a5523] is committing.
26/02/13 11:52:36 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 65, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@761a5523] committed.
26/02/13 11:52:36 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/65 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.65.1b4422f4-3983-4eeb-97dd-29e5998c47b3.tmp
26/02/13 11:52:36 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.65.1b4422f4-3983-4eeb-97dd-29e5998c47b3.tmp to file:/tmp/spark-checkpoint-enrichment/commits/65
26/02/13 11:52:36 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "fc5c6452-bd70-4c90-becf-772d3bebdb57",
  "name" : null,
  "timestamp" : "2026-02-13T11:52:35.437Z",
  "batchId" : 65,
  "numInputRows" : 87,
  "inputRowsPerSecond" : 7909.09090909091,
  "processedRowsPerSecond" : 92.84951974386338,
  "durationMs" : {
    "addBatch" : 715,
    "commitOffsets" : 56,
    "getBatch" : 0,
    "latestOffset" : 2,
    "queryPlanning" : 51,
    "triggerExecution" : 937,
    "walCommit" : 113
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 5298,
        "1" : 5799,
        "0" : 7431
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 5328,
        "1" : 5832,
        "0" : 7455
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 5328,
        "1" : 5832,
        "0" : 7455
      }
    },
    "numInputRows" : 87,
    "inputRowsPerSecond" : 7909.09090909091,
    "processedRowsPerSecond" : 92.84951974386338,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@7fe9b5de",
    "numOutputRows" : 23
  }
}
26/02/13 11:52:36 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/66 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.66.9141c37d-ea6f-40a7-841d-59a20f554c6b.tmp
26/02/13 11:52:36 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.66.9141c37d-ea6f-40a7-841d-59a20f554c6b.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/66
26/02/13 11:52:36 INFO MicroBatchExecution: Committed offsets for batch 66. Metadata OffsetSeqMetadata(0,1770983556376,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 11:52:36 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:52:36 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:52:36 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:52:36 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:52:36 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:52:36 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:52:36 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#58315 - origin_code.nullCount#58314) > 0)
26/02/13 11:52:36 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#58320 - destination_code.nullCount#58319) > 0)
26/02/13 11:52:36 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#58350 - callsign.nullCount#58349) > 0)
26/02/13 11:52:36 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:52:36 INFO DAGScheduler: Got job 141 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 11:52:36 INFO DAGScheduler: Final stage: ResultStage 215 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:52:36 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 214)
26/02/13 11:52:36 INFO DAGScheduler: Missing parents: List()
26/02/13 11:52:36 INFO DAGScheduler: Submitting ResultStage 215 (MapPartitionsRDD[769] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:52:36 INFO MemoryStore: Block broadcast_211 stored as values in memory (estimated size 99.3 KiB, free 433.8 MiB)
26/02/13 11:52:36 INFO MemoryStore: Block broadcast_211_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 433.7 MiB)
26/02/13 11:52:36 INFO BlockManagerInfo: Added broadcast_211_piece0 in memory on spark-master:40291 (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 11:52:36 INFO SparkContext: Created broadcast 211 from broadcast at DAGScheduler.scala:1585
26/02/13 11:52:36 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 215 (MapPartitionsRDD[769] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 11:52:36 INFO TaskSchedulerImpl: Adding task set 215.0 with 4 tasks resource profile 0
26/02/13 11:52:36 INFO TaskSetManager: Starting task 0.0 in stage 215.0 (TID 469) (172.18.0.15, executor 0, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:52:36 INFO TaskSetManager: Starting task 1.0 in stage 215.0 (TID 470) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:52:36 INFO BlockManagerInfo: Added broadcast_211_piece0 in memory on 172.18.0.15:34959 (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 11:52:36 INFO TaskSetManager: Starting task 2.0 in stage 215.0 (TID 471) (172.18.0.15, executor 0, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:52:36 INFO TaskSetManager: Finished task 1.0 in stage 215.0 (TID 470) in 19 ms on 172.18.0.15 (executor 0) (1/4)
26/02/13 11:52:36 INFO TaskSetManager: Starting task 3.0 in stage 215.0 (TID 472) (172.18.0.15, executor 0, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:52:36 INFO TaskSetManager: Finished task 0.0 in stage 215.0 (TID 469) in 25 ms on 172.18.0.15 (executor 0) (2/4)
26/02/13 11:52:36 INFO TaskSetManager: Finished task 2.0 in stage 215.0 (TID 471) in 15 ms on 172.18.0.15 (executor 0) (3/4)
26/02/13 11:52:36 INFO TaskSetManager: Finished task 3.0 in stage 215.0 (TID 472) in 16 ms on 172.18.0.15 (executor 0) (4/4)
26/02/13 11:52:36 INFO TaskSchedulerImpl: Removed TaskSet 215.0, whose tasks have all completed, from pool 
26/02/13 11:52:36 INFO DAGScheduler: ResultStage 215 (start at NativeMethodAccessorImpl.java:0) finished in 0.047 s
26/02/13 11:52:36 INFO DAGScheduler: Job 141 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:52:36 INFO TaskSchedulerImpl: Killing all running tasks in stage 215: Stage finished
26/02/13 11:52:36 INFO DAGScheduler: Job 141 finished: start at NativeMethodAccessorImpl.java:0, took 0.050314 s
26/02/13 11:52:36 INFO MemoryStore: Block broadcast_212_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.6 MiB)
26/02/13 11:52:36 INFO BlockManagerInfo: Added broadcast_212_piece0 in memory on spark-master:40291 (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 11:52:36 INFO SparkContext: Created broadcast 212 from start at NativeMethodAccessorImpl.java:0
26/02/13 11:52:36 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 66, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@10e1daa0]. The input RDD has 3 partitions.
26/02/13 11:52:36 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:52:36 INFO DAGScheduler: Got job 142 (start at NativeMethodAccessorImpl.java:0) with 3 output partitions
26/02/13 11:52:36 INFO DAGScheduler: Final stage: ResultStage 216 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:52:36 INFO DAGScheduler: Parents of final stage: List()
26/02/13 11:52:36 INFO DAGScheduler: Missing parents: List()
26/02/13 11:52:36 INFO DAGScheduler: Submitting ResultStage 216 (MapPartitionsRDD[775] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:52:36 INFO MemoryStore: Block broadcast_213 stored as values in memory (estimated size 49.9 KiB, free 433.6 MiB)
26/02/13 11:52:36 INFO MemoryStore: Block broadcast_213_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.6 MiB)
26/02/13 11:52:36 INFO BlockManagerInfo: Added broadcast_213_piece0 in memory on spark-master:40291 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 11:52:36 INFO SparkContext: Created broadcast 213 from broadcast at DAGScheduler.scala:1585
26/02/13 11:52:36 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 216 (MapPartitionsRDD[775] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
26/02/13 11:52:36 INFO TaskSchedulerImpl: Adding task set 216.0 with 3 tasks resource profile 0
26/02/13 11:52:36 INFO TaskSetManager: Starting task 1.0 in stage 216.0 (TID 473) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:52:36 INFO TaskSetManager: Starting task 0.0 in stage 216.0 (TID 474) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:52:36 INFO TaskSetManager: Starting task 2.0 in stage 216.0 (TID 475) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:52:36 INFO BlockManagerInfo: Added broadcast_213_piece0 in memory on 172.18.0.15:34959 (size: 19.5 KiB, free: 434.0 MiB)
26/02/13 11:52:36 INFO BlockManagerInfo: Added broadcast_213_piece0 in memory on 172.18.0.14:40939 (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 11:52:36 INFO BlockManagerInfo: Added broadcast_212_piece0 in memory on 172.18.0.14:40939 (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 11:52:36 INFO BlockManagerInfo: Added broadcast_212_piece0 in memory on 172.18.0.15:34959 (size: 107.2 KiB, free: 433.9 MiB)
26/02/13 11:52:36 INFO TaskSetManager: Finished task 2.0 in stage 216.0 (TID 475) in 55 ms on 172.18.0.14 (executor 1) (1/3)
26/02/13 11:52:36 INFO TaskSetManager: Finished task 0.0 in stage 216.0 (TID 474) in 57 ms on 172.18.0.14 (executor 1) (2/3)
26/02/13 11:52:36 INFO TaskSetManager: Finished task 1.0 in stage 216.0 (TID 473) in 60 ms on 172.18.0.15 (executor 0) (3/3)
26/02/13 11:52:36 INFO TaskSchedulerImpl: Removed TaskSet 216.0, whose tasks have all completed, from pool 
26/02/13 11:52:36 INFO DAGScheduler: ResultStage 216 (start at NativeMethodAccessorImpl.java:0) finished in 0.066 s
26/02/13 11:52:36 INFO DAGScheduler: Job 142 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:52:36 INFO TaskSchedulerImpl: Killing all running tasks in stage 216: Stage finished
26/02/13 11:52:36 INFO DAGScheduler: Job 142 finished: start at NativeMethodAccessorImpl.java:0, took 0.066823 s
26/02/13 11:52:36 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 66, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@10e1daa0] is committing.
26/02/13 11:52:36 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 66, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@10e1daa0] committed.
26/02/13 11:52:36 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/66 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.66.70a3ddec-8d4e-4152-a983-f8f1d41d6469.tmp
26/02/13 11:52:36 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.66.70a3ddec-8d4e-4152-a983-f8f1d41d6469.tmp to file:/tmp/spark-checkpoint-enrichment/commits/66
26/02/13 11:52:36 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "fc5c6452-bd70-4c90-becf-772d3bebdb57",
  "name" : null,
  "timestamp" : "2026-02-13T11:52:36.375Z",
  "batchId" : 66,
  "numInputRows" : 160,
  "inputRowsPerSecond" : 170.5756929637527,
  "processedRowsPerSecond" : 483.3836858006042,
  "durationMs" : {
    "addBatch" : 195,
    "commitOffsets" : 58,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 22,
    "triggerExecution" : 331,
    "walCommit" : 55
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 5328,
        "1" : 5832,
        "0" : 7455
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 5378,
        "1" : 5889,
        "0" : 7508
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 5378,
        "1" : 5889,
        "0" : 7508
      }
    },
    "numInputRows" : 160,
    "inputRowsPerSecond" : 170.5756929637527,
    "processedRowsPerSecond" : 483.3836858006042,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@7fe9b5de",
    "numOutputRows" : 31
  }
}
26/02/13 11:52:38 INFO BlockManagerInfo: Removed broadcast_211_piece0 on spark-master:40291 in memory (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 11:52:38 INFO BlockManagerInfo: Removed broadcast_211_piece0 on 172.18.0.15:34959 in memory (size: 33.3 KiB, free: 433.9 MiB)
26/02/13 11:52:38 INFO BlockManagerInfo: Removed broadcast_209_piece0 on spark-master:40291 in memory (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 11:52:38 INFO BlockManagerInfo: Removed broadcast_209_piece0 on 172.18.0.15:34959 in memory (size: 107.2 KiB, free: 434.0 MiB)
26/02/13 11:52:38 INFO BlockManagerInfo: Removed broadcast_209_piece0 on 172.18.0.14:40939 in memory (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 11:52:38 INFO BlockManagerInfo: Removed broadcast_210_piece0 on spark-master:40291 in memory (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 11:52:38 INFO BlockManagerInfo: Removed broadcast_210_piece0 on 172.18.0.15:34959 in memory (size: 19.5 KiB, free: 434.0 MiB)
26/02/13 11:52:38 INFO BlockManagerInfo: Removed broadcast_210_piece0 on 172.18.0.14:40939 in memory (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 11:52:38 INFO BlockManagerInfo: Removed broadcast_208_piece0 on spark-master:40291 in memory (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 11:52:38 INFO BlockManagerInfo: Removed broadcast_208_piece0 on 172.18.0.15:34959 in memory (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 11:52:38 INFO BlockManagerInfo: Removed broadcast_213_piece0 on spark-master:40291 in memory (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 11:52:38 INFO BlockManagerInfo: Removed broadcast_213_piece0 on 172.18.0.15:34959 in memory (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 11:52:38 INFO BlockManagerInfo: Removed broadcast_213_piece0 on 172.18.0.14:40939 in memory (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 11:52:46 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 11:52:51 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/67 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.67.a9f988b1-8a3e-49f3-b639-fee083e20ae2.tmp
26/02/13 11:52:51 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.67.a9f988b1-8a3e-49f3-b639-fee083e20ae2.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/67
26/02/13 11:52:51 INFO MicroBatchExecution: Committed offsets for batch 67. Metadata OffsetSeqMetadata(0,1770983571425,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 11:52:51 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:52:51 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:52:51 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:52:51 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:52:51 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:52:51 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:52:51 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#59169 - origin_code.nullCount#59168) > 0)
26/02/13 11:52:51 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#59174 - destination_code.nullCount#59173) > 0)
26/02/13 11:52:51 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#59204 - callsign.nullCount#59203) > 0)
26/02/13 11:52:51 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:52:51 INFO DAGScheduler: Got job 143 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 11:52:51 INFO DAGScheduler: Final stage: ResultStage 218 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:52:51 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 217)
26/02/13 11:52:51 INFO DAGScheduler: Missing parents: List()
26/02/13 11:52:51 INFO DAGScheduler: Submitting ResultStage 218 (MapPartitionsRDD[780] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:52:51 INFO MemoryStore: Block broadcast_214 stored as values in memory (estimated size 99.3 KiB, free 434.0 MiB)
26/02/13 11:52:51 INFO MemoryStore: Block broadcast_214_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 433.9 MiB)
26/02/13 11:52:51 INFO BlockManagerInfo: Added broadcast_214_piece0 in memory on spark-master:40291 (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 11:52:51 INFO SparkContext: Created broadcast 214 from broadcast at DAGScheduler.scala:1585
26/02/13 11:52:51 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 218 (MapPartitionsRDD[780] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 11:52:51 INFO TaskSchedulerImpl: Adding task set 218.0 with 4 tasks resource profile 0
26/02/13 11:52:51 INFO TaskSetManager: Starting task 0.0 in stage 218.0 (TID 476) (172.18.0.15, executor 0, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:52:51 INFO TaskSetManager: Starting task 1.0 in stage 218.0 (TID 477) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:52:51 INFO BlockManagerInfo: Added broadcast_214_piece0 in memory on 172.18.0.15:34959 (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 11:52:51 INFO TaskSetManager: Starting task 2.0 in stage 218.0 (TID 478) (172.18.0.15, executor 0, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:52:51 INFO TaskSetManager: Finished task 1.0 in stage 218.0 (TID 477) in 19 ms on 172.18.0.15 (executor 0) (1/4)
26/02/13 11:52:51 INFO TaskSetManager: Starting task 3.0 in stage 218.0 (TID 479) (172.18.0.15, executor 0, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:52:51 INFO TaskSetManager: Finished task 0.0 in stage 218.0 (TID 476) in 21 ms on 172.18.0.15 (executor 0) (2/4)
26/02/13 11:52:51 INFO TaskSetManager: Finished task 2.0 in stage 218.0 (TID 478) in 15 ms on 172.18.0.15 (executor 0) (3/4)
26/02/13 11:52:51 INFO TaskSetManager: Finished task 3.0 in stage 218.0 (TID 479) in 14 ms on 172.18.0.15 (executor 0) (4/4)
26/02/13 11:52:51 INFO TaskSchedulerImpl: Removed TaskSet 218.0, whose tasks have all completed, from pool 
26/02/13 11:52:51 INFO DAGScheduler: ResultStage 218 (start at NativeMethodAccessorImpl.java:0) finished in 0.042 s
26/02/13 11:52:51 INFO DAGScheduler: Job 143 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:52:51 INFO TaskSchedulerImpl: Killing all running tasks in stage 218: Stage finished
26/02/13 11:52:51 INFO DAGScheduler: Job 143 finished: start at NativeMethodAccessorImpl.java:0, took 0.044181 s
26/02/13 11:52:51 INFO MemoryStore: Block broadcast_215_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.8 MiB)
26/02/13 11:52:51 INFO BlockManagerInfo: Added broadcast_215_piece0 in memory on spark-master:40291 (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 11:52:51 INFO SparkContext: Created broadcast 215 from start at NativeMethodAccessorImpl.java:0
26/02/13 11:52:51 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 67, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@1c3460bf]. The input RDD has 3 partitions.
26/02/13 11:52:51 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:52:51 INFO DAGScheduler: Got job 144 (start at NativeMethodAccessorImpl.java:0) with 3 output partitions
26/02/13 11:52:51 INFO DAGScheduler: Final stage: ResultStage 219 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:52:51 INFO DAGScheduler: Parents of final stage: List()
26/02/13 11:52:51 INFO DAGScheduler: Missing parents: List()
26/02/13 11:52:51 INFO DAGScheduler: Submitting ResultStage 219 (MapPartitionsRDD[786] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:52:51 INFO MemoryStore: Block broadcast_216 stored as values in memory (estimated size 49.9 KiB, free 433.8 MiB)
26/02/13 11:52:51 INFO MemoryStore: Block broadcast_216_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.8 MiB)
26/02/13 11:52:51 INFO BlockManagerInfo: Added broadcast_216_piece0 in memory on spark-master:40291 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 11:52:51 INFO SparkContext: Created broadcast 216 from broadcast at DAGScheduler.scala:1585
26/02/13 11:52:51 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 219 (MapPartitionsRDD[786] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
26/02/13 11:52:51 INFO TaskSchedulerImpl: Adding task set 219.0 with 3 tasks resource profile 0
26/02/13 11:52:51 INFO TaskSetManager: Starting task 1.0 in stage 219.0 (TID 480) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:52:51 INFO TaskSetManager: Starting task 0.0 in stage 219.0 (TID 481) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:52:51 INFO TaskSetManager: Starting task 2.0 in stage 219.0 (TID 482) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:52:51 INFO BlockManagerInfo: Removed broadcast_212_piece0 on spark-master:40291 in memory (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 11:52:51 INFO BlockManagerInfo: Removed broadcast_212_piece0 on 172.18.0.15:34959 in memory (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 11:52:51 INFO BlockManagerInfo: Removed broadcast_212_piece0 on 172.18.0.14:40939 in memory (size: 107.2 KiB, free: 434.4 MiB)
26/02/13 11:52:51 INFO BlockManagerInfo: Added broadcast_216_piece0 in memory on 172.18.0.15:34959 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 11:52:51 INFO BlockManagerInfo: Added broadcast_216_piece0 in memory on 172.18.0.14:40939 (size: 19.5 KiB, free: 434.4 MiB)
26/02/13 11:52:51 INFO BlockManagerInfo: Added broadcast_215_piece0 in memory on 172.18.0.15:34959 (size: 107.2 KiB, free: 434.0 MiB)
26/02/13 11:52:51 INFO BlockManagerInfo: Added broadcast_215_piece0 in memory on 172.18.0.14:40939 (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 11:52:52 INFO TaskSetManager: Finished task 2.0 in stage 219.0 (TID 482) in 635 ms on 172.18.0.14 (executor 1) (1/3)
26/02/13 11:52:52 INFO TaskSetManager: Finished task 0.0 in stage 219.0 (TID 481) in 636 ms on 172.18.0.14 (executor 1) (2/3)
26/02/13 11:52:52 INFO TaskSetManager: Finished task 1.0 in stage 219.0 (TID 480) in 654 ms on 172.18.0.15 (executor 0) (3/3)
26/02/13 11:52:52 INFO TaskSchedulerImpl: Removed TaskSet 219.0, whose tasks have all completed, from pool 
26/02/13 11:52:52 INFO DAGScheduler: ResultStage 219 (start at NativeMethodAccessorImpl.java:0) finished in 0.674 s
26/02/13 11:52:52 INFO DAGScheduler: Job 144 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:52:52 INFO TaskSchedulerImpl: Killing all running tasks in stage 219: Stage finished
26/02/13 11:52:52 INFO DAGScheduler: Job 144 finished: start at NativeMethodAccessorImpl.java:0, took 0.675687 s
26/02/13 11:52:52 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 67, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@1c3460bf] is committing.
26/02/13 11:52:52 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 67, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@1c3460bf] committed.
26/02/13 11:52:52 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/67 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.67.e170b42a-3f38-4f59-8d59-38b64fdcaaf3.tmp
26/02/13 11:52:52 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.67.e170b42a-3f38-4f59-8d59-38b64fdcaaf3.tmp to file:/tmp/spark-checkpoint-enrichment/commits/67
26/02/13 11:52:52 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "fc5c6452-bd70-4c90-becf-772d3bebdb57",
  "name" : null,
  "timestamp" : "2026-02-13T11:52:51.423Z",
  "batchId" : 67,
  "numInputRows" : 8,
  "inputRowsPerSecond" : 727.2727272727274,
  "processedRowsPerSecond" : 8.048289738430583,
  "durationMs" : {
    "addBatch" : 813,
    "commitOffsets" : 69,
    "getBatch" : 0,
    "latestOffset" : 2,
    "queryPlanning" : 31,
    "triggerExecution" : 994,
    "walCommit" : 78
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 5378,
        "1" : 5889,
        "0" : 7508
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 5379,
        "1" : 5894,
        "0" : 7510
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 5379,
        "1" : 5894,
        "0" : 7510
      }
    },
    "numInputRows" : 8,
    "inputRowsPerSecond" : 727.2727272727274,
    "processedRowsPerSecond" : 8.048289738430583,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@7fe9b5de",
    "numOutputRows" : 3
  }
}
26/02/13 11:52:52 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/68 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.68.0b069e50-28b2-42e9-8bdc-05acd8bc763f.tmp
26/02/13 11:52:52 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.68.0b069e50-28b2-42e9-8bdc-05acd8bc763f.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/68
26/02/13 11:52:52 INFO MicroBatchExecution: Committed offsets for batch 68. Metadata OffsetSeqMetadata(0,1770983572419,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 11:52:52 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:52:52 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:52:52 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:52:52 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:52:52 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:52:52 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:52:52 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#60023 - origin_code.nullCount#60022) > 0)
26/02/13 11:52:52 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#60028 - destination_code.nullCount#60027) > 0)
26/02/13 11:52:52 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#60058 - callsign.nullCount#60057) > 0)
26/02/13 11:52:52 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:52:52 INFO DAGScheduler: Got job 145 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 11:52:52 INFO DAGScheduler: Final stage: ResultStage 221 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:52:52 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 220)
26/02/13 11:52:52 INFO DAGScheduler: Missing parents: List()
26/02/13 11:52:52 INFO DAGScheduler: Submitting ResultStage 221 (MapPartitionsRDD[791] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:52:52 INFO MemoryStore: Block broadcast_217 stored as values in memory (estimated size 99.3 KiB, free 433.8 MiB)
26/02/13 11:52:52 INFO MemoryStore: Block broadcast_217_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 433.7 MiB)
26/02/13 11:52:52 INFO BlockManagerInfo: Added broadcast_217_piece0 in memory on spark-master:40291 (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 11:52:52 INFO SparkContext: Created broadcast 217 from broadcast at DAGScheduler.scala:1585
26/02/13 11:52:52 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 221 (MapPartitionsRDD[791] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 11:52:52 INFO TaskSchedulerImpl: Adding task set 221.0 with 4 tasks resource profile 0
26/02/13 11:52:52 INFO TaskSetManager: Starting task 0.0 in stage 221.0 (TID 483) (172.18.0.15, executor 0, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:52:52 INFO TaskSetManager: Starting task 1.0 in stage 221.0 (TID 484) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:52:52 INFO BlockManagerInfo: Added broadcast_217_piece0 in memory on 172.18.0.15:34959 (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 11:52:52 INFO TaskSetManager: Starting task 2.0 in stage 221.0 (TID 485) (172.18.0.15, executor 0, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:52:52 INFO TaskSetManager: Finished task 1.0 in stage 221.0 (TID 484) in 19 ms on 172.18.0.15 (executor 0) (1/4)
26/02/13 11:52:52 INFO TaskSetManager: Starting task 3.0 in stage 221.0 (TID 486) (172.18.0.15, executor 0, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:52:52 INFO TaskSetManager: Finished task 0.0 in stage 221.0 (TID 483) in 24 ms on 172.18.0.15 (executor 0) (2/4)
26/02/13 11:52:52 INFO TaskSetManager: Finished task 2.0 in stage 221.0 (TID 485) in 13 ms on 172.18.0.15 (executor 0) (3/4)
26/02/13 11:52:52 INFO TaskSetManager: Finished task 3.0 in stage 221.0 (TID 486) in 15 ms on 172.18.0.15 (executor 0) (4/4)
26/02/13 11:52:52 INFO TaskSchedulerImpl: Removed TaskSet 221.0, whose tasks have all completed, from pool 
26/02/13 11:52:52 INFO DAGScheduler: ResultStage 221 (start at NativeMethodAccessorImpl.java:0) finished in 0.043 s
26/02/13 11:52:52 INFO DAGScheduler: Job 145 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:52:52 INFO TaskSchedulerImpl: Killing all running tasks in stage 221: Stage finished
26/02/13 11:52:52 INFO DAGScheduler: Job 145 finished: start at NativeMethodAccessorImpl.java:0, took 0.045352 s
26/02/13 11:52:52 INFO MemoryStore: Block broadcast_218_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.6 MiB)
26/02/13 11:52:52 INFO BlockManagerInfo: Added broadcast_218_piece0 in memory on spark-master:40291 (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 11:52:52 INFO SparkContext: Created broadcast 218 from start at NativeMethodAccessorImpl.java:0
26/02/13 11:52:52 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 68, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@73eb883f]. The input RDD has 3 partitions.
26/02/13 11:52:52 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:52:52 INFO DAGScheduler: Got job 146 (start at NativeMethodAccessorImpl.java:0) with 3 output partitions
26/02/13 11:52:52 INFO DAGScheduler: Final stage: ResultStage 222 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:52:52 INFO DAGScheduler: Parents of final stage: List()
26/02/13 11:52:52 INFO DAGScheduler: Missing parents: List()
26/02/13 11:52:52 INFO DAGScheduler: Submitting ResultStage 222 (MapPartitionsRDD[797] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:52:52 INFO MemoryStore: Block broadcast_219 stored as values in memory (estimated size 49.9 KiB, free 433.6 MiB)
26/02/13 11:52:52 INFO MemoryStore: Block broadcast_219_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.6 MiB)
26/02/13 11:52:52 INFO BlockManagerInfo: Added broadcast_219_piece0 in memory on spark-master:40291 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 11:52:52 INFO SparkContext: Created broadcast 219 from broadcast at DAGScheduler.scala:1585
26/02/13 11:52:52 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 222 (MapPartitionsRDD[797] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
26/02/13 11:52:52 INFO TaskSchedulerImpl: Adding task set 222.0 with 3 tasks resource profile 0
26/02/13 11:52:52 INFO TaskSetManager: Starting task 0.0 in stage 222.0 (TID 487) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:52:52 INFO TaskSetManager: Starting task 1.0 in stage 222.0 (TID 488) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:52:52 INFO TaskSetManager: Starting task 2.0 in stage 222.0 (TID 489) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:52:52 INFO BlockManagerInfo: Added broadcast_219_piece0 in memory on 172.18.0.14:40939 (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 11:52:52 INFO BlockManagerInfo: Added broadcast_219_piece0 in memory on 172.18.0.15:34959 (size: 19.5 KiB, free: 434.0 MiB)
26/02/13 11:52:52 INFO BlockManagerInfo: Added broadcast_218_piece0 in memory on 172.18.0.15:34959 (size: 107.2 KiB, free: 433.9 MiB)
26/02/13 11:52:52 INFO BlockManagerInfo: Added broadcast_218_piece0 in memory on 172.18.0.14:40939 (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 11:52:52 INFO TaskSetManager: Finished task 1.0 in stage 222.0 (TID 488) in 73 ms on 172.18.0.15 (executor 0) (1/3)
26/02/13 11:52:52 INFO TaskSetManager: Finished task 2.0 in stage 222.0 (TID 489) in 78 ms on 172.18.0.14 (executor 1) (2/3)
26/02/13 11:52:52 INFO TaskSetManager: Finished task 0.0 in stage 222.0 (TID 487) in 79 ms on 172.18.0.14 (executor 1) (3/3)
26/02/13 11:52:52 INFO TaskSchedulerImpl: Removed TaskSet 222.0, whose tasks have all completed, from pool 
26/02/13 11:52:52 INFO DAGScheduler: ResultStage 222 (start at NativeMethodAccessorImpl.java:0) finished in 0.084 s
26/02/13 11:52:52 INFO DAGScheduler: Job 146 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:52:52 INFO TaskSchedulerImpl: Killing all running tasks in stage 222: Stage finished
26/02/13 11:52:52 INFO DAGScheduler: Job 146 finished: start at NativeMethodAccessorImpl.java:0, took 0.085850 s
26/02/13 11:52:52 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 68, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@73eb883f] is committing.
26/02/13 11:52:52 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 68, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@73eb883f] committed.
26/02/13 11:52:52 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/68 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.68.f82000cd-32a3-44a9-82b3-aa52ef634d4f.tmp
26/02/13 11:52:52 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.68.f82000cd-32a3-44a9-82b3-aa52ef634d4f.tmp to file:/tmp/spark-checkpoint-enrichment/commits/68
26/02/13 11:52:52 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "fc5c6452-bd70-4c90-becf-772d3bebdb57",
  "name" : null,
  "timestamp" : "2026-02-13T11:52:52.418Z",
  "batchId" : 68,
  "numInputRows" : 237,
  "inputRowsPerSecond" : 238.19095477386935,
  "processedRowsPerSecond" : 496.8553459119497,
  "durationMs" : {
    "addBatch" : 253,
    "commitOffsets" : 81,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 62,
    "triggerExecution" : 477,
    "walCommit" : 78
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 5379,
        "1" : 5894,
        "0" : 7510
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 5458,
        "1" : 5978,
        "0" : 7584
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 5458,
        "1" : 5978,
        "0" : 7584
      }
    },
    "numInputRows" : 237,
    "inputRowsPerSecond" : 238.19095477386935,
    "processedRowsPerSecond" : 496.8553459119497,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@7fe9b5de",
    "numOutputRows" : 52
  }
}
26/02/13 11:52:55 INFO BlockManagerInfo: Removed broadcast_216_piece0 on spark-master:40291 in memory (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 11:52:55 INFO BlockManagerInfo: Removed broadcast_216_piece0 on 172.18.0.15:34959 in memory (size: 19.5 KiB, free: 433.9 MiB)
26/02/13 11:52:55 INFO BlockManagerInfo: Removed broadcast_216_piece0 on 172.18.0.14:40939 in memory (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 11:52:55 INFO BlockManagerInfo: Removed broadcast_217_piece0 on spark-master:40291 in memory (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 11:52:55 INFO BlockManagerInfo: Removed broadcast_217_piece0 on 172.18.0.15:34959 in memory (size: 33.3 KiB, free: 433.9 MiB)
26/02/13 11:52:55 INFO BlockManagerInfo: Removed broadcast_215_piece0 on spark-master:40291 in memory (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 11:52:55 INFO BlockManagerInfo: Removed broadcast_215_piece0 on 172.18.0.15:34959 in memory (size: 107.2 KiB, free: 434.0 MiB)
26/02/13 11:52:55 INFO BlockManagerInfo: Removed broadcast_215_piece0 on 172.18.0.14:40939 in memory (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 11:52:55 INFO BlockManagerInfo: Removed broadcast_219_piece0 on spark-master:40291 in memory (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 11:52:55 INFO BlockManagerInfo: Removed broadcast_219_piece0 on 172.18.0.15:34959 in memory (size: 19.5 KiB, free: 434.0 MiB)
26/02/13 11:52:55 INFO BlockManagerInfo: Removed broadcast_219_piece0 on 172.18.0.14:40939 in memory (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 11:52:55 INFO BlockManagerInfo: Removed broadcast_214_piece0 on spark-master:40291 in memory (size: 33.3 KiB, free: 434.3 MiB)
26/02/13 11:52:55 INFO BlockManagerInfo: Removed broadcast_214_piece0 on 172.18.0.15:34959 in memory (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 11:53:02 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 11:53:07 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/69 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.69.e10b4140-adc4-4c42-8791-5202c65687ad.tmp
26/02/13 11:53:07 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.69.e10b4140-adc4-4c42-8791-5202c65687ad.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/69
26/02/13 11:53:07 INFO MicroBatchExecution: Committed offsets for batch 69. Metadata OffsetSeqMetadata(0,1770983587344,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 11:53:07 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:53:07 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:53:07 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:53:07 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:53:07 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:53:07 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:53:07 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#60877 - origin_code.nullCount#60876) > 0)
26/02/13 11:53:07 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#60882 - destination_code.nullCount#60881) > 0)
26/02/13 11:53:07 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#60912 - callsign.nullCount#60911) > 0)
26/02/13 11:53:07 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:53:07 INFO DAGScheduler: Got job 147 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 11:53:07 INFO DAGScheduler: Final stage: ResultStage 224 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:53:07 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 223)
26/02/13 11:53:07 INFO DAGScheduler: Missing parents: List()
26/02/13 11:53:07 INFO DAGScheduler: Submitting ResultStage 224 (MapPartitionsRDD[802] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:53:07 INFO MemoryStore: Block broadcast_220 stored as values in memory (estimated size 99.3 KiB, free 434.0 MiB)
26/02/13 11:53:07 INFO MemoryStore: Block broadcast_220_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 433.9 MiB)
26/02/13 11:53:07 INFO BlockManagerInfo: Added broadcast_220_piece0 in memory on spark-master:40291 (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 11:53:07 INFO SparkContext: Created broadcast 220 from broadcast at DAGScheduler.scala:1585
26/02/13 11:53:07 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 224 (MapPartitionsRDD[802] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 11:53:07 INFO TaskSchedulerImpl: Adding task set 224.0 with 4 tasks resource profile 0
26/02/13 11:53:07 INFO TaskSetManager: Starting task 0.0 in stage 224.0 (TID 490) (172.18.0.15, executor 0, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:53:07 INFO TaskSetManager: Starting task 1.0 in stage 224.0 (TID 491) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:53:07 INFO BlockManagerInfo: Added broadcast_220_piece0 in memory on 172.18.0.15:34959 (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 11:53:07 INFO TaskSetManager: Starting task 2.0 in stage 224.0 (TID 492) (172.18.0.15, executor 0, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:53:07 INFO TaskSetManager: Finished task 0.0 in stage 224.0 (TID 490) in 29 ms on 172.18.0.15 (executor 0) (1/4)
26/02/13 11:53:07 INFO TaskSetManager: Starting task 3.0 in stage 224.0 (TID 493) (172.18.0.15, executor 0, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:53:07 INFO TaskSetManager: Finished task 1.0 in stage 224.0 (TID 491) in 29 ms on 172.18.0.15 (executor 0) (2/4)
26/02/13 11:53:07 INFO TaskSetManager: Finished task 2.0 in stage 224.0 (TID 492) in 22 ms on 172.18.0.15 (executor 0) (3/4)
26/02/13 11:53:07 INFO TaskSetManager: Finished task 3.0 in stage 224.0 (TID 493) in 19 ms on 172.18.0.15 (executor 0) (4/4)
26/02/13 11:53:07 INFO TaskSchedulerImpl: Removed TaskSet 224.0, whose tasks have all completed, from pool 
26/02/13 11:53:07 INFO DAGScheduler: ResultStage 224 (start at NativeMethodAccessorImpl.java:0) finished in 0.058 s
26/02/13 11:53:07 INFO DAGScheduler: Job 147 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:53:07 INFO TaskSchedulerImpl: Killing all running tasks in stage 224: Stage finished
26/02/13 11:53:07 INFO DAGScheduler: Job 147 finished: start at NativeMethodAccessorImpl.java:0, took 0.059081 s
26/02/13 11:53:07 INFO MemoryStore: Block broadcast_221_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.8 MiB)
26/02/13 11:53:07 INFO BlockManagerInfo: Added broadcast_221_piece0 in memory on spark-master:40291 (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 11:53:07 INFO SparkContext: Created broadcast 221 from start at NativeMethodAccessorImpl.java:0
26/02/13 11:53:07 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 69, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@5369d8bc]. The input RDD has 3 partitions.
26/02/13 11:53:07 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:53:07 INFO DAGScheduler: Got job 148 (start at NativeMethodAccessorImpl.java:0) with 3 output partitions
26/02/13 11:53:07 INFO DAGScheduler: Final stage: ResultStage 225 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:53:07 INFO DAGScheduler: Parents of final stage: List()
26/02/13 11:53:07 INFO DAGScheduler: Missing parents: List()
26/02/13 11:53:07 INFO DAGScheduler: Submitting ResultStage 225 (MapPartitionsRDD[808] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:53:07 INFO MemoryStore: Block broadcast_222 stored as values in memory (estimated size 49.9 KiB, free 433.8 MiB)
26/02/13 11:53:07 INFO MemoryStore: Block broadcast_222_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.8 MiB)
26/02/13 11:53:07 INFO BlockManagerInfo: Added broadcast_222_piece0 in memory on spark-master:40291 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 11:53:07 INFO SparkContext: Created broadcast 222 from broadcast at DAGScheduler.scala:1585
26/02/13 11:53:07 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 225 (MapPartitionsRDD[808] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
26/02/13 11:53:07 INFO TaskSchedulerImpl: Adding task set 225.0 with 3 tasks resource profile 0
26/02/13 11:53:07 INFO TaskSetManager: Starting task 0.0 in stage 225.0 (TID 494) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:53:07 INFO TaskSetManager: Starting task 1.0 in stage 225.0 (TID 495) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:53:07 INFO TaskSetManager: Starting task 2.0 in stage 225.0 (TID 496) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:53:07 INFO BlockManagerInfo: Added broadcast_222_piece0 in memory on 172.18.0.14:40939 (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 11:53:07 INFO BlockManagerInfo: Added broadcast_222_piece0 in memory on 172.18.0.15:34959 (size: 19.5 KiB, free: 434.0 MiB)
26/02/13 11:53:07 INFO BlockManagerInfo: Removed broadcast_218_piece0 on spark-master:40291 in memory (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 11:53:07 INFO BlockManagerInfo: Removed broadcast_218_piece0 on 172.18.0.14:40939 in memory (size: 107.2 KiB, free: 434.4 MiB)
26/02/13 11:53:07 INFO BlockManagerInfo: Removed broadcast_218_piece0 on 172.18.0.15:34959 in memory (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 11:53:07 INFO BlockManagerInfo: Added broadcast_221_piece0 in memory on 172.18.0.14:40939 (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 11:53:07 INFO BlockManagerInfo: Added broadcast_221_piece0 in memory on 172.18.0.15:34959 (size: 107.2 KiB, free: 434.0 MiB)
26/02/13 11:53:08 INFO TaskSetManager: Finished task 0.0 in stage 225.0 (TID 494) in 592 ms on 172.18.0.14 (executor 1) (1/3)
26/02/13 11:53:08 INFO TaskSetManager: Finished task 2.0 in stage 225.0 (TID 496) in 591 ms on 172.18.0.14 (executor 1) (2/3)
26/02/13 11:53:08 INFO TaskSetManager: Finished task 1.0 in stage 225.0 (TID 495) in 728 ms on 172.18.0.15 (executor 0) (3/3)
26/02/13 11:53:08 INFO TaskSchedulerImpl: Removed TaskSet 225.0, whose tasks have all completed, from pool 
26/02/13 11:53:08 INFO DAGScheduler: ResultStage 225 (start at NativeMethodAccessorImpl.java:0) finished in 0.734 s
26/02/13 11:53:08 INFO DAGScheduler: Job 148 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:53:08 INFO TaskSchedulerImpl: Killing all running tasks in stage 225: Stage finished
26/02/13 11:53:08 INFO DAGScheduler: Job 148 finished: start at NativeMethodAccessorImpl.java:0, took 0.736573 s
26/02/13 11:53:08 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 69, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@5369d8bc] is committing.
26/02/13 11:53:08 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 69, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@5369d8bc] committed.
26/02/13 11:53:08 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/69 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.69.bd279838-9d47-45f4-a19d-d98c243ff175.tmp
26/02/13 11:53:08 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.69.bd279838-9d47-45f4-a19d-d98c243ff175.tmp to file:/tmp/spark-checkpoint-enrichment/commits/69
26/02/13 11:53:08 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "fc5c6452-bd70-4c90-becf-772d3bebdb57",
  "name" : null,
  "timestamp" : "2026-02-13T11:53:07.343Z",
  "batchId" : 69,
  "numInputRows" : 99,
  "inputRowsPerSecond" : 8250.0,
  "processedRowsPerSecond" : 95.28392685274302,
  "durationMs" : {
    "addBatch" : 898,
    "commitOffsets" : 57,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 22,
    "triggerExecution" : 1039,
    "walCommit" : 61
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 5458,
        "1" : 5978,
        "0" : 7584
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 5489,
        "1" : 6015,
        "0" : 7615
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 5489,
        "1" : 6015,
        "0" : 7615
      }
    },
    "numInputRows" : 99,
    "inputRowsPerSecond" : 8250.0,
    "processedRowsPerSecond" : 95.28392685274302,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@7fe9b5de",
    "numOutputRows" : 25
  }
}
26/02/13 11:53:08 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/70 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.70.4e1ada3a-dd5d-4208-a40a-cecaf4ba0cdc.tmp
26/02/13 11:53:08 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.70.4e1ada3a-dd5d-4208-a40a-cecaf4ba0cdc.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/70
26/02/13 11:53:08 INFO MicroBatchExecution: Committed offsets for batch 70. Metadata OffsetSeqMetadata(0,1770983588384,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 11:53:08 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:53:08 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:53:08 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:53:08 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:53:08 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:53:08 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:53:08 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#61731 - origin_code.nullCount#61730) > 0)
26/02/13 11:53:08 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#61736 - destination_code.nullCount#61735) > 0)
26/02/13 11:53:08 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#61766 - callsign.nullCount#61765) > 0)
26/02/13 11:53:08 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:53:08 INFO DAGScheduler: Got job 149 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 11:53:08 INFO DAGScheduler: Final stage: ResultStage 227 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:53:08 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 226)
26/02/13 11:53:08 INFO DAGScheduler: Missing parents: List()
26/02/13 11:53:08 INFO DAGScheduler: Submitting ResultStage 227 (MapPartitionsRDD[813] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:53:08 INFO MemoryStore: Block broadcast_223 stored as values in memory (estimated size 99.3 KiB, free 433.8 MiB)
26/02/13 11:53:08 INFO MemoryStore: Block broadcast_223_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 433.7 MiB)
26/02/13 11:53:08 INFO BlockManagerInfo: Added broadcast_223_piece0 in memory on spark-master:40291 (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 11:53:08 INFO SparkContext: Created broadcast 223 from broadcast at DAGScheduler.scala:1585
26/02/13 11:53:08 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 227 (MapPartitionsRDD[813] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 11:53:08 INFO TaskSchedulerImpl: Adding task set 227.0 with 4 tasks resource profile 0
26/02/13 11:53:08 INFO TaskSetManager: Starting task 0.0 in stage 227.0 (TID 497) (172.18.0.15, executor 0, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:53:08 INFO TaskSetManager: Starting task 1.0 in stage 227.0 (TID 498) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:53:08 INFO BlockManagerInfo: Added broadcast_223_piece0 in memory on 172.18.0.15:34959 (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 11:53:08 INFO TaskSetManager: Starting task 2.0 in stage 227.0 (TID 499) (172.18.0.15, executor 0, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:53:08 INFO TaskSetManager: Finished task 0.0 in stage 227.0 (TID 497) in 22 ms on 172.18.0.15 (executor 0) (1/4)
26/02/13 11:53:08 INFO TaskSetManager: Starting task 3.0 in stage 227.0 (TID 500) (172.18.0.15, executor 0, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:53:08 INFO TaskSetManager: Finished task 1.0 in stage 227.0 (TID 498) in 25 ms on 172.18.0.15 (executor 0) (2/4)
26/02/13 11:53:08 INFO TaskSetManager: Finished task 2.0 in stage 227.0 (TID 499) in 11 ms on 172.18.0.15 (executor 0) (3/4)
26/02/13 11:53:08 INFO TaskSetManager: Finished task 3.0 in stage 227.0 (TID 500) in 11 ms on 172.18.0.15 (executor 0) (4/4)
26/02/13 11:53:08 INFO TaskSchedulerImpl: Removed TaskSet 227.0, whose tasks have all completed, from pool 
26/02/13 11:53:08 INFO DAGScheduler: ResultStage 227 (start at NativeMethodAccessorImpl.java:0) finished in 0.047 s
26/02/13 11:53:08 INFO DAGScheduler: Job 149 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:53:08 INFO TaskSchedulerImpl: Killing all running tasks in stage 227: Stage finished
26/02/13 11:53:08 INFO DAGScheduler: Job 149 finished: start at NativeMethodAccessorImpl.java:0, took 0.050202 s
26/02/13 11:53:08 INFO MemoryStore: Block broadcast_224_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.6 MiB)
26/02/13 11:53:08 INFO BlockManagerInfo: Added broadcast_224_piece0 in memory on spark-master:40291 (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 11:53:08 INFO SparkContext: Created broadcast 224 from start at NativeMethodAccessorImpl.java:0
26/02/13 11:53:08 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 70, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@6ab28ca0]. The input RDD has 3 partitions.
26/02/13 11:53:08 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:53:08 INFO DAGScheduler: Got job 150 (start at NativeMethodAccessorImpl.java:0) with 3 output partitions
26/02/13 11:53:08 INFO DAGScheduler: Final stage: ResultStage 228 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:53:08 INFO DAGScheduler: Parents of final stage: List()
26/02/13 11:53:08 INFO DAGScheduler: Missing parents: List()
26/02/13 11:53:08 INFO DAGScheduler: Submitting ResultStage 228 (MapPartitionsRDD[819] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:53:08 INFO MemoryStore: Block broadcast_225 stored as values in memory (estimated size 49.9 KiB, free 433.6 MiB)
26/02/13 11:53:08 INFO MemoryStore: Block broadcast_225_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.6 MiB)
26/02/13 11:53:08 INFO BlockManagerInfo: Added broadcast_225_piece0 in memory on spark-master:40291 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 11:53:08 INFO SparkContext: Created broadcast 225 from broadcast at DAGScheduler.scala:1585
26/02/13 11:53:08 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 228 (MapPartitionsRDD[819] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
26/02/13 11:53:08 INFO TaskSchedulerImpl: Adding task set 228.0 with 3 tasks resource profile 0
26/02/13 11:53:08 INFO TaskSetManager: Starting task 0.0 in stage 228.0 (TID 501) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:53:08 INFO TaskSetManager: Starting task 1.0 in stage 228.0 (TID 502) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:53:08 INFO TaskSetManager: Starting task 2.0 in stage 228.0 (TID 503) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:53:08 INFO BlockManagerInfo: Added broadcast_225_piece0 in memory on 172.18.0.15:34959 (size: 19.5 KiB, free: 434.0 MiB)
26/02/13 11:53:08 INFO BlockManagerInfo: Added broadcast_225_piece0 in memory on 172.18.0.14:40939 (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 11:53:08 INFO BlockManagerInfo: Added broadcast_224_piece0 in memory on 172.18.0.15:34959 (size: 107.2 KiB, free: 433.9 MiB)
26/02/13 11:53:08 INFO BlockManagerInfo: Added broadcast_224_piece0 in memory on 172.18.0.14:40939 (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 11:53:08 INFO TaskSetManager: Finished task 1.0 in stage 228.0 (TID 502) in 61 ms on 172.18.0.15 (executor 0) (1/3)
26/02/13 11:53:08 INFO TaskSetManager: Finished task 2.0 in stage 228.0 (TID 503) in 72 ms on 172.18.0.14 (executor 1) (2/3)
26/02/13 11:53:08 INFO TaskSetManager: Finished task 0.0 in stage 228.0 (TID 501) in 77 ms on 172.18.0.14 (executor 1) (3/3)
26/02/13 11:53:08 INFO TaskSchedulerImpl: Removed TaskSet 228.0, whose tasks have all completed, from pool 
26/02/13 11:53:08 INFO DAGScheduler: ResultStage 228 (start at NativeMethodAccessorImpl.java:0) finished in 0.082 s
26/02/13 11:53:08 INFO DAGScheduler: Job 150 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:53:08 INFO TaskSchedulerImpl: Killing all running tasks in stage 228: Stage finished
26/02/13 11:53:08 INFO DAGScheduler: Job 150 finished: start at NativeMethodAccessorImpl.java:0, took 0.083308 s
26/02/13 11:53:08 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 70, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@6ab28ca0] is committing.
26/02/13 11:53:08 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 70, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@6ab28ca0] committed.
26/02/13 11:53:08 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/70 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.70.2c1f92bc-56db-4ba4-a95c-2476ddb0fa88.tmp
26/02/13 11:53:08 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.70.2c1f92bc-56db-4ba4-a95c-2476ddb0fa88.tmp to file:/tmp/spark-checkpoint-enrichment/commits/70
26/02/13 11:53:08 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "fc5c6452-bd70-4c90-becf-772d3bebdb57",
  "name" : null,
  "timestamp" : "2026-02-13T11:53:08.383Z",
  "batchId" : 70,
  "numInputRows" : 147,
  "inputRowsPerSecond" : 141.34615384615384,
  "processedRowsPerSecond" : 403.84615384615387,
  "durationMs" : {
    "addBatch" : 220,
    "commitOffsets" : 61,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 25,
    "triggerExecution" : 364,
    "walCommit" : 57
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 5489,
        "1" : 6015,
        "0" : 7615
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 5538,
        "1" : 6068,
        "0" : 7660
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 5538,
        "1" : 6068,
        "0" : 7660
      }
    },
    "numInputRows" : 147,
    "inputRowsPerSecond" : 141.34615384615384,
    "processedRowsPerSecond" : 403.84615384615387,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@7fe9b5de",
    "numOutputRows" : 31
  }
}
26/02/13 11:53:11 INFO BlockManagerInfo: Removed broadcast_222_piece0 on spark-master:40291 in memory (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 11:53:11 INFO BlockManagerInfo: Removed broadcast_222_piece0 on 172.18.0.14:40939 in memory (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 11:53:11 INFO BlockManagerInfo: Removed broadcast_222_piece0 on 172.18.0.15:34959 in memory (size: 19.5 KiB, free: 433.9 MiB)
26/02/13 11:53:11 INFO BlockManagerInfo: Removed broadcast_223_piece0 on spark-master:40291 in memory (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 11:53:11 INFO BlockManagerInfo: Removed broadcast_223_piece0 on 172.18.0.15:34959 in memory (size: 33.3 KiB, free: 433.9 MiB)
26/02/13 11:53:11 INFO BlockManagerInfo: Removed broadcast_221_piece0 on spark-master:40291 in memory (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 11:53:11 INFO BlockManagerInfo: Removed broadcast_221_piece0 on 172.18.0.14:40939 in memory (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 11:53:11 INFO BlockManagerInfo: Removed broadcast_221_piece0 on 172.18.0.15:34959 in memory (size: 107.2 KiB, free: 434.0 MiB)
26/02/13 11:53:11 INFO BlockManagerInfo: Removed broadcast_220_piece0 on spark-master:40291 in memory (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 11:53:11 INFO BlockManagerInfo: Removed broadcast_220_piece0 on 172.18.0.15:34959 in memory (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 11:53:11 INFO BlockManagerInfo: Removed broadcast_225_piece0 on spark-master:40291 in memory (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 11:53:11 INFO BlockManagerInfo: Removed broadcast_225_piece0 on 172.18.0.14:40939 in memory (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 11:53:11 INFO BlockManagerInfo: Removed broadcast_225_piece0 on 172.18.0.15:34959 in memory (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 11:53:18 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 11:53:23 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/71 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.71.82ce52c9-3ac5-42a9-832f-2772dd1d2e9d.tmp
26/02/13 11:53:23 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.71.82ce52c9-3ac5-42a9-832f-2772dd1d2e9d.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/71
26/02/13 11:53:23 INFO MicroBatchExecution: Committed offsets for batch 71. Metadata OffsetSeqMetadata(0,1770983603713,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 11:53:23 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:53:23 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:53:23 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:53:23 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:53:23 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:53:23 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:53:23 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#62585 - origin_code.nullCount#62584) > 0)
26/02/13 11:53:23 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#62590 - destination_code.nullCount#62589) > 0)
26/02/13 11:53:23 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#62620 - callsign.nullCount#62619) > 0)
26/02/13 11:53:23 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:53:23 INFO DAGScheduler: Got job 151 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 11:53:23 INFO DAGScheduler: Final stage: ResultStage 230 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:53:23 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 229)
26/02/13 11:53:23 INFO DAGScheduler: Missing parents: List()
26/02/13 11:53:23 INFO DAGScheduler: Submitting ResultStage 230 (MapPartitionsRDD[824] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:53:23 INFO MemoryStore: Block broadcast_226 stored as values in memory (estimated size 99.3 KiB, free 434.0 MiB)
26/02/13 11:53:23 INFO MemoryStore: Block broadcast_226_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 433.9 MiB)
26/02/13 11:53:23 INFO BlockManagerInfo: Added broadcast_226_piece0 in memory on spark-master:40291 (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 11:53:23 INFO SparkContext: Created broadcast 226 from broadcast at DAGScheduler.scala:1585
26/02/13 11:53:23 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 230 (MapPartitionsRDD[824] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 11:53:23 INFO TaskSchedulerImpl: Adding task set 230.0 with 4 tasks resource profile 0
26/02/13 11:53:23 INFO TaskSetManager: Starting task 0.0 in stage 230.0 (TID 504) (172.18.0.15, executor 0, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:53:23 INFO TaskSetManager: Starting task 1.0 in stage 230.0 (TID 505) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:53:23 INFO BlockManagerInfo: Added broadcast_226_piece0 in memory on 172.18.0.15:34959 (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 11:53:23 INFO TaskSetManager: Starting task 2.0 in stage 230.0 (TID 506) (172.18.0.15, executor 0, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:53:23 INFO TaskSetManager: Starting task 3.0 in stage 230.0 (TID 507) (172.18.0.15, executor 0, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:53:23 INFO TaskSetManager: Finished task 1.0 in stage 230.0 (TID 505) in 24 ms on 172.18.0.15 (executor 0) (1/4)
26/02/13 11:53:23 INFO TaskSetManager: Finished task 0.0 in stage 230.0 (TID 504) in 25 ms on 172.18.0.15 (executor 0) (2/4)
26/02/13 11:53:24 INFO TaskSetManager: Finished task 2.0 in stage 230.0 (TID 506) in 16 ms on 172.18.0.15 (executor 0) (3/4)
26/02/13 11:53:24 INFO TaskSetManager: Finished task 3.0 in stage 230.0 (TID 507) in 18 ms on 172.18.0.15 (executor 0) (4/4)
26/02/13 11:53:24 INFO TaskSchedulerImpl: Removed TaskSet 230.0, whose tasks have all completed, from pool 
26/02/13 11:53:24 INFO DAGScheduler: ResultStage 230 (start at NativeMethodAccessorImpl.java:0) finished in 0.056 s
26/02/13 11:53:24 INFO DAGScheduler: Job 151 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:53:24 INFO TaskSchedulerImpl: Killing all running tasks in stage 230: Stage finished
26/02/13 11:53:24 INFO DAGScheduler: Job 151 finished: start at NativeMethodAccessorImpl.java:0, took 0.058660 s
26/02/13 11:53:24 INFO MemoryStore: Block broadcast_227_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.8 MiB)
26/02/13 11:53:24 INFO BlockManagerInfo: Added broadcast_227_piece0 in memory on spark-master:40291 (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 11:53:24 INFO SparkContext: Created broadcast 227 from start at NativeMethodAccessorImpl.java:0
26/02/13 11:53:24 INFO BlockManagerInfo: Removed broadcast_224_piece0 on spark-master:40291 in memory (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 11:53:24 INFO BlockManagerInfo: Removed broadcast_224_piece0 on 172.18.0.15:34959 in memory (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 11:53:24 INFO BlockManagerInfo: Removed broadcast_224_piece0 on 172.18.0.14:40939 in memory (size: 107.2 KiB, free: 434.4 MiB)
26/02/13 11:53:24 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 71, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@5d004d31]. The input RDD has 3 partitions.
26/02/13 11:53:24 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:53:24 INFO DAGScheduler: Got job 152 (start at NativeMethodAccessorImpl.java:0) with 3 output partitions
26/02/13 11:53:24 INFO DAGScheduler: Final stage: ResultStage 231 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:53:24 INFO DAGScheduler: Parents of final stage: List()
26/02/13 11:53:24 INFO DAGScheduler: Missing parents: List()
26/02/13 11:53:24 INFO DAGScheduler: Submitting ResultStage 231 (MapPartitionsRDD[830] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:53:24 INFO MemoryStore: Block broadcast_228 stored as values in memory (estimated size 49.9 KiB, free 433.9 MiB)
26/02/13 11:53:24 INFO MemoryStore: Block broadcast_228_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.9 MiB)
26/02/13 11:53:24 INFO BlockManagerInfo: Added broadcast_228_piece0 in memory on spark-master:40291 (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 11:53:24 INFO SparkContext: Created broadcast 228 from broadcast at DAGScheduler.scala:1585
26/02/13 11:53:24 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 231 (MapPartitionsRDD[830] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
26/02/13 11:53:24 INFO TaskSchedulerImpl: Adding task set 231.0 with 3 tasks resource profile 0
26/02/13 11:53:24 INFO TaskSetManager: Starting task 0.0 in stage 231.0 (TID 508) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:53:24 INFO TaskSetManager: Starting task 1.0 in stage 231.0 (TID 509) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:53:24 INFO TaskSetManager: Starting task 2.0 in stage 231.0 (TID 510) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:53:24 INFO BlockManagerInfo: Added broadcast_228_piece0 in memory on 172.18.0.15:34959 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 11:53:24 INFO BlockManagerInfo: Added broadcast_228_piece0 in memory on 172.18.0.14:40939 (size: 19.5 KiB, free: 434.4 MiB)
26/02/13 11:53:24 INFO BlockManagerInfo: Added broadcast_227_piece0 in memory on 172.18.0.15:34959 (size: 107.2 KiB, free: 434.0 MiB)
26/02/13 11:53:24 INFO BlockManagerInfo: Added broadcast_227_piece0 in memory on 172.18.0.14:40939 (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 11:53:24 INFO TaskSetManager: Finished task 2.0 in stage 231.0 (TID 510) in 567 ms on 172.18.0.14 (executor 1) (1/3)
26/02/13 11:53:24 INFO TaskSetManager: Finished task 0.0 in stage 231.0 (TID 508) in 568 ms on 172.18.0.14 (executor 1) (2/3)
26/02/13 11:53:24 INFO TaskSetManager: Finished task 1.0 in stage 231.0 (TID 509) in 570 ms on 172.18.0.15 (executor 0) (3/3)
26/02/13 11:53:24 INFO TaskSchedulerImpl: Removed TaskSet 231.0, whose tasks have all completed, from pool 
26/02/13 11:53:24 INFO DAGScheduler: ResultStage 231 (start at NativeMethodAccessorImpl.java:0) finished in 0.577 s
26/02/13 11:53:24 INFO DAGScheduler: Job 152 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:53:24 INFO TaskSchedulerImpl: Killing all running tasks in stage 231: Stage finished
26/02/13 11:53:24 INFO DAGScheduler: Job 152 finished: start at NativeMethodAccessorImpl.java:0, took 0.578891 s
26/02/13 11:53:24 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 71, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@5d004d31] is committing.
26/02/13 11:53:24 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 71, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@5d004d31] committed.
26/02/13 11:53:24 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/71 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.71.7a5d7ac1-1cf6-418f-a149-a61f7d4698a5.tmp
26/02/13 11:53:24 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.71.7a5d7ac1-1cf6-418f-a149-a61f7d4698a5.tmp to file:/tmp/spark-checkpoint-enrichment/commits/71
26/02/13 11:53:24 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "fc5c6452-bd70-4c90-becf-772d3bebdb57",
  "name" : null,
  "timestamp" : "2026-02-13T11:53:23.711Z",
  "batchId" : 71,
  "numInputRows" : 90,
  "inputRowsPerSecond" : 8181.818181818182,
  "processedRowsPerSecond" : 93.45794392523365,
  "durationMs" : {
    "addBatch" : 752,
    "commitOffsets" : 59,
    "getBatch" : 0,
    "latestOffset" : 2,
    "queryPlanning" : 51,
    "triggerExecution" : 963,
    "walCommit" : 98
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 5538,
        "1" : 6068,
        "0" : 7660
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 5569,
        "1" : 6102,
        "0" : 7685
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 5569,
        "1" : 6102,
        "0" : 7685
      }
    },
    "numInputRows" : 90,
    "inputRowsPerSecond" : 8181.818181818182,
    "processedRowsPerSecond" : 93.45794392523365,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@7fe9b5de",
    "numOutputRows" : 24
  }
}
26/02/13 11:53:24 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/72 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.72.904b2f23-c0d2-4d78-97e5-1eb90e6196cc.tmp
26/02/13 11:53:24 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.72.904b2f23-c0d2-4d78-97e5-1eb90e6196cc.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/72
26/02/13 11:53:24 INFO MicroBatchExecution: Committed offsets for batch 72. Metadata OffsetSeqMetadata(0,1770983604675,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 11:53:24 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:53:24 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:53:24 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:53:24 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:53:24 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:53:24 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:53:24 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#63439 - origin_code.nullCount#63438) > 0)
26/02/13 11:53:24 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#63444 - destination_code.nullCount#63443) > 0)
26/02/13 11:53:24 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#63474 - callsign.nullCount#63473) > 0)
26/02/13 11:53:24 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:53:24 INFO DAGScheduler: Got job 153 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 11:53:24 INFO DAGScheduler: Final stage: ResultStage 233 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:53:24 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 232)
26/02/13 11:53:24 INFO DAGScheduler: Missing parents: List()
26/02/13 11:53:24 INFO DAGScheduler: Submitting ResultStage 233 (MapPartitionsRDD[835] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:53:24 INFO MemoryStore: Block broadcast_229 stored as values in memory (estimated size 99.3 KiB, free 433.8 MiB)
26/02/13 11:53:24 INFO MemoryStore: Block broadcast_229_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 433.7 MiB)
26/02/13 11:53:24 INFO BlockManagerInfo: Added broadcast_229_piece0 in memory on spark-master:40291 (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 11:53:24 INFO SparkContext: Created broadcast 229 from broadcast at DAGScheduler.scala:1585
26/02/13 11:53:24 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 233 (MapPartitionsRDD[835] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 11:53:24 INFO TaskSchedulerImpl: Adding task set 233.0 with 4 tasks resource profile 0
26/02/13 11:53:24 INFO TaskSetManager: Starting task 0.0 in stage 233.0 (TID 511) (172.18.0.15, executor 0, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:53:24 INFO TaskSetManager: Starting task 1.0 in stage 233.0 (TID 512) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:53:24 INFO BlockManagerInfo: Added broadcast_229_piece0 in memory on 172.18.0.15:34959 (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 11:53:24 INFO TaskSetManager: Starting task 2.0 in stage 233.0 (TID 513) (172.18.0.15, executor 0, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:53:24 INFO TaskSetManager: Finished task 0.0 in stage 233.0 (TID 511) in 17 ms on 172.18.0.15 (executor 0) (1/4)
26/02/13 11:53:24 INFO TaskSetManager: Starting task 3.0 in stage 233.0 (TID 514) (172.18.0.15, executor 0, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:53:24 INFO TaskSetManager: Finished task 1.0 in stage 233.0 (TID 512) in 22 ms on 172.18.0.15 (executor 0) (2/4)
26/02/13 11:53:24 INFO TaskSetManager: Finished task 3.0 in stage 233.0 (TID 514) in 15 ms on 172.18.0.15 (executor 0) (3/4)
26/02/13 11:53:24 INFO TaskSetManager: Finished task 2.0 in stage 233.0 (TID 513) in 23 ms on 172.18.0.15 (executor 0) (4/4)
26/02/13 11:53:24 INFO TaskSchedulerImpl: Removed TaskSet 233.0, whose tasks have all completed, from pool 
26/02/13 11:53:24 INFO DAGScheduler: ResultStage 233 (start at NativeMethodAccessorImpl.java:0) finished in 0.046 s
26/02/13 11:53:24 INFO DAGScheduler: Job 153 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:53:24 INFO TaskSchedulerImpl: Killing all running tasks in stage 233: Stage finished
26/02/13 11:53:24 INFO DAGScheduler: Job 153 finished: start at NativeMethodAccessorImpl.java:0, took 0.049457 s
26/02/13 11:53:24 INFO MemoryStore: Block broadcast_230_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.6 MiB)
26/02/13 11:53:24 INFO BlockManagerInfo: Added broadcast_230_piece0 in memory on spark-master:40291 (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 11:53:24 INFO SparkContext: Created broadcast 230 from start at NativeMethodAccessorImpl.java:0
26/02/13 11:53:24 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 72, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@259ae96d]. The input RDD has 3 partitions.
26/02/13 11:53:24 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:53:24 INFO DAGScheduler: Got job 154 (start at NativeMethodAccessorImpl.java:0) with 3 output partitions
26/02/13 11:53:24 INFO DAGScheduler: Final stage: ResultStage 234 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:53:24 INFO DAGScheduler: Parents of final stage: List()
26/02/13 11:53:24 INFO DAGScheduler: Missing parents: List()
26/02/13 11:53:24 INFO DAGScheduler: Submitting ResultStage 234 (MapPartitionsRDD[841] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:53:24 INFO MemoryStore: Block broadcast_231 stored as values in memory (estimated size 49.9 KiB, free 433.6 MiB)
26/02/13 11:53:24 INFO MemoryStore: Block broadcast_231_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.6 MiB)
26/02/13 11:53:24 INFO BlockManagerInfo: Added broadcast_231_piece0 in memory on spark-master:40291 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 11:53:24 INFO SparkContext: Created broadcast 231 from broadcast at DAGScheduler.scala:1585
26/02/13 11:53:24 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 234 (MapPartitionsRDD[841] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
26/02/13 11:53:24 INFO TaskSchedulerImpl: Adding task set 234.0 with 3 tasks resource profile 0
26/02/13 11:53:24 INFO TaskSetManager: Starting task 1.0 in stage 234.0 (TID 515) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:53:24 INFO TaskSetManager: Starting task 0.0 in stage 234.0 (TID 516) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:53:24 INFO TaskSetManager: Starting task 2.0 in stage 234.0 (TID 517) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:53:24 INFO BlockManagerInfo: Added broadcast_231_piece0 in memory on 172.18.0.14:40939 (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 11:53:24 INFO BlockManagerInfo: Added broadcast_231_piece0 in memory on 172.18.0.15:34959 (size: 19.5 KiB, free: 434.0 MiB)
26/02/13 11:53:24 INFO BlockManagerInfo: Added broadcast_230_piece0 in memory on 172.18.0.14:40939 (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 11:53:24 INFO BlockManagerInfo: Added broadcast_230_piece0 in memory on 172.18.0.15:34959 (size: 107.2 KiB, free: 433.9 MiB)
26/02/13 11:53:24 INFO TaskSetManager: Finished task 0.0 in stage 234.0 (TID 516) in 51 ms on 172.18.0.14 (executor 1) (1/3)
26/02/13 11:53:24 INFO TaskSetManager: Finished task 1.0 in stage 234.0 (TID 515) in 60 ms on 172.18.0.15 (executor 0) (2/3)
26/02/13 11:53:24 INFO TaskSetManager: Finished task 2.0 in stage 234.0 (TID 517) in 60 ms on 172.18.0.14 (executor 1) (3/3)
26/02/13 11:53:24 INFO TaskSchedulerImpl: Removed TaskSet 234.0, whose tasks have all completed, from pool 
26/02/13 11:53:24 INFO DAGScheduler: ResultStage 234 (start at NativeMethodAccessorImpl.java:0) finished in 0.066 s
26/02/13 11:53:24 INFO DAGScheduler: Job 154 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:53:24 INFO TaskSchedulerImpl: Killing all running tasks in stage 234: Stage finished
26/02/13 11:53:24 INFO DAGScheduler: Job 154 finished: start at NativeMethodAccessorImpl.java:0, took 0.067924 s
26/02/13 11:53:24 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 72, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@259ae96d] is committing.
26/02/13 11:53:24 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 72, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@259ae96d] committed.
26/02/13 11:53:24 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/72 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.72.679788ad-2fb6-4f69-999b-ac16eaf149e6.tmp
26/02/13 11:53:25 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.72.679788ad-2fb6-4f69-999b-ac16eaf149e6.tmp to file:/tmp/spark-checkpoint-enrichment/commits/72
26/02/13 11:53:25 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "fc5c6452-bd70-4c90-becf-772d3bebdb57",
  "name" : null,
  "timestamp" : "2026-02-13T11:53:24.674Z",
  "batchId" : 72,
  "numInputRows" : 157,
  "inputRowsPerSecond" : 163.03219106957425,
  "processedRowsPerSecond" : 446.0227272727273,
  "durationMs" : {
    "addBatch" : 192,
    "commitOffsets" : 65,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 32,
    "triggerExecution" : 352,
    "walCommit" : 61
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 5569,
        "1" : 6102,
        "0" : 7685
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 5618,
        "1" : 6159,
        "0" : 7736
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 5618,
        "1" : 6159,
        "0" : 7736
      }
    },
    "numInputRows" : 157,
    "inputRowsPerSecond" : 163.03219106957425,
    "processedRowsPerSecond" : 446.0227272727273,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@7fe9b5de",
    "numOutputRows" : 32
  }
}
26/02/13 11:53:27 INFO BlockManagerInfo: Removed broadcast_228_piece0 on spark-master:40291 in memory (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 11:53:27 INFO BlockManagerInfo: Removed broadcast_228_piece0 on 172.18.0.15:34959 in memory (size: 19.5 KiB, free: 433.9 MiB)
26/02/13 11:53:27 INFO BlockManagerInfo: Removed broadcast_228_piece0 on 172.18.0.14:40939 in memory (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 11:53:27 INFO BlockManagerInfo: Removed broadcast_229_piece0 on spark-master:40291 in memory (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 11:53:27 INFO BlockManagerInfo: Removed broadcast_229_piece0 on 172.18.0.15:34959 in memory (size: 33.3 KiB, free: 433.9 MiB)
26/02/13 11:53:27 INFO BlockManagerInfo: Removed broadcast_226_piece0 on 172.18.0.15:34959 in memory (size: 33.3 KiB, free: 433.9 MiB)
26/02/13 11:53:27 INFO BlockManagerInfo: Removed broadcast_226_piece0 on spark-master:40291 in memory (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 11:53:27 INFO BlockManagerInfo: Removed broadcast_231_piece0 on spark-master:40291 in memory (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 11:53:27 INFO BlockManagerInfo: Removed broadcast_231_piece0 on 172.18.0.15:34959 in memory (size: 19.5 KiB, free: 434.0 MiB)
26/02/13 11:53:27 INFO BlockManagerInfo: Removed broadcast_231_piece0 on 172.18.0.14:40939 in memory (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 11:53:27 INFO BlockManagerInfo: Removed broadcast_227_piece0 on spark-master:40291 in memory (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 11:53:27 INFO BlockManagerInfo: Removed broadcast_227_piece0 on 172.18.0.15:34959 in memory (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 11:53:27 INFO BlockManagerInfo: Removed broadcast_227_piece0 on 172.18.0.14:40939 in memory (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 11:53:35 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 11:53:42 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/73 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.73.c22ddf0c-c7ab-4172-aac5-86113cf883a4.tmp
26/02/13 11:53:42 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.73.c22ddf0c-c7ab-4172-aac5-86113cf883a4.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/73
26/02/13 11:53:42 INFO MicroBatchExecution: Committed offsets for batch 73. Metadata OffsetSeqMetadata(0,1770983622151,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 11:53:42 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:53:42 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:53:42 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:53:42 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:53:42 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:53:42 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:53:42 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#64293 - origin_code.nullCount#64292) > 0)
26/02/13 11:53:42 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#64298 - destination_code.nullCount#64297) > 0)
26/02/13 11:53:42 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#64328 - callsign.nullCount#64327) > 0)
26/02/13 11:53:42 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:53:42 INFO DAGScheduler: Got job 155 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 11:53:42 INFO DAGScheduler: Final stage: ResultStage 236 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:53:42 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 235)
26/02/13 11:53:42 INFO DAGScheduler: Missing parents: List()
26/02/13 11:53:42 INFO DAGScheduler: Submitting ResultStage 236 (MapPartitionsRDD[846] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:53:42 INFO MemoryStore: Block broadcast_232 stored as values in memory (estimated size 99.3 KiB, free 434.0 MiB)
26/02/13 11:53:42 INFO MemoryStore: Block broadcast_232_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 433.9 MiB)
26/02/13 11:53:42 INFO BlockManagerInfo: Added broadcast_232_piece0 in memory on spark-master:40291 (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 11:53:42 INFO SparkContext: Created broadcast 232 from broadcast at DAGScheduler.scala:1585
26/02/13 11:53:42 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 236 (MapPartitionsRDD[846] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 11:53:42 INFO TaskSchedulerImpl: Adding task set 236.0 with 4 tasks resource profile 0
26/02/13 11:53:42 INFO TaskSetManager: Starting task 0.0 in stage 236.0 (TID 518) (172.18.0.15, executor 0, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:53:42 INFO TaskSetManager: Starting task 1.0 in stage 236.0 (TID 519) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:53:42 INFO BlockManagerInfo: Added broadcast_232_piece0 in memory on 172.18.0.15:34959 (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 11:53:42 INFO TaskSetManager: Starting task 2.0 in stage 236.0 (TID 520) (172.18.0.15, executor 0, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:53:42 INFO TaskSetManager: Finished task 1.0 in stage 236.0 (TID 519) in 15 ms on 172.18.0.15 (executor 0) (1/4)
26/02/13 11:53:42 INFO TaskSetManager: Starting task 3.0 in stage 236.0 (TID 521) (172.18.0.15, executor 0, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:53:42 INFO TaskSetManager: Finished task 0.0 in stage 236.0 (TID 518) in 16 ms on 172.18.0.15 (executor 0) (2/4)
26/02/13 11:53:42 INFO TaskSetManager: Finished task 3.0 in stage 236.0 (TID 521) in 10 ms on 172.18.0.15 (executor 0) (3/4)
26/02/13 11:53:42 INFO TaskSetManager: Finished task 2.0 in stage 236.0 (TID 520) in 15 ms on 172.18.0.15 (executor 0) (4/4)
26/02/13 11:53:42 INFO TaskSchedulerImpl: Removed TaskSet 236.0, whose tasks have all completed, from pool 
26/02/13 11:53:42 INFO DAGScheduler: ResultStage 236 (start at NativeMethodAccessorImpl.java:0) finished in 0.038 s
26/02/13 11:53:42 INFO DAGScheduler: Job 155 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:53:42 INFO TaskSchedulerImpl: Killing all running tasks in stage 236: Stage finished
26/02/13 11:53:42 INFO DAGScheduler: Job 155 finished: start at NativeMethodAccessorImpl.java:0, took 0.040223 s
26/02/13 11:53:42 INFO MemoryStore: Block broadcast_233_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.8 MiB)
26/02/13 11:53:42 INFO BlockManagerInfo: Added broadcast_233_piece0 in memory on spark-master:40291 (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 11:53:42 INFO SparkContext: Created broadcast 233 from start at NativeMethodAccessorImpl.java:0
26/02/13 11:53:42 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 73, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@61a56e46]. The input RDD has 3 partitions.
26/02/13 11:53:42 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:53:42 INFO DAGScheduler: Got job 156 (start at NativeMethodAccessorImpl.java:0) with 3 output partitions
26/02/13 11:53:42 INFO DAGScheduler: Final stage: ResultStage 237 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:53:42 INFO DAGScheduler: Parents of final stage: List()
26/02/13 11:53:42 INFO DAGScheduler: Missing parents: List()
26/02/13 11:53:42 INFO DAGScheduler: Submitting ResultStage 237 (MapPartitionsRDD[852] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:53:42 INFO MemoryStore: Block broadcast_234 stored as values in memory (estimated size 49.9 KiB, free 433.8 MiB)
26/02/13 11:53:42 INFO MemoryStore: Block broadcast_234_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.8 MiB)
26/02/13 11:53:42 INFO BlockManagerInfo: Added broadcast_234_piece0 in memory on spark-master:40291 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 11:53:42 INFO SparkContext: Created broadcast 234 from broadcast at DAGScheduler.scala:1585
26/02/13 11:53:42 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 237 (MapPartitionsRDD[852] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
26/02/13 11:53:42 INFO TaskSchedulerImpl: Adding task set 237.0 with 3 tasks resource profile 0
26/02/13 11:53:42 INFO TaskSetManager: Starting task 0.0 in stage 237.0 (TID 522) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:53:42 INFO TaskSetManager: Starting task 1.0 in stage 237.0 (TID 523) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:53:42 INFO TaskSetManager: Starting task 2.0 in stage 237.0 (TID 524) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:53:42 INFO BlockManagerInfo: Removed broadcast_230_piece0 on spark-master:40291 in memory (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 11:53:42 INFO BlockManagerInfo: Removed broadcast_230_piece0 on 172.18.0.14:40939 in memory (size: 107.2 KiB, free: 434.4 MiB)
26/02/13 11:53:42 INFO BlockManagerInfo: Removed broadcast_230_piece0 on 172.18.0.15:34959 in memory (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 11:53:42 INFO BlockManagerInfo: Added broadcast_234_piece0 in memory on 172.18.0.14:40939 (size: 19.5 KiB, free: 434.4 MiB)
26/02/13 11:53:42 INFO BlockManagerInfo: Added broadcast_234_piece0 in memory on 172.18.0.15:34959 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 11:53:42 INFO BlockManagerInfo: Added broadcast_233_piece0 in memory on 172.18.0.14:40939 (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 11:53:42 INFO BlockManagerInfo: Added broadcast_233_piece0 in memory on 172.18.0.15:34959 (size: 107.2 KiB, free: 434.0 MiB)
26/02/13 11:53:42 INFO TaskSetManager: Finished task 0.0 in stage 237.0 (TID 522) in 578 ms on 172.18.0.14 (executor 1) (1/3)
26/02/13 11:53:42 INFO TaskSetManager: Finished task 2.0 in stage 237.0 (TID 524) in 578 ms on 172.18.0.14 (executor 1) (2/3)
26/02/13 11:53:42 INFO TaskSetManager: Finished task 1.0 in stage 237.0 (TID 523) in 583 ms on 172.18.0.15 (executor 0) (3/3)
26/02/13 11:53:42 INFO TaskSchedulerImpl: Removed TaskSet 237.0, whose tasks have all completed, from pool 
26/02/13 11:53:42 INFO DAGScheduler: ResultStage 237 (start at NativeMethodAccessorImpl.java:0) finished in 0.587 s
26/02/13 11:53:42 INFO DAGScheduler: Job 156 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:53:42 INFO TaskSchedulerImpl: Killing all running tasks in stage 237: Stage finished
26/02/13 11:53:42 INFO DAGScheduler: Job 156 finished: start at NativeMethodAccessorImpl.java:0, took 0.588742 s
26/02/13 11:53:42 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 73, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@61a56e46] is committing.
26/02/13 11:53:42 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 73, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@61a56e46] committed.
26/02/13 11:53:43 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/73 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.73.702d4923-ba99-4abb-aca8-e61b930fc477.tmp
26/02/13 11:53:43 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.73.702d4923-ba99-4abb-aca8-e61b930fc477.tmp to file:/tmp/spark-checkpoint-enrichment/commits/73
26/02/13 11:53:43 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "fc5c6452-bd70-4c90-becf-772d3bebdb57",
  "name" : null,
  "timestamp" : "2026-02-13T11:53:42.150Z",
  "batchId" : 73,
  "numInputRows" : 111,
  "inputRowsPerSecond" : 9250.0,
  "processedRowsPerSecond" : 121.97802197802197,
  "durationMs" : {
    "addBatch" : 705,
    "commitOffsets" : 66,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 50,
    "triggerExecution" : 910,
    "walCommit" : 88
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 5618,
        "1" : 6159,
        "0" : 7736
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 5655,
        "1" : 6196,
        "0" : 7773
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 5655,
        "1" : 6196,
        "0" : 7773
      }
    },
    "numInputRows" : 111,
    "inputRowsPerSecond" : 9250.0,
    "processedRowsPerSecond" : 121.97802197802197,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@7fe9b5de",
    "numOutputRows" : 28
  }
}
26/02/13 11:53:43 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/74 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.74.dd5e44c7-49bc-4f95-8f08-b220c4f3f821.tmp
26/02/13 11:53:43 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.74.dd5e44c7-49bc-4f95-8f08-b220c4f3f821.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/74
26/02/13 11:53:43 INFO MicroBatchExecution: Committed offsets for batch 74. Metadata OffsetSeqMetadata(0,1770983623062,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 11:53:43 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:53:43 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:53:43 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:53:43 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:53:43 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:53:43 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:53:43 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#65147 - origin_code.nullCount#65146) > 0)
26/02/13 11:53:43 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#65152 - destination_code.nullCount#65151) > 0)
26/02/13 11:53:43 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#65182 - callsign.nullCount#65181) > 0)
26/02/13 11:53:43 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:53:43 INFO DAGScheduler: Got job 157 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 11:53:43 INFO DAGScheduler: Final stage: ResultStage 239 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:53:43 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 238)
26/02/13 11:53:43 INFO DAGScheduler: Missing parents: List()
26/02/13 11:53:43 INFO DAGScheduler: Submitting ResultStage 239 (MapPartitionsRDD[857] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:53:43 INFO MemoryStore: Block broadcast_235 stored as values in memory (estimated size 99.3 KiB, free 433.8 MiB)
26/02/13 11:53:43 INFO MemoryStore: Block broadcast_235_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 433.7 MiB)
26/02/13 11:53:43 INFO BlockManagerInfo: Added broadcast_235_piece0 in memory on spark-master:40291 (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 11:53:43 INFO SparkContext: Created broadcast 235 from broadcast at DAGScheduler.scala:1585
26/02/13 11:53:43 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 239 (MapPartitionsRDD[857] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 11:53:43 INFO TaskSchedulerImpl: Adding task set 239.0 with 4 tasks resource profile 0
26/02/13 11:53:43 INFO TaskSetManager: Starting task 0.0 in stage 239.0 (TID 525) (172.18.0.15, executor 0, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:53:43 INFO TaskSetManager: Starting task 1.0 in stage 239.0 (TID 526) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:53:43 INFO BlockManagerInfo: Added broadcast_235_piece0 in memory on 172.18.0.15:34959 (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 11:53:43 INFO TaskSetManager: Starting task 2.0 in stage 239.0 (TID 527) (172.18.0.15, executor 0, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:53:43 INFO TaskSetManager: Finished task 0.0 in stage 239.0 (TID 525) in 16 ms on 172.18.0.15 (executor 0) (1/4)
26/02/13 11:53:43 INFO TaskSetManager: Starting task 3.0 in stage 239.0 (TID 528) (172.18.0.15, executor 0, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:53:43 INFO TaskSetManager: Finished task 1.0 in stage 239.0 (TID 526) in 29 ms on 172.18.0.15 (executor 0) (2/4)
26/02/13 11:53:43 INFO TaskSetManager: Finished task 2.0 in stage 239.0 (TID 527) in 24 ms on 172.18.0.15 (executor 0) (3/4)
26/02/13 11:53:43 INFO TaskSetManager: Finished task 3.0 in stage 239.0 (TID 528) in 18 ms on 172.18.0.15 (executor 0) (4/4)
26/02/13 11:53:43 INFO TaskSchedulerImpl: Removed TaskSet 239.0, whose tasks have all completed, from pool 
26/02/13 11:53:43 INFO DAGScheduler: ResultStage 239 (start at NativeMethodAccessorImpl.java:0) finished in 0.052 s
26/02/13 11:53:43 INFO DAGScheduler: Job 157 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:53:43 INFO TaskSchedulerImpl: Killing all running tasks in stage 239: Stage finished
26/02/13 11:53:43 INFO DAGScheduler: Job 157 finished: start at NativeMethodAccessorImpl.java:0, took 0.053665 s
26/02/13 11:53:43 INFO MemoryStore: Block broadcast_236_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.6 MiB)
26/02/13 11:53:43 INFO BlockManagerInfo: Added broadcast_236_piece0 in memory on spark-master:40291 (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 11:53:43 INFO SparkContext: Created broadcast 236 from start at NativeMethodAccessorImpl.java:0
26/02/13 11:53:43 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 74, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@129599d6]. The input RDD has 3 partitions.
26/02/13 11:53:43 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:53:43 INFO DAGScheduler: Got job 158 (start at NativeMethodAccessorImpl.java:0) with 3 output partitions
26/02/13 11:53:43 INFO DAGScheduler: Final stage: ResultStage 240 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:53:43 INFO DAGScheduler: Parents of final stage: List()
26/02/13 11:53:43 INFO DAGScheduler: Missing parents: List()
26/02/13 11:53:43 INFO DAGScheduler: Submitting ResultStage 240 (MapPartitionsRDD[863] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:53:43 INFO MemoryStore: Block broadcast_237 stored as values in memory (estimated size 49.9 KiB, free 433.6 MiB)
26/02/13 11:53:43 INFO MemoryStore: Block broadcast_237_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.6 MiB)
26/02/13 11:53:43 INFO BlockManagerInfo: Added broadcast_237_piece0 in memory on spark-master:40291 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 11:53:43 INFO SparkContext: Created broadcast 237 from broadcast at DAGScheduler.scala:1585
26/02/13 11:53:43 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 240 (MapPartitionsRDD[863] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
26/02/13 11:53:43 INFO TaskSchedulerImpl: Adding task set 240.0 with 3 tasks resource profile 0
26/02/13 11:53:43 INFO TaskSetManager: Starting task 1.0 in stage 240.0 (TID 529) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:53:43 INFO TaskSetManager: Starting task 0.0 in stage 240.0 (TID 530) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:53:43 INFO TaskSetManager: Starting task 2.0 in stage 240.0 (TID 531) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:53:43 INFO BlockManagerInfo: Added broadcast_237_piece0 in memory on 172.18.0.15:34959 (size: 19.5 KiB, free: 434.0 MiB)
26/02/13 11:53:43 INFO BlockManagerInfo: Added broadcast_237_piece0 in memory on 172.18.0.14:40939 (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 11:53:43 INFO BlockManagerInfo: Added broadcast_236_piece0 in memory on 172.18.0.14:40939 (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 11:53:43 INFO BlockManagerInfo: Added broadcast_236_piece0 in memory on 172.18.0.15:34959 (size: 107.2 KiB, free: 433.9 MiB)
26/02/13 11:53:43 INFO TaskSetManager: Finished task 2.0 in stage 240.0 (TID 531) in 59 ms on 172.18.0.14 (executor 1) (1/3)
26/02/13 11:53:43 INFO TaskSetManager: Finished task 0.0 in stage 240.0 (TID 530) in 61 ms on 172.18.0.14 (executor 1) (2/3)
26/02/13 11:53:43 INFO TaskSetManager: Finished task 1.0 in stage 240.0 (TID 529) in 68 ms on 172.18.0.15 (executor 0) (3/3)
26/02/13 11:53:43 INFO TaskSchedulerImpl: Removed TaskSet 240.0, whose tasks have all completed, from pool 
26/02/13 11:53:43 INFO DAGScheduler: ResultStage 240 (start at NativeMethodAccessorImpl.java:0) finished in 0.074 s
26/02/13 11:53:43 INFO DAGScheduler: Job 158 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:53:43 INFO TaskSchedulerImpl: Killing all running tasks in stage 240: Stage finished
26/02/13 11:53:43 INFO DAGScheduler: Job 158 finished: start at NativeMethodAccessorImpl.java:0, took 0.075351 s
26/02/13 11:53:43 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 74, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@129599d6] is committing.
26/02/13 11:53:43 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 74, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@129599d6] committed.
26/02/13 11:53:43 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/74 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.74.c9d92a63-a448-48fc-84dc-c15355ee6a6e.tmp
26/02/13 11:53:43 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.74.c9d92a63-a448-48fc-84dc-c15355ee6a6e.tmp to file:/tmp/spark-checkpoint-enrichment/commits/74
26/02/13 11:53:43 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "fc5c6452-bd70-4c90-becf-772d3bebdb57",
  "name" : null,
  "timestamp" : "2026-02-13T11:53:43.061Z",
  "batchId" : 74,
  "numInputRows" : 137,
  "inputRowsPerSecond" : 150.38419319429198,
  "processedRowsPerSecond" : 397.10144927536237,
  "durationMs" : {
    "addBatch" : 207,
    "commitOffsets" : 58,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 21,
    "triggerExecution" : 345,
    "walCommit" : 57
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 5655,
        "1" : 6196,
        "0" : 7773
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 5697,
        "1" : 6251,
        "0" : 7813
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 5697,
        "1" : 6251,
        "0" : 7813
      }
    },
    "numInputRows" : 137,
    "inputRowsPerSecond" : 150.38419319429198,
    "processedRowsPerSecond" : 397.10144927536237,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@7fe9b5de",
    "numOutputRows" : 28
  }
}
26/02/13 11:53:46 INFO BlockManagerInfo: Removed broadcast_234_piece0 on spark-master:40291 in memory (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 11:53:46 INFO BlockManagerInfo: Removed broadcast_234_piece0 on 172.18.0.14:40939 in memory (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 11:53:46 INFO BlockManagerInfo: Removed broadcast_234_piece0 on 172.18.0.15:34959 in memory (size: 19.5 KiB, free: 433.9 MiB)
26/02/13 11:53:46 INFO BlockManagerInfo: Removed broadcast_232_piece0 on spark-master:40291 in memory (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 11:53:46 INFO BlockManagerInfo: Removed broadcast_232_piece0 on 172.18.0.15:34959 in memory (size: 33.3 KiB, free: 433.9 MiB)
26/02/13 11:53:46 INFO BlockManagerInfo: Removed broadcast_233_piece0 on spark-master:40291 in memory (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 11:53:46 INFO BlockManagerInfo: Removed broadcast_233_piece0 on 172.18.0.14:40939 in memory (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 11:53:46 INFO BlockManagerInfo: Removed broadcast_233_piece0 on 172.18.0.15:34959 in memory (size: 107.2 KiB, free: 434.0 MiB)
26/02/13 11:53:46 INFO BlockManagerInfo: Removed broadcast_235_piece0 on spark-master:40291 in memory (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 11:53:46 INFO BlockManagerInfo: Removed broadcast_235_piece0 on 172.18.0.15:34959 in memory (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 11:53:46 INFO BlockManagerInfo: Removed broadcast_237_piece0 on spark-master:40291 in memory (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 11:53:46 INFO BlockManagerInfo: Removed broadcast_237_piece0 on 172.18.0.15:34959 in memory (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 11:53:46 INFO BlockManagerInfo: Removed broadcast_237_piece0 on 172.18.0.14:40939 in memory (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 11:53:53 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 11:53:58 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/75 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.75.0f194c4d-f887-4a72-a720-625e2f85dabd.tmp
26/02/13 11:53:58 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.75.0f194c4d-f887-4a72-a720-625e2f85dabd.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/75
26/02/13 11:53:58 INFO MicroBatchExecution: Committed offsets for batch 75. Metadata OffsetSeqMetadata(0,1770983638604,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 11:53:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:53:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:53:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:53:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:53:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:53:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:53:58 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#66001 - origin_code.nullCount#66000) > 0)
26/02/13 11:53:58 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#66006 - destination_code.nullCount#66005) > 0)
26/02/13 11:53:58 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#66036 - callsign.nullCount#66035) > 0)
26/02/13 11:53:58 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:53:58 INFO DAGScheduler: Got job 159 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 11:53:58 INFO DAGScheduler: Final stage: ResultStage 242 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:53:58 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 241)
26/02/13 11:53:58 INFO DAGScheduler: Missing parents: List()
26/02/13 11:53:58 INFO DAGScheduler: Submitting ResultStage 242 (MapPartitionsRDD[868] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:53:58 INFO MemoryStore: Block broadcast_238 stored as values in memory (estimated size 99.3 KiB, free 434.0 MiB)
26/02/13 11:53:58 INFO MemoryStore: Block broadcast_238_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 433.9 MiB)
26/02/13 11:53:58 INFO BlockManagerInfo: Added broadcast_238_piece0 in memory on spark-master:40291 (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 11:53:58 INFO SparkContext: Created broadcast 238 from broadcast at DAGScheduler.scala:1585
26/02/13 11:53:58 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 242 (MapPartitionsRDD[868] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 11:53:58 INFO TaskSchedulerImpl: Adding task set 242.0 with 4 tasks resource profile 0
26/02/13 11:53:58 INFO TaskSetManager: Starting task 0.0 in stage 242.0 (TID 532) (172.18.0.15, executor 0, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:53:58 INFO TaskSetManager: Starting task 1.0 in stage 242.0 (TID 533) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:53:58 INFO BlockManagerInfo: Added broadcast_238_piece0 in memory on 172.18.0.15:34959 (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 11:53:58 INFO TaskSetManager: Starting task 2.0 in stage 242.0 (TID 534) (172.18.0.15, executor 0, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:53:58 INFO TaskSetManager: Finished task 1.0 in stage 242.0 (TID 533) in 15 ms on 172.18.0.15 (executor 0) (1/4)
26/02/13 11:53:58 INFO TaskSetManager: Starting task 3.0 in stage 242.0 (TID 535) (172.18.0.15, executor 0, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:53:58 INFO TaskSetManager: Finished task 0.0 in stage 242.0 (TID 532) in 16 ms on 172.18.0.15 (executor 0) (2/4)
26/02/13 11:53:58 INFO TaskSetManager: Finished task 2.0 in stage 242.0 (TID 534) in 13 ms on 172.18.0.15 (executor 0) (3/4)
26/02/13 11:53:58 INFO TaskSetManager: Finished task 3.0 in stage 242.0 (TID 535) in 14 ms on 172.18.0.15 (executor 0) (4/4)
26/02/13 11:53:58 INFO TaskSchedulerImpl: Removed TaskSet 242.0, whose tasks have all completed, from pool 
26/02/13 11:53:58 INFO DAGScheduler: ResultStage 242 (start at NativeMethodAccessorImpl.java:0) finished in 0.039 s
26/02/13 11:53:58 INFO DAGScheduler: Job 159 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:53:58 INFO TaskSchedulerImpl: Killing all running tasks in stage 242: Stage finished
26/02/13 11:53:58 INFO DAGScheduler: Job 159 finished: start at NativeMethodAccessorImpl.java:0, took 0.040994 s
26/02/13 11:53:58 INFO BlockManagerInfo: Removed broadcast_238_piece0 on spark-master:40291 in memory (size: 33.3 KiB, free: 434.3 MiB)
26/02/13 11:53:58 INFO BlockManagerInfo: Removed broadcast_238_piece0 on 172.18.0.15:34959 in memory (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 11:53:58 INFO MemoryStore: Block broadcast_239_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 434.0 MiB)
26/02/13 11:53:58 INFO BlockManagerInfo: Added broadcast_239_piece0 in memory on spark-master:40291 (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 11:53:58 INFO SparkContext: Created broadcast 239 from start at NativeMethodAccessorImpl.java:0
26/02/13 11:53:58 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 75, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@1988b7fd]. The input RDD has 3 partitions.
26/02/13 11:53:58 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:53:58 INFO DAGScheduler: Got job 160 (start at NativeMethodAccessorImpl.java:0) with 3 output partitions
26/02/13 11:53:58 INFO DAGScheduler: Final stage: ResultStage 243 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:53:58 INFO DAGScheduler: Parents of final stage: List()
26/02/13 11:53:58 INFO DAGScheduler: Missing parents: List()
26/02/13 11:53:58 INFO DAGScheduler: Submitting ResultStage 243 (MapPartitionsRDD[874] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:53:58 INFO MemoryStore: Block broadcast_240 stored as values in memory (estimated size 49.9 KiB, free 433.9 MiB)
26/02/13 11:53:58 INFO MemoryStore: Block broadcast_240_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.9 MiB)
26/02/13 11:53:58 INFO BlockManagerInfo: Added broadcast_240_piece0 in memory on spark-master:40291 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 11:53:58 INFO SparkContext: Created broadcast 240 from broadcast at DAGScheduler.scala:1585
26/02/13 11:53:58 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 243 (MapPartitionsRDD[874] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
26/02/13 11:53:58 INFO TaskSchedulerImpl: Adding task set 243.0 with 3 tasks resource profile 0
26/02/13 11:53:58 INFO TaskSetManager: Starting task 0.0 in stage 243.0 (TID 536) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:53:58 INFO TaskSetManager: Starting task 1.0 in stage 243.0 (TID 537) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:53:58 INFO TaskSetManager: Starting task 2.0 in stage 243.0 (TID 538) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:53:58 INFO BlockManagerInfo: Added broadcast_240_piece0 in memory on 172.18.0.14:40939 (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 11:53:58 INFO BlockManagerInfo: Added broadcast_240_piece0 in memory on 172.18.0.15:34959 (size: 19.5 KiB, free: 434.0 MiB)
26/02/13 11:53:58 INFO BlockManagerInfo: Added broadcast_239_piece0 in memory on 172.18.0.15:34959 (size: 107.2 KiB, free: 433.9 MiB)
26/02/13 11:53:58 INFO BlockManagerInfo: Added broadcast_239_piece0 in memory on 172.18.0.14:40939 (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 11:53:58 INFO BlockManagerInfo: Removed broadcast_236_piece0 on spark-master:40291 in memory (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 11:53:58 INFO BlockManagerInfo: Removed broadcast_236_piece0 on 172.18.0.14:40939 in memory (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 11:53:58 INFO BlockManagerInfo: Removed broadcast_236_piece0 on 172.18.0.15:34959 in memory (size: 107.2 KiB, free: 434.0 MiB)
26/02/13 11:53:59 INFO TaskSetManager: Finished task 1.0 in stage 243.0 (TID 537) in 579 ms on 172.18.0.15 (executor 0) (1/3)
26/02/13 11:53:59 INFO TaskSetManager: Finished task 0.0 in stage 243.0 (TID 536) in 587 ms on 172.18.0.14 (executor 1) (2/3)
26/02/13 11:53:59 INFO TaskSetManager: Finished task 2.0 in stage 243.0 (TID 538) in 586 ms on 172.18.0.14 (executor 1) (3/3)
26/02/13 11:53:59 INFO TaskSchedulerImpl: Removed TaskSet 243.0, whose tasks have all completed, from pool 
26/02/13 11:53:59 INFO DAGScheduler: ResultStage 243 (start at NativeMethodAccessorImpl.java:0) finished in 0.594 s
26/02/13 11:53:59 INFO DAGScheduler: Job 160 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:53:59 INFO TaskSchedulerImpl: Killing all running tasks in stage 243: Stage finished
26/02/13 11:53:59 INFO DAGScheduler: Job 160 finished: start at NativeMethodAccessorImpl.java:0, took 0.595152 s
26/02/13 11:53:59 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 75, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@1988b7fd] is committing.
26/02/13 11:53:59 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 75, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@1988b7fd] committed.
26/02/13 11:53:59 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/75 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.75.ed0b9c95-37d1-4969-bbb0-ff435029835a.tmp
26/02/13 11:53:59 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.75.ed0b9c95-37d1-4969-bbb0-ff435029835a.tmp to file:/tmp/spark-checkpoint-enrichment/commits/75
26/02/13 11:53:59 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "fc5c6452-bd70-4c90-becf-772d3bebdb57",
  "name" : null,
  "timestamp" : "2026-02-13T11:53:58.602Z",
  "batchId" : 75,
  "numInputRows" : 107,
  "inputRowsPerSecond" : 9727.272727272728,
  "processedRowsPerSecond" : 118.10154525386314,
  "durationMs" : {
    "addBatch" : 737,
    "commitOffsets" : 66,
    "getBatch" : 0,
    "latestOffset" : 2,
    "queryPlanning" : 28,
    "triggerExecution" : 906,
    "walCommit" : 72
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 5697,
        "1" : 6251,
        "0" : 7813
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 5729,
        "1" : 6294,
        "0" : 7845
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 5729,
        "1" : 6294,
        "0" : 7845
      }
    },
    "numInputRows" : 107,
    "inputRowsPerSecond" : 9727.272727272728,
    "processedRowsPerSecond" : 118.10154525386314,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@7fe9b5de",
    "numOutputRows" : 25
  }
}
26/02/13 11:53:59 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/76 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.76.67e6cc0f-b9eb-40f2-ac21-960af4745965.tmp
26/02/13 11:53:59 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.76.67e6cc0f-b9eb-40f2-ac21-960af4745965.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/76
26/02/13 11:53:59 INFO MicroBatchExecution: Committed offsets for batch 76. Metadata OffsetSeqMetadata(0,1770983639510,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 11:53:59 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:53:59 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:53:59 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:53:59 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:53:59 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:53:59 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:53:59 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#66855 - origin_code.nullCount#66854) > 0)
26/02/13 11:53:59 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#66860 - destination_code.nullCount#66859) > 0)
26/02/13 11:53:59 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#66890 - callsign.nullCount#66889) > 0)
26/02/13 11:53:59 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:53:59 INFO DAGScheduler: Got job 161 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 11:53:59 INFO DAGScheduler: Final stage: ResultStage 245 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:53:59 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 244)
26/02/13 11:53:59 INFO DAGScheduler: Missing parents: List()
26/02/13 11:53:59 INFO DAGScheduler: Submitting ResultStage 245 (MapPartitionsRDD[879] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:53:59 INFO MemoryStore: Block broadcast_241 stored as values in memory (estimated size 99.3 KiB, free 433.9 MiB)
26/02/13 11:53:59 INFO MemoryStore: Block broadcast_241_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 433.9 MiB)
26/02/13 11:53:59 INFO BlockManagerInfo: Added broadcast_241_piece0 in memory on spark-master:40291 (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 11:53:59 INFO SparkContext: Created broadcast 241 from broadcast at DAGScheduler.scala:1585
26/02/13 11:53:59 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 245 (MapPartitionsRDD[879] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 11:53:59 INFO TaskSchedulerImpl: Adding task set 245.0 with 4 tasks resource profile 0
26/02/13 11:53:59 INFO TaskSetManager: Starting task 0.0 in stage 245.0 (TID 539) (172.18.0.15, executor 0, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:53:59 INFO TaskSetManager: Starting task 1.0 in stage 245.0 (TID 540) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:53:59 INFO BlockManagerInfo: Added broadcast_241_piece0 in memory on 172.18.0.15:34959 (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 11:53:59 INFO TaskSetManager: Starting task 2.0 in stage 245.0 (TID 541) (172.18.0.15, executor 0, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:53:59 INFO TaskSetManager: Finished task 1.0 in stage 245.0 (TID 540) in 16 ms on 172.18.0.15 (executor 0) (1/4)
26/02/13 11:53:59 INFO TaskSetManager: Starting task 3.0 in stage 245.0 (TID 542) (172.18.0.15, executor 0, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:53:59 INFO TaskSetManager: Finished task 0.0 in stage 245.0 (TID 539) in 18 ms on 172.18.0.15 (executor 0) (2/4)
26/02/13 11:53:59 INFO TaskSetManager: Finished task 2.0 in stage 245.0 (TID 541) in 13 ms on 172.18.0.15 (executor 0) (3/4)
26/02/13 11:53:59 INFO TaskSetManager: Finished task 3.0 in stage 245.0 (TID 542) in 15 ms on 172.18.0.15 (executor 0) (4/4)
26/02/13 11:53:59 INFO TaskSchedulerImpl: Removed TaskSet 245.0, whose tasks have all completed, from pool 
26/02/13 11:53:59 INFO DAGScheduler: ResultStage 245 (start at NativeMethodAccessorImpl.java:0) finished in 0.039 s
26/02/13 11:53:59 INFO DAGScheduler: Job 161 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:53:59 INFO TaskSchedulerImpl: Killing all running tasks in stage 245: Stage finished
26/02/13 11:53:59 INFO DAGScheduler: Job 161 finished: start at NativeMethodAccessorImpl.java:0, took 0.041015 s
26/02/13 11:53:59 INFO MemoryStore: Block broadcast_242_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.8 MiB)
26/02/13 11:53:59 INFO BlockManagerInfo: Added broadcast_242_piece0 in memory on spark-master:40291 (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 11:53:59 INFO SparkContext: Created broadcast 242 from start at NativeMethodAccessorImpl.java:0
26/02/13 11:53:59 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 76, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@6008356e]. The input RDD has 3 partitions.
26/02/13 11:53:59 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:53:59 INFO DAGScheduler: Got job 162 (start at NativeMethodAccessorImpl.java:0) with 3 output partitions
26/02/13 11:53:59 INFO DAGScheduler: Final stage: ResultStage 246 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:53:59 INFO DAGScheduler: Parents of final stage: List()
26/02/13 11:53:59 INFO DAGScheduler: Missing parents: List()
26/02/13 11:53:59 INFO DAGScheduler: Submitting ResultStage 246 (MapPartitionsRDD[885] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:53:59 INFO MemoryStore: Block broadcast_243 stored as values in memory (estimated size 49.9 KiB, free 433.7 MiB)
26/02/13 11:53:59 INFO MemoryStore: Block broadcast_243_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.7 MiB)
26/02/13 11:53:59 INFO BlockManagerInfo: Added broadcast_243_piece0 in memory on spark-master:40291 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 11:53:59 INFO SparkContext: Created broadcast 243 from broadcast at DAGScheduler.scala:1585
26/02/13 11:53:59 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 246 (MapPartitionsRDD[885] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
26/02/13 11:53:59 INFO TaskSchedulerImpl: Adding task set 246.0 with 3 tasks resource profile 0
26/02/13 11:53:59 INFO TaskSetManager: Starting task 1.0 in stage 246.0 (TID 543) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:53:59 INFO TaskSetManager: Starting task 0.0 in stage 246.0 (TID 544) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:53:59 INFO TaskSetManager: Starting task 2.0 in stage 246.0 (TID 545) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:53:59 INFO BlockManagerInfo: Added broadcast_243_piece0 in memory on 172.18.0.14:40939 (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 11:53:59 INFO BlockManagerInfo: Added broadcast_243_piece0 in memory on 172.18.0.15:34959 (size: 19.5 KiB, free: 434.0 MiB)
26/02/13 11:53:59 INFO BlockManagerInfo: Added broadcast_242_piece0 in memory on 172.18.0.15:34959 (size: 107.2 KiB, free: 433.9 MiB)
26/02/13 11:53:59 INFO BlockManagerInfo: Added broadcast_242_piece0 in memory on 172.18.0.14:40939 (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 11:53:59 INFO TaskSetManager: Finished task 1.0 in stage 246.0 (TID 543) in 55 ms on 172.18.0.15 (executor 0) (1/3)
26/02/13 11:53:59 INFO TaskSetManager: Finished task 2.0 in stage 246.0 (TID 545) in 82 ms on 172.18.0.14 (executor 1) (2/3)
26/02/13 11:53:59 INFO TaskSetManager: Finished task 0.0 in stage 246.0 (TID 544) in 83 ms on 172.18.0.14 (executor 1) (3/3)
26/02/13 11:53:59 INFO TaskSchedulerImpl: Removed TaskSet 246.0, whose tasks have all completed, from pool 
26/02/13 11:53:59 INFO DAGScheduler: ResultStage 246 (start at NativeMethodAccessorImpl.java:0) finished in 0.089 s
26/02/13 11:53:59 INFO DAGScheduler: Job 162 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:53:59 INFO TaskSchedulerImpl: Killing all running tasks in stage 246: Stage finished
26/02/13 11:53:59 INFO DAGScheduler: Job 162 finished: start at NativeMethodAccessorImpl.java:0, took 0.090933 s
26/02/13 11:53:59 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 76, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@6008356e] is committing.
26/02/13 11:53:59 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 76, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@6008356e] committed.
26/02/13 11:53:59 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/76 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.76.e446a01a-5b6d-4737-ad61-f06c274a7d77.tmp
26/02/13 11:53:59 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.76.e446a01a-5b6d-4737-ad61-f06c274a7d77.tmp to file:/tmp/spark-checkpoint-enrichment/commits/76
26/02/13 11:53:59 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "fc5c6452-bd70-4c90-becf-772d3bebdb57",
  "name" : null,
  "timestamp" : "2026-02-13T11:53:59.509Z",
  "batchId" : 76,
  "numInputRows" : 144,
  "inputRowsPerSecond" : 158.7651598676957,
  "processedRowsPerSecond" : 400.0,
  "durationMs" : {
    "addBatch" : 219,
    "commitOffsets" : 57,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 26,
    "triggerExecution" : 360,
    "walCommit" : 55
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 5729,
        "1" : 6294,
        "0" : 7845
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 5777,
        "1" : 6345,
        "0" : 7890
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 5777,
        "1" : 6345,
        "0" : 7890
      }
    },
    "numInputRows" : 144,
    "inputRowsPerSecond" : 158.7651598676957,
    "processedRowsPerSecond" : 400.0,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@7fe9b5de",
    "numOutputRows" : 32
  }
}
26/02/13 11:54:03 INFO BlockManagerInfo: Removed broadcast_241_piece0 on spark-master:40291 in memory (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 11:54:03 INFO BlockManagerInfo: Removed broadcast_241_piece0 on 172.18.0.15:34959 in memory (size: 33.3 KiB, free: 433.9 MiB)
26/02/13 11:54:03 INFO BlockManagerInfo: Removed broadcast_240_piece0 on spark-master:40291 in memory (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 11:54:03 INFO BlockManagerInfo: Removed broadcast_240_piece0 on 172.18.0.14:40939 in memory (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 11:54:03 INFO BlockManagerInfo: Removed broadcast_240_piece0 on 172.18.0.15:34959 in memory (size: 19.5 KiB, free: 433.9 MiB)
26/02/13 11:54:03 INFO BlockManagerInfo: Removed broadcast_239_piece0 on spark-master:40291 in memory (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 11:54:03 INFO BlockManagerInfo: Removed broadcast_239_piece0 on 172.18.0.15:34959 in memory (size: 107.2 KiB, free: 434.0 MiB)
26/02/13 11:54:03 INFO BlockManagerInfo: Removed broadcast_239_piece0 on 172.18.0.14:40939 in memory (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 11:54:03 INFO BlockManagerInfo: Removed broadcast_243_piece0 on spark-master:40291 in memory (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 11:54:03 INFO BlockManagerInfo: Removed broadcast_243_piece0 on 172.18.0.14:40939 in memory (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 11:54:03 INFO BlockManagerInfo: Removed broadcast_243_piece0 on 172.18.0.15:34959 in memory (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 11:54:09 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 11:54:15 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/77 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.77.f1c37473-f15e-4de3-8c63-c2bf22541da6.tmp
26/02/13 11:54:15 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.77.f1c37473-f15e-4de3-8c63-c2bf22541da6.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/77
26/02/13 11:54:15 INFO MicroBatchExecution: Committed offsets for batch 77. Metadata OffsetSeqMetadata(0,1770983655482,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 11:54:15 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:54:15 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:54:15 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:54:15 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:54:15 INFO BlockManagerInfo: Removed broadcast_242_piece0 on spark-master:40291 in memory (size: 107.2 KiB, free: 434.4 MiB)
26/02/13 11:54:15 INFO BlockManagerInfo: Removed broadcast_242_piece0 on 172.18.0.15:34959 in memory (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 11:54:15 INFO BlockManagerInfo: Removed broadcast_242_piece0 on 172.18.0.14:40939 in memory (size: 107.2 KiB, free: 434.4 MiB)
26/02/13 11:54:15 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:54:15 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:54:15 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#67709 - origin_code.nullCount#67708) > 0)
26/02/13 11:54:15 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#67714 - destination_code.nullCount#67713) > 0)
26/02/13 11:54:15 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#67744 - callsign.nullCount#67743) > 0)
26/02/13 11:54:15 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:54:15 INFO DAGScheduler: Got job 163 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 11:54:15 INFO DAGScheduler: Final stage: ResultStage 248 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:54:15 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 247)
26/02/13 11:54:15 INFO DAGScheduler: Missing parents: List()
26/02/13 11:54:15 INFO DAGScheduler: Submitting ResultStage 248 (MapPartitionsRDD[890] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:54:15 INFO MemoryStore: Block broadcast_244 stored as values in memory (estimated size 99.3 KiB, free 434.1 MiB)
26/02/13 11:54:15 INFO MemoryStore: Block broadcast_244_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 434.0 MiB)
26/02/13 11:54:15 INFO BlockManagerInfo: Added broadcast_244_piece0 in memory on spark-master:40291 (size: 33.3 KiB, free: 434.3 MiB)
26/02/13 11:54:15 INFO SparkContext: Created broadcast 244 from broadcast at DAGScheduler.scala:1585
26/02/13 11:54:15 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 248 (MapPartitionsRDD[890] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 11:54:15 INFO TaskSchedulerImpl: Adding task set 248.0 with 4 tasks resource profile 0
26/02/13 11:54:15 INFO TaskSetManager: Starting task 0.0 in stage 248.0 (TID 546) (172.18.0.15, executor 0, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:54:15 INFO TaskSetManager: Starting task 1.0 in stage 248.0 (TID 547) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:54:15 INFO BlockManagerInfo: Added broadcast_244_piece0 in memory on 172.18.0.15:34959 (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 11:54:15 INFO TaskSetManager: Starting task 2.0 in stage 248.0 (TID 548) (172.18.0.15, executor 0, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:54:15 INFO TaskSetManager: Finished task 0.0 in stage 248.0 (TID 546) in 23 ms on 172.18.0.15 (executor 0) (1/4)
26/02/13 11:54:15 INFO TaskSetManager: Starting task 3.0 in stage 248.0 (TID 549) (172.18.0.15, executor 0, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:54:15 INFO TaskSetManager: Finished task 1.0 in stage 248.0 (TID 547) in 25 ms on 172.18.0.15 (executor 0) (2/4)
26/02/13 11:54:15 INFO TaskSetManager: Finished task 2.0 in stage 248.0 (TID 548) in 17 ms on 172.18.0.15 (executor 0) (3/4)
26/02/13 11:54:15 INFO TaskSetManager: Finished task 3.0 in stage 248.0 (TID 549) in 16 ms on 172.18.0.15 (executor 0) (4/4)
26/02/13 11:54:15 INFO DAGScheduler: ResultStage 248 (start at NativeMethodAccessorImpl.java:0) finished in 0.047 s
26/02/13 11:54:15 INFO TaskSchedulerImpl: Removed TaskSet 248.0, whose tasks have all completed, from pool 
26/02/13 11:54:15 INFO DAGScheduler: Job 163 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:54:15 INFO TaskSchedulerImpl: Killing all running tasks in stage 248: Stage finished
26/02/13 11:54:15 INFO DAGScheduler: Job 163 finished: start at NativeMethodAccessorImpl.java:0, took 0.049781 s
26/02/13 11:54:15 INFO MemoryStore: Block broadcast_245_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.9 MiB)
26/02/13 11:54:15 INFO BlockManagerInfo: Added broadcast_245_piece0 in memory on spark-master:40291 (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 11:54:15 INFO SparkContext: Created broadcast 245 from start at NativeMethodAccessorImpl.java:0
26/02/13 11:54:15 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 77, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@7e0c030f]. The input RDD has 2 partitions.
26/02/13 11:54:15 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:54:15 INFO DAGScheduler: Got job 164 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/02/13 11:54:15 INFO DAGScheduler: Final stage: ResultStage 249 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:54:15 INFO DAGScheduler: Parents of final stage: List()
26/02/13 11:54:15 INFO DAGScheduler: Missing parents: List()
26/02/13 11:54:15 INFO DAGScheduler: Submitting ResultStage 249 (MapPartitionsRDD[896] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:54:15 INFO MemoryStore: Block broadcast_246 stored as values in memory (estimated size 49.9 KiB, free 433.9 MiB)
26/02/13 11:54:15 INFO MemoryStore: Block broadcast_246_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.9 MiB)
26/02/13 11:54:15 INFO BlockManagerInfo: Added broadcast_246_piece0 in memory on spark-master:40291 (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 11:54:15 INFO SparkContext: Created broadcast 246 from broadcast at DAGScheduler.scala:1585
26/02/13 11:54:15 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 249 (MapPartitionsRDD[896] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/02/13 11:54:15 INFO TaskSchedulerImpl: Adding task set 249.0 with 2 tasks resource profile 0
26/02/13 11:54:15 INFO TaskSetManager: Starting task 0.0 in stage 249.0 (TID 550) (172.18.0.15, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:54:15 INFO TaskSetManager: Starting task 1.0 in stage 249.0 (TID 551) (172.18.0.14, executor 1, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:54:15 INFO BlockManagerInfo: Added broadcast_246_piece0 in memory on 172.18.0.15:34959 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 11:54:15 INFO BlockManagerInfo: Added broadcast_246_piece0 in memory on 172.18.0.14:40939 (size: 19.5 KiB, free: 434.4 MiB)
26/02/13 11:54:15 INFO BlockManagerInfo: Added broadcast_245_piece0 in memory on 172.18.0.15:34959 (size: 107.2 KiB, free: 434.0 MiB)
26/02/13 11:54:15 INFO BlockManagerInfo: Added broadcast_245_piece0 in memory on 172.18.0.14:40939 (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 11:54:16 INFO TaskSetManager: Finished task 0.0 in stage 249.0 (TID 550) in 547 ms on 172.18.0.15 (executor 0) (1/2)
26/02/13 11:54:16 INFO TaskSetManager: Finished task 1.0 in stage 249.0 (TID 551) in 550 ms on 172.18.0.14 (executor 1) (2/2)
26/02/13 11:54:16 INFO TaskSchedulerImpl: Removed TaskSet 249.0, whose tasks have all completed, from pool 
26/02/13 11:54:16 INFO DAGScheduler: ResultStage 249 (start at NativeMethodAccessorImpl.java:0) finished in 0.555 s
26/02/13 11:54:16 INFO DAGScheduler: Job 164 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:54:16 INFO TaskSchedulerImpl: Killing all running tasks in stage 249: Stage finished
26/02/13 11:54:16 INFO DAGScheduler: Job 164 finished: start at NativeMethodAccessorImpl.java:0, took 0.556590 s
26/02/13 11:54:16 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 77, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@7e0c030f] is committing.
26/02/13 11:54:16 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 77, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@7e0c030f] committed.
26/02/13 11:54:16 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/77 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.77.8b9a9919-8897-4dbc-b61e-f59de3c192c0.tmp
26/02/13 11:54:16 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.77.8b9a9919-8897-4dbc-b61e-f59de3c192c0.tmp to file:/tmp/spark-checkpoint-enrichment/commits/77
26/02/13 11:54:16 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "fc5c6452-bd70-4c90-becf-772d3bebdb57",
  "name" : null,
  "timestamp" : "2026-02-13T11:54:15.481Z",
  "batchId" : 77,
  "numInputRows" : 5,
  "inputRowsPerSecond" : 416.6666666666667,
  "processedRowsPerSecond" : 5.63063063063063,
  "durationMs" : {
    "addBatch" : 738,
    "commitOffsets" : 63,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 24,
    "triggerExecution" : 888,
    "walCommit" : 62
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 5777,
        "1" : 6345,
        "0" : 7890
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 5777,
        "1" : 6348,
        "0" : 7892
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 5777,
        "1" : 6348,
        "0" : 7892
      }
    },
    "numInputRows" : 5,
    "inputRowsPerSecond" : 416.6666666666667,
    "processedRowsPerSecond" : 5.63063063063063,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@7fe9b5de",
    "numOutputRows" : 2
  }
}
26/02/13 11:54:16 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/78 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.78.6e3b63ab-0381-41a4-aa2c-8fa31376d6f7.tmp
26/02/13 11:54:16 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.78.6e3b63ab-0381-41a4-aa2c-8fa31376d6f7.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/78
26/02/13 11:54:16 INFO MicroBatchExecution: Committed offsets for batch 78. Metadata OffsetSeqMetadata(0,1770983656371,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 11:54:16 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:54:16 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:54:16 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:54:16 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:54:16 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:54:16 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:54:16 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#68563 - origin_code.nullCount#68562) > 0)
26/02/13 11:54:16 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#68568 - destination_code.nullCount#68567) > 0)
26/02/13 11:54:16 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#68598 - callsign.nullCount#68597) > 0)
26/02/13 11:54:16 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:54:16 INFO DAGScheduler: Got job 165 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 11:54:16 INFO DAGScheduler: Final stage: ResultStage 251 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:54:16 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 250)
26/02/13 11:54:16 INFO DAGScheduler: Missing parents: List()
26/02/13 11:54:16 INFO DAGScheduler: Submitting ResultStage 251 (MapPartitionsRDD[901] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:54:16 INFO MemoryStore: Block broadcast_247 stored as values in memory (estimated size 99.3 KiB, free 433.8 MiB)
26/02/13 11:54:16 INFO MemoryStore: Block broadcast_247_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 433.7 MiB)
26/02/13 11:54:16 INFO BlockManagerInfo: Added broadcast_247_piece0 in memory on spark-master:40291 (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 11:54:16 INFO SparkContext: Created broadcast 247 from broadcast at DAGScheduler.scala:1585
26/02/13 11:54:16 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 251 (MapPartitionsRDD[901] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 11:54:16 INFO TaskSchedulerImpl: Adding task set 251.0 with 4 tasks resource profile 0
26/02/13 11:54:16 INFO TaskSetManager: Starting task 0.0 in stage 251.0 (TID 552) (172.18.0.15, executor 0, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:54:16 INFO TaskSetManager: Starting task 1.0 in stage 251.0 (TID 553) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:54:16 INFO BlockManagerInfo: Added broadcast_247_piece0 in memory on 172.18.0.15:34959 (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 11:54:16 INFO TaskSetManager: Starting task 2.0 in stage 251.0 (TID 554) (172.18.0.15, executor 0, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:54:16 INFO TaskSetManager: Finished task 0.0 in stage 251.0 (TID 552) in 21 ms on 172.18.0.15 (executor 0) (1/4)
26/02/13 11:54:16 INFO TaskSetManager: Starting task 3.0 in stage 251.0 (TID 555) (172.18.0.15, executor 0, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:54:16 INFO TaskSetManager: Finished task 1.0 in stage 251.0 (TID 553) in 21 ms on 172.18.0.15 (executor 0) (2/4)
26/02/13 11:54:16 INFO TaskSetManager: Finished task 3.0 in stage 251.0 (TID 555) in 12 ms on 172.18.0.15 (executor 0) (3/4)
26/02/13 11:54:16 INFO TaskSetManager: Finished task 2.0 in stage 251.0 (TID 554) in 13 ms on 172.18.0.15 (executor 0) (4/4)
26/02/13 11:54:16 INFO TaskSchedulerImpl: Removed TaskSet 251.0, whose tasks have all completed, from pool 
26/02/13 11:54:16 INFO DAGScheduler: ResultStage 251 (start at NativeMethodAccessorImpl.java:0) finished in 0.039 s
26/02/13 11:54:16 INFO DAGScheduler: Job 165 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:54:16 INFO TaskSchedulerImpl: Killing all running tasks in stage 251: Stage finished
26/02/13 11:54:16 INFO DAGScheduler: Job 165 finished: start at NativeMethodAccessorImpl.java:0, took 0.040889 s
26/02/13 11:54:16 INFO MemoryStore: Block broadcast_248_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.6 MiB)
26/02/13 11:54:16 INFO BlockManagerInfo: Added broadcast_248_piece0 in memory on spark-master:40291 (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 11:54:16 INFO SparkContext: Created broadcast 248 from start at NativeMethodAccessorImpl.java:0
26/02/13 11:54:16 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 78, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@4c60a56b]. The input RDD has 3 partitions.
26/02/13 11:54:16 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:54:16 INFO DAGScheduler: Got job 166 (start at NativeMethodAccessorImpl.java:0) with 3 output partitions
26/02/13 11:54:16 INFO DAGScheduler: Final stage: ResultStage 252 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:54:16 INFO DAGScheduler: Parents of final stage: List()
26/02/13 11:54:16 INFO DAGScheduler: Missing parents: List()
26/02/13 11:54:16 INFO DAGScheduler: Submitting ResultStage 252 (MapPartitionsRDD[907] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:54:16 INFO MemoryStore: Block broadcast_249 stored as values in memory (estimated size 49.9 KiB, free 433.6 MiB)
26/02/13 11:54:16 INFO MemoryStore: Block broadcast_249_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.6 MiB)
26/02/13 11:54:16 INFO BlockManagerInfo: Added broadcast_249_piece0 in memory on spark-master:40291 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 11:54:16 INFO SparkContext: Created broadcast 249 from broadcast at DAGScheduler.scala:1585
26/02/13 11:54:16 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 252 (MapPartitionsRDD[907] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
26/02/13 11:54:16 INFO TaskSchedulerImpl: Adding task set 252.0 with 3 tasks resource profile 0
26/02/13 11:54:16 INFO BlockManagerInfo: Removed broadcast_244_piece0 on spark-master:40291 in memory (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 11:54:16 INFO TaskSetManager: Starting task 1.0 in stage 252.0 (TID 556) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:54:16 INFO TaskSetManager: Starting task 0.0 in stage 252.0 (TID 557) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:54:16 INFO TaskSetManager: Starting task 2.0 in stage 252.0 (TID 558) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:54:16 INFO BlockManagerInfo: Removed broadcast_244_piece0 on 172.18.0.15:34959 in memory (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 11:54:16 INFO BlockManagerInfo: Removed broadcast_245_piece0 on 172.18.0.14:40939 in memory (size: 107.2 KiB, free: 434.4 MiB)
26/02/13 11:54:16 INFO BlockManagerInfo: Removed broadcast_245_piece0 on spark-master:40291 in memory (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 11:54:16 INFO BlockManagerInfo: Removed broadcast_245_piece0 on 172.18.0.15:34959 in memory (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 11:54:16 INFO BlockManagerInfo: Added broadcast_249_piece0 in memory on 172.18.0.14:40939 (size: 19.5 KiB, free: 434.4 MiB)
26/02/13 11:54:16 INFO BlockManagerInfo: Added broadcast_249_piece0 in memory on 172.18.0.15:34959 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 11:54:16 INFO BlockManagerInfo: Removed broadcast_246_piece0 on spark-master:40291 in memory (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 11:54:16 INFO BlockManagerInfo: Removed broadcast_246_piece0 on 172.18.0.14:40939 in memory (size: 19.5 KiB, free: 434.4 MiB)
26/02/13 11:54:16 INFO BlockManagerInfo: Removed broadcast_246_piece0 on 172.18.0.15:34959 in memory (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 11:54:16 INFO BlockManagerInfo: Added broadcast_248_piece0 in memory on 172.18.0.14:40939 (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 11:54:16 INFO BlockManagerInfo: Removed broadcast_247_piece0 on spark-master:40291 in memory (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 11:54:16 INFO BlockManagerInfo: Removed broadcast_247_piece0 on 172.18.0.15:34959 in memory (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 11:54:16 INFO BlockManagerInfo: Added broadcast_248_piece0 in memory on 172.18.0.15:34959 (size: 107.2 KiB, free: 434.0 MiB)
26/02/13 11:54:16 INFO TaskSetManager: Finished task 2.0 in stage 252.0 (TID 558) in 75 ms on 172.18.0.14 (executor 1) (1/3)
26/02/13 11:54:16 INFO TaskSetManager: Finished task 1.0 in stage 252.0 (TID 556) in 93 ms on 172.18.0.15 (executor 0) (2/3)
26/02/13 11:54:17 INFO TaskSetManager: Finished task 0.0 in stage 252.0 (TID 557) in 577 ms on 172.18.0.14 (executor 1) (3/3)
26/02/13 11:54:17 INFO TaskSchedulerImpl: Removed TaskSet 252.0, whose tasks have all completed, from pool 
26/02/13 11:54:17 INFO DAGScheduler: ResultStage 252 (start at NativeMethodAccessorImpl.java:0) finished in 0.590 s
26/02/13 11:54:17 INFO DAGScheduler: Job 166 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:54:17 INFO TaskSchedulerImpl: Killing all running tasks in stage 252: Stage finished
26/02/13 11:54:17 INFO DAGScheduler: Job 166 finished: start at NativeMethodAccessorImpl.java:0, took 0.592121 s
26/02/13 11:54:17 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 78, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@4c60a56b] is committing.
26/02/13 11:54:17 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 78, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@4c60a56b] committed.
26/02/13 11:54:17 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/78 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.78.250571b9-bfe3-45c8-aa0e-abf12de1976e.tmp
26/02/13 11:54:17 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.78.250571b9-bfe3-45c8-aa0e-abf12de1976e.tmp to file:/tmp/spark-checkpoint-enrichment/commits/78
26/02/13 11:54:17 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "fc5c6452-bd70-4c90-becf-772d3bebdb57",
  "name" : null,
  "timestamp" : "2026-02-13T11:54:16.369Z",
  "batchId" : 78,
  "numInputRows" : 246,
  "inputRowsPerSecond" : 277.02702702702703,
  "processedRowsPerSecond" : 278.5956964892412,
  "durationMs" : {
    "addBatch" : 721,
    "commitOffsets" : 67,
    "getBatch" : 0,
    "latestOffset" : 2,
    "queryPlanning" : 32,
    "triggerExecution" : 883,
    "walCommit" : 61
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 5777,
        "1" : 6348,
        "0" : 7892
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 5860,
        "1" : 6437,
        "0" : 7966
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 5860,
        "1" : 6437,
        "0" : 7966
      }
    },
    "numInputRows" : 246,
    "inputRowsPerSecond" : 277.02702702702703,
    "processedRowsPerSecond" : 278.5956964892412,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@7fe9b5de",
    "numOutputRows" : 54
  }
}
26/02/13 11:54:18 INFO BlockManagerInfo: Removed broadcast_249_piece0 on spark-master:40291 in memory (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 11:54:18 INFO BlockManagerInfo: Removed broadcast_249_piece0 on 172.18.0.15:34959 in memory (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 11:54:18 INFO BlockManagerInfo: Removed broadcast_249_piece0 on 172.18.0.14:40939 in memory (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 11:54:27 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 11:54:34 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/79 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.79.9b608925-879a-47da-b481-49491da75ac0.tmp
26/02/13 11:54:34 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.79.9b608925-879a-47da-b481-49491da75ac0.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/79
26/02/13 11:54:34 INFO MicroBatchExecution: Committed offsets for batch 79. Metadata OffsetSeqMetadata(0,1770983674235,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 11:54:34 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:54:34 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:54:34 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:54:34 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:54:34 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:54:34 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:54:34 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#69417 - origin_code.nullCount#69416) > 0)
26/02/13 11:54:34 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#69422 - destination_code.nullCount#69421) > 0)
26/02/13 11:54:34 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#69452 - callsign.nullCount#69451) > 0)
26/02/13 11:54:34 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:54:34 INFO DAGScheduler: Got job 167 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 11:54:34 INFO DAGScheduler: Final stage: ResultStage 254 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:54:34 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 253)
26/02/13 11:54:34 INFO DAGScheduler: Missing parents: List()
26/02/13 11:54:34 INFO DAGScheduler: Submitting ResultStage 254 (MapPartitionsRDD[912] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:54:34 INFO MemoryStore: Block broadcast_250 stored as values in memory (estimated size 99.3 KiB, free 434.0 MiB)
26/02/13 11:54:34 INFO MemoryStore: Block broadcast_250_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 433.9 MiB)
26/02/13 11:54:34 INFO BlockManagerInfo: Added broadcast_250_piece0 in memory on spark-master:40291 (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 11:54:34 INFO SparkContext: Created broadcast 250 from broadcast at DAGScheduler.scala:1585
26/02/13 11:54:34 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 254 (MapPartitionsRDD[912] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 11:54:34 INFO TaskSchedulerImpl: Adding task set 254.0 with 4 tasks resource profile 0
26/02/13 11:54:34 INFO TaskSetManager: Starting task 0.0 in stage 254.0 (TID 559) (172.18.0.15, executor 0, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:54:34 INFO TaskSetManager: Starting task 1.0 in stage 254.0 (TID 560) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:54:34 INFO BlockManagerInfo: Added broadcast_250_piece0 in memory on 172.18.0.15:34959 (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 11:54:34 INFO TaskSetManager: Starting task 2.0 in stage 254.0 (TID 561) (172.18.0.15, executor 0, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:54:34 INFO TaskSetManager: Finished task 1.0 in stage 254.0 (TID 560) in 17 ms on 172.18.0.15 (executor 0) (1/4)
26/02/13 11:54:34 INFO TaskSetManager: Starting task 3.0 in stage 254.0 (TID 562) (172.18.0.15, executor 0, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:54:34 INFO TaskSetManager: Finished task 0.0 in stage 254.0 (TID 559) in 23 ms on 172.18.0.15 (executor 0) (2/4)
26/02/13 11:54:34 INFO TaskSetManager: Finished task 2.0 in stage 254.0 (TID 561) in 10 ms on 172.18.0.15 (executor 0) (3/4)
26/02/13 11:54:34 INFO TaskSetManager: Finished task 3.0 in stage 254.0 (TID 562) in 10 ms on 172.18.0.15 (executor 0) (4/4)
26/02/13 11:54:34 INFO TaskSchedulerImpl: Removed TaskSet 254.0, whose tasks have all completed, from pool 
26/02/13 11:54:34 INFO DAGScheduler: ResultStage 254 (start at NativeMethodAccessorImpl.java:0) finished in 0.040 s
26/02/13 11:54:34 INFO DAGScheduler: Job 167 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:54:34 INFO TaskSchedulerImpl: Killing all running tasks in stage 254: Stage finished
26/02/13 11:54:34 INFO DAGScheduler: Job 167 finished: start at NativeMethodAccessorImpl.java:0, took 0.041951 s
26/02/13 11:54:34 INFO MemoryStore: Block broadcast_251_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.8 MiB)
26/02/13 11:54:34 INFO BlockManagerInfo: Added broadcast_251_piece0 in memory on spark-master:40291 (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 11:54:34 INFO SparkContext: Created broadcast 251 from start at NativeMethodAccessorImpl.java:0
26/02/13 11:54:34 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 79, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@4b1f61f1]. The input RDD has 2 partitions.
26/02/13 11:54:34 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:54:34 INFO DAGScheduler: Got job 168 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/02/13 11:54:34 INFO DAGScheduler: Final stage: ResultStage 255 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:54:34 INFO DAGScheduler: Parents of final stage: List()
26/02/13 11:54:34 INFO DAGScheduler: Missing parents: List()
26/02/13 11:54:34 INFO DAGScheduler: Submitting ResultStage 255 (MapPartitionsRDD[918] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:54:34 INFO MemoryStore: Block broadcast_252 stored as values in memory (estimated size 49.9 KiB, free 433.8 MiB)
26/02/13 11:54:34 INFO MemoryStore: Block broadcast_252_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.8 MiB)
26/02/13 11:54:34 INFO BlockManagerInfo: Added broadcast_252_piece0 in memory on spark-master:40291 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 11:54:34 INFO SparkContext: Created broadcast 252 from broadcast at DAGScheduler.scala:1585
26/02/13 11:54:34 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 255 (MapPartitionsRDD[918] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/02/13 11:54:34 INFO TaskSchedulerImpl: Adding task set 255.0 with 2 tasks resource profile 0
26/02/13 11:54:34 INFO TaskSetManager: Starting task 1.0 in stage 255.0 (TID 563) (172.18.0.14, executor 1, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:54:34 INFO TaskSetManager: Starting task 0.0 in stage 255.0 (TID 564) (172.18.0.15, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:54:34 INFO BlockManagerInfo: Added broadcast_252_piece0 in memory on 172.18.0.15:34959 (size: 19.5 KiB, free: 434.0 MiB)
26/02/13 11:54:34 INFO BlockManagerInfo: Added broadcast_252_piece0 in memory on 172.18.0.14:40939 (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 11:54:34 INFO BlockManagerInfo: Added broadcast_251_piece0 in memory on 172.18.0.15:34959 (size: 107.2 KiB, free: 433.9 MiB)
26/02/13 11:54:34 INFO BlockManagerInfo: Added broadcast_251_piece0 in memory on 172.18.0.14:40939 (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 11:54:35 INFO TaskSetManager: Finished task 0.0 in stage 255.0 (TID 564) in 551 ms on 172.18.0.15 (executor 0) (1/2)
26/02/13 11:54:35 INFO TaskSetManager: Finished task 1.0 in stage 255.0 (TID 563) in 552 ms on 172.18.0.14 (executor 1) (2/2)
26/02/13 11:54:35 INFO TaskSchedulerImpl: Removed TaskSet 255.0, whose tasks have all completed, from pool 
26/02/13 11:54:35 INFO DAGScheduler: ResultStage 255 (start at NativeMethodAccessorImpl.java:0) finished in 0.558 s
26/02/13 11:54:35 INFO DAGScheduler: Job 168 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:54:35 INFO TaskSchedulerImpl: Killing all running tasks in stage 255: Stage finished
26/02/13 11:54:35 INFO DAGScheduler: Job 168 finished: start at NativeMethodAccessorImpl.java:0, took 0.560541 s
26/02/13 11:54:35 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 79, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@4b1f61f1] is committing.
26/02/13 11:54:35 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 79, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@4b1f61f1] committed.
26/02/13 11:54:35 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/79 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.79.49ec5f8e-c82b-4f29-9bd8-49341fbf9a51.tmp
26/02/13 11:54:35 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.79.49ec5f8e-c82b-4f29-9bd8-49341fbf9a51.tmp to file:/tmp/spark-checkpoint-enrichment/commits/79
26/02/13 11:54:35 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "fc5c6452-bd70-4c90-becf-772d3bebdb57",
  "name" : null,
  "timestamp" : "2026-02-13T11:54:34.234Z",
  "batchId" : 79,
  "numInputRows" : 5,
  "inputRowsPerSecond" : 416.6666666666667,
  "processedRowsPerSecond" : 5.973715651135006,
  "durationMs" : {
    "addBatch" : 689,
    "commitOffsets" : 60,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 23,
    "triggerExecution" : 837,
    "walCommit" : 64
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 5860,
        "1" : 6437,
        "0" : 7966
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 5860,
        "1" : 6440,
        "0" : 7968
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 5860,
        "1" : 6440,
        "0" : 7968
      }
    },
    "numInputRows" : 5,
    "inputRowsPerSecond" : 416.6666666666667,
    "processedRowsPerSecond" : 5.973715651135006,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@7fe9b5de",
    "numOutputRows" : 2
  }
}
26/02/13 11:54:35 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/80 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.80.8c76f8b2-12ac-48a7-8083-eb1281b91504.tmp
26/02/13 11:54:35 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.80.8c76f8b2-12ac-48a7-8083-eb1281b91504.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/80
26/02/13 11:54:35 INFO MicroBatchExecution: Committed offsets for batch 80. Metadata OffsetSeqMetadata(0,1770983675072,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 11:54:35 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:54:35 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:54:35 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:54:35 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:54:35 INFO BlockManagerInfo: Removed broadcast_252_piece0 on spark-master:40291 in memory (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 11:54:35 INFO BlockManagerInfo: Removed broadcast_252_piece0 on 172.18.0.15:34959 in memory (size: 19.5 KiB, free: 433.9 MiB)
26/02/13 11:54:35 INFO BlockManagerInfo: Removed broadcast_252_piece0 on 172.18.0.14:40939 in memory (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 11:54:35 INFO BlockManagerInfo: Removed broadcast_251_piece0 on spark-master:40291 in memory (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 11:54:35 INFO BlockManagerInfo: Removed broadcast_251_piece0 on 172.18.0.14:40939 in memory (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 11:54:35 INFO BlockManagerInfo: Removed broadcast_251_piece0 on 172.18.0.15:34959 in memory (size: 107.2 KiB, free: 434.0 MiB)
26/02/13 11:54:35 INFO BlockManagerInfo: Removed broadcast_250_piece0 on spark-master:40291 in memory (size: 33.3 KiB, free: 434.3 MiB)
26/02/13 11:54:35 INFO BlockManagerInfo: Removed broadcast_250_piece0 on 172.18.0.15:34959 in memory (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 11:54:35 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:54:35 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:54:35 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#70271 - origin_code.nullCount#70270) > 0)
26/02/13 11:54:35 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#70276 - destination_code.nullCount#70275) > 0)
26/02/13 11:54:35 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#70306 - callsign.nullCount#70305) > 0)
26/02/13 11:54:35 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:54:35 INFO DAGScheduler: Got job 169 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 11:54:35 INFO DAGScheduler: Final stage: ResultStage 257 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:54:35 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 256)
26/02/13 11:54:35 INFO DAGScheduler: Missing parents: List()
26/02/13 11:54:35 INFO DAGScheduler: Submitting ResultStage 257 (MapPartitionsRDD[923] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:54:35 INFO MemoryStore: Block broadcast_253 stored as values in memory (estimated size 99.3 KiB, free 434.0 MiB)
26/02/13 11:54:35 INFO MemoryStore: Block broadcast_253_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 433.9 MiB)
26/02/13 11:54:35 INFO BlockManagerInfo: Added broadcast_253_piece0 in memory on spark-master:40291 (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 11:54:35 INFO SparkContext: Created broadcast 253 from broadcast at DAGScheduler.scala:1585
26/02/13 11:54:35 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 257 (MapPartitionsRDD[923] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 11:54:35 INFO TaskSchedulerImpl: Adding task set 257.0 with 4 tasks resource profile 0
26/02/13 11:54:35 INFO TaskSetManager: Starting task 0.0 in stage 257.0 (TID 565) (172.18.0.15, executor 0, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:54:35 INFO TaskSetManager: Starting task 1.0 in stage 257.0 (TID 566) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:54:35 INFO BlockManagerInfo: Added broadcast_253_piece0 in memory on 172.18.0.15:34959 (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 11:54:35 INFO TaskSetManager: Starting task 2.0 in stage 257.0 (TID 567) (172.18.0.15, executor 0, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:54:35 INFO TaskSetManager: Finished task 1.0 in stage 257.0 (TID 566) in 16 ms on 172.18.0.15 (executor 0) (1/4)
26/02/13 11:54:35 INFO TaskSetManager: Starting task 3.0 in stage 257.0 (TID 568) (172.18.0.15, executor 0, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:54:35 INFO TaskSetManager: Finished task 0.0 in stage 257.0 (TID 565) in 18 ms on 172.18.0.15 (executor 0) (2/4)
26/02/13 11:54:35 INFO TaskSetManager: Finished task 2.0 in stage 257.0 (TID 567) in 12 ms on 172.18.0.15 (executor 0) (3/4)
26/02/13 11:54:35 INFO TaskSetManager: Finished task 3.0 in stage 257.0 (TID 568) in 15 ms on 172.18.0.15 (executor 0) (4/4)
26/02/13 11:54:35 INFO TaskSchedulerImpl: Removed TaskSet 257.0, whose tasks have all completed, from pool 
26/02/13 11:54:35 INFO DAGScheduler: ResultStage 257 (start at NativeMethodAccessorImpl.java:0) finished in 0.042 s
26/02/13 11:54:35 INFO DAGScheduler: Job 169 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:54:35 INFO TaskSchedulerImpl: Killing all running tasks in stage 257: Stage finished
26/02/13 11:54:35 INFO DAGScheduler: Job 169 finished: start at NativeMethodAccessorImpl.java:0, took 0.043447 s
26/02/13 11:54:35 INFO MemoryStore: Block broadcast_254_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.8 MiB)
26/02/13 11:54:35 INFO BlockManagerInfo: Added broadcast_254_piece0 in memory on spark-master:40291 (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 11:54:35 INFO SparkContext: Created broadcast 254 from start at NativeMethodAccessorImpl.java:0
26/02/13 11:54:35 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 80, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@12fd5c32]. The input RDD has 3 partitions.
26/02/13 11:54:35 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:54:35 INFO DAGScheduler: Got job 170 (start at NativeMethodAccessorImpl.java:0) with 3 output partitions
26/02/13 11:54:35 INFO DAGScheduler: Final stage: ResultStage 258 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:54:35 INFO DAGScheduler: Parents of final stage: List()
26/02/13 11:54:35 INFO DAGScheduler: Missing parents: List()
26/02/13 11:54:35 INFO DAGScheduler: Submitting ResultStage 258 (MapPartitionsRDD[929] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:54:35 INFO MemoryStore: Block broadcast_255 stored as values in memory (estimated size 49.9 KiB, free 433.8 MiB)
26/02/13 11:54:35 INFO MemoryStore: Block broadcast_255_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.8 MiB)
26/02/13 11:54:35 INFO BlockManagerInfo: Added broadcast_255_piece0 in memory on spark-master:40291 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 11:54:35 INFO SparkContext: Created broadcast 255 from broadcast at DAGScheduler.scala:1585
26/02/13 11:54:35 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 258 (MapPartitionsRDD[929] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
26/02/13 11:54:35 INFO TaskSchedulerImpl: Adding task set 258.0 with 3 tasks resource profile 0
26/02/13 11:54:35 INFO TaskSetManager: Starting task 0.0 in stage 258.0 (TID 569) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:54:35 INFO TaskSetManager: Starting task 1.0 in stage 258.0 (TID 570) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:54:35 INFO TaskSetManager: Starting task 2.0 in stage 258.0 (TID 571) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:54:35 INFO BlockManagerInfo: Removed broadcast_248_piece0 on spark-master:40291 in memory (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 11:54:35 INFO BlockManagerInfo: Removed broadcast_248_piece0 on 172.18.0.14:40939 in memory (size: 107.2 KiB, free: 434.4 MiB)
26/02/13 11:54:35 INFO BlockManagerInfo: Removed broadcast_248_piece0 on 172.18.0.15:34959 in memory (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 11:54:35 INFO BlockManagerInfo: Added broadcast_255_piece0 in memory on 172.18.0.15:34959 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 11:54:35 INFO BlockManagerInfo: Added broadcast_255_piece0 in memory on 172.18.0.14:40939 (size: 19.5 KiB, free: 434.4 MiB)
26/02/13 11:54:35 INFO BlockManagerInfo: Added broadcast_254_piece0 in memory on 172.18.0.15:34959 (size: 107.2 KiB, free: 434.0 MiB)
26/02/13 11:54:35 INFO BlockManagerInfo: Added broadcast_254_piece0 in memory on 172.18.0.14:40939 (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 11:54:35 INFO TaskSetManager: Finished task 2.0 in stage 258.0 (TID 571) in 66 ms on 172.18.0.14 (executor 1) (1/3)
26/02/13 11:54:35 INFO TaskSetManager: Finished task 1.0 in stage 258.0 (TID 570) in 66 ms on 172.18.0.15 (executor 0) (2/3)
26/02/13 11:54:35 INFO TaskSetManager: Finished task 0.0 in stage 258.0 (TID 569) in 570 ms on 172.18.0.14 (executor 1) (3/3)
26/02/13 11:54:35 INFO TaskSchedulerImpl: Removed TaskSet 258.0, whose tasks have all completed, from pool 
26/02/13 11:54:35 INFO DAGScheduler: ResultStage 258 (start at NativeMethodAccessorImpl.java:0) finished in 0.575 s
26/02/13 11:54:35 INFO DAGScheduler: Job 170 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:54:35 INFO TaskSchedulerImpl: Killing all running tasks in stage 258: Stage finished
26/02/13 11:54:35 INFO DAGScheduler: Job 170 finished: start at NativeMethodAccessorImpl.java:0, took 0.575826 s
26/02/13 11:54:35 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 80, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@12fd5c32] is committing.
26/02/13 11:54:35 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 80, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@12fd5c32] committed.
26/02/13 11:54:35 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/80 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.80.f23051b8-185b-4e68-b224-de57049d7439.tmp
26/02/13 11:54:35 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.80.f23051b8-185b-4e68-b224-de57049d7439.tmp to file:/tmp/spark-checkpoint-enrichment/commits/80
26/02/13 11:54:35 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "fc5c6452-bd70-4c90-becf-772d3bebdb57",
  "name" : null,
  "timestamp" : "2026-02-13T11:54:35.071Z",
  "batchId" : 80,
  "numInputRows" : 247,
  "inputRowsPerSecond" : 295.1015531660693,
  "processedRowsPerSecond" : 282.9324169530355,
  "durationMs" : {
    "addBatch" : 715,
    "commitOffsets" : 63,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 24,
    "triggerExecution" : 873,
    "walCommit" : 69
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 5860,
        "1" : 6440,
        "0" : 7968
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 5943,
        "1" : 6531,
        "0" : 8041
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 5943,
        "1" : 6531,
        "0" : 8041
      }
    },
    "numInputRows" : 247,
    "inputRowsPerSecond" : 295.1015531660693,
    "processedRowsPerSecond" : 282.9324169530355,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@7fe9b5de",
    "numOutputRows" : 54
  }
}
26/02/13 11:54:45 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 11:54:47 INFO BlockManagerInfo: Removed broadcast_253_piece0 on spark-master:40291 in memory (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 11:54:47 INFO BlockManagerInfo: Removed broadcast_253_piece0 on 172.18.0.15:34959 in memory (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 11:54:47 INFO BlockManagerInfo: Removed broadcast_255_piece0 on spark-master:40291 in memory (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 11:54:47 INFO BlockManagerInfo: Removed broadcast_255_piece0 on 172.18.0.15:34959 in memory (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 11:54:47 INFO BlockManagerInfo: Removed broadcast_255_piece0 on 172.18.0.14:40939 in memory (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 11:54:50 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/81 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.81.e397ccfc-1910-4948-bd55-5dde084c3bba.tmp
26/02/13 11:54:50 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.81.e397ccfc-1910-4948-bd55-5dde084c3bba.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/81
26/02/13 11:54:50 INFO MicroBatchExecution: Committed offsets for batch 81. Metadata OffsetSeqMetadata(0,1770983690779,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 11:54:50 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:54:50 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:54:50 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:54:50 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:54:50 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:54:50 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:54:50 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#71125 - origin_code.nullCount#71124) > 0)
26/02/13 11:54:50 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#71130 - destination_code.nullCount#71129) > 0)
26/02/13 11:54:50 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#71160 - callsign.nullCount#71159) > 0)
26/02/13 11:54:50 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:54:50 INFO DAGScheduler: Got job 171 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 11:54:50 INFO DAGScheduler: Final stage: ResultStage 260 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:54:50 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 259)
26/02/13 11:54:50 INFO DAGScheduler: Missing parents: List()
26/02/13 11:54:50 INFO DAGScheduler: Submitting ResultStage 260 (MapPartitionsRDD[934] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:54:50 INFO MemoryStore: Block broadcast_256 stored as values in memory (estimated size 99.3 KiB, free 434.0 MiB)
26/02/13 11:54:50 INFO MemoryStore: Block broadcast_256_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 433.9 MiB)
26/02/13 11:54:50 INFO BlockManagerInfo: Added broadcast_256_piece0 in memory on spark-master:40291 (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 11:54:50 INFO SparkContext: Created broadcast 256 from broadcast at DAGScheduler.scala:1585
26/02/13 11:54:50 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 260 (MapPartitionsRDD[934] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 11:54:50 INFO TaskSchedulerImpl: Adding task set 260.0 with 4 tasks resource profile 0
26/02/13 11:54:50 INFO TaskSetManager: Starting task 0.0 in stage 260.0 (TID 572) (172.18.0.15, executor 0, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:54:50 INFO TaskSetManager: Starting task 1.0 in stage 260.0 (TID 573) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:54:50 INFO BlockManagerInfo: Added broadcast_256_piece0 in memory on 172.18.0.15:34959 (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 11:54:51 INFO TaskSetManager: Starting task 2.0 in stage 260.0 (TID 574) (172.18.0.15, executor 0, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:54:51 INFO TaskSetManager: Starting task 3.0 in stage 260.0 (TID 575) (172.18.0.15, executor 0, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:54:51 INFO TaskSetManager: Finished task 1.0 in stage 260.0 (TID 573) in 33 ms on 172.18.0.15 (executor 0) (1/4)
26/02/13 11:54:51 INFO TaskSetManager: Finished task 0.0 in stage 260.0 (TID 572) in 33 ms on 172.18.0.15 (executor 0) (2/4)
26/02/13 11:54:51 INFO TaskSetManager: Finished task 2.0 in stage 260.0 (TID 574) in 19 ms on 172.18.0.15 (executor 0) (3/4)
26/02/13 11:54:51 INFO TaskSetManager: Finished task 3.0 in stage 260.0 (TID 575) in 19 ms on 172.18.0.15 (executor 0) (4/4)
26/02/13 11:54:51 INFO TaskSchedulerImpl: Removed TaskSet 260.0, whose tasks have all completed, from pool 
26/02/13 11:54:51 INFO DAGScheduler: ResultStage 260 (start at NativeMethodAccessorImpl.java:0) finished in 0.063 s
26/02/13 11:54:51 INFO DAGScheduler: Job 171 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:54:51 INFO TaskSchedulerImpl: Killing all running tasks in stage 260: Stage finished
26/02/13 11:54:51 INFO DAGScheduler: Job 171 finished: start at NativeMethodAccessorImpl.java:0, took 0.066644 s
26/02/13 11:54:51 INFO MemoryStore: Block broadcast_257_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.8 MiB)
26/02/13 11:54:51 INFO BlockManagerInfo: Added broadcast_257_piece0 in memory on spark-master:40291 (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 11:54:51 INFO SparkContext: Created broadcast 257 from start at NativeMethodAccessorImpl.java:0
26/02/13 11:54:51 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 81, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@19213d0e]. The input RDD has 3 partitions.
26/02/13 11:54:51 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:54:51 INFO DAGScheduler: Got job 172 (start at NativeMethodAccessorImpl.java:0) with 3 output partitions
26/02/13 11:54:51 INFO DAGScheduler: Final stage: ResultStage 261 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:54:51 INFO DAGScheduler: Parents of final stage: List()
26/02/13 11:54:51 INFO DAGScheduler: Missing parents: List()
26/02/13 11:54:51 INFO DAGScheduler: Submitting ResultStage 261 (MapPartitionsRDD[940] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:54:51 INFO MemoryStore: Block broadcast_258 stored as values in memory (estimated size 49.9 KiB, free 433.8 MiB)
26/02/13 11:54:51 INFO MemoryStore: Block broadcast_258_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.8 MiB)
26/02/13 11:54:51 INFO BlockManagerInfo: Added broadcast_258_piece0 in memory on spark-master:40291 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 11:54:51 INFO SparkContext: Created broadcast 258 from broadcast at DAGScheduler.scala:1585
26/02/13 11:54:51 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 261 (MapPartitionsRDD[940] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
26/02/13 11:54:51 INFO TaskSchedulerImpl: Adding task set 261.0 with 3 tasks resource profile 0
26/02/13 11:54:51 INFO TaskSetManager: Starting task 1.0 in stage 261.0 (TID 576) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:54:51 INFO TaskSetManager: Starting task 0.0 in stage 261.0 (TID 577) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:54:51 INFO TaskSetManager: Starting task 2.0 in stage 261.0 (TID 578) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:54:51 INFO BlockManagerInfo: Added broadcast_258_piece0 in memory on 172.18.0.15:34959 (size: 19.5 KiB, free: 434.0 MiB)
26/02/13 11:54:51 INFO BlockManagerInfo: Added broadcast_258_piece0 in memory on 172.18.0.14:40939 (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 11:54:51 INFO BlockManagerInfo: Added broadcast_257_piece0 in memory on 172.18.0.15:34959 (size: 107.2 KiB, free: 433.9 MiB)
26/02/13 11:54:51 INFO BlockManagerInfo: Added broadcast_257_piece0 in memory on 172.18.0.14:40939 (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 11:54:51 INFO TaskSetManager: Finished task 2.0 in stage 261.0 (TID 578) in 569 ms on 172.18.0.14 (executor 1) (1/3)
26/02/13 11:54:51 INFO TaskSetManager: Finished task 1.0 in stage 261.0 (TID 576) in 572 ms on 172.18.0.15 (executor 0) (2/3)
26/02/13 11:54:51 INFO TaskSetManager: Finished task 0.0 in stage 261.0 (TID 577) in 575 ms on 172.18.0.14 (executor 1) (3/3)
26/02/13 11:54:51 INFO TaskSchedulerImpl: Removed TaskSet 261.0, whose tasks have all completed, from pool 
26/02/13 11:54:51 INFO DAGScheduler: ResultStage 261 (start at NativeMethodAccessorImpl.java:0) finished in 0.582 s
26/02/13 11:54:51 INFO DAGScheduler: Job 172 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:54:51 INFO TaskSchedulerImpl: Killing all running tasks in stage 261: Stage finished
26/02/13 11:54:51 INFO DAGScheduler: Job 172 finished: start at NativeMethodAccessorImpl.java:0, took 0.584389 s
26/02/13 11:54:51 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 81, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@19213d0e] is committing.
26/02/13 11:54:51 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 81, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@19213d0e] committed.
26/02/13 11:54:51 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/81 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.81.b88c8043-ed2b-41f2-b0a1-77fef7d3387b.tmp
26/02/13 11:54:51 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.81.b88c8043-ed2b-41f2-b0a1-77fef7d3387b.tmp to file:/tmp/spark-checkpoint-enrichment/commits/81
26/02/13 11:54:51 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "fc5c6452-bd70-4c90-becf-772d3bebdb57",
  "name" : null,
  "timestamp" : "2026-02-13T11:54:50.777Z",
  "batchId" : 81,
  "numInputRows" : 10,
  "inputRowsPerSecond" : 833.3333333333334,
  "processedRowsPerSecond" : 10.649627263045794,
  "durationMs" : {
    "addBatch" : 770,
    "commitOffsets" : 72,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 30,
    "triggerExecution" : 939,
    "walCommit" : 64
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 5943,
        "1" : 6531,
        "0" : 8041
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 5946,
        "1" : 6537,
        "0" : 8042
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 5946,
        "1" : 6537,
        "0" : 8042
      }
    },
    "numInputRows" : 10,
    "inputRowsPerSecond" : 833.3333333333334,
    "processedRowsPerSecond" : 10.649627263045794,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@7fe9b5de",
    "numOutputRows" : 2
  }
}
26/02/13 11:54:51 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/82 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.82.fb8d24da-1857-4361-998f-cc2334ecefc2.tmp
26/02/13 11:54:51 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.82.fb8d24da-1857-4361-998f-cc2334ecefc2.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/82
26/02/13 11:54:51 INFO MicroBatchExecution: Committed offsets for batch 82. Metadata OffsetSeqMetadata(0,1770983691718,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 11:54:51 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:54:51 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:54:51 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:54:51 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:54:51 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:54:51 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:54:51 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#71979 - origin_code.nullCount#71978) > 0)
26/02/13 11:54:51 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#71984 - destination_code.nullCount#71983) > 0)
26/02/13 11:54:51 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#72014 - callsign.nullCount#72013) > 0)
26/02/13 11:54:51 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:54:51 INFO DAGScheduler: Got job 173 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 11:54:51 INFO DAGScheduler: Final stage: ResultStage 263 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:54:51 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 262)
26/02/13 11:54:51 INFO DAGScheduler: Missing parents: List()
26/02/13 11:54:51 INFO DAGScheduler: Submitting ResultStage 263 (MapPartitionsRDD[945] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:54:51 INFO MemoryStore: Block broadcast_259 stored as values in memory (estimated size 99.3 KiB, free 433.7 MiB)
26/02/13 11:54:51 INFO MemoryStore: Block broadcast_259_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 433.6 MiB)
26/02/13 11:54:51 INFO BlockManagerInfo: Added broadcast_259_piece0 in memory on spark-master:40291 (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 11:54:51 INFO SparkContext: Created broadcast 259 from broadcast at DAGScheduler.scala:1585
26/02/13 11:54:51 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 263 (MapPartitionsRDD[945] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 11:54:51 INFO TaskSchedulerImpl: Adding task set 263.0 with 4 tasks resource profile 0
26/02/13 11:54:51 INFO TaskSetManager: Starting task 0.0 in stage 263.0 (TID 579) (172.18.0.15, executor 0, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:54:51 INFO TaskSetManager: Starting task 1.0 in stage 263.0 (TID 580) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:54:51 INFO BlockManagerInfo: Added broadcast_259_piece0 in memory on 172.18.0.15:34959 (size: 33.3 KiB, free: 433.9 MiB)
26/02/13 11:54:51 INFO TaskSetManager: Starting task 2.0 in stage 263.0 (TID 581) (172.18.0.15, executor 0, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:54:51 INFO TaskSetManager: Finished task 0.0 in stage 263.0 (TID 579) in 22 ms on 172.18.0.15 (executor 0) (1/4)
26/02/13 11:54:51 INFO TaskSetManager: Starting task 3.0 in stage 263.0 (TID 582) (172.18.0.15, executor 0, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:54:51 INFO TaskSetManager: Finished task 1.0 in stage 263.0 (TID 580) in 22 ms on 172.18.0.15 (executor 0) (2/4)
26/02/13 11:54:51 INFO TaskSetManager: Finished task 2.0 in stage 263.0 (TID 581) in 12 ms on 172.18.0.15 (executor 0) (3/4)
26/02/13 11:54:51 INFO TaskSetManager: Finished task 3.0 in stage 263.0 (TID 582) in 19 ms on 172.18.0.15 (executor 0) (4/4)
26/02/13 11:54:51 INFO TaskSchedulerImpl: Removed TaskSet 263.0, whose tasks have all completed, from pool 
26/02/13 11:54:51 INFO DAGScheduler: ResultStage 263 (start at NativeMethodAccessorImpl.java:0) finished in 0.049 s
26/02/13 11:54:51 INFO DAGScheduler: Job 173 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:54:51 INFO TaskSchedulerImpl: Killing all running tasks in stage 263: Stage finished
26/02/13 11:54:51 INFO DAGScheduler: Job 173 finished: start at NativeMethodAccessorImpl.java:0, took 0.051155 s
26/02/13 11:54:51 INFO MemoryStore: Block broadcast_260_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.5 MiB)
26/02/13 11:54:51 INFO BlockManagerInfo: Added broadcast_260_piece0 in memory on spark-master:40291 (size: 107.2 KiB, free: 434.0 MiB)
26/02/13 11:54:51 INFO SparkContext: Created broadcast 260 from start at NativeMethodAccessorImpl.java:0
26/02/13 11:54:51 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 82, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@320fc1d2]. The input RDD has 3 partitions.
26/02/13 11:54:51 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:54:51 INFO DAGScheduler: Got job 174 (start at NativeMethodAccessorImpl.java:0) with 3 output partitions
26/02/13 11:54:51 INFO DAGScheduler: Final stage: ResultStage 264 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:54:51 INFO DAGScheduler: Parents of final stage: List()
26/02/13 11:54:51 INFO DAGScheduler: Missing parents: List()
26/02/13 11:54:51 INFO DAGScheduler: Submitting ResultStage 264 (MapPartitionsRDD[951] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:54:51 INFO MemoryStore: Block broadcast_261 stored as values in memory (estimated size 49.9 KiB, free 433.5 MiB)
26/02/13 11:54:51 INFO MemoryStore: Block broadcast_261_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.5 MiB)
26/02/13 11:54:51 INFO BlockManagerInfo: Added broadcast_261_piece0 in memory on spark-master:40291 (size: 19.5 KiB, free: 433.9 MiB)
26/02/13 11:54:51 INFO SparkContext: Created broadcast 261 from broadcast at DAGScheduler.scala:1585
26/02/13 11:54:51 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 264 (MapPartitionsRDD[951] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
26/02/13 11:54:51 INFO TaskSchedulerImpl: Adding task set 264.0 with 3 tasks resource profile 0
26/02/13 11:54:51 INFO TaskSetManager: Starting task 1.0 in stage 264.0 (TID 583) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:54:51 INFO TaskSetManager: Starting task 0.0 in stage 264.0 (TID 584) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:54:51 INFO TaskSetManager: Starting task 2.0 in stage 264.0 (TID 585) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:54:51 INFO BlockManagerInfo: Added broadcast_261_piece0 in memory on 172.18.0.15:34959 (size: 19.5 KiB, free: 433.9 MiB)
26/02/13 11:54:51 INFO BlockManagerInfo: Added broadcast_261_piece0 in memory on 172.18.0.14:40939 (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 11:54:51 INFO BlockManagerInfo: Added broadcast_260_piece0 in memory on 172.18.0.15:34959 (size: 107.2 KiB, free: 433.7 MiB)
26/02/13 11:54:51 INFO BlockManagerInfo: Added broadcast_260_piece0 in memory on 172.18.0.14:40939 (size: 107.2 KiB, free: 434.0 MiB)
26/02/13 11:54:52 INFO TaskSetManager: Finished task 2.0 in stage 264.0 (TID 585) in 65 ms on 172.18.0.14 (executor 1) (1/3)
26/02/13 11:54:52 INFO TaskSetManager: Finished task 0.0 in stage 264.0 (TID 584) in 68 ms on 172.18.0.14 (executor 1) (2/3)
26/02/13 11:54:52 INFO TaskSetManager: Finished task 1.0 in stage 264.0 (TID 583) in 72 ms on 172.18.0.15 (executor 0) (3/3)
26/02/13 11:54:52 INFO TaskSchedulerImpl: Removed TaskSet 264.0, whose tasks have all completed, from pool 
26/02/13 11:54:52 INFO DAGScheduler: ResultStage 264 (start at NativeMethodAccessorImpl.java:0) finished in 0.076 s
26/02/13 11:54:52 INFO DAGScheduler: Job 174 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:54:52 INFO TaskSchedulerImpl: Killing all running tasks in stage 264: Stage finished
26/02/13 11:54:52 INFO DAGScheduler: Job 174 finished: start at NativeMethodAccessorImpl.java:0, took 0.077331 s
26/02/13 11:54:52 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 82, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@320fc1d2] is committing.
26/02/13 11:54:52 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 82, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@320fc1d2] committed.
26/02/13 11:54:52 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/82 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.82.6b715142-ec33-4e00-86ee-02d5f729159c.tmp
26/02/13 11:54:52 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.82.6b715142-ec33-4e00-86ee-02d5f729159c.tmp to file:/tmp/spark-checkpoint-enrichment/commits/82
26/02/13 11:54:52 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "fc5c6452-bd70-4c90-becf-772d3bebdb57",
  "name" : null,
  "timestamp" : "2026-02-13T11:54:51.717Z",
  "batchId" : 82,
  "numInputRows" : 240,
  "inputRowsPerSecond" : 255.31914893617022,
  "processedRowsPerSecond" : 634.9206349206349,
  "durationMs" : {
    "addBatch" : 230,
    "commitOffsets" : 57,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 25,
    "triggerExecution" : 378,
    "walCommit" : 65
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 5946,
        "1" : 6537,
        "0" : 8042
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 6026,
        "1" : 6625,
        "0" : 8114
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 6026,
        "1" : 6625,
        "0" : 8114
      }
    },
    "numInputRows" : 240,
    "inputRowsPerSecond" : 255.31914893617022,
    "processedRowsPerSecond" : 634.9206349206349,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@7fe9b5de",
    "numOutputRows" : 53
  }
}
26/02/13 11:54:55 INFO BlockManagerInfo: Removed broadcast_259_piece0 on spark-master:40291 in memory (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 11:54:55 INFO BlockManagerInfo: Removed broadcast_259_piece0 on 172.18.0.15:34959 in memory (size: 33.3 KiB, free: 433.8 MiB)
26/02/13 11:54:55 INFO BlockManagerInfo: Removed broadcast_257_piece0 on spark-master:40291 in memory (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 11:54:55 INFO BlockManagerInfo: Removed broadcast_257_piece0 on 172.18.0.15:34959 in memory (size: 107.2 KiB, free: 433.9 MiB)
26/02/13 11:54:55 INFO BlockManagerInfo: Removed broadcast_257_piece0 on 172.18.0.14:40939 in memory (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 11:54:55 INFO BlockManagerInfo: Removed broadcast_258_piece0 on spark-master:40291 in memory (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 11:54:55 INFO BlockManagerInfo: Removed broadcast_258_piece0 on 172.18.0.14:40939 in memory (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 11:54:55 INFO BlockManagerInfo: Removed broadcast_258_piece0 on 172.18.0.15:34959 in memory (size: 19.5 KiB, free: 433.9 MiB)
26/02/13 11:54:55 INFO BlockManagerInfo: Removed broadcast_261_piece0 on spark-master:40291 in memory (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 11:54:55 INFO BlockManagerInfo: Removed broadcast_261_piece0 on 172.18.0.15:34959 in memory (size: 19.5 KiB, free: 433.9 MiB)
26/02/13 11:54:55 INFO BlockManagerInfo: Removed broadcast_261_piece0 on 172.18.0.14:40939 in memory (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 11:54:55 INFO BlockManagerInfo: Removed broadcast_256_piece0 on spark-master:40291 in memory (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 11:54:55 INFO BlockManagerInfo: Removed broadcast_256_piece0 on 172.18.0.15:34959 in memory (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 11:55:02 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 11:55:07 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/83 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.83.e35bf8c8-62a7-4c11-8c6f-2d6352d77c97.tmp
26/02/13 11:55:07 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.83.e35bf8c8-62a7-4c11-8c6f-2d6352d77c97.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/83
26/02/13 11:55:07 INFO MicroBatchExecution: Committed offsets for batch 83. Metadata OffsetSeqMetadata(0,1770983707204,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 11:55:07 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:55:07 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:55:07 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:55:07 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:55:07 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:55:07 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:55:07 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#72833 - origin_code.nullCount#72832) > 0)
26/02/13 11:55:07 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#72838 - destination_code.nullCount#72837) > 0)
26/02/13 11:55:07 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#72868 - callsign.nullCount#72867) > 0)
26/02/13 11:55:07 INFO BlockManagerInfo: Removed broadcast_254_piece0 on spark-master:40291 in memory (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 11:55:07 INFO BlockManagerInfo: Removed broadcast_254_piece0 on 172.18.0.15:34959 in memory (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 11:55:07 INFO BlockManagerInfo: Removed broadcast_254_piece0 on 172.18.0.14:40939 in memory (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 11:55:07 INFO BlockManagerInfo: Removed broadcast_260_piece0 on spark-master:40291 in memory (size: 107.2 KiB, free: 434.4 MiB)
26/02/13 11:55:07 INFO BlockManagerInfo: Removed broadcast_260_piece0 on 172.18.0.14:40939 in memory (size: 107.2 KiB, free: 434.4 MiB)
26/02/13 11:55:07 INFO BlockManagerInfo: Removed broadcast_260_piece0 on 172.18.0.15:34959 in memory (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 11:55:07 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:55:07 INFO DAGScheduler: Got job 175 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 11:55:07 INFO DAGScheduler: Final stage: ResultStage 266 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:55:07 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 265)
26/02/13 11:55:07 INFO DAGScheduler: Missing parents: List()
26/02/13 11:55:07 INFO DAGScheduler: Submitting ResultStage 266 (MapPartitionsRDD[956] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:55:07 INFO MemoryStore: Block broadcast_262 stored as values in memory (estimated size 99.3 KiB, free 434.1 MiB)
26/02/13 11:55:07 INFO MemoryStore: Block broadcast_262_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 434.0 MiB)
26/02/13 11:55:07 INFO BlockManagerInfo: Added broadcast_262_piece0 in memory on spark-master:40291 (size: 33.3 KiB, free: 434.3 MiB)
26/02/13 11:55:07 INFO SparkContext: Created broadcast 262 from broadcast at DAGScheduler.scala:1585
26/02/13 11:55:07 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 266 (MapPartitionsRDD[956] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 11:55:07 INFO TaskSchedulerImpl: Adding task set 266.0 with 4 tasks resource profile 0
26/02/13 11:55:07 INFO TaskSetManager: Starting task 0.0 in stage 266.0 (TID 586) (172.18.0.15, executor 0, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:55:07 INFO TaskSetManager: Starting task 1.0 in stage 266.0 (TID 587) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:55:07 INFO BlockManagerInfo: Added broadcast_262_piece0 in memory on 172.18.0.15:34959 (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 11:55:07 INFO TaskSetManager: Starting task 2.0 in stage 266.0 (TID 588) (172.18.0.15, executor 0, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:55:07 INFO TaskSetManager: Finished task 1.0 in stage 266.0 (TID 587) in 14 ms on 172.18.0.15 (executor 0) (1/4)
26/02/13 11:55:07 INFO TaskSetManager: Starting task 3.0 in stage 266.0 (TID 589) (172.18.0.15, executor 0, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:55:07 INFO TaskSetManager: Finished task 0.0 in stage 266.0 (TID 586) in 14 ms on 172.18.0.15 (executor 0) (2/4)
26/02/13 11:55:07 INFO TaskSetManager: Finished task 2.0 in stage 266.0 (TID 588) in 13 ms on 172.18.0.15 (executor 0) (3/4)
26/02/13 11:55:07 INFO TaskSetManager: Finished task 3.0 in stage 266.0 (TID 589) in 12 ms on 172.18.0.15 (executor 0) (4/4)
26/02/13 11:55:07 INFO TaskSchedulerImpl: Removed TaskSet 266.0, whose tasks have all completed, from pool 
26/02/13 11:55:07 INFO DAGScheduler: ResultStage 266 (start at NativeMethodAccessorImpl.java:0) finished in 0.031 s
26/02/13 11:55:07 INFO DAGScheduler: Job 175 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:55:07 INFO TaskSchedulerImpl: Killing all running tasks in stage 266: Stage finished
26/02/13 11:55:07 INFO DAGScheduler: Job 175 finished: start at NativeMethodAccessorImpl.java:0, took 0.034189 s
26/02/13 11:55:07 INFO MemoryStore: Block broadcast_263_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.9 MiB)
26/02/13 11:55:07 INFO BlockManagerInfo: Added broadcast_263_piece0 in memory on spark-master:40291 (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 11:55:07 INFO SparkContext: Created broadcast 263 from start at NativeMethodAccessorImpl.java:0
26/02/13 11:55:07 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 83, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@58bc0866]. The input RDD has 3 partitions.
26/02/13 11:55:07 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:55:07 INFO DAGScheduler: Got job 176 (start at NativeMethodAccessorImpl.java:0) with 3 output partitions
26/02/13 11:55:07 INFO DAGScheduler: Final stage: ResultStage 267 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:55:07 INFO DAGScheduler: Parents of final stage: List()
26/02/13 11:55:07 INFO DAGScheduler: Missing parents: List()
26/02/13 11:55:07 INFO DAGScheduler: Submitting ResultStage 267 (MapPartitionsRDD[962] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:55:07 INFO MemoryStore: Block broadcast_264 stored as values in memory (estimated size 49.9 KiB, free 433.9 MiB)
26/02/13 11:55:07 INFO MemoryStore: Block broadcast_264_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.9 MiB)
26/02/13 11:55:07 INFO BlockManagerInfo: Added broadcast_264_piece0 in memory on spark-master:40291 (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 11:55:07 INFO SparkContext: Created broadcast 264 from broadcast at DAGScheduler.scala:1585
26/02/13 11:55:07 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 267 (MapPartitionsRDD[962] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
26/02/13 11:55:07 INFO TaskSchedulerImpl: Adding task set 267.0 with 3 tasks resource profile 0
26/02/13 11:55:07 INFO TaskSetManager: Starting task 0.0 in stage 267.0 (TID 590) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:55:07 INFO TaskSetManager: Starting task 1.0 in stage 267.0 (TID 591) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:55:07 INFO TaskSetManager: Starting task 2.0 in stage 267.0 (TID 592) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:55:07 INFO BlockManagerInfo: Added broadcast_264_piece0 in memory on 172.18.0.15:34959 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 11:55:07 INFO BlockManagerInfo: Added broadcast_264_piece0 in memory on 172.18.0.14:40939 (size: 19.5 KiB, free: 434.4 MiB)
26/02/13 11:55:07 INFO BlockManagerInfo: Added broadcast_263_piece0 in memory on 172.18.0.14:40939 (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 11:55:07 INFO BlockManagerInfo: Added broadcast_263_piece0 in memory on 172.18.0.15:34959 (size: 107.2 KiB, free: 434.0 MiB)
26/02/13 11:55:08 INFO TaskSetManager: Finished task 2.0 in stage 267.0 (TID 592) in 541 ms on 172.18.0.14 (executor 1) (1/3)
26/02/13 11:55:08 INFO TaskSetManager: Finished task 0.0 in stage 267.0 (TID 590) in 545 ms on 172.18.0.14 (executor 1) (2/3)
26/02/13 11:55:08 INFO TaskSetManager: Finished task 1.0 in stage 267.0 (TID 591) in 549 ms on 172.18.0.15 (executor 0) (3/3)
26/02/13 11:55:08 INFO TaskSchedulerImpl: Removed TaskSet 267.0, whose tasks have all completed, from pool 
26/02/13 11:55:08 INFO DAGScheduler: ResultStage 267 (start at NativeMethodAccessorImpl.java:0) finished in 0.554 s
26/02/13 11:55:08 INFO DAGScheduler: Job 176 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:55:08 INFO TaskSchedulerImpl: Killing all running tasks in stage 267: Stage finished
26/02/13 11:55:08 INFO DAGScheduler: Job 176 finished: start at NativeMethodAccessorImpl.java:0, took 0.555446 s
26/02/13 11:55:08 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 83, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@58bc0866] is committing.
26/02/13 11:55:08 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 83, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@58bc0866] committed.
26/02/13 11:55:08 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/83 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.83.75bf2f14-cd75-4f3c-b93c-285bdc781ba2.tmp
26/02/13 11:55:08 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.83.75bf2f14-cd75-4f3c-b93c-285bdc781ba2.tmp to file:/tmp/spark-checkpoint-enrichment/commits/83
26/02/13 11:55:08 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "fc5c6452-bd70-4c90-becf-772d3bebdb57",
  "name" : null,
  "timestamp" : "2026-02-13T11:55:07.202Z",
  "batchId" : 83,
  "numInputRows" : 11,
  "inputRowsPerSecond" : 1000.0000000000001,
  "processedRowsPerSecond" : 12.45753114382786,
  "durationMs" : {
    "addBatch" : 699,
    "commitOffsets" : 61,
    "getBatch" : 0,
    "latestOffset" : 2,
    "queryPlanning" : 48,
    "triggerExecution" : 883,
    "walCommit" : 72
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 6026,
        "1" : 6625,
        "0" : 8114
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 6029,
        "1" : 6631,
        "0" : 8116
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 6029,
        "1" : 6631,
        "0" : 8116
      }
    },
    "numInputRows" : 11,
    "inputRowsPerSecond" : 1000.0000000000001,
    "processedRowsPerSecond" : 12.45753114382786,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@7fe9b5de",
    "numOutputRows" : 2
  }
}
26/02/13 11:55:08 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/84 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.84.6422b548-49cf-4020-a920-27c8f665d16b.tmp
26/02/13 11:55:08 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.84.6422b548-49cf-4020-a920-27c8f665d16b.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/84
26/02/13 11:55:08 INFO MicroBatchExecution: Committed offsets for batch 84. Metadata OffsetSeqMetadata(0,1770983708087,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 11:55:08 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:55:08 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:55:08 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:55:08 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:55:08 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:55:08 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:55:08 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#73687 - origin_code.nullCount#73686) > 0)
26/02/13 11:55:08 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#73692 - destination_code.nullCount#73691) > 0)
26/02/13 11:55:08 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#73722 - callsign.nullCount#73721) > 0)
26/02/13 11:55:08 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:55:08 INFO DAGScheduler: Got job 177 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 11:55:08 INFO DAGScheduler: Final stage: ResultStage 269 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:55:08 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 268)
26/02/13 11:55:08 INFO DAGScheduler: Missing parents: List()
26/02/13 11:55:08 INFO DAGScheduler: Submitting ResultStage 269 (MapPartitionsRDD[967] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:55:08 INFO MemoryStore: Block broadcast_265 stored as values in memory (estimated size 99.3 KiB, free 433.8 MiB)
26/02/13 11:55:08 INFO MemoryStore: Block broadcast_265_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 433.7 MiB)
26/02/13 11:55:08 INFO BlockManagerInfo: Added broadcast_265_piece0 in memory on spark-master:40291 (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 11:55:08 INFO SparkContext: Created broadcast 265 from broadcast at DAGScheduler.scala:1585
26/02/13 11:55:08 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 269 (MapPartitionsRDD[967] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 11:55:08 INFO TaskSchedulerImpl: Adding task set 269.0 with 4 tasks resource profile 0
26/02/13 11:55:08 INFO TaskSetManager: Starting task 0.0 in stage 269.0 (TID 593) (172.18.0.15, executor 0, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:55:08 INFO TaskSetManager: Starting task 1.0 in stage 269.0 (TID 594) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:55:08 INFO BlockManagerInfo: Added broadcast_265_piece0 in memory on 172.18.0.15:34959 (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 11:55:08 INFO TaskSetManager: Starting task 2.0 in stage 269.0 (TID 595) (172.18.0.15, executor 0, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:55:08 INFO TaskSetManager: Finished task 0.0 in stage 269.0 (TID 593) in 14 ms on 172.18.0.15 (executor 0) (1/4)
26/02/13 11:55:08 INFO TaskSetManager: Starting task 3.0 in stage 269.0 (TID 596) (172.18.0.15, executor 0, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:55:08 INFO TaskSetManager: Finished task 1.0 in stage 269.0 (TID 594) in 19 ms on 172.18.0.15 (executor 0) (2/4)
26/02/13 11:55:08 INFO TaskSetManager: Finished task 2.0 in stage 269.0 (TID 595) in 9 ms on 172.18.0.15 (executor 0) (3/4)
26/02/13 11:55:08 INFO TaskSetManager: Finished task 3.0 in stage 269.0 (TID 596) in 12 ms on 172.18.0.15 (executor 0) (4/4)
26/02/13 11:55:08 INFO TaskSchedulerImpl: Removed TaskSet 269.0, whose tasks have all completed, from pool 
26/02/13 11:55:08 INFO DAGScheduler: ResultStage 269 (start at NativeMethodAccessorImpl.java:0) finished in 0.036 s
26/02/13 11:55:08 INFO DAGScheduler: Job 177 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:55:08 INFO TaskSchedulerImpl: Killing all running tasks in stage 269: Stage finished
26/02/13 11:55:08 INFO DAGScheduler: Job 177 finished: start at NativeMethodAccessorImpl.java:0, took 0.037295 s
26/02/13 11:55:08 INFO MemoryStore: Block broadcast_266_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.6 MiB)
26/02/13 11:55:08 INFO BlockManagerInfo: Added broadcast_266_piece0 in memory on spark-master:40291 (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 11:55:08 INFO SparkContext: Created broadcast 266 from start at NativeMethodAccessorImpl.java:0
26/02/13 11:55:08 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 84, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@36cfd0e3]. The input RDD has 3 partitions.
26/02/13 11:55:08 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:55:08 INFO DAGScheduler: Got job 178 (start at NativeMethodAccessorImpl.java:0) with 3 output partitions
26/02/13 11:55:08 INFO DAGScheduler: Final stage: ResultStage 270 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:55:08 INFO DAGScheduler: Parents of final stage: List()
26/02/13 11:55:08 INFO DAGScheduler: Missing parents: List()
26/02/13 11:55:08 INFO DAGScheduler: Submitting ResultStage 270 (MapPartitionsRDD[973] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:55:08 INFO MemoryStore: Block broadcast_267 stored as values in memory (estimated size 49.9 KiB, free 433.6 MiB)
26/02/13 11:55:08 INFO MemoryStore: Block broadcast_267_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.6 MiB)
26/02/13 11:55:08 INFO BlockManagerInfo: Added broadcast_267_piece0 in memory on spark-master:40291 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 11:55:08 INFO SparkContext: Created broadcast 267 from broadcast at DAGScheduler.scala:1585
26/02/13 11:55:08 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 270 (MapPartitionsRDD[973] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
26/02/13 11:55:08 INFO TaskSchedulerImpl: Adding task set 270.0 with 3 tasks resource profile 0
26/02/13 11:55:08 INFO TaskSetManager: Starting task 0.0 in stage 270.0 (TID 597) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:55:08 INFO TaskSetManager: Starting task 1.0 in stage 270.0 (TID 598) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:55:08 INFO TaskSetManager: Starting task 2.0 in stage 270.0 (TID 599) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:55:08 INFO BlockManagerInfo: Added broadcast_267_piece0 in memory on 172.18.0.15:34959 (size: 19.5 KiB, free: 434.0 MiB)
26/02/13 11:55:08 INFO BlockManagerInfo: Added broadcast_267_piece0 in memory on 172.18.0.14:40939 (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 11:55:08 INFO BlockManagerInfo: Added broadcast_266_piece0 in memory on 172.18.0.15:34959 (size: 107.2 KiB, free: 433.9 MiB)
26/02/13 11:55:08 INFO BlockManagerInfo: Added broadcast_266_piece0 in memory on 172.18.0.14:40939 (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 11:55:08 INFO TaskSetManager: Finished task 0.0 in stage 270.0 (TID 597) in 56 ms on 172.18.0.14 (executor 1) (1/3)
26/02/13 11:55:08 INFO TaskSetManager: Finished task 2.0 in stage 270.0 (TID 599) in 55 ms on 172.18.0.14 (executor 1) (2/3)
26/02/13 11:55:08 INFO TaskSetManager: Finished task 1.0 in stage 270.0 (TID 598) in 58 ms on 172.18.0.15 (executor 0) (3/3)
26/02/13 11:55:08 INFO TaskSchedulerImpl: Removed TaskSet 270.0, whose tasks have all completed, from pool 
26/02/13 11:55:08 INFO DAGScheduler: ResultStage 270 (start at NativeMethodAccessorImpl.java:0) finished in 0.061 s
26/02/13 11:55:08 INFO DAGScheduler: Job 178 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:55:08 INFO TaskSchedulerImpl: Killing all running tasks in stage 270: Stage finished
26/02/13 11:55:08 INFO DAGScheduler: Job 178 finished: start at NativeMethodAccessorImpl.java:0, took 0.062406 s
26/02/13 11:55:08 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 84, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@36cfd0e3] is committing.
26/02/13 11:55:08 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 84, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@36cfd0e3] committed.
26/02/13 11:55:08 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/84 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.84.1cbe82fb-6a60-4406-8691-d557331dbd6e.tmp
26/02/13 11:55:08 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.84.1cbe82fb-6a60-4406-8691-d557331dbd6e.tmp to file:/tmp/spark-checkpoint-enrichment/commits/84
26/02/13 11:55:08 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "fc5c6452-bd70-4c90-becf-772d3bebdb57",
  "name" : null,
  "timestamp" : "2026-02-13T11:55:08.086Z",
  "batchId" : 84,
  "numInputRows" : 237,
  "inputRowsPerSecond" : 268.0995475113122,
  "processedRowsPerSecond" : 705.3571428571428,
  "durationMs" : {
    "addBatch" : 176,
    "commitOffsets" : 58,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 42,
    "triggerExecution" : 336,
    "walCommit" : 58
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 6029,
        "1" : 6631,
        "0" : 8116
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 6109,
        "1" : 6718,
        "0" : 8186
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 6109,
        "1" : 6718,
        "0" : 8186
      }
    },
    "numInputRows" : 237,
    "inputRowsPerSecond" : 268.0995475113122,
    "processedRowsPerSecond" : 705.3571428571428,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@7fe9b5de",
    "numOutputRows" : 53
  }
}
26/02/13 11:55:08 INFO BlockManagerInfo: Removed broadcast_264_piece0 on spark-master:40291 in memory (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 11:55:08 INFO BlockManagerInfo: Removed broadcast_264_piece0 on 172.18.0.15:34959 in memory (size: 19.5 KiB, free: 433.9 MiB)
26/02/13 11:55:08 INFO BlockManagerInfo: Removed broadcast_264_piece0 on 172.18.0.14:40939 in memory (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 11:55:08 INFO BlockManagerInfo: Removed broadcast_262_piece0 on spark-master:40291 in memory (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 11:55:08 INFO BlockManagerInfo: Removed broadcast_262_piece0 on 172.18.0.15:34959 in memory (size: 33.3 KiB, free: 433.9 MiB)
26/02/13 11:55:08 INFO BlockManagerInfo: Removed broadcast_267_piece0 on spark-master:40291 in memory (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 11:55:08 INFO BlockManagerInfo: Removed broadcast_267_piece0 on 172.18.0.15:34959 in memory (size: 19.5 KiB, free: 433.9 MiB)
26/02/13 11:55:08 INFO BlockManagerInfo: Removed broadcast_267_piece0 on 172.18.0.14:40939 in memory (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 11:55:08 INFO BlockManagerInfo: Removed broadcast_265_piece0 on spark-master:40291 in memory (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 11:55:08 INFO BlockManagerInfo: Removed broadcast_265_piece0 on 172.18.0.15:34959 in memory (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 11:55:08 INFO BlockManagerInfo: Removed broadcast_263_piece0 on spark-master:40291 in memory (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 11:55:08 INFO BlockManagerInfo: Removed broadcast_263_piece0 on 172.18.0.15:34959 in memory (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 11:55:08 INFO BlockManagerInfo: Removed broadcast_263_piece0 on 172.18.0.14:40939 in memory (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 11:55:18 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 11:55:27 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/85 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.85.36ae0538-d91a-49fd-b980-9c45089e4467.tmp
26/02/13 11:55:27 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.85.36ae0538-d91a-49fd-b980-9c45089e4467.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/85
26/02/13 11:55:27 INFO MicroBatchExecution: Committed offsets for batch 85. Metadata OffsetSeqMetadata(0,1770983727322,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 11:55:27 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:55:27 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:55:27 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:55:27 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:55:27 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:55:27 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:55:27 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#74541 - origin_code.nullCount#74540) > 0)
26/02/13 11:55:27 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#74546 - destination_code.nullCount#74545) > 0)
26/02/13 11:55:27 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#74576 - callsign.nullCount#74575) > 0)
26/02/13 11:55:27 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:55:27 INFO DAGScheduler: Got job 179 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 11:55:27 INFO DAGScheduler: Final stage: ResultStage 272 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:55:27 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 271)
26/02/13 11:55:27 INFO DAGScheduler: Missing parents: List()
26/02/13 11:55:27 INFO DAGScheduler: Submitting ResultStage 272 (MapPartitionsRDD[978] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:55:27 INFO MemoryStore: Block broadcast_268 stored as values in memory (estimated size 99.3 KiB, free 434.0 MiB)
26/02/13 11:55:27 INFO MemoryStore: Block broadcast_268_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 433.9 MiB)
26/02/13 11:55:27 INFO BlockManagerInfo: Added broadcast_268_piece0 in memory on spark-master:40291 (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 11:55:27 INFO SparkContext: Created broadcast 268 from broadcast at DAGScheduler.scala:1585
26/02/13 11:55:27 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 272 (MapPartitionsRDD[978] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 11:55:27 INFO TaskSchedulerImpl: Adding task set 272.0 with 4 tasks resource profile 0
26/02/13 11:55:27 INFO TaskSetManager: Starting task 0.0 in stage 272.0 (TID 600) (172.18.0.15, executor 0, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:55:27 INFO TaskSetManager: Starting task 1.0 in stage 272.0 (TID 601) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:55:27 INFO BlockManagerInfo: Added broadcast_268_piece0 in memory on 172.18.0.15:34959 (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 11:55:27 INFO TaskSetManager: Starting task 2.0 in stage 272.0 (TID 602) (172.18.0.15, executor 0, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:55:27 INFO TaskSetManager: Finished task 0.0 in stage 272.0 (TID 600) in 19 ms on 172.18.0.15 (executor 0) (1/4)
26/02/13 11:55:27 INFO TaskSetManager: Starting task 3.0 in stage 272.0 (TID 603) (172.18.0.15, executor 0, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:55:27 INFO TaskSetManager: Finished task 1.0 in stage 272.0 (TID 601) in 19 ms on 172.18.0.15 (executor 0) (2/4)
26/02/13 11:55:27 INFO TaskSetManager: Finished task 2.0 in stage 272.0 (TID 602) in 13 ms on 172.18.0.15 (executor 0) (3/4)
26/02/13 11:55:27 INFO TaskSetManager: Finished task 3.0 in stage 272.0 (TID 603) in 15 ms on 172.18.0.15 (executor 0) (4/4)
26/02/13 11:55:27 INFO TaskSchedulerImpl: Removed TaskSet 272.0, whose tasks have all completed, from pool 
26/02/13 11:55:27 INFO DAGScheduler: ResultStage 272 (start at NativeMethodAccessorImpl.java:0) finished in 0.040 s
26/02/13 11:55:27 INFO DAGScheduler: Job 179 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:55:27 INFO TaskSchedulerImpl: Killing all running tasks in stage 272: Stage finished
26/02/13 11:55:27 INFO DAGScheduler: Job 179 finished: start at NativeMethodAccessorImpl.java:0, took 0.043043 s
26/02/13 11:55:27 INFO MemoryStore: Block broadcast_269_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.8 MiB)
26/02/13 11:55:27 INFO BlockManagerInfo: Added broadcast_269_piece0 in memory on spark-master:40291 (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 11:55:27 INFO SparkContext: Created broadcast 269 from start at NativeMethodAccessorImpl.java:0
26/02/13 11:55:27 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 85, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@34b6eac7]. The input RDD has 3 partitions.
26/02/13 11:55:27 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:55:27 INFO DAGScheduler: Got job 180 (start at NativeMethodAccessorImpl.java:0) with 3 output partitions
26/02/13 11:55:27 INFO DAGScheduler: Final stage: ResultStage 273 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:55:27 INFO DAGScheduler: Parents of final stage: List()
26/02/13 11:55:27 INFO DAGScheduler: Missing parents: List()
26/02/13 11:55:27 INFO DAGScheduler: Submitting ResultStage 273 (MapPartitionsRDD[984] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:55:27 INFO MemoryStore: Block broadcast_270 stored as values in memory (estimated size 49.9 KiB, free 433.8 MiB)
26/02/13 11:55:27 INFO MemoryStore: Block broadcast_270_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.8 MiB)
26/02/13 11:55:27 INFO BlockManagerInfo: Added broadcast_270_piece0 in memory on spark-master:40291 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 11:55:27 INFO SparkContext: Created broadcast 270 from broadcast at DAGScheduler.scala:1585
26/02/13 11:55:27 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 273 (MapPartitionsRDD[984] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
26/02/13 11:55:27 INFO TaskSchedulerImpl: Adding task set 273.0 with 3 tasks resource profile 0
26/02/13 11:55:27 INFO TaskSetManager: Starting task 1.0 in stage 273.0 (TID 604) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:55:27 INFO TaskSetManager: Starting task 0.0 in stage 273.0 (TID 605) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:55:27 INFO TaskSetManager: Starting task 2.0 in stage 273.0 (TID 606) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:55:27 INFO BlockManagerInfo: Added broadcast_270_piece0 in memory on 172.18.0.15:34959 (size: 19.5 KiB, free: 434.0 MiB)
26/02/13 11:55:27 INFO BlockManagerInfo: Added broadcast_270_piece0 in memory on 172.18.0.14:40939 (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 11:55:27 INFO BlockManagerInfo: Added broadcast_269_piece0 in memory on 172.18.0.15:34959 (size: 107.2 KiB, free: 433.9 MiB)
26/02/13 11:55:27 INFO BlockManagerInfo: Added broadcast_269_piece0 in memory on 172.18.0.14:40939 (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 11:55:28 INFO TaskSetManager: Finished task 0.0 in stage 273.0 (TID 605) in 549 ms on 172.18.0.14 (executor 1) (1/3)
26/02/13 11:55:28 INFO TaskSetManager: Finished task 2.0 in stage 273.0 (TID 606) in 550 ms on 172.18.0.14 (executor 1) (2/3)
26/02/13 11:55:28 INFO TaskSetManager: Finished task 1.0 in stage 273.0 (TID 604) in 561 ms on 172.18.0.15 (executor 0) (3/3)
26/02/13 11:55:28 INFO TaskSchedulerImpl: Removed TaskSet 273.0, whose tasks have all completed, from pool 
26/02/13 11:55:28 INFO DAGScheduler: ResultStage 273 (start at NativeMethodAccessorImpl.java:0) finished in 0.566 s
26/02/13 11:55:28 INFO DAGScheduler: Job 180 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:55:28 INFO TaskSchedulerImpl: Killing all running tasks in stage 273: Stage finished
26/02/13 11:55:28 INFO DAGScheduler: Job 180 finished: start at NativeMethodAccessorImpl.java:0, took 0.566923 s
26/02/13 11:55:28 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 85, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@34b6eac7] is committing.
26/02/13 11:55:28 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 85, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@34b6eac7] committed.
26/02/13 11:55:28 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/85 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.85.f65a1161-dcd0-4175-a848-5ca15ce6545a.tmp
26/02/13 11:55:28 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.85.f65a1161-dcd0-4175-a848-5ca15ce6545a.tmp to file:/tmp/spark-checkpoint-enrichment/commits/85
26/02/13 11:55:28 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "fc5c6452-bd70-4c90-becf-772d3bebdb57",
  "name" : null,
  "timestamp" : "2026-02-13T11:55:27.320Z",
  "batchId" : 85,
  "numInputRows" : 99,
  "inputRowsPerSecond" : 9000.0,
  "processedRowsPerSecond" : 106.33727175080558,
  "durationMs" : {
    "addBatch" : 701,
    "commitOffsets" : 61,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 50,
    "triggerExecution" : 931,
    "walCommit" : 116
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 6109,
        "1" : 6718,
        "0" : 8186
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 6141,
        "1" : 6755,
        "0" : 8216
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 6141,
        "1" : 6755,
        "0" : 8216
      }
    },
    "numInputRows" : 99,
    "inputRowsPerSecond" : 9000.0,
    "processedRowsPerSecond" : 106.33727175080558,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@7fe9b5de",
    "numOutputRows" : 24
  }
}
26/02/13 11:55:28 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/86 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.86.89181e04-cf5d-4349-beb0-007f41312daf.tmp
26/02/13 11:55:28 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.86.89181e04-cf5d-4349-beb0-007f41312daf.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/86
26/02/13 11:55:28 INFO MicroBatchExecution: Committed offsets for batch 86. Metadata OffsetSeqMetadata(0,1770983728252,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 11:55:28 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:55:28 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:55:28 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:55:28 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:55:28 INFO BlockManagerInfo: Removed broadcast_270_piece0 on spark-master:40291 in memory (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 11:55:28 INFO BlockManagerInfo: Removed broadcast_270_piece0 on 172.18.0.15:34959 in memory (size: 19.5 KiB, free: 433.9 MiB)
26/02/13 11:55:28 INFO BlockManagerInfo: Removed broadcast_270_piece0 on 172.18.0.14:40939 in memory (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 11:55:28 INFO BlockManagerInfo: Removed broadcast_269_piece0 on spark-master:40291 in memory (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 11:55:28 INFO BlockManagerInfo: Removed broadcast_269_piece0 on 172.18.0.14:40939 in memory (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 11:55:28 INFO BlockManagerInfo: Removed broadcast_269_piece0 on 172.18.0.15:34959 in memory (size: 107.2 KiB, free: 434.0 MiB)
26/02/13 11:55:28 INFO BlockManagerInfo: Removed broadcast_268_piece0 on spark-master:40291 in memory (size: 33.3 KiB, free: 434.3 MiB)
26/02/13 11:55:28 INFO BlockManagerInfo: Removed broadcast_268_piece0 on 172.18.0.15:34959 in memory (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 11:55:28 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:55:28 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:55:28 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#75395 - origin_code.nullCount#75394) > 0)
26/02/13 11:55:28 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#75400 - destination_code.nullCount#75399) > 0)
26/02/13 11:55:28 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#75430 - callsign.nullCount#75429) > 0)
26/02/13 11:55:28 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:55:28 INFO DAGScheduler: Got job 181 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 11:55:28 INFO DAGScheduler: Final stage: ResultStage 275 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:55:28 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 274)
26/02/13 11:55:28 INFO DAGScheduler: Missing parents: List()
26/02/13 11:55:28 INFO DAGScheduler: Submitting ResultStage 275 (MapPartitionsRDD[989] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:55:28 INFO MemoryStore: Block broadcast_271 stored as values in memory (estimated size 99.3 KiB, free 434.0 MiB)
26/02/13 11:55:28 INFO MemoryStore: Block broadcast_271_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 433.9 MiB)
26/02/13 11:55:28 INFO BlockManagerInfo: Added broadcast_271_piece0 in memory on spark-master:40291 (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 11:55:28 INFO SparkContext: Created broadcast 271 from broadcast at DAGScheduler.scala:1585
26/02/13 11:55:28 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 275 (MapPartitionsRDD[989] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 11:55:28 INFO TaskSchedulerImpl: Adding task set 275.0 with 4 tasks resource profile 0
26/02/13 11:55:28 INFO TaskSetManager: Starting task 0.0 in stage 275.0 (TID 607) (172.18.0.15, executor 0, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:55:28 INFO TaskSetManager: Starting task 1.0 in stage 275.0 (TID 608) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:55:28 INFO BlockManagerInfo: Added broadcast_271_piece0 in memory on 172.18.0.15:34959 (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 11:55:28 INFO TaskSetManager: Starting task 2.0 in stage 275.0 (TID 609) (172.18.0.15, executor 0, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:55:28 INFO TaskSetManager: Starting task 3.0 in stage 275.0 (TID 610) (172.18.0.15, executor 0, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:55:28 INFO TaskSetManager: Finished task 1.0 in stage 275.0 (TID 608) in 17 ms on 172.18.0.15 (executor 0) (1/4)
26/02/13 11:55:28 INFO TaskSetManager: Finished task 0.0 in stage 275.0 (TID 607) in 17 ms on 172.18.0.15 (executor 0) (2/4)
26/02/13 11:55:28 INFO TaskSetManager: Finished task 2.0 in stage 275.0 (TID 609) in 12 ms on 172.18.0.15 (executor 0) (3/4)
26/02/13 11:55:28 INFO TaskSetManager: Finished task 3.0 in stage 275.0 (TID 610) in 12 ms on 172.18.0.15 (executor 0) (4/4)
26/02/13 11:55:28 INFO TaskSchedulerImpl: Removed TaskSet 275.0, whose tasks have all completed, from pool 
26/02/13 11:55:28 INFO DAGScheduler: ResultStage 275 (start at NativeMethodAccessorImpl.java:0) finished in 0.037 s
26/02/13 11:55:28 INFO DAGScheduler: Job 181 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:55:28 INFO TaskSchedulerImpl: Killing all running tasks in stage 275: Stage finished
26/02/13 11:55:28 INFO DAGScheduler: Job 181 finished: start at NativeMethodAccessorImpl.java:0, took 0.038151 s
26/02/13 11:55:28 INFO MemoryStore: Block broadcast_272_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.8 MiB)
26/02/13 11:55:28 INFO BlockManagerInfo: Added broadcast_272_piece0 in memory on spark-master:40291 (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 11:55:28 INFO SparkContext: Created broadcast 272 from start at NativeMethodAccessorImpl.java:0
26/02/13 11:55:28 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 86, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@13994ef1]. The input RDD has 3 partitions.
26/02/13 11:55:28 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:55:28 INFO DAGScheduler: Got job 182 (start at NativeMethodAccessorImpl.java:0) with 3 output partitions
26/02/13 11:55:28 INFO DAGScheduler: Final stage: ResultStage 276 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:55:28 INFO DAGScheduler: Parents of final stage: List()
26/02/13 11:55:28 INFO DAGScheduler: Missing parents: List()
26/02/13 11:55:28 INFO DAGScheduler: Submitting ResultStage 276 (MapPartitionsRDD[995] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:55:28 INFO MemoryStore: Block broadcast_273 stored as values in memory (estimated size 49.9 KiB, free 433.9 MiB)
26/02/13 11:55:28 INFO BlockManagerInfo: Removed broadcast_266_piece0 on 172.18.0.14:40939 in memory (size: 107.2 KiB, free: 434.4 MiB)
26/02/13 11:55:28 INFO BlockManagerInfo: Removed broadcast_266_piece0 on spark-master:40291 in memory (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 11:55:28 INFO BlockManagerInfo: Removed broadcast_266_piece0 on 172.18.0.15:34959 in memory (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 11:55:28 INFO MemoryStore: Block broadcast_273_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.9 MiB)
26/02/13 11:55:28 INFO BlockManagerInfo: Added broadcast_273_piece0 in memory on spark-master:40291 (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 11:55:28 INFO SparkContext: Created broadcast 273 from broadcast at DAGScheduler.scala:1585
26/02/13 11:55:28 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 276 (MapPartitionsRDD[995] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
26/02/13 11:55:28 INFO TaskSchedulerImpl: Adding task set 276.0 with 3 tasks resource profile 0
26/02/13 11:55:28 INFO TaskSetManager: Starting task 1.0 in stage 276.0 (TID 611) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:55:28 INFO TaskSetManager: Starting task 0.0 in stage 276.0 (TID 612) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:55:28 INFO TaskSetManager: Starting task 2.0 in stage 276.0 (TID 613) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:55:28 INFO BlockManagerInfo: Added broadcast_273_piece0 in memory on 172.18.0.15:34959 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 11:55:28 INFO BlockManagerInfo: Added broadcast_273_piece0 in memory on 172.18.0.14:40939 (size: 19.5 KiB, free: 434.4 MiB)
26/02/13 11:55:28 INFO BlockManagerInfo: Added broadcast_272_piece0 in memory on 172.18.0.15:34959 (size: 107.2 KiB, free: 434.0 MiB)
26/02/13 11:55:28 INFO BlockManagerInfo: Added broadcast_272_piece0 in memory on 172.18.0.14:40939 (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 11:55:28 INFO TaskSetManager: Finished task 1.0 in stage 276.0 (TID 611) in 63 ms on 172.18.0.15 (executor 0) (1/3)
26/02/13 11:55:28 INFO TaskSetManager: Finished task 0.0 in stage 276.0 (TID 612) in 79 ms on 172.18.0.14 (executor 1) (2/3)
26/02/13 11:55:28 INFO TaskSetManager: Finished task 2.0 in stage 276.0 (TID 613) in 79 ms on 172.18.0.14 (executor 1) (3/3)
26/02/13 11:55:28 INFO TaskSchedulerImpl: Removed TaskSet 276.0, whose tasks have all completed, from pool 
26/02/13 11:55:28 INFO DAGScheduler: ResultStage 276 (start at NativeMethodAccessorImpl.java:0) finished in 0.089 s
26/02/13 11:55:28 INFO DAGScheduler: Job 182 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:55:28 INFO TaskSchedulerImpl: Killing all running tasks in stage 276: Stage finished
26/02/13 11:55:28 INFO DAGScheduler: Job 182 finished: start at NativeMethodAccessorImpl.java:0, took 0.092156 s
26/02/13 11:55:28 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 86, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@13994ef1] is committing.
26/02/13 11:55:28 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 86, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@13994ef1] committed.
26/02/13 11:55:28 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/86 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.86.ec669de6-d357-40a3-89ba-d1cace5eee93.tmp
26/02/13 11:55:28 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.86.ec669de6-d357-40a3-89ba-d1cace5eee93.tmp to file:/tmp/spark-checkpoint-enrichment/commits/86
26/02/13 11:55:28 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "fc5c6452-bd70-4c90-becf-772d3bebdb57",
  "name" : null,
  "timestamp" : "2026-02-13T11:55:28.251Z",
  "batchId" : 86,
  "numInputRows" : 148,
  "inputRowsPerSecond" : 158.968850698174,
  "processedRowsPerSecond" : 371.85929648241205,
  "durationMs" : {
    "addBatch" : 235,
    "commitOffsets" : 73,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 27,
    "triggerExecution" : 398,
    "walCommit" : 61
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 6141,
        "1" : 6755,
        "0" : 8216
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 6192,
        "1" : 6810,
        "0" : 8258
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 6192,
        "1" : 6810,
        "0" : 8258
      }
    },
    "numInputRows" : 148,
    "inputRowsPerSecond" : 158.968850698174,
    "processedRowsPerSecond" : 371.85929648241205,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@7fe9b5de",
    "numOutputRows" : 31
  }
}
26/02/13 11:55:38 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 11:55:41 INFO BlockManagerInfo: Removed broadcast_273_piece0 on spark-master:40291 in memory (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 11:55:41 INFO BlockManagerInfo: Removed broadcast_273_piece0 on 172.18.0.15:34959 in memory (size: 19.5 KiB, free: 434.0 MiB)
26/02/13 11:55:41 INFO BlockManagerInfo: Removed broadcast_273_piece0 on 172.18.0.14:40939 in memory (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 11:55:41 INFO BlockManagerInfo: Removed broadcast_271_piece0 on spark-master:40291 in memory (size: 33.3 KiB, free: 434.3 MiB)
26/02/13 11:55:41 INFO BlockManagerInfo: Removed broadcast_271_piece0 on 172.18.0.15:34959 in memory (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 11:55:43 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/87 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.87.600493be-aa07-4e97-b0ef-9d67f5f1b6fc.tmp
26/02/13 11:55:43 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.87.600493be-aa07-4e97-b0ef-9d67f5f1b6fc.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/87
26/02/13 11:55:43 INFO MicroBatchExecution: Committed offsets for batch 87. Metadata OffsetSeqMetadata(0,1770983743557,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 11:55:43 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:55:43 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:55:43 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:55:43 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:55:43 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:55:43 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:55:43 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#76249 - origin_code.nullCount#76248) > 0)
26/02/13 11:55:43 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#76254 - destination_code.nullCount#76253) > 0)
26/02/13 11:55:43 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#76284 - callsign.nullCount#76283) > 0)
26/02/13 11:55:43 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:55:43 INFO DAGScheduler: Got job 183 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 11:55:43 INFO DAGScheduler: Final stage: ResultStage 278 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:55:43 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 277)
26/02/13 11:55:43 INFO DAGScheduler: Missing parents: List()
26/02/13 11:55:43 INFO DAGScheduler: Submitting ResultStage 278 (MapPartitionsRDD[1000] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:55:43 INFO MemoryStore: Block broadcast_274 stored as values in memory (estimated size 99.3 KiB, free 434.0 MiB)
26/02/13 11:55:43 INFO MemoryStore: Block broadcast_274_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 433.9 MiB)
26/02/13 11:55:43 INFO BlockManagerInfo: Added broadcast_274_piece0 in memory on spark-master:40291 (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 11:55:43 INFO SparkContext: Created broadcast 274 from broadcast at DAGScheduler.scala:1585
26/02/13 11:55:43 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 278 (MapPartitionsRDD[1000] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 11:55:43 INFO TaskSchedulerImpl: Adding task set 278.0 with 4 tasks resource profile 0
26/02/13 11:55:43 INFO TaskSetManager: Starting task 0.0 in stage 278.0 (TID 614) (172.18.0.15, executor 0, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:55:43 INFO TaskSetManager: Starting task 1.0 in stage 278.0 (TID 615) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:55:43 INFO BlockManagerInfo: Added broadcast_274_piece0 in memory on 172.18.0.15:34959 (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 11:55:43 INFO TaskSetManager: Starting task 2.0 in stage 278.0 (TID 616) (172.18.0.15, executor 0, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:55:43 INFO TaskSetManager: Finished task 0.0 in stage 278.0 (TID 614) in 18 ms on 172.18.0.15 (executor 0) (1/4)
26/02/13 11:55:43 INFO TaskSetManager: Starting task 3.0 in stage 278.0 (TID 617) (172.18.0.15, executor 0, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:55:43 INFO TaskSetManager: Finished task 1.0 in stage 278.0 (TID 615) in 19 ms on 172.18.0.15 (executor 0) (2/4)
26/02/13 11:55:43 INFO TaskSetManager: Finished task 3.0 in stage 278.0 (TID 617) in 10 ms on 172.18.0.15 (executor 0) (3/4)
26/02/13 11:55:43 INFO TaskSetManager: Finished task 2.0 in stage 278.0 (TID 616) in 13 ms on 172.18.0.15 (executor 0) (4/4)
26/02/13 11:55:43 INFO TaskSchedulerImpl: Removed TaskSet 278.0, whose tasks have all completed, from pool 
26/02/13 11:55:43 INFO DAGScheduler: ResultStage 278 (start at NativeMethodAccessorImpl.java:0) finished in 0.039 s
26/02/13 11:55:43 INFO DAGScheduler: Job 183 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:55:43 INFO TaskSchedulerImpl: Killing all running tasks in stage 278: Stage finished
26/02/13 11:55:43 INFO DAGScheduler: Job 183 finished: start at NativeMethodAccessorImpl.java:0, took 0.040156 s
26/02/13 11:55:43 INFO MemoryStore: Block broadcast_275_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.8 MiB)
26/02/13 11:55:43 INFO BlockManagerInfo: Added broadcast_275_piece0 in memory on spark-master:40291 (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 11:55:43 INFO SparkContext: Created broadcast 275 from start at NativeMethodAccessorImpl.java:0
26/02/13 11:55:43 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 87, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@5c3abbee]. The input RDD has 3 partitions.
26/02/13 11:55:43 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:55:43 INFO DAGScheduler: Got job 184 (start at NativeMethodAccessorImpl.java:0) with 3 output partitions
26/02/13 11:55:43 INFO DAGScheduler: Final stage: ResultStage 279 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:55:43 INFO DAGScheduler: Parents of final stage: List()
26/02/13 11:55:43 INFO DAGScheduler: Missing parents: List()
26/02/13 11:55:43 INFO DAGScheduler: Submitting ResultStage 279 (MapPartitionsRDD[1006] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:55:43 INFO MemoryStore: Block broadcast_276 stored as values in memory (estimated size 49.9 KiB, free 433.8 MiB)
26/02/13 11:55:43 INFO MemoryStore: Block broadcast_276_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.8 MiB)
26/02/13 11:55:43 INFO BlockManagerInfo: Added broadcast_276_piece0 in memory on spark-master:40291 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 11:55:43 INFO SparkContext: Created broadcast 276 from broadcast at DAGScheduler.scala:1585
26/02/13 11:55:43 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 279 (MapPartitionsRDD[1006] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
26/02/13 11:55:43 INFO TaskSchedulerImpl: Adding task set 279.0 with 3 tasks resource profile 0
26/02/13 11:55:43 INFO TaskSetManager: Starting task 1.0 in stage 279.0 (TID 618) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:55:43 INFO TaskSetManager: Starting task 0.0 in stage 279.0 (TID 619) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:55:43 INFO TaskSetManager: Starting task 2.0 in stage 279.0 (TID 620) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:55:43 INFO BlockManagerInfo: Added broadcast_276_piece0 in memory on 172.18.0.15:34959 (size: 19.5 KiB, free: 434.0 MiB)
26/02/13 11:55:43 INFO BlockManagerInfo: Added broadcast_276_piece0 in memory on 172.18.0.14:40939 (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 11:55:43 INFO BlockManagerInfo: Added broadcast_275_piece0 in memory on 172.18.0.15:34959 (size: 107.2 KiB, free: 433.9 MiB)
26/02/13 11:55:43 INFO BlockManagerInfo: Added broadcast_275_piece0 in memory on 172.18.0.14:40939 (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 11:55:44 INFO TaskSetManager: Finished task 1.0 in stage 279.0 (TID 618) in 559 ms on 172.18.0.15 (executor 0) (1/3)
26/02/13 11:55:44 INFO TaskSetManager: Finished task 0.0 in stage 279.0 (TID 619) in 562 ms on 172.18.0.14 (executor 1) (2/3)
26/02/13 11:55:44 INFO TaskSetManager: Finished task 2.0 in stage 279.0 (TID 620) in 563 ms on 172.18.0.14 (executor 1) (3/3)
26/02/13 11:55:44 INFO TaskSchedulerImpl: Removed TaskSet 279.0, whose tasks have all completed, from pool 
26/02/13 11:55:44 INFO DAGScheduler: ResultStage 279 (start at NativeMethodAccessorImpl.java:0) finished in 0.578 s
26/02/13 11:55:44 INFO DAGScheduler: Job 184 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:55:44 INFO TaskSchedulerImpl: Killing all running tasks in stage 279: Stage finished
26/02/13 11:55:44 INFO DAGScheduler: Job 184 finished: start at NativeMethodAccessorImpl.java:0, took 0.580470 s
26/02/13 11:55:44 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 87, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@5c3abbee] is committing.
26/02/13 11:55:44 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 87, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@5c3abbee] committed.
26/02/13 11:55:44 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/87 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.87.77e1cc5d-b03b-48a0-b3b9-4e7ec3185c93.tmp
26/02/13 11:55:44 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.87.77e1cc5d-b03b-48a0-b3b9-4e7ec3185c93.tmp to file:/tmp/spark-checkpoint-enrichment/commits/87
26/02/13 11:55:44 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "fc5c6452-bd70-4c90-becf-772d3bebdb57",
  "name" : null,
  "timestamp" : "2026-02-13T11:55:43.555Z",
  "batchId" : 87,
  "numInputRows" : 11,
  "inputRowsPerSecond" : 916.6666666666666,
  "processedRowsPerSecond" : 12.702078521939955,
  "durationMs" : {
    "addBatch" : 695,
    "commitOffsets" : 63,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 21,
    "triggerExecution" : 866,
    "walCommit" : 85
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 6192,
        "1" : 6810,
        "0" : 8258
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 6195,
        "1" : 6816,
        "0" : 8260
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 6195,
        "1" : 6816,
        "0" : 8260
      }
    },
    "numInputRows" : 11,
    "inputRowsPerSecond" : 916.6666666666666,
    "processedRowsPerSecond" : 12.702078521939955,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@7fe9b5de",
    "numOutputRows" : 2
  }
}
26/02/13 11:55:44 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/88 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.88.67c3ebe8-6bdd-4ba1-9a71-618546298790.tmp
26/02/13 11:55:44 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.88.67c3ebe8-6bdd-4ba1-9a71-618546298790.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/88
26/02/13 11:55:44 INFO MicroBatchExecution: Committed offsets for batch 88. Metadata OffsetSeqMetadata(0,1770983744423,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 11:55:44 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:55:44 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:55:44 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:55:44 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:55:44 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:55:44 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:55:44 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#77103 - origin_code.nullCount#77102) > 0)
26/02/13 11:55:44 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#77108 - destination_code.nullCount#77107) > 0)
26/02/13 11:55:44 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#77138 - callsign.nullCount#77137) > 0)
26/02/13 11:55:44 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:55:44 INFO DAGScheduler: Got job 185 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 11:55:44 INFO DAGScheduler: Final stage: ResultStage 281 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:55:44 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 280)
26/02/13 11:55:44 INFO DAGScheduler: Missing parents: List()
26/02/13 11:55:44 INFO DAGScheduler: Submitting ResultStage 281 (MapPartitionsRDD[1011] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:55:44 INFO MemoryStore: Block broadcast_277 stored as values in memory (estimated size 99.3 KiB, free 433.7 MiB)
26/02/13 11:55:44 INFO MemoryStore: Block broadcast_277_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 433.6 MiB)
26/02/13 11:55:44 INFO BlockManagerInfo: Added broadcast_277_piece0 in memory on spark-master:40291 (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 11:55:44 INFO SparkContext: Created broadcast 277 from broadcast at DAGScheduler.scala:1585
26/02/13 11:55:44 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 281 (MapPartitionsRDD[1011] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 11:55:44 INFO TaskSchedulerImpl: Adding task set 281.0 with 4 tasks resource profile 0
26/02/13 11:55:44 INFO TaskSetManager: Starting task 0.0 in stage 281.0 (TID 621) (172.18.0.15, executor 0, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:55:44 INFO TaskSetManager: Starting task 1.0 in stage 281.0 (TID 622) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:55:44 INFO BlockManagerInfo: Added broadcast_277_piece0 in memory on 172.18.0.15:34959 (size: 33.3 KiB, free: 433.9 MiB)
26/02/13 11:55:44 INFO TaskSetManager: Starting task 2.0 in stage 281.0 (TID 623) (172.18.0.15, executor 0, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:55:44 INFO TaskSetManager: Finished task 1.0 in stage 281.0 (TID 622) in 15 ms on 172.18.0.15 (executor 0) (1/4)
26/02/13 11:55:44 INFO TaskSetManager: Starting task 3.0 in stage 281.0 (TID 624) (172.18.0.15, executor 0, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:55:44 INFO TaskSetManager: Finished task 0.0 in stage 281.0 (TID 621) in 16 ms on 172.18.0.15 (executor 0) (2/4)
26/02/13 11:55:44 INFO TaskSetManager: Finished task 2.0 in stage 281.0 (TID 623) in 13 ms on 172.18.0.15 (executor 0) (3/4)
26/02/13 11:55:44 INFO TaskSetManager: Finished task 3.0 in stage 281.0 (TID 624) in 14 ms on 172.18.0.15 (executor 0) (4/4)
26/02/13 11:55:44 INFO TaskSchedulerImpl: Removed TaskSet 281.0, whose tasks have all completed, from pool 
26/02/13 11:55:44 INFO DAGScheduler: ResultStage 281 (start at NativeMethodAccessorImpl.java:0) finished in 0.035 s
26/02/13 11:55:44 INFO DAGScheduler: Job 185 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:55:44 INFO TaskSchedulerImpl: Killing all running tasks in stage 281: Stage finished
26/02/13 11:55:44 INFO DAGScheduler: Job 185 finished: start at NativeMethodAccessorImpl.java:0, took 0.036260 s
26/02/13 11:55:44 INFO MemoryStore: Block broadcast_278_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.5 MiB)
26/02/13 11:55:44 INFO BlockManagerInfo: Added broadcast_278_piece0 in memory on spark-master:40291 (size: 107.2 KiB, free: 434.0 MiB)
26/02/13 11:55:44 INFO SparkContext: Created broadcast 278 from start at NativeMethodAccessorImpl.java:0
26/02/13 11:55:44 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 88, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@73d308a7]. The input RDD has 3 partitions.
26/02/13 11:55:44 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:55:44 INFO DAGScheduler: Got job 186 (start at NativeMethodAccessorImpl.java:0) with 3 output partitions
26/02/13 11:55:44 INFO DAGScheduler: Final stage: ResultStage 282 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:55:44 INFO DAGScheduler: Parents of final stage: List()
26/02/13 11:55:44 INFO DAGScheduler: Missing parents: List()
26/02/13 11:55:44 INFO DAGScheduler: Submitting ResultStage 282 (MapPartitionsRDD[1017] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:55:44 INFO MemoryStore: Block broadcast_279 stored as values in memory (estimated size 49.9 KiB, free 433.5 MiB)
26/02/13 11:55:44 INFO MemoryStore: Block broadcast_279_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.5 MiB)
26/02/13 11:55:44 INFO BlockManagerInfo: Added broadcast_279_piece0 in memory on spark-master:40291 (size: 19.5 KiB, free: 433.9 MiB)
26/02/13 11:55:44 INFO SparkContext: Created broadcast 279 from broadcast at DAGScheduler.scala:1585
26/02/13 11:55:44 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 282 (MapPartitionsRDD[1017] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
26/02/13 11:55:44 INFO TaskSchedulerImpl: Adding task set 282.0 with 3 tasks resource profile 0
26/02/13 11:55:44 INFO TaskSetManager: Starting task 1.0 in stage 282.0 (TID 625) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:55:44 INFO TaskSetManager: Starting task 0.0 in stage 282.0 (TID 626) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:55:44 INFO TaskSetManager: Starting task 2.0 in stage 282.0 (TID 627) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:55:44 INFO BlockManagerInfo: Added broadcast_279_piece0 in memory on 172.18.0.15:34959 (size: 19.5 KiB, free: 433.9 MiB)
26/02/13 11:55:44 INFO BlockManagerInfo: Added broadcast_279_piece0 in memory on 172.18.0.14:40939 (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 11:55:44 INFO BlockManagerInfo: Added broadcast_278_piece0 in memory on 172.18.0.15:34959 (size: 107.2 KiB, free: 433.7 MiB)
26/02/13 11:55:44 INFO BlockManagerInfo: Added broadcast_278_piece0 in memory on 172.18.0.14:40939 (size: 107.2 KiB, free: 434.0 MiB)
26/02/13 11:55:44 INFO TaskSetManager: Finished task 1.0 in stage 282.0 (TID 625) in 61 ms on 172.18.0.15 (executor 0) (1/3)
26/02/13 11:55:44 INFO TaskSetManager: Finished task 2.0 in stage 282.0 (TID 627) in 65 ms on 172.18.0.14 (executor 1) (2/3)
26/02/13 11:55:44 INFO TaskSetManager: Finished task 0.0 in stage 282.0 (TID 626) in 69 ms on 172.18.0.14 (executor 1) (3/3)
26/02/13 11:55:44 INFO TaskSchedulerImpl: Removed TaskSet 282.0, whose tasks have all completed, from pool 
26/02/13 11:55:44 INFO DAGScheduler: ResultStage 282 (start at NativeMethodAccessorImpl.java:0) finished in 0.073 s
26/02/13 11:55:44 INFO DAGScheduler: Job 186 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:55:44 INFO TaskSchedulerImpl: Killing all running tasks in stage 282: Stage finished
26/02/13 11:55:44 INFO DAGScheduler: Job 186 finished: start at NativeMethodAccessorImpl.java:0, took 0.074775 s
26/02/13 11:55:44 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 88, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@73d308a7] is committing.
26/02/13 11:55:44 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 88, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@73d308a7] committed.
26/02/13 11:55:44 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/88 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.88.66a1f66f-d162-49c6-bfb7-f2414ed56bc2.tmp
26/02/13 11:55:44 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.88.66a1f66f-d162-49c6-bfb7-f2414ed56bc2.tmp to file:/tmp/spark-checkpoint-enrichment/commits/88
26/02/13 11:55:44 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "fc5c6452-bd70-4c90-becf-772d3bebdb57",
  "name" : null,
  "timestamp" : "2026-02-13T11:55:44.422Z",
  "batchId" : 88,
  "numInputRows" : 238,
  "inputRowsPerSecond" : 274.5098039215686,
  "processedRowsPerSecond" : 685.8789625360231,
  "durationMs" : {
    "addBatch" : 185,
    "commitOffsets" : 61,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 45,
    "triggerExecution" : 347,
    "walCommit" : 55
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 6195,
        "1" : 6816,
        "0" : 8260
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 6274,
        "1" : 6904,
        "0" : 8331
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 6274,
        "1" : 6904,
        "0" : 8331
      }
    },
    "numInputRows" : 238,
    "inputRowsPerSecond" : 274.5098039215686,
    "processedRowsPerSecond" : 685.8789625360231,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@7fe9b5de",
    "numOutputRows" : 53
  }
}
26/02/13 11:55:47 INFO BlockManagerInfo: Removed broadcast_277_piece0 on spark-master:40291 in memory (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 11:55:47 INFO BlockManagerInfo: Removed broadcast_277_piece0 on 172.18.0.15:34959 in memory (size: 33.3 KiB, free: 433.8 MiB)
26/02/13 11:55:47 INFO BlockManagerInfo: Removed broadcast_275_piece0 on spark-master:40291 in memory (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 11:55:47 INFO BlockManagerInfo: Removed broadcast_275_piece0 on 172.18.0.14:40939 in memory (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 11:55:47 INFO BlockManagerInfo: Removed broadcast_275_piece0 on 172.18.0.15:34959 in memory (size: 107.2 KiB, free: 433.9 MiB)
26/02/13 11:55:47 INFO BlockManagerInfo: Removed broadcast_274_piece0 on spark-master:40291 in memory (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 11:55:47 INFO BlockManagerInfo: Removed broadcast_274_piece0 on 172.18.0.15:34959 in memory (size: 33.3 KiB, free: 433.9 MiB)
26/02/13 11:55:47 INFO BlockManagerInfo: Removed broadcast_276_piece0 on spark-master:40291 in memory (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 11:55:47 INFO BlockManagerInfo: Removed broadcast_276_piece0 on 172.18.0.14:40939 in memory (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 11:55:47 INFO BlockManagerInfo: Removed broadcast_276_piece0 on 172.18.0.15:34959 in memory (size: 19.5 KiB, free: 433.9 MiB)
26/02/13 11:55:47 INFO BlockManagerInfo: Removed broadcast_279_piece0 on spark-master:40291 in memory (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 11:55:47 INFO BlockManagerInfo: Removed broadcast_279_piece0 on 172.18.0.14:40939 in memory (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 11:55:47 INFO BlockManagerInfo: Removed broadcast_279_piece0 on 172.18.0.15:34959 in memory (size: 19.5 KiB, free: 434.0 MiB)
26/02/13 11:55:54 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 11:55:59 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/89 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.89.8666af1d-1beb-4c07-8325-5eb10edee3fd.tmp
26/02/13 11:55:59 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.89.8666af1d-1beb-4c07-8325-5eb10edee3fd.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/89
26/02/13 11:55:59 INFO MicroBatchExecution: Committed offsets for batch 89. Metadata OffsetSeqMetadata(0,1770983759880,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 11:55:59 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:55:59 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:55:59 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:55:59 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:56:00 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:56:00 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:56:00 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#77957 - origin_code.nullCount#77956) > 0)
26/02/13 11:56:00 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#77962 - destination_code.nullCount#77961) > 0)
26/02/13 11:56:00 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#77992 - callsign.nullCount#77991) > 0)
26/02/13 11:56:00 INFO BlockManagerInfo: Removed broadcast_278_piece0 on spark-master:40291 in memory (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 11:56:00 INFO BlockManagerInfo: Removed broadcast_278_piece0 on 172.18.0.14:40939 in memory (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 11:56:00 INFO BlockManagerInfo: Removed broadcast_278_piece0 on 172.18.0.15:34959 in memory (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 11:56:00 INFO BlockManagerInfo: Removed broadcast_272_piece0 on spark-master:40291 in memory (size: 107.2 KiB, free: 434.4 MiB)
26/02/13 11:56:00 INFO BlockManagerInfo: Removed broadcast_272_piece0 on 172.18.0.14:40939 in memory (size: 107.2 KiB, free: 434.4 MiB)
26/02/13 11:56:00 INFO BlockManagerInfo: Removed broadcast_272_piece0 on 172.18.0.15:34959 in memory (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 11:56:00 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:56:00 INFO DAGScheduler: Got job 187 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 11:56:00 INFO DAGScheduler: Final stage: ResultStage 284 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:56:00 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 283)
26/02/13 11:56:00 INFO DAGScheduler: Missing parents: List()
26/02/13 11:56:00 INFO DAGScheduler: Submitting ResultStage 284 (MapPartitionsRDD[1022] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:56:00 INFO MemoryStore: Block broadcast_280 stored as values in memory (estimated size 99.3 KiB, free 434.1 MiB)
26/02/13 11:56:00 INFO MemoryStore: Block broadcast_280_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 434.0 MiB)
26/02/13 11:56:00 INFO BlockManagerInfo: Added broadcast_280_piece0 in memory on spark-master:40291 (size: 33.3 KiB, free: 434.3 MiB)
26/02/13 11:56:00 INFO SparkContext: Created broadcast 280 from broadcast at DAGScheduler.scala:1585
26/02/13 11:56:00 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 284 (MapPartitionsRDD[1022] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 11:56:00 INFO TaskSchedulerImpl: Adding task set 284.0 with 4 tasks resource profile 0
26/02/13 11:56:00 INFO TaskSetManager: Starting task 0.0 in stage 284.0 (TID 628) (172.18.0.15, executor 0, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:56:00 INFO TaskSetManager: Starting task 1.0 in stage 284.0 (TID 629) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:56:00 INFO BlockManagerInfo: Added broadcast_280_piece0 in memory on 172.18.0.15:34959 (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 11:56:00 INFO TaskSetManager: Starting task 2.0 in stage 284.0 (TID 630) (172.18.0.15, executor 0, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:56:00 INFO TaskSetManager: Finished task 1.0 in stage 284.0 (TID 629) in 19 ms on 172.18.0.15 (executor 0) (1/4)
26/02/13 11:56:00 INFO TaskSetManager: Starting task 3.0 in stage 284.0 (TID 631) (172.18.0.15, executor 0, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:56:00 INFO TaskSetManager: Finished task 0.0 in stage 284.0 (TID 628) in 21 ms on 172.18.0.15 (executor 0) (2/4)
26/02/13 11:56:00 INFO TaskSetManager: Finished task 2.0 in stage 284.0 (TID 630) in 12 ms on 172.18.0.15 (executor 0) (3/4)
26/02/13 11:56:00 INFO TaskSetManager: Finished task 3.0 in stage 284.0 (TID 631) in 11 ms on 172.18.0.15 (executor 0) (4/4)
26/02/13 11:56:00 INFO TaskSchedulerImpl: Removed TaskSet 284.0, whose tasks have all completed, from pool 
26/02/13 11:56:00 INFO DAGScheduler: ResultStage 284 (start at NativeMethodAccessorImpl.java:0) finished in 0.039 s
26/02/13 11:56:00 INFO DAGScheduler: Job 187 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:56:00 INFO TaskSchedulerImpl: Killing all running tasks in stage 284: Stage finished
26/02/13 11:56:00 INFO DAGScheduler: Job 187 finished: start at NativeMethodAccessorImpl.java:0, took 0.041546 s
26/02/13 11:56:00 INFO MemoryStore: Block broadcast_281_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.9 MiB)
26/02/13 11:56:00 INFO BlockManagerInfo: Added broadcast_281_piece0 in memory on spark-master:40291 (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 11:56:00 INFO SparkContext: Created broadcast 281 from start at NativeMethodAccessorImpl.java:0
26/02/13 11:56:00 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 89, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@308a8b3b]. The input RDD has 3 partitions.
26/02/13 11:56:00 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:56:00 INFO DAGScheduler: Got job 188 (start at NativeMethodAccessorImpl.java:0) with 3 output partitions
26/02/13 11:56:00 INFO DAGScheduler: Final stage: ResultStage 285 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:56:00 INFO DAGScheduler: Parents of final stage: List()
26/02/13 11:56:00 INFO DAGScheduler: Missing parents: List()
26/02/13 11:56:00 INFO DAGScheduler: Submitting ResultStage 285 (MapPartitionsRDD[1028] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:56:00 INFO MemoryStore: Block broadcast_282 stored as values in memory (estimated size 49.9 KiB, free 433.9 MiB)
26/02/13 11:56:00 INFO MemoryStore: Block broadcast_282_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.9 MiB)
26/02/13 11:56:00 INFO BlockManagerInfo: Added broadcast_282_piece0 in memory on spark-master:40291 (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 11:56:00 INFO SparkContext: Created broadcast 282 from broadcast at DAGScheduler.scala:1585
26/02/13 11:56:00 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 285 (MapPartitionsRDD[1028] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
26/02/13 11:56:00 INFO TaskSchedulerImpl: Adding task set 285.0 with 3 tasks resource profile 0
26/02/13 11:56:00 INFO TaskSetManager: Starting task 1.0 in stage 285.0 (TID 632) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:56:00 INFO TaskSetManager: Starting task 0.0 in stage 285.0 (TID 633) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:56:00 INFO TaskSetManager: Starting task 2.0 in stage 285.0 (TID 634) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:56:00 INFO BlockManagerInfo: Added broadcast_282_piece0 in memory on 172.18.0.15:34959 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 11:56:00 INFO BlockManagerInfo: Added broadcast_282_piece0 in memory on 172.18.0.14:40939 (size: 19.5 KiB, free: 434.4 MiB)
26/02/13 11:56:00 INFO BlockManagerInfo: Added broadcast_281_piece0 in memory on 172.18.0.14:40939 (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 11:56:00 INFO BlockManagerInfo: Added broadcast_281_piece0 in memory on 172.18.0.15:34959 (size: 107.2 KiB, free: 434.0 MiB)
26/02/13 11:56:00 INFO TaskSetManager: Finished task 2.0 in stage 285.0 (TID 634) in 537 ms on 172.18.0.14 (executor 1) (1/3)
26/02/13 11:56:00 INFO TaskSetManager: Finished task 0.0 in stage 285.0 (TID 633) in 541 ms on 172.18.0.14 (executor 1) (2/3)
26/02/13 11:56:00 INFO TaskSetManager: Finished task 1.0 in stage 285.0 (TID 632) in 546 ms on 172.18.0.15 (executor 0) (3/3)
26/02/13 11:56:00 INFO TaskSchedulerImpl: Removed TaskSet 285.0, whose tasks have all completed, from pool 
26/02/13 11:56:00 INFO DAGScheduler: ResultStage 285 (start at NativeMethodAccessorImpl.java:0) finished in 0.551 s
26/02/13 11:56:00 INFO DAGScheduler: Job 188 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:56:00 INFO TaskSchedulerImpl: Killing all running tasks in stage 285: Stage finished
26/02/13 11:56:00 INFO DAGScheduler: Job 188 finished: start at NativeMethodAccessorImpl.java:0, took 0.552747 s
26/02/13 11:56:00 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 89, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@308a8b3b] is committing.
26/02/13 11:56:00 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 89, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@308a8b3b] committed.
26/02/13 11:56:00 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/89 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.89.33ecbada-1516-440c-9125-45c36d23462a.tmp
26/02/13 11:56:00 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.89.33ecbada-1516-440c-9125-45c36d23462a.tmp to file:/tmp/spark-checkpoint-enrichment/commits/89
26/02/13 11:56:00 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "fc5c6452-bd70-4c90-becf-772d3bebdb57",
  "name" : null,
  "timestamp" : "2026-02-13T11:55:59.878Z",
  "batchId" : 89,
  "numInputRows" : 7,
  "inputRowsPerSecond" : 636.3636363636364,
  "processedRowsPerSecond" : 8.158508158508159,
  "durationMs" : {
    "addBatch" : 698,
    "commitOffsets" : 71,
    "getBatch" : 0,
    "latestOffset" : 2,
    "queryPlanning" : 24,
    "triggerExecution" : 858,
    "walCommit" : 62
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 6274,
        "1" : 6904,
        "0" : 8331
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 6275,
        "1" : 6909,
        "0" : 8332
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 6275,
        "1" : 6909,
        "0" : 8332
      }
    },
    "numInputRows" : 7,
    "inputRowsPerSecond" : 636.3636363636364,
    "processedRowsPerSecond" : 8.158508158508159,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@7fe9b5de",
    "numOutputRows" : 2
  }
}
26/02/13 11:56:00 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/90 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.90.c642be08-6b30-4a1b-9c09-e748e63e077c.tmp
26/02/13 11:56:00 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.90.c642be08-6b30-4a1b-9c09-e748e63e077c.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/90
26/02/13 11:56:00 INFO MicroBatchExecution: Committed offsets for batch 90. Metadata OffsetSeqMetadata(0,1770983760738,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 11:56:00 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:56:00 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:56:00 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:56:00 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:56:00 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:56:00 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:56:00 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#78811 - origin_code.nullCount#78810) > 0)
26/02/13 11:56:00 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#78816 - destination_code.nullCount#78815) > 0)
26/02/13 11:56:00 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#78846 - callsign.nullCount#78845) > 0)
26/02/13 11:56:00 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:56:00 INFO DAGScheduler: Got job 189 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 11:56:00 INFO DAGScheduler: Final stage: ResultStage 287 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:56:00 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 286)
26/02/13 11:56:00 INFO DAGScheduler: Missing parents: List()
26/02/13 11:56:00 INFO DAGScheduler: Submitting ResultStage 287 (MapPartitionsRDD[1033] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:56:00 INFO MemoryStore: Block broadcast_283 stored as values in memory (estimated size 99.3 KiB, free 433.8 MiB)
26/02/13 11:56:00 INFO MemoryStore: Block broadcast_283_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 433.7 MiB)
26/02/13 11:56:00 INFO BlockManagerInfo: Added broadcast_283_piece0 in memory on spark-master:40291 (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 11:56:00 INFO SparkContext: Created broadcast 283 from broadcast at DAGScheduler.scala:1585
26/02/13 11:56:00 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 287 (MapPartitionsRDD[1033] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 11:56:00 INFO TaskSchedulerImpl: Adding task set 287.0 with 4 tasks resource profile 0
26/02/13 11:56:00 INFO TaskSetManager: Starting task 0.0 in stage 287.0 (TID 635) (172.18.0.15, executor 0, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:56:00 INFO TaskSetManager: Starting task 1.0 in stage 287.0 (TID 636) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:56:00 INFO BlockManagerInfo: Added broadcast_283_piece0 in memory on 172.18.0.15:34959 (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 11:56:00 INFO TaskSetManager: Starting task 2.0 in stage 287.0 (TID 637) (172.18.0.15, executor 0, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:56:00 INFO TaskSetManager: Starting task 3.0 in stage 287.0 (TID 638) (172.18.0.15, executor 0, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:56:00 INFO TaskSetManager: Finished task 0.0 in stage 287.0 (TID 635) in 21 ms on 172.18.0.15 (executor 0) (1/4)
26/02/13 11:56:00 INFO TaskSetManager: Finished task 1.0 in stage 287.0 (TID 636) in 21 ms on 172.18.0.15 (executor 0) (2/4)
26/02/13 11:56:00 INFO TaskSetManager: Finished task 3.0 in stage 287.0 (TID 638) in 14 ms on 172.18.0.15 (executor 0) (3/4)
26/02/13 11:56:00 INFO TaskSetManager: Finished task 2.0 in stage 287.0 (TID 637) in 15 ms on 172.18.0.15 (executor 0) (4/4)
26/02/13 11:56:00 INFO TaskSchedulerImpl: Removed TaskSet 287.0, whose tasks have all completed, from pool 
26/02/13 11:56:00 INFO DAGScheduler: ResultStage 287 (start at NativeMethodAccessorImpl.java:0) finished in 0.042 s
26/02/13 11:56:00 INFO DAGScheduler: Job 189 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:56:00 INFO TaskSchedulerImpl: Killing all running tasks in stage 287: Stage finished
26/02/13 11:56:00 INFO DAGScheduler: Job 189 finished: start at NativeMethodAccessorImpl.java:0, took 0.044732 s
26/02/13 11:56:00 INFO MemoryStore: Block broadcast_284_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.6 MiB)
26/02/13 11:56:00 INFO BlockManagerInfo: Added broadcast_284_piece0 in memory on spark-master:40291 (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 11:56:00 INFO SparkContext: Created broadcast 284 from start at NativeMethodAccessorImpl.java:0
26/02/13 11:56:00 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 90, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@7308be7b]. The input RDD has 3 partitions.
26/02/13 11:56:00 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:56:00 INFO DAGScheduler: Got job 190 (start at NativeMethodAccessorImpl.java:0) with 3 output partitions
26/02/13 11:56:00 INFO DAGScheduler: Final stage: ResultStage 288 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:56:00 INFO DAGScheduler: Parents of final stage: List()
26/02/13 11:56:00 INFO DAGScheduler: Missing parents: List()
26/02/13 11:56:00 INFO DAGScheduler: Submitting ResultStage 288 (MapPartitionsRDD[1039] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:56:00 INFO MemoryStore: Block broadcast_285 stored as values in memory (estimated size 49.9 KiB, free 433.6 MiB)
26/02/13 11:56:00 INFO MemoryStore: Block broadcast_285_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.6 MiB)
26/02/13 11:56:00 INFO BlockManagerInfo: Added broadcast_285_piece0 in memory on spark-master:40291 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 11:56:00 INFO SparkContext: Created broadcast 285 from broadcast at DAGScheduler.scala:1585
26/02/13 11:56:00 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 288 (MapPartitionsRDD[1039] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
26/02/13 11:56:00 INFO TaskSchedulerImpl: Adding task set 288.0 with 3 tasks resource profile 0
26/02/13 11:56:00 INFO TaskSetManager: Starting task 0.0 in stage 288.0 (TID 639) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:56:00 INFO TaskSetManager: Starting task 1.0 in stage 288.0 (TID 640) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:56:00 INFO TaskSetManager: Starting task 2.0 in stage 288.0 (TID 641) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:56:00 INFO BlockManagerInfo: Added broadcast_285_piece0 in memory on 172.18.0.14:40939 (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 11:56:00 INFO BlockManagerInfo: Added broadcast_285_piece0 in memory on 172.18.0.15:34959 (size: 19.5 KiB, free: 434.0 MiB)
26/02/13 11:56:01 INFO BlockManagerInfo: Added broadcast_284_piece0 in memory on 172.18.0.15:34959 (size: 107.2 KiB, free: 433.9 MiB)
26/02/13 11:56:01 INFO BlockManagerInfo: Added broadcast_284_piece0 in memory on 172.18.0.14:40939 (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 11:56:01 INFO TaskSetManager: Finished task 2.0 in stage 288.0 (TID 641) in 58 ms on 172.18.0.14 (executor 1) (1/3)
26/02/13 11:56:01 INFO TaskSetManager: Finished task 0.0 in stage 288.0 (TID 639) in 63 ms on 172.18.0.14 (executor 1) (2/3)
26/02/13 11:56:01 INFO TaskSetManager: Finished task 1.0 in stage 288.0 (TID 640) in 73 ms on 172.18.0.15 (executor 0) (3/3)
26/02/13 11:56:01 INFO TaskSchedulerImpl: Removed TaskSet 288.0, whose tasks have all completed, from pool 
26/02/13 11:56:01 INFO DAGScheduler: ResultStage 288 (start at NativeMethodAccessorImpl.java:0) finished in 0.082 s
26/02/13 11:56:01 INFO DAGScheduler: Job 190 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:56:01 INFO TaskSchedulerImpl: Killing all running tasks in stage 288: Stage finished
26/02/13 11:56:01 INFO DAGScheduler: Job 190 finished: start at NativeMethodAccessorImpl.java:0, took 0.085274 s
26/02/13 11:56:01 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 90, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@7308be7b] is committing.
26/02/13 11:56:01 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 90, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@7308be7b] committed.
26/02/13 11:56:01 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/90 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.90.058f76d8-1aeb-4f46-bf31-7699adf6c929.tmp
26/02/13 11:56:01 INFO BlockManagerInfo: Removed broadcast_282_piece0 on spark-master:40291 in memory (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 11:56:01 INFO BlockManagerInfo: Removed broadcast_282_piece0 on 172.18.0.14:40939 in memory (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 11:56:01 INFO BlockManagerInfo: Removed broadcast_282_piece0 on 172.18.0.15:34959 in memory (size: 19.5 KiB, free: 433.9 MiB)
26/02/13 11:56:01 INFO BlockManagerInfo: Removed broadcast_283_piece0 on spark-master:40291 in memory (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 11:56:01 INFO BlockManagerInfo: Removed broadcast_283_piece0 on 172.18.0.15:34959 in memory (size: 33.3 KiB, free: 433.9 MiB)
26/02/13 11:56:01 INFO BlockManagerInfo: Removed broadcast_285_piece0 on spark-master:40291 in memory (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 11:56:01 INFO BlockManagerInfo: Removed broadcast_285_piece0 on 172.18.0.15:34959 in memory (size: 19.5 KiB, free: 433.9 MiB)
26/02/13 11:56:01 INFO BlockManagerInfo: Removed broadcast_285_piece0 on 172.18.0.14:40939 in memory (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 11:56:01 INFO BlockManagerInfo: Removed broadcast_280_piece0 on spark-master:40291 in memory (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 11:56:01 INFO BlockManagerInfo: Removed broadcast_280_piece0 on 172.18.0.15:34959 in memory (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 11:56:01 INFO BlockManagerInfo: Removed broadcast_281_piece0 on spark-master:40291 in memory (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 11:56:01 INFO BlockManagerInfo: Removed broadcast_281_piece0 on 172.18.0.14:40939 in memory (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 11:56:01 INFO BlockManagerInfo: Removed broadcast_281_piece0 on 172.18.0.15:34959 in memory (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 11:56:01 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.90.058f76d8-1aeb-4f46-bf31-7699adf6c929.tmp to file:/tmp/spark-checkpoint-enrichment/commits/90
26/02/13 11:56:01 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "fc5c6452-bd70-4c90-becf-772d3bebdb57",
  "name" : null,
  "timestamp" : "2026-02-13T11:56:00.737Z",
  "batchId" : 90,
  "numInputRows" : 246,
  "inputRowsPerSecond" : 286.37951105937134,
  "processedRowsPerSecond" : 578.8235294117648,
  "durationMs" : {
    "addBatch" : 220,
    "commitOffsets" : 107,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 26,
    "triggerExecution" : 425,
    "walCommit" : 71
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 6275,
        "1" : 6909,
        "0" : 8332
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 6355,
        "1" : 7001,
        "0" : 8406
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 6355,
        "1" : 7001,
        "0" : 8406
      }
    },
    "numInputRows" : 246,
    "inputRowsPerSecond" : 286.37951105937134,
    "processedRowsPerSecond" : 578.8235294117648,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@7fe9b5de",
    "numOutputRows" : 53
  }
}
26/02/13 11:56:11 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 11:56:16 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/91 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.91.ed456893-42d7-4aab-8027-2b9a4d593d2a.tmp
26/02/13 11:56:16 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.91.ed456893-42d7-4aab-8027-2b9a4d593d2a.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/91
26/02/13 11:56:16 INFO MicroBatchExecution: Committed offsets for batch 91. Metadata OffsetSeqMetadata(0,1770983776259,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 11:56:16 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:56:16 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:56:16 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:56:16 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:56:16 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:56:16 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:56:16 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#79665 - origin_code.nullCount#79664) > 0)
26/02/13 11:56:16 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#79670 - destination_code.nullCount#79669) > 0)
26/02/13 11:56:16 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#79700 - callsign.nullCount#79699) > 0)
26/02/13 11:56:16 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:56:16 INFO DAGScheduler: Got job 191 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 11:56:16 INFO DAGScheduler: Final stage: ResultStage 290 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:56:16 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 289)
26/02/13 11:56:16 INFO DAGScheduler: Missing parents: List()
26/02/13 11:56:16 INFO DAGScheduler: Submitting ResultStage 290 (MapPartitionsRDD[1044] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:56:16 INFO MemoryStore: Block broadcast_286 stored as values in memory (estimated size 99.3 KiB, free 434.0 MiB)
26/02/13 11:56:16 INFO MemoryStore: Block broadcast_286_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 433.9 MiB)
26/02/13 11:56:16 INFO BlockManagerInfo: Added broadcast_286_piece0 in memory on spark-master:40291 (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 11:56:16 INFO SparkContext: Created broadcast 286 from broadcast at DAGScheduler.scala:1585
26/02/13 11:56:16 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 290 (MapPartitionsRDD[1044] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 11:56:16 INFO TaskSchedulerImpl: Adding task set 290.0 with 4 tasks resource profile 0
26/02/13 11:56:16 INFO TaskSetManager: Starting task 0.0 in stage 290.0 (TID 642) (172.18.0.15, executor 0, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:56:16 INFO TaskSetManager: Starting task 1.0 in stage 290.0 (TID 643) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:56:16 INFO BlockManagerInfo: Added broadcast_286_piece0 in memory on 172.18.0.15:34959 (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 11:56:16 INFO TaskSetManager: Starting task 2.0 in stage 290.0 (TID 644) (172.18.0.15, executor 0, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:56:16 INFO TaskSetManager: Finished task 1.0 in stage 290.0 (TID 643) in 14 ms on 172.18.0.15 (executor 0) (1/4)
26/02/13 11:56:16 INFO TaskSetManager: Starting task 3.0 in stage 290.0 (TID 645) (172.18.0.15, executor 0, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:56:16 INFO TaskSetManager: Finished task 0.0 in stage 290.0 (TID 642) in 18 ms on 172.18.0.15 (executor 0) (2/4)
26/02/13 11:56:16 INFO TaskSetManager: Finished task 2.0 in stage 290.0 (TID 644) in 10 ms on 172.18.0.15 (executor 0) (3/4)
26/02/13 11:56:16 INFO TaskSetManager: Finished task 3.0 in stage 290.0 (TID 645) in 18 ms on 172.18.0.15 (executor 0) (4/4)
26/02/13 11:56:16 INFO TaskSchedulerImpl: Removed TaskSet 290.0, whose tasks have all completed, from pool 
26/02/13 11:56:16 INFO DAGScheduler: ResultStage 290 (start at NativeMethodAccessorImpl.java:0) finished in 0.040 s
26/02/13 11:56:16 INFO DAGScheduler: Job 191 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:56:16 INFO TaskSchedulerImpl: Killing all running tasks in stage 290: Stage finished
26/02/13 11:56:16 INFO DAGScheduler: Job 191 finished: start at NativeMethodAccessorImpl.java:0, took 0.041669 s
26/02/13 11:56:16 INFO MemoryStore: Block broadcast_287_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.8 MiB)
26/02/13 11:56:16 INFO BlockManagerInfo: Added broadcast_287_piece0 in memory on spark-master:40291 (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 11:56:16 INFO SparkContext: Created broadcast 287 from start at NativeMethodAccessorImpl.java:0
26/02/13 11:56:16 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 91, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@6ad2e4ed]. The input RDD has 3 partitions.
26/02/13 11:56:16 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:56:16 INFO DAGScheduler: Got job 192 (start at NativeMethodAccessorImpl.java:0) with 3 output partitions
26/02/13 11:56:16 INFO DAGScheduler: Final stage: ResultStage 291 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:56:16 INFO DAGScheduler: Parents of final stage: List()
26/02/13 11:56:16 INFO DAGScheduler: Missing parents: List()
26/02/13 11:56:16 INFO DAGScheduler: Submitting ResultStage 291 (MapPartitionsRDD[1050] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:56:16 INFO MemoryStore: Block broadcast_288 stored as values in memory (estimated size 49.9 KiB, free 433.8 MiB)
26/02/13 11:56:16 INFO MemoryStore: Block broadcast_288_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.8 MiB)
26/02/13 11:56:16 INFO BlockManagerInfo: Added broadcast_288_piece0 in memory on spark-master:40291 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 11:56:16 INFO SparkContext: Created broadcast 288 from broadcast at DAGScheduler.scala:1585
26/02/13 11:56:16 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 291 (MapPartitionsRDD[1050] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
26/02/13 11:56:16 INFO TaskSchedulerImpl: Adding task set 291.0 with 3 tasks resource profile 0
26/02/13 11:56:16 INFO TaskSetManager: Starting task 1.0 in stage 291.0 (TID 646) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:56:16 INFO TaskSetManager: Starting task 0.0 in stage 291.0 (TID 647) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:56:16 INFO TaskSetManager: Starting task 2.0 in stage 291.0 (TID 648) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:56:16 INFO BlockManagerInfo: Added broadcast_288_piece0 in memory on 172.18.0.15:34959 (size: 19.5 KiB, free: 434.0 MiB)
26/02/13 11:56:16 INFO BlockManagerInfo: Added broadcast_288_piece0 in memory on 172.18.0.14:40939 (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 11:56:16 INFO BlockManagerInfo: Added broadcast_287_piece0 in memory on 172.18.0.15:34959 (size: 107.2 KiB, free: 433.9 MiB)
26/02/13 11:56:16 INFO BlockManagerInfo: Added broadcast_287_piece0 in memory on 172.18.0.14:40939 (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 11:56:17 INFO TaskSetManager: Finished task 1.0 in stage 291.0 (TID 646) in 551 ms on 172.18.0.15 (executor 0) (1/3)
26/02/13 11:56:17 INFO TaskSetManager: Finished task 0.0 in stage 291.0 (TID 647) in 560 ms on 172.18.0.14 (executor 1) (2/3)
26/02/13 11:56:17 INFO TaskSetManager: Finished task 2.0 in stage 291.0 (TID 648) in 561 ms on 172.18.0.14 (executor 1) (3/3)
26/02/13 11:56:17 INFO TaskSchedulerImpl: Removed TaskSet 291.0, whose tasks have all completed, from pool 
26/02/13 11:56:17 INFO DAGScheduler: ResultStage 291 (start at NativeMethodAccessorImpl.java:0) finished in 0.567 s
26/02/13 11:56:17 INFO DAGScheduler: Job 192 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:56:17 INFO TaskSchedulerImpl: Killing all running tasks in stage 291: Stage finished
26/02/13 11:56:17 INFO DAGScheduler: Job 192 finished: start at NativeMethodAccessorImpl.java:0, took 0.568906 s
26/02/13 11:56:17 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 91, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@6ad2e4ed] is committing.
26/02/13 11:56:17 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 91, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@6ad2e4ed] committed.
26/02/13 11:56:17 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/91 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.91.180e05ae-0bee-488b-92d7-a88d5413408d.tmp
26/02/13 11:56:17 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.91.180e05ae-0bee-488b-92d7-a88d5413408d.tmp to file:/tmp/spark-checkpoint-enrichment/commits/91
26/02/13 11:56:17 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "fc5c6452-bd70-4c90-becf-772d3bebdb57",
  "name" : null,
  "timestamp" : "2026-02-13T11:56:16.257Z",
  "batchId" : 91,
  "numInputRows" : 100,
  "inputRowsPerSecond" : 8333.333333333334,
  "processedRowsPerSecond" : 116.14401858304298,
  "durationMs" : {
    "addBatch" : 687,
    "commitOffsets" : 67,
    "getBatch" : 0,
    "latestOffset" : 2,
    "queryPlanning" : 35,
    "triggerExecution" : 861,
    "walCommit" : 70
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 6355,
        "1" : 7001,
        "0" : 8406
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 6388,
        "1" : 7038,
        "0" : 8436
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 6388,
        "1" : 7038,
        "0" : 8436
      }
    },
    "numInputRows" : 100,
    "inputRowsPerSecond" : 8333.333333333334,
    "processedRowsPerSecond" : 116.14401858304298,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@7fe9b5de",
    "numOutputRows" : 24
  }
}
26/02/13 11:56:17 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/92 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.92.768977d0-d594-4a50-818c-9600d217da4a.tmp
26/02/13 11:56:17 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.92.768977d0-d594-4a50-818c-9600d217da4a.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/92
26/02/13 11:56:17 INFO MicroBatchExecution: Committed offsets for batch 92. Metadata OffsetSeqMetadata(0,1770983777120,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 11:56:17 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:56:17 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:56:17 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:56:17 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:56:17 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:56:17 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:56:17 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#80519 - origin_code.nullCount#80518) > 0)
26/02/13 11:56:17 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#80524 - destination_code.nullCount#80523) > 0)
26/02/13 11:56:17 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#80554 - callsign.nullCount#80553) > 0)
26/02/13 11:56:17 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:56:17 INFO DAGScheduler: Got job 193 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 11:56:17 INFO DAGScheduler: Final stage: ResultStage 293 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:56:17 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 292)
26/02/13 11:56:17 INFO DAGScheduler: Missing parents: List()
26/02/13 11:56:17 INFO DAGScheduler: Submitting ResultStage 293 (MapPartitionsRDD[1055] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:56:17 INFO MemoryStore: Block broadcast_289 stored as values in memory (estimated size 99.3 KiB, free 433.7 MiB)
26/02/13 11:56:17 INFO MemoryStore: Block broadcast_289_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 433.6 MiB)
26/02/13 11:56:17 INFO BlockManagerInfo: Added broadcast_289_piece0 in memory on spark-master:40291 (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 11:56:17 INFO SparkContext: Created broadcast 289 from broadcast at DAGScheduler.scala:1585
26/02/13 11:56:17 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 293 (MapPartitionsRDD[1055] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 11:56:17 INFO TaskSchedulerImpl: Adding task set 293.0 with 4 tasks resource profile 0
26/02/13 11:56:17 INFO TaskSetManager: Starting task 0.0 in stage 293.0 (TID 649) (172.18.0.15, executor 0, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:56:17 INFO TaskSetManager: Starting task 1.0 in stage 293.0 (TID 650) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:56:17 INFO BlockManagerInfo: Added broadcast_289_piece0 in memory on 172.18.0.15:34959 (size: 33.3 KiB, free: 433.9 MiB)
26/02/13 11:56:17 INFO TaskSetManager: Starting task 2.0 in stage 293.0 (TID 651) (172.18.0.15, executor 0, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:56:17 INFO TaskSetManager: Finished task 0.0 in stage 293.0 (TID 649) in 12 ms on 172.18.0.15 (executor 0) (1/4)
26/02/13 11:56:17 INFO TaskSetManager: Starting task 3.0 in stage 293.0 (TID 652) (172.18.0.15, executor 0, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:56:17 INFO TaskSetManager: Finished task 1.0 in stage 293.0 (TID 650) in 18 ms on 172.18.0.15 (executor 0) (2/4)
26/02/13 11:56:17 INFO TaskSetManager: Finished task 2.0 in stage 293.0 (TID 651) in 9 ms on 172.18.0.15 (executor 0) (3/4)
26/02/13 11:56:17 INFO TaskSetManager: Finished task 3.0 in stage 293.0 (TID 652) in 15 ms on 172.18.0.15 (executor 0) (4/4)
26/02/13 11:56:17 INFO TaskSchedulerImpl: Removed TaskSet 293.0, whose tasks have all completed, from pool 
26/02/13 11:56:17 INFO DAGScheduler: ResultStage 293 (start at NativeMethodAccessorImpl.java:0) finished in 0.040 s
26/02/13 11:56:17 INFO DAGScheduler: Job 193 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:56:17 INFO TaskSchedulerImpl: Killing all running tasks in stage 293: Stage finished
26/02/13 11:56:17 INFO DAGScheduler: Job 193 finished: start at NativeMethodAccessorImpl.java:0, took 0.041333 s
26/02/13 11:56:17 INFO BlockManagerInfo: Removed broadcast_289_piece0 on spark-master:40291 in memory (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 11:56:17 INFO BlockManagerInfo: Removed broadcast_289_piece0 on 172.18.0.15:34959 in memory (size: 33.3 KiB, free: 433.9 MiB)
26/02/13 11:56:17 INFO MemoryStore: Block broadcast_290_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.6 MiB)
26/02/13 11:56:17 INFO BlockManagerInfo: Added broadcast_290_piece0 in memory on spark-master:40291 (size: 107.2 KiB, free: 434.0 MiB)
26/02/13 11:56:17 INFO SparkContext: Created broadcast 290 from start at NativeMethodAccessorImpl.java:0
26/02/13 11:56:17 INFO BlockManagerInfo: Removed broadcast_287_piece0 on spark-master:40291 in memory (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 11:56:17 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 92, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@41cd3164]. The input RDD has 3 partitions.
26/02/13 11:56:17 INFO BlockManagerInfo: Removed broadcast_287_piece0 on 172.18.0.15:34959 in memory (size: 107.2 KiB, free: 434.0 MiB)
26/02/13 11:56:17 INFO BlockManagerInfo: Removed broadcast_287_piece0 on 172.18.0.14:40939 in memory (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 11:56:17 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:56:17 INFO DAGScheduler: Got job 194 (start at NativeMethodAccessorImpl.java:0) with 3 output partitions
26/02/13 11:56:17 INFO DAGScheduler: Final stage: ResultStage 294 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:56:17 INFO DAGScheduler: Parents of final stage: List()
26/02/13 11:56:17 INFO DAGScheduler: Missing parents: List()
26/02/13 11:56:17 INFO DAGScheduler: Submitting ResultStage 294 (MapPartitionsRDD[1061] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:56:17 INFO MemoryStore: Block broadcast_291 stored as values in memory (estimated size 49.9 KiB, free 433.7 MiB)
26/02/13 11:56:17 INFO MemoryStore: Block broadcast_291_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.7 MiB)
26/02/13 11:56:17 INFO BlockManagerInfo: Added broadcast_291_piece0 in memory on spark-master:40291 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 11:56:17 INFO BlockManagerInfo: Removed broadcast_286_piece0 on spark-master:40291 in memory (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 11:56:17 INFO SparkContext: Created broadcast 291 from broadcast at DAGScheduler.scala:1585
26/02/13 11:56:17 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 294 (MapPartitionsRDD[1061] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
26/02/13 11:56:17 INFO TaskSchedulerImpl: Adding task set 294.0 with 3 tasks resource profile 0
26/02/13 11:56:17 INFO TaskSetManager: Starting task 0.0 in stage 294.0 (TID 653) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:56:17 INFO TaskSetManager: Starting task 1.0 in stage 294.0 (TID 654) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:56:17 INFO TaskSetManager: Starting task 2.0 in stage 294.0 (TID 655) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:56:17 INFO BlockManagerInfo: Removed broadcast_286_piece0 on 172.18.0.15:34959 in memory (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 11:56:17 INFO BlockManagerInfo: Removed broadcast_288_piece0 on spark-master:40291 in memory (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 11:56:17 INFO BlockManagerInfo: Added broadcast_291_piece0 in memory on 172.18.0.14:40939 (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 11:56:17 INFO BlockManagerInfo: Added broadcast_291_piece0 in memory on 172.18.0.15:34959 (size: 19.5 KiB, free: 434.0 MiB)
26/02/13 11:56:17 INFO BlockManagerInfo: Removed broadcast_288_piece0 on 172.18.0.15:34959 in memory (size: 19.5 KiB, free: 434.0 MiB)
26/02/13 11:56:17 INFO BlockManagerInfo: Removed broadcast_288_piece0 on 172.18.0.14:40939 in memory (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 11:56:17 INFO BlockManagerInfo: Added broadcast_290_piece0 in memory on 172.18.0.14:40939 (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 11:56:17 INFO BlockManagerInfo: Added broadcast_290_piece0 in memory on 172.18.0.15:34959 (size: 107.2 KiB, free: 433.9 MiB)
26/02/13 11:56:17 INFO TaskSetManager: Finished task 2.0 in stage 294.0 (TID 655) in 65 ms on 172.18.0.14 (executor 1) (1/3)
26/02/13 11:56:17 INFO TaskSetManager: Finished task 0.0 in stage 294.0 (TID 653) in 66 ms on 172.18.0.14 (executor 1) (2/3)
26/02/13 11:56:17 INFO TaskSetManager: Finished task 1.0 in stage 294.0 (TID 654) in 74 ms on 172.18.0.15 (executor 0) (3/3)
26/02/13 11:56:17 INFO TaskSchedulerImpl: Removed TaskSet 294.0, whose tasks have all completed, from pool 
26/02/13 11:56:17 INFO DAGScheduler: ResultStage 294 (start at NativeMethodAccessorImpl.java:0) finished in 0.083 s
26/02/13 11:56:17 INFO DAGScheduler: Job 194 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:56:17 INFO TaskSchedulerImpl: Killing all running tasks in stage 294: Stage finished
26/02/13 11:56:17 INFO DAGScheduler: Job 194 finished: start at NativeMethodAccessorImpl.java:0, took 0.088735 s
26/02/13 11:56:17 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 92, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@41cd3164] is committing.
26/02/13 11:56:17 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 92, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@41cd3164] committed.
26/02/13 11:56:17 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/92 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.92.c8dd295d-217f-49de-a179-45222764f1d0.tmp
26/02/13 11:56:17 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.92.c8dd295d-217f-49de-a179-45222764f1d0.tmp to file:/tmp/spark-checkpoint-enrichment/commits/92
26/02/13 11:56:17 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "fc5c6452-bd70-4c90-becf-772d3bebdb57",
  "name" : null,
  "timestamp" : "2026-02-13T11:56:17.119Z",
  "batchId" : 92,
  "numInputRows" : 152,
  "inputRowsPerSecond" : 176.33410672853827,
  "processedRowsPerSecond" : 398.9501312335958,
  "durationMs" : {
    "addBatch" : 216,
    "commitOffsets" : 80,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 25,
    "triggerExecution" : 381,
    "walCommit" : 58
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 6388,
        "1" : 7038,
        "0" : 8436
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 6435,
        "1" : 7098,
        "0" : 8481
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 6435,
        "1" : 7098,
        "0" : 8481
      }
    },
    "numInputRows" : 152,
    "inputRowsPerSecond" : 176.33410672853827,
    "processedRowsPerSecond" : 398.9501312335958,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@7fe9b5de",
    "numOutputRows" : 31
  }
}
26/02/13 11:56:17 INFO BlockManagerInfo: Removed broadcast_291_piece0 on spark-master:40291 in memory (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 11:56:17 INFO BlockManagerInfo: Removed broadcast_291_piece0 on 172.18.0.15:34959 in memory (size: 19.5 KiB, free: 434.0 MiB)
26/02/13 11:56:17 INFO BlockManagerInfo: Removed broadcast_291_piece0 on 172.18.0.14:40939 in memory (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 11:56:27 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 11:56:31 INFO BlockManagerInfo: Removed broadcast_284_piece0 on spark-master:40291 in memory (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 11:56:31 INFO BlockManagerInfo: Removed broadcast_284_piece0 on 172.18.0.14:40939 in memory (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 11:56:31 INFO BlockManagerInfo: Removed broadcast_284_piece0 on 172.18.0.15:34959 in memory (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 11:56:32 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/93 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.93.2fe5af74-1407-4f07-b515-d8bcf7ccd0ac.tmp
26/02/13 11:56:32 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.93.2fe5af74-1407-4f07-b515-d8bcf7ccd0ac.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/93
26/02/13 11:56:32 INFO MicroBatchExecution: Committed offsets for batch 93. Metadata OffsetSeqMetadata(0,1770983792518,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 11:56:32 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:56:32 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:56:32 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:56:32 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:56:32 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:56:32 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:56:32 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#81373 - origin_code.nullCount#81372) > 0)
26/02/13 11:56:32 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#81378 - destination_code.nullCount#81377) > 0)
26/02/13 11:56:32 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#81408 - callsign.nullCount#81407) > 0)
26/02/13 11:56:32 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:56:32 INFO DAGScheduler: Got job 195 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 11:56:32 INFO DAGScheduler: Final stage: ResultStage 296 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:56:32 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 295)
26/02/13 11:56:32 INFO DAGScheduler: Missing parents: List()
26/02/13 11:56:32 INFO DAGScheduler: Submitting ResultStage 296 (MapPartitionsRDD[1066] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:56:32 INFO MemoryStore: Block broadcast_292 stored as values in memory (estimated size 99.3 KiB, free 434.0 MiB)
26/02/13 11:56:32 INFO MemoryStore: Block broadcast_292_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 433.9 MiB)
26/02/13 11:56:32 INFO BlockManagerInfo: Added broadcast_292_piece0 in memory on spark-master:40291 (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 11:56:32 INFO SparkContext: Created broadcast 292 from broadcast at DAGScheduler.scala:1585
26/02/13 11:56:32 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 296 (MapPartitionsRDD[1066] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 11:56:32 INFO TaskSchedulerImpl: Adding task set 296.0 with 4 tasks resource profile 0
26/02/13 11:56:32 INFO TaskSetManager: Starting task 0.0 in stage 296.0 (TID 656) (172.18.0.15, executor 0, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:56:32 INFO TaskSetManager: Starting task 1.0 in stage 296.0 (TID 657) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:56:32 INFO BlockManagerInfo: Added broadcast_292_piece0 in memory on 172.18.0.15:34959 (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 11:56:32 INFO TaskSetManager: Starting task 2.0 in stage 296.0 (TID 658) (172.18.0.15, executor 0, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:56:32 INFO TaskSetManager: Finished task 1.0 in stage 296.0 (TID 657) in 25 ms on 172.18.0.15 (executor 0) (1/4)
26/02/13 11:56:32 INFO TaskSetManager: Starting task 3.0 in stage 296.0 (TID 659) (172.18.0.15, executor 0, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:56:32 INFO TaskSetManager: Finished task 0.0 in stage 296.0 (TID 656) in 27 ms on 172.18.0.15 (executor 0) (2/4)
26/02/13 11:56:32 INFO TaskSetManager: Finished task 2.0 in stage 296.0 (TID 658) in 20 ms on 172.18.0.15 (executor 0) (3/4)
26/02/13 11:56:32 INFO TaskSetManager: Finished task 3.0 in stage 296.0 (TID 659) in 21 ms on 172.18.0.15 (executor 0) (4/4)
26/02/13 11:56:32 INFO TaskSchedulerImpl: Removed TaskSet 296.0, whose tasks have all completed, from pool 
26/02/13 11:56:32 INFO DAGScheduler: ResultStage 296 (start at NativeMethodAccessorImpl.java:0) finished in 0.052 s
26/02/13 11:56:32 INFO DAGScheduler: Job 195 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:56:32 INFO TaskSchedulerImpl: Killing all running tasks in stage 296: Stage finished
26/02/13 11:56:32 INFO DAGScheduler: Job 195 finished: start at NativeMethodAccessorImpl.java:0, took 0.053697 s
26/02/13 11:56:32 INFO MemoryStore: Block broadcast_293_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.8 MiB)
26/02/13 11:56:32 INFO BlockManagerInfo: Added broadcast_293_piece0 in memory on spark-master:40291 (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 11:56:32 INFO SparkContext: Created broadcast 293 from start at NativeMethodAccessorImpl.java:0
26/02/13 11:56:32 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 93, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@1ff36ddf]. The input RDD has 3 partitions.
26/02/13 11:56:32 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:56:32 INFO DAGScheduler: Got job 196 (start at NativeMethodAccessorImpl.java:0) with 3 output partitions
26/02/13 11:56:32 INFO DAGScheduler: Final stage: ResultStage 297 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:56:32 INFO DAGScheduler: Parents of final stage: List()
26/02/13 11:56:32 INFO DAGScheduler: Missing parents: List()
26/02/13 11:56:32 INFO DAGScheduler: Submitting ResultStage 297 (MapPartitionsRDD[1072] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:56:32 INFO MemoryStore: Block broadcast_294 stored as values in memory (estimated size 49.9 KiB, free 433.8 MiB)
26/02/13 11:56:32 INFO MemoryStore: Block broadcast_294_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.8 MiB)
26/02/13 11:56:32 INFO BlockManagerInfo: Added broadcast_294_piece0 in memory on spark-master:40291 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 11:56:32 INFO SparkContext: Created broadcast 294 from broadcast at DAGScheduler.scala:1585
26/02/13 11:56:32 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 297 (MapPartitionsRDD[1072] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
26/02/13 11:56:32 INFO TaskSchedulerImpl: Adding task set 297.0 with 3 tasks resource profile 0
26/02/13 11:56:32 INFO TaskSetManager: Starting task 0.0 in stage 297.0 (TID 660) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:56:32 INFO TaskSetManager: Starting task 1.0 in stage 297.0 (TID 661) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:56:32 INFO TaskSetManager: Starting task 2.0 in stage 297.0 (TID 662) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:56:32 INFO BlockManagerInfo: Added broadcast_294_piece0 in memory on 172.18.0.15:34959 (size: 19.5 KiB, free: 434.0 MiB)
26/02/13 11:56:32 INFO BlockManagerInfo: Added broadcast_294_piece0 in memory on 172.18.0.14:40939 (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 11:56:32 INFO BlockManagerInfo: Added broadcast_293_piece0 in memory on 172.18.0.15:34959 (size: 107.2 KiB, free: 433.9 MiB)
26/02/13 11:56:32 INFO BlockManagerInfo: Added broadcast_293_piece0 in memory on 172.18.0.14:40939 (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 11:56:33 INFO TaskSetManager: Finished task 1.0 in stage 297.0 (TID 661) in 553 ms on 172.18.0.15 (executor 0) (1/3)
26/02/13 11:56:33 INFO TaskSetManager: Finished task 2.0 in stage 297.0 (TID 662) in 572 ms on 172.18.0.14 (executor 1) (2/3)
26/02/13 11:56:33 INFO TaskSetManager: Finished task 0.0 in stage 297.0 (TID 660) in 573 ms on 172.18.0.14 (executor 1) (3/3)
26/02/13 11:56:33 INFO TaskSchedulerImpl: Removed TaskSet 297.0, whose tasks have all completed, from pool 
26/02/13 11:56:33 INFO DAGScheduler: ResultStage 297 (start at NativeMethodAccessorImpl.java:0) finished in 0.578 s
26/02/13 11:56:33 INFO DAGScheduler: Job 196 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:56:33 INFO TaskSchedulerImpl: Killing all running tasks in stage 297: Stage finished
26/02/13 11:56:33 INFO DAGScheduler: Job 196 finished: start at NativeMethodAccessorImpl.java:0, took 0.579187 s
26/02/13 11:56:33 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 93, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@1ff36ddf] is committing.
26/02/13 11:56:33 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 93, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@1ff36ddf] committed.
26/02/13 11:56:33 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/93 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.93.923552ba-bfd6-42c2-8cd1-31ed3ab5d6ba.tmp
26/02/13 11:56:33 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.93.923552ba-bfd6-42c2-8cd1-31ed3ab5d6ba.tmp to file:/tmp/spark-checkpoint-enrichment/commits/93
26/02/13 11:56:33 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "fc5c6452-bd70-4c90-becf-772d3bebdb57",
  "name" : null,
  "timestamp" : "2026-02-13T11:56:32.516Z",
  "batchId" : 93,
  "numInputRows" : 46,
  "inputRowsPerSecond" : 3833.333333333333,
  "processedRowsPerSecond" : 49.09284951974386,
  "durationMs" : {
    "addBatch" : 712,
    "commitOffsets" : 60,
    "getBatch" : 0,
    "latestOffset" : 2,
    "queryPlanning" : 45,
    "triggerExecution" : 937,
    "walCommit" : 117
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 6435,
        "1" : 7098,
        "0" : 8481
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 6449,
        "1" : 7120,
        "0" : 8491
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 6449,
        "1" : 7120,
        "0" : 8491
      }
    },
    "numInputRows" : 46,
    "inputRowsPerSecond" : 3833.333333333333,
    "processedRowsPerSecond" : 49.09284951974386,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@7fe9b5de",
    "numOutputRows" : 12
  }
}
26/02/13 11:56:33 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/94 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.94.bc926c3e-6397-414e-a846-abd9068857dc.tmp
26/02/13 11:56:33 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.94.bc926c3e-6397-414e-a846-abd9068857dc.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/94
26/02/13 11:56:33 INFO MicroBatchExecution: Committed offsets for batch 94. Metadata OffsetSeqMetadata(0,1770983793455,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 11:56:33 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:56:33 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:56:33 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:56:33 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:56:33 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:56:33 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:56:33 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#82227 - origin_code.nullCount#82226) > 0)
26/02/13 11:56:33 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#82232 - destination_code.nullCount#82231) > 0)
26/02/13 11:56:33 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#82262 - callsign.nullCount#82261) > 0)
26/02/13 11:56:33 INFO BlockManagerInfo: Removed broadcast_293_piece0 on spark-master:40291 in memory (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 11:56:33 INFO BlockManagerInfo: Removed broadcast_293_piece0 on 172.18.0.15:34959 in memory (size: 107.2 KiB, free: 434.0 MiB)
26/02/13 11:56:33 INFO BlockManagerInfo: Removed broadcast_293_piece0 on 172.18.0.14:40939 in memory (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 11:56:33 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:56:33 INFO DAGScheduler: Got job 197 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 11:56:33 INFO DAGScheduler: Final stage: ResultStage 299 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:56:33 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 298)
26/02/13 11:56:33 INFO DAGScheduler: Missing parents: List()
26/02/13 11:56:33 INFO DAGScheduler: Submitting ResultStage 299 (MapPartitionsRDD[1077] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:56:33 INFO BlockManagerInfo: Removed broadcast_294_piece0 on spark-master:40291 in memory (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 11:56:33 INFO BlockManagerInfo: Removed broadcast_294_piece0 on 172.18.0.15:34959 in memory (size: 19.5 KiB, free: 434.0 MiB)
26/02/13 11:56:33 INFO BlockManagerInfo: Removed broadcast_294_piece0 on 172.18.0.14:40939 in memory (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 11:56:33 INFO MemoryStore: Block broadcast_295 stored as values in memory (estimated size 99.3 KiB, free 433.8 MiB)
26/02/13 11:56:33 INFO MemoryStore: Block broadcast_295_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 433.9 MiB)
26/02/13 11:56:33 INFO BlockManagerInfo: Removed broadcast_292_piece0 on spark-master:40291 in memory (size: 33.3 KiB, free: 434.3 MiB)
26/02/13 11:56:33 INFO BlockManagerInfo: Added broadcast_295_piece0 in memory on spark-master:40291 (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 11:56:33 INFO SparkContext: Created broadcast 295 from broadcast at DAGScheduler.scala:1585
26/02/13 11:56:33 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 299 (MapPartitionsRDD[1077] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 11:56:33 INFO TaskSchedulerImpl: Adding task set 299.0 with 4 tasks resource profile 0
26/02/13 11:56:33 INFO BlockManagerInfo: Removed broadcast_292_piece0 on 172.18.0.15:34959 in memory (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 11:56:33 INFO TaskSetManager: Starting task 0.0 in stage 299.0 (TID 663) (172.18.0.15, executor 0, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:56:33 INFO TaskSetManager: Starting task 1.0 in stage 299.0 (TID 664) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:56:33 INFO BlockManagerInfo: Added broadcast_295_piece0 in memory on 172.18.0.15:34959 (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 11:56:33 INFO TaskSetManager: Starting task 2.0 in stage 299.0 (TID 665) (172.18.0.15, executor 0, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:56:33 INFO TaskSetManager: Finished task 1.0 in stage 299.0 (TID 664) in 16 ms on 172.18.0.15 (executor 0) (1/4)
26/02/13 11:56:33 INFO TaskSetManager: Starting task 3.0 in stage 299.0 (TID 666) (172.18.0.15, executor 0, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:56:33 INFO TaskSetManager: Finished task 0.0 in stage 299.0 (TID 663) in 16 ms on 172.18.0.15 (executor 0) (2/4)
26/02/13 11:56:33 INFO TaskSetManager: Finished task 3.0 in stage 299.0 (TID 666) in 13 ms on 172.18.0.15 (executor 0) (3/4)
26/02/13 11:56:33 INFO TaskSetManager: Finished task 2.0 in stage 299.0 (TID 665) in 14 ms on 172.18.0.15 (executor 0) (4/4)
26/02/13 11:56:33 INFO TaskSchedulerImpl: Removed TaskSet 299.0, whose tasks have all completed, from pool 
26/02/13 11:56:33 INFO DAGScheduler: ResultStage 299 (start at NativeMethodAccessorImpl.java:0) finished in 0.038 s
26/02/13 11:56:33 INFO DAGScheduler: Job 197 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:56:33 INFO TaskSchedulerImpl: Killing all running tasks in stage 299: Stage finished
26/02/13 11:56:33 INFO DAGScheduler: Job 197 finished: start at NativeMethodAccessorImpl.java:0, took 0.040354 s
26/02/13 11:56:33 INFO MemoryStore: Block broadcast_296_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.8 MiB)
26/02/13 11:56:33 INFO BlockManagerInfo: Added broadcast_296_piece0 in memory on spark-master:40291 (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 11:56:33 INFO SparkContext: Created broadcast 296 from start at NativeMethodAccessorImpl.java:0
26/02/13 11:56:33 INFO BlockManagerInfo: Removed broadcast_295_piece0 on spark-master:40291 in memory (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 11:56:33 INFO BlockManagerInfo: Removed broadcast_295_piece0 on 172.18.0.15:34959 in memory (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 11:56:33 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 94, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@7a393436]. The input RDD has 3 partitions.
26/02/13 11:56:33 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:56:33 INFO DAGScheduler: Got job 198 (start at NativeMethodAccessorImpl.java:0) with 3 output partitions
26/02/13 11:56:33 INFO DAGScheduler: Final stage: ResultStage 300 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:56:33 INFO DAGScheduler: Parents of final stage: List()
26/02/13 11:56:33 INFO DAGScheduler: Missing parents: List()
26/02/13 11:56:33 INFO DAGScheduler: Submitting ResultStage 300 (MapPartitionsRDD[1083] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:56:33 INFO MemoryStore: Block broadcast_297 stored as values in memory (estimated size 49.9 KiB, free 433.9 MiB)
26/02/13 11:56:33 INFO MemoryStore: Block broadcast_297_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.9 MiB)
26/02/13 11:56:33 INFO BlockManagerInfo: Added broadcast_297_piece0 in memory on spark-master:40291 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 11:56:33 INFO SparkContext: Created broadcast 297 from broadcast at DAGScheduler.scala:1585
26/02/13 11:56:33 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 300 (MapPartitionsRDD[1083] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
26/02/13 11:56:33 INFO TaskSchedulerImpl: Adding task set 300.0 with 3 tasks resource profile 0
26/02/13 11:56:33 INFO TaskSetManager: Starting task 0.0 in stage 300.0 (TID 667) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:56:33 INFO TaskSetManager: Starting task 1.0 in stage 300.0 (TID 668) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:56:33 INFO TaskSetManager: Starting task 2.0 in stage 300.0 (TID 669) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:56:33 INFO BlockManagerInfo: Added broadcast_297_piece0 in memory on 172.18.0.15:34959 (size: 19.5 KiB, free: 434.0 MiB)
26/02/13 11:56:33 INFO BlockManagerInfo: Added broadcast_297_piece0 in memory on 172.18.0.14:40939 (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 11:56:33 INFO BlockManagerInfo: Added broadcast_296_piece0 in memory on 172.18.0.15:34959 (size: 107.2 KiB, free: 433.9 MiB)
26/02/13 11:56:33 INFO BlockManagerInfo: Added broadcast_296_piece0 in memory on 172.18.0.14:40939 (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 11:56:33 INFO TaskSetManager: Finished task 2.0 in stage 300.0 (TID 669) in 53 ms on 172.18.0.14 (executor 1) (1/3)
26/02/13 11:56:33 INFO TaskSetManager: Finished task 1.0 in stage 300.0 (TID 668) in 55 ms on 172.18.0.15 (executor 0) (2/3)
26/02/13 11:56:33 INFO TaskSetManager: Finished task 0.0 in stage 300.0 (TID 667) in 55 ms on 172.18.0.14 (executor 1) (3/3)
26/02/13 11:56:33 INFO TaskSchedulerImpl: Removed TaskSet 300.0, whose tasks have all completed, from pool 
26/02/13 11:56:33 INFO DAGScheduler: ResultStage 300 (start at NativeMethodAccessorImpl.java:0) finished in 0.062 s
26/02/13 11:56:33 INFO DAGScheduler: Job 198 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:56:33 INFO TaskSchedulerImpl: Killing all running tasks in stage 300: Stage finished
26/02/13 11:56:33 INFO DAGScheduler: Job 198 finished: start at NativeMethodAccessorImpl.java:0, took 0.063176 s
26/02/13 11:56:33 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 94, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@7a393436] is committing.
26/02/13 11:56:33 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 94, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@7a393436] committed.
26/02/13 11:56:33 INFO BlockManagerInfo: Removed broadcast_290_piece0 on spark-master:40291 in memory (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 11:56:33 INFO BlockManagerInfo: Removed broadcast_290_piece0 on 172.18.0.14:40939 in memory (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 11:56:33 INFO BlockManagerInfo: Removed broadcast_290_piece0 on 172.18.0.15:34959 in memory (size: 107.2 KiB, free: 434.0 MiB)
26/02/13 11:56:33 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/94 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.94.adad371f-dc6a-4b58-9698-1ca5692bc1c3.tmp
26/02/13 11:56:33 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.94.adad371f-dc6a-4b58-9698-1ca5692bc1c3.tmp to file:/tmp/spark-checkpoint-enrichment/commits/94
26/02/13 11:56:33 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "fc5c6452-bd70-4c90-becf-772d3bebdb57",
  "name" : null,
  "timestamp" : "2026-02-13T11:56:33.454Z",
  "batchId" : 94,
  "numInputRows" : 205,
  "inputRowsPerSecond" : 218.55010660980813,
  "processedRowsPerSecond" : 601.1730205278592,
  "durationMs" : {
    "addBatch" : 190,
    "commitOffsets" : 67,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 22,
    "triggerExecution" : 341,
    "walCommit" : 60
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 6449,
        "1" : 7120,
        "0" : 8491
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 6515,
        "1" : 7192,
        "0" : 8558
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 6515,
        "1" : 7192,
        "0" : 8558
      }
    },
    "numInputRows" : 205,
    "inputRowsPerSecond" : 218.55010660980813,
    "processedRowsPerSecond" : 601.1730205278592,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@7fe9b5de",
    "numOutputRows" : 41
  }
}
26/02/13 11:56:43 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 11:56:47 INFO BlockManagerInfo: Removed broadcast_297_piece0 on spark-master:40291 in memory (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 11:56:47 INFO BlockManagerInfo: Removed broadcast_297_piece0 on 172.18.0.15:34959 in memory (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 11:56:47 INFO BlockManagerInfo: Removed broadcast_297_piece0 on 172.18.0.14:40939 in memory (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 11:56:52 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/95 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.95.454d919f-0c54-4ac2-9851-630166f08c51.tmp
26/02/13 11:56:52 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.95.454d919f-0c54-4ac2-9851-630166f08c51.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/95
26/02/13 11:56:52 INFO MicroBatchExecution: Committed offsets for batch 95. Metadata OffsetSeqMetadata(0,1770983812808,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 11:56:52 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:56:52 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:56:52 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:56:52 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:56:52 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:56:52 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:56:52 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#83081 - origin_code.nullCount#83080) > 0)
26/02/13 11:56:52 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#83086 - destination_code.nullCount#83085) > 0)
26/02/13 11:56:52 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#83116 - callsign.nullCount#83115) > 0)
26/02/13 11:56:53 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:56:53 INFO DAGScheduler: Got job 199 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 11:56:53 INFO DAGScheduler: Final stage: ResultStage 302 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:56:53 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 301)
26/02/13 11:56:53 INFO DAGScheduler: Missing parents: List()
26/02/13 11:56:53 INFO DAGScheduler: Submitting ResultStage 302 (MapPartitionsRDD[1088] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:56:53 INFO MemoryStore: Block broadcast_298 stored as values in memory (estimated size 99.3 KiB, free 434.0 MiB)
26/02/13 11:56:53 INFO MemoryStore: Block broadcast_298_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 433.9 MiB)
26/02/13 11:56:53 INFO BlockManagerInfo: Added broadcast_298_piece0 in memory on spark-master:40291 (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 11:56:53 INFO SparkContext: Created broadcast 298 from broadcast at DAGScheduler.scala:1585
26/02/13 11:56:53 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 302 (MapPartitionsRDD[1088] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 11:56:53 INFO TaskSchedulerImpl: Adding task set 302.0 with 4 tasks resource profile 0
26/02/13 11:56:53 INFO TaskSetManager: Starting task 0.0 in stage 302.0 (TID 670) (172.18.0.15, executor 0, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:56:53 INFO TaskSetManager: Starting task 1.0 in stage 302.0 (TID 671) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:56:53 INFO BlockManagerInfo: Added broadcast_298_piece0 in memory on 172.18.0.15:34959 (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 11:56:53 INFO TaskSetManager: Starting task 2.0 in stage 302.0 (TID 672) (172.18.0.15, executor 0, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:56:53 INFO TaskSetManager: Finished task 0.0 in stage 302.0 (TID 670) in 18 ms on 172.18.0.15 (executor 0) (1/4)
26/02/13 11:56:53 INFO TaskSetManager: Starting task 3.0 in stage 302.0 (TID 673) (172.18.0.15, executor 0, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:56:53 INFO TaskSetManager: Finished task 1.0 in stage 302.0 (TID 671) in 24 ms on 172.18.0.15 (executor 0) (2/4)
26/02/13 11:56:53 INFO TaskSetManager: Finished task 2.0 in stage 302.0 (TID 672) in 13 ms on 172.18.0.15 (executor 0) (3/4)
26/02/13 11:56:53 INFO TaskSetManager: Finished task 3.0 in stage 302.0 (TID 673) in 26 ms on 172.18.0.15 (executor 0) (4/4)
26/02/13 11:56:53 INFO TaskSchedulerImpl: Removed TaskSet 302.0, whose tasks have all completed, from pool 
26/02/13 11:56:53 INFO DAGScheduler: ResultStage 302 (start at NativeMethodAccessorImpl.java:0) finished in 0.060 s
26/02/13 11:56:53 INFO DAGScheduler: Job 199 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:56:53 INFO TaskSchedulerImpl: Killing all running tasks in stage 302: Stage finished
26/02/13 11:56:53 INFO DAGScheduler: Job 199 finished: start at NativeMethodAccessorImpl.java:0, took 0.062729 s
26/02/13 11:56:53 INFO MemoryStore: Block broadcast_299_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.8 MiB)
26/02/13 11:56:53 INFO BlockManagerInfo: Added broadcast_299_piece0 in memory on spark-master:40291 (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 11:56:53 INFO SparkContext: Created broadcast 299 from start at NativeMethodAccessorImpl.java:0
26/02/13 11:56:53 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 95, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@57b8df18]. The input RDD has 3 partitions.
26/02/13 11:56:53 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:56:53 INFO DAGScheduler: Got job 200 (start at NativeMethodAccessorImpl.java:0) with 3 output partitions
26/02/13 11:56:53 INFO DAGScheduler: Final stage: ResultStage 303 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:56:53 INFO DAGScheduler: Parents of final stage: List()
26/02/13 11:56:53 INFO DAGScheduler: Missing parents: List()
26/02/13 11:56:53 INFO DAGScheduler: Submitting ResultStage 303 (MapPartitionsRDD[1094] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:56:53 INFO MemoryStore: Block broadcast_300 stored as values in memory (estimated size 49.9 KiB, free 433.8 MiB)
26/02/13 11:56:53 INFO MemoryStore: Block broadcast_300_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.8 MiB)
26/02/13 11:56:53 INFO BlockManagerInfo: Added broadcast_300_piece0 in memory on spark-master:40291 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 11:56:53 INFO SparkContext: Created broadcast 300 from broadcast at DAGScheduler.scala:1585
26/02/13 11:56:53 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 303 (MapPartitionsRDD[1094] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
26/02/13 11:56:53 INFO TaskSchedulerImpl: Adding task set 303.0 with 3 tasks resource profile 0
26/02/13 11:56:53 INFO TaskSetManager: Starting task 1.0 in stage 303.0 (TID 674) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:56:53 INFO TaskSetManager: Starting task 0.0 in stage 303.0 (TID 675) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:56:53 INFO TaskSetManager: Starting task 2.0 in stage 303.0 (TID 676) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:56:53 INFO BlockManagerInfo: Added broadcast_300_piece0 in memory on 172.18.0.14:40939 (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 11:56:53 INFO BlockManagerInfo: Added broadcast_300_piece0 in memory on 172.18.0.15:34959 (size: 19.5 KiB, free: 434.0 MiB)
26/02/13 11:56:53 INFO BlockManagerInfo: Added broadcast_299_piece0 in memory on 172.18.0.15:34959 (size: 107.2 KiB, free: 433.9 MiB)
26/02/13 11:56:53 INFO BlockManagerInfo: Added broadcast_299_piece0 in memory on 172.18.0.14:40939 (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 11:56:53 INFO TaskSetManager: Finished task 2.0 in stage 303.0 (TID 676) in 551 ms on 172.18.0.14 (executor 1) (1/3)
26/02/13 11:56:53 INFO TaskSetManager: Finished task 1.0 in stage 303.0 (TID 674) in 556 ms on 172.18.0.15 (executor 0) (2/3)
26/02/13 11:56:53 INFO TaskSetManager: Finished task 0.0 in stage 303.0 (TID 675) in 556 ms on 172.18.0.14 (executor 1) (3/3)
26/02/13 11:56:53 INFO TaskSchedulerImpl: Removed TaskSet 303.0, whose tasks have all completed, from pool 
26/02/13 11:56:53 INFO DAGScheduler: ResultStage 303 (start at NativeMethodAccessorImpl.java:0) finished in 0.563 s
26/02/13 11:56:53 INFO DAGScheduler: Job 200 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:56:53 INFO TaskSchedulerImpl: Killing all running tasks in stage 303: Stage finished
26/02/13 11:56:53 INFO DAGScheduler: Job 200 finished: start at NativeMethodAccessorImpl.java:0, took 0.565037 s
26/02/13 11:56:53 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 95, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@57b8df18] is committing.
26/02/13 11:56:53 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 95, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@57b8df18] committed.
26/02/13 11:56:53 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/95 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.95.f142ef01-d50b-4bcd-8351-b339591963a8.tmp
26/02/13 11:56:53 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.95.f142ef01-d50b-4bcd-8351-b339591963a8.tmp to file:/tmp/spark-checkpoint-enrichment/commits/95
26/02/13 11:56:53 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "fc5c6452-bd70-4c90-becf-772d3bebdb57",
  "name" : null,
  "timestamp" : "2026-02-13T11:56:52.806Z",
  "batchId" : 95,
  "numInputRows" : 92,
  "inputRowsPerSecond" : 7666.666666666666,
  "processedRowsPerSecond" : 100.76670317634172,
  "durationMs" : {
    "addBatch" : 712,
    "commitOffsets" : 65,
    "getBatch" : 0,
    "latestOffset" : 2,
    "queryPlanning" : 24,
    "triggerExecution" : 913,
    "walCommit" : 110
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 6515,
        "1" : 7192,
        "0" : 8558
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 6547,
        "1" : 7226,
        "0" : 8584
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 6547,
        "1" : 7226,
        "0" : 8584
      }
    },
    "numInputRows" : 92,
    "inputRowsPerSecond" : 7666.666666666666,
    "processedRowsPerSecond" : 100.76670317634172,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@7fe9b5de",
    "numOutputRows" : 22
  }
}
26/02/13 11:56:53 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/96 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.96.e21ce06c-c2a8-4d1a-93fb-af0cbeac98f1.tmp
26/02/13 11:56:53 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.96.e21ce06c-c2a8-4d1a-93fb-af0cbeac98f1.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/96
26/02/13 11:56:53 INFO MicroBatchExecution: Committed offsets for batch 96. Metadata OffsetSeqMetadata(0,1770983813721,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 11:56:53 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:56:53 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:56:53 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:56:53 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:56:53 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:56:53 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:56:53 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#83935 - origin_code.nullCount#83934) > 0)
26/02/13 11:56:53 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#83940 - destination_code.nullCount#83939) > 0)
26/02/13 11:56:53 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#83970 - callsign.nullCount#83969) > 0)
26/02/13 11:56:53 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:56:53 INFO DAGScheduler: Got job 201 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 11:56:53 INFO DAGScheduler: Final stage: ResultStage 305 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:56:53 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 304)
26/02/13 11:56:53 INFO DAGScheduler: Missing parents: List()
26/02/13 11:56:53 INFO DAGScheduler: Submitting ResultStage 305 (MapPartitionsRDD[1099] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:56:53 INFO MemoryStore: Block broadcast_301 stored as values in memory (estimated size 99.3 KiB, free 433.7 MiB)
26/02/13 11:56:53 INFO MemoryStore: Block broadcast_301_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 433.6 MiB)
26/02/13 11:56:53 INFO BlockManagerInfo: Added broadcast_301_piece0 in memory on spark-master:40291 (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 11:56:53 INFO SparkContext: Created broadcast 301 from broadcast at DAGScheduler.scala:1585
26/02/13 11:56:53 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 305 (MapPartitionsRDD[1099] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 11:56:53 INFO TaskSchedulerImpl: Adding task set 305.0 with 4 tasks resource profile 0
26/02/13 11:56:53 INFO TaskSetManager: Starting task 0.0 in stage 305.0 (TID 677) (172.18.0.15, executor 0, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:56:53 INFO TaskSetManager: Starting task 1.0 in stage 305.0 (TID 678) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:56:53 INFO BlockManagerInfo: Added broadcast_301_piece0 in memory on 172.18.0.15:34959 (size: 33.3 KiB, free: 433.9 MiB)
26/02/13 11:56:53 INFO TaskSetManager: Starting task 2.0 in stage 305.0 (TID 679) (172.18.0.15, executor 0, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:56:53 INFO TaskSetManager: Finished task 0.0 in stage 305.0 (TID 677) in 18 ms on 172.18.0.15 (executor 0) (1/4)
26/02/13 11:56:53 INFO TaskSetManager: Starting task 3.0 in stage 305.0 (TID 680) (172.18.0.15, executor 0, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:56:53 INFO TaskSetManager: Finished task 1.0 in stage 305.0 (TID 678) in 22 ms on 172.18.0.15 (executor 0) (2/4)
26/02/13 11:56:53 INFO TaskSetManager: Finished task 2.0 in stage 305.0 (TID 679) in 18 ms on 172.18.0.15 (executor 0) (3/4)
26/02/13 11:56:53 INFO TaskSetManager: Finished task 3.0 in stage 305.0 (TID 680) in 18 ms on 172.18.0.15 (executor 0) (4/4)
26/02/13 11:56:53 INFO TaskSchedulerImpl: Removed TaskSet 305.0, whose tasks have all completed, from pool 
26/02/13 11:56:53 INFO DAGScheduler: ResultStage 305 (start at NativeMethodAccessorImpl.java:0) finished in 0.048 s
26/02/13 11:56:53 INFO DAGScheduler: Job 201 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:56:53 INFO TaskSchedulerImpl: Killing all running tasks in stage 305: Stage finished
26/02/13 11:56:53 INFO DAGScheduler: Job 201 finished: start at NativeMethodAccessorImpl.java:0, took 0.049854 s
26/02/13 11:56:53 INFO MemoryStore: Block broadcast_302_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.5 MiB)
26/02/13 11:56:53 INFO BlockManagerInfo: Added broadcast_302_piece0 in memory on spark-master:40291 (size: 107.2 KiB, free: 434.0 MiB)
26/02/13 11:56:53 INFO SparkContext: Created broadcast 302 from start at NativeMethodAccessorImpl.java:0
26/02/13 11:56:53 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 96, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@1e51d68a]. The input RDD has 3 partitions.
26/02/13 11:56:53 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:56:53 INFO DAGScheduler: Got job 202 (start at NativeMethodAccessorImpl.java:0) with 3 output partitions
26/02/13 11:56:53 INFO DAGScheduler: Final stage: ResultStage 306 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:56:53 INFO DAGScheduler: Parents of final stage: List()
26/02/13 11:56:53 INFO DAGScheduler: Missing parents: List()
26/02/13 11:56:53 INFO DAGScheduler: Submitting ResultStage 306 (MapPartitionsRDD[1105] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:56:53 INFO MemoryStore: Block broadcast_303 stored as values in memory (estimated size 49.9 KiB, free 433.5 MiB)
26/02/13 11:56:53 INFO MemoryStore: Block broadcast_303_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.5 MiB)
26/02/13 11:56:53 INFO BlockManagerInfo: Added broadcast_303_piece0 in memory on spark-master:40291 (size: 19.5 KiB, free: 433.9 MiB)
26/02/13 11:56:53 INFO SparkContext: Created broadcast 303 from broadcast at DAGScheduler.scala:1585
26/02/13 11:56:53 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 306 (MapPartitionsRDD[1105] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
26/02/13 11:56:53 INFO TaskSchedulerImpl: Adding task set 306.0 with 3 tasks resource profile 0
26/02/13 11:56:53 INFO TaskSetManager: Starting task 0.0 in stage 306.0 (TID 681) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:56:53 INFO TaskSetManager: Starting task 1.0 in stage 306.0 (TID 682) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:56:53 INFO TaskSetManager: Starting task 2.0 in stage 306.0 (TID 683) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:56:53 INFO BlockManagerInfo: Added broadcast_303_piece0 in memory on 172.18.0.15:34959 (size: 19.5 KiB, free: 433.9 MiB)
26/02/13 11:56:53 INFO BlockManagerInfo: Added broadcast_303_piece0 in memory on 172.18.0.14:40939 (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 11:56:53 INFO BlockManagerInfo: Added broadcast_302_piece0 in memory on 172.18.0.15:34959 (size: 107.2 KiB, free: 433.7 MiB)
26/02/13 11:56:53 INFO BlockManagerInfo: Added broadcast_302_piece0 in memory on 172.18.0.14:40939 (size: 107.2 KiB, free: 434.0 MiB)
26/02/13 11:56:53 INFO TaskSetManager: Finished task 0.0 in stage 306.0 (TID 681) in 48 ms on 172.18.0.14 (executor 1) (1/3)
26/02/13 11:56:53 INFO TaskSetManager: Finished task 2.0 in stage 306.0 (TID 683) in 48 ms on 172.18.0.14 (executor 1) (2/3)
26/02/13 11:56:53 INFO TaskSetManager: Finished task 1.0 in stage 306.0 (TID 682) in 49 ms on 172.18.0.15 (executor 0) (3/3)
26/02/13 11:56:53 INFO TaskSchedulerImpl: Removed TaskSet 306.0, whose tasks have all completed, from pool 
26/02/13 11:56:53 INFO DAGScheduler: ResultStage 306 (start at NativeMethodAccessorImpl.java:0) finished in 0.053 s
26/02/13 11:56:53 INFO DAGScheduler: Job 202 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:56:53 INFO TaskSchedulerImpl: Killing all running tasks in stage 306: Stage finished
26/02/13 11:56:53 INFO DAGScheduler: Job 202 finished: start at NativeMethodAccessorImpl.java:0, took 0.054007 s
26/02/13 11:56:53 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 96, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@1e51d68a] is committing.
26/02/13 11:56:53 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 96, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@1e51d68a] committed.
26/02/13 11:56:54 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/96 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.96.1e9e5693-0e87-4659-8137-fcc3b3e44181.tmp
26/02/13 11:56:54 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.96.1e9e5693-0e87-4659-8137-fcc3b3e44181.tmp to file:/tmp/spark-checkpoint-enrichment/commits/96
26/02/13 11:56:54 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "fc5c6452-bd70-4c90-becf-772d3bebdb57",
  "name" : null,
  "timestamp" : "2026-02-13T11:56:53.720Z",
  "batchId" : 96,
  "numInputRows" : 157,
  "inputRowsPerSecond" : 171.77242888402625,
  "processedRowsPerSecond" : 472.8915662650602,
  "durationMs" : {
    "addBatch" : 189,
    "commitOffsets" : 58,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 23,
    "triggerExecution" : 332,
    "walCommit" : 61
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 6547,
        "1" : 7226,
        "0" : 8584
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 6594,
        "1" : 7287,
        "0" : 8633
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 6594,
        "1" : 7287,
        "0" : 8633
      }
    },
    "numInputRows" : 157,
    "inputRowsPerSecond" : 171.77242888402625,
    "processedRowsPerSecond" : 472.8915662650602,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@7fe9b5de",
    "numOutputRows" : 30
  }
}
26/02/13 11:56:57 INFO BlockManagerInfo: Removed broadcast_298_piece0 on spark-master:40291 in memory (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 11:56:57 INFO BlockManagerInfo: Removed broadcast_298_piece0 on 172.18.0.15:34959 in memory (size: 33.3 KiB, free: 433.8 MiB)
26/02/13 11:56:57 INFO BlockManagerInfo: Removed broadcast_300_piece0 on spark-master:40291 in memory (size: 19.5 KiB, free: 434.0 MiB)
26/02/13 11:56:57 INFO BlockManagerInfo: Removed broadcast_300_piece0 on 172.18.0.15:34959 in memory (size: 19.5 KiB, free: 433.8 MiB)
26/02/13 11:56:57 INFO BlockManagerInfo: Removed broadcast_300_piece0 on 172.18.0.14:40939 in memory (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 11:56:57 INFO BlockManagerInfo: Removed broadcast_299_piece0 on spark-master:40291 in memory (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 11:56:57 INFO BlockManagerInfo: Removed broadcast_299_piece0 on 172.18.0.15:34959 in memory (size: 107.2 KiB, free: 433.9 MiB)
26/02/13 11:56:57 INFO BlockManagerInfo: Removed broadcast_299_piece0 on 172.18.0.14:40939 in memory (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 11:56:57 INFO BlockManagerInfo: Removed broadcast_296_piece0 on spark-master:40291 in memory (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 11:56:57 INFO BlockManagerInfo: Removed broadcast_296_piece0 on 172.18.0.14:40939 in memory (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 11:56:57 INFO BlockManagerInfo: Removed broadcast_296_piece0 on 172.18.0.15:34959 in memory (size: 107.2 KiB, free: 434.0 MiB)
26/02/13 11:56:57 INFO BlockManagerInfo: Removed broadcast_303_piece0 on spark-master:40291 in memory (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 11:56:57 INFO BlockManagerInfo: Removed broadcast_303_piece0 on 172.18.0.15:34959 in memory (size: 19.5 KiB, free: 434.0 MiB)
26/02/13 11:56:57 INFO BlockManagerInfo: Removed broadcast_303_piece0 on 172.18.0.14:40939 in memory (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 11:56:57 INFO BlockManagerInfo: Removed broadcast_301_piece0 on spark-master:40291 in memory (size: 33.3 KiB, free: 434.3 MiB)
26/02/13 11:56:57 INFO BlockManagerInfo: Removed broadcast_301_piece0 on 172.18.0.15:34959 in memory (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 11:57:04 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 11:57:09 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/97 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.97.0e1d7af5-ac3a-444c-960c-74b0e06fad12.tmp
26/02/13 11:57:09 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.97.0e1d7af5-ac3a-444c-960c-74b0e06fad12.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/97
26/02/13 11:57:09 INFO MicroBatchExecution: Committed offsets for batch 97. Metadata OffsetSeqMetadata(0,1770983829669,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 11:57:09 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:57:09 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:57:09 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:57:09 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:57:09 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:57:09 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:57:09 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#84789 - origin_code.nullCount#84788) > 0)
26/02/13 11:57:09 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#84794 - destination_code.nullCount#84793) > 0)
26/02/13 11:57:09 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#84824 - callsign.nullCount#84823) > 0)
26/02/13 11:57:09 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:57:09 INFO DAGScheduler: Got job 203 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 11:57:09 INFO DAGScheduler: Final stage: ResultStage 308 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:57:09 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 307)
26/02/13 11:57:09 INFO DAGScheduler: Missing parents: List()
26/02/13 11:57:09 INFO DAGScheduler: Submitting ResultStage 308 (MapPartitionsRDD[1110] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:57:09 INFO MemoryStore: Block broadcast_304 stored as values in memory (estimated size 99.3 KiB, free 434.0 MiB)
26/02/13 11:57:09 INFO MemoryStore: Block broadcast_304_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 433.9 MiB)
26/02/13 11:57:09 INFO BlockManagerInfo: Added broadcast_304_piece0 in memory on spark-master:40291 (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 11:57:09 INFO SparkContext: Created broadcast 304 from broadcast at DAGScheduler.scala:1585
26/02/13 11:57:09 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 308 (MapPartitionsRDD[1110] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 11:57:09 INFO TaskSchedulerImpl: Adding task set 308.0 with 4 tasks resource profile 0
26/02/13 11:57:09 INFO TaskSetManager: Starting task 0.0 in stage 308.0 (TID 684) (172.18.0.15, executor 0, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:57:09 INFO TaskSetManager: Starting task 1.0 in stage 308.0 (TID 685) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:57:09 INFO BlockManagerInfo: Removed broadcast_302_piece0 on spark-master:40291 in memory (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 11:57:09 INFO BlockManagerInfo: Added broadcast_304_piece0 in memory on 172.18.0.15:34959 (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 11:57:09 INFO BlockManagerInfo: Removed broadcast_302_piece0 on 172.18.0.15:34959 in memory (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 11:57:09 INFO BlockManagerInfo: Removed broadcast_302_piece0 on 172.18.0.14:40939 in memory (size: 107.2 KiB, free: 434.4 MiB)
26/02/13 11:57:09 INFO TaskSetManager: Starting task 2.0 in stage 308.0 (TID 686) (172.18.0.15, executor 0, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:57:09 INFO TaskSetManager: Finished task 1.0 in stage 308.0 (TID 685) in 33 ms on 172.18.0.15 (executor 0) (1/4)
26/02/13 11:57:09 INFO TaskSetManager: Starting task 3.0 in stage 308.0 (TID 687) (172.18.0.15, executor 0, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:57:09 INFO TaskSetManager: Finished task 0.0 in stage 308.0 (TID 684) in 34 ms on 172.18.0.15 (executor 0) (2/4)
26/02/13 11:57:09 INFO TaskSetManager: Finished task 2.0 in stage 308.0 (TID 686) in 14 ms on 172.18.0.15 (executor 0) (3/4)
26/02/13 11:57:09 INFO TaskSetManager: Finished task 3.0 in stage 308.0 (TID 687) in 15 ms on 172.18.0.15 (executor 0) (4/4)
26/02/13 11:57:09 INFO TaskSchedulerImpl: Removed TaskSet 308.0, whose tasks have all completed, from pool 
26/02/13 11:57:09 INFO DAGScheduler: ResultStage 308 (start at NativeMethodAccessorImpl.java:0) finished in 0.055 s
26/02/13 11:57:09 INFO DAGScheduler: Job 203 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:57:09 INFO TaskSchedulerImpl: Killing all running tasks in stage 308: Stage finished
26/02/13 11:57:09 INFO DAGScheduler: Job 203 finished: start at NativeMethodAccessorImpl.java:0, took 0.056990 s
26/02/13 11:57:09 INFO MemoryStore: Block broadcast_305_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.9 MiB)
26/02/13 11:57:09 INFO BlockManagerInfo: Added broadcast_305_piece0 in memory on spark-master:40291 (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 11:57:09 INFO SparkContext: Created broadcast 305 from start at NativeMethodAccessorImpl.java:0
26/02/13 11:57:09 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 97, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@23fcf206]. The input RDD has 2 partitions.
26/02/13 11:57:09 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:57:09 INFO DAGScheduler: Got job 204 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/02/13 11:57:09 INFO DAGScheduler: Final stage: ResultStage 309 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:57:09 INFO DAGScheduler: Parents of final stage: List()
26/02/13 11:57:09 INFO DAGScheduler: Missing parents: List()
26/02/13 11:57:09 INFO DAGScheduler: Submitting ResultStage 309 (MapPartitionsRDD[1116] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:57:09 INFO MemoryStore: Block broadcast_306 stored as values in memory (estimated size 49.9 KiB, free 433.9 MiB)
26/02/13 11:57:09 INFO MemoryStore: Block broadcast_306_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.9 MiB)
26/02/13 11:57:09 INFO BlockManagerInfo: Added broadcast_306_piece0 in memory on spark-master:40291 (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 11:57:09 INFO SparkContext: Created broadcast 306 from broadcast at DAGScheduler.scala:1585
26/02/13 11:57:09 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 309 (MapPartitionsRDD[1116] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/02/13 11:57:09 INFO TaskSchedulerImpl: Adding task set 309.0 with 2 tasks resource profile 0
26/02/13 11:57:09 INFO TaskSetManager: Starting task 1.0 in stage 309.0 (TID 688) (172.18.0.14, executor 1, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:57:09 INFO TaskSetManager: Starting task 0.0 in stage 309.0 (TID 689) (172.18.0.15, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:57:09 INFO BlockManagerInfo: Added broadcast_306_piece0 in memory on 172.18.0.14:40939 (size: 19.5 KiB, free: 434.4 MiB)
26/02/13 11:57:09 INFO BlockManagerInfo: Added broadcast_306_piece0 in memory on 172.18.0.15:34959 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 11:57:09 INFO BlockManagerInfo: Added broadcast_305_piece0 in memory on 172.18.0.14:40939 (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 11:57:09 INFO BlockManagerInfo: Added broadcast_305_piece0 in memory on 172.18.0.15:34959 (size: 107.2 KiB, free: 434.0 MiB)
26/02/13 11:57:10 INFO TaskSetManager: Finished task 1.0 in stage 309.0 (TID 688) in 539 ms on 172.18.0.14 (executor 1) (1/2)
26/02/13 11:57:10 INFO TaskSetManager: Finished task 0.0 in stage 309.0 (TID 689) in 541 ms on 172.18.0.15 (executor 0) (2/2)
26/02/13 11:57:10 INFO TaskSchedulerImpl: Removed TaskSet 309.0, whose tasks have all completed, from pool 
26/02/13 11:57:10 INFO DAGScheduler: ResultStage 309 (start at NativeMethodAccessorImpl.java:0) finished in 0.545 s
26/02/13 11:57:10 INFO DAGScheduler: Job 204 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:57:10 INFO TaskSchedulerImpl: Killing all running tasks in stage 309: Stage finished
26/02/13 11:57:10 INFO DAGScheduler: Job 204 finished: start at NativeMethodAccessorImpl.java:0, took 0.546654 s
26/02/13 11:57:10 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 97, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@23fcf206] is committing.
26/02/13 11:57:10 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 97, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@23fcf206] committed.
26/02/13 11:57:10 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/97 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.97.e10a6220-1373-4999-8398-c23a31f71bef.tmp
26/02/13 11:57:10 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.97.e10a6220-1373-4999-8398-c23a31f71bef.tmp to file:/tmp/spark-checkpoint-enrichment/commits/97
26/02/13 11:57:10 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "fc5c6452-bd70-4c90-becf-772d3bebdb57",
  "name" : null,
  "timestamp" : "2026-02-13T11:57:09.667Z",
  "batchId" : 97,
  "numInputRows" : 5,
  "inputRowsPerSecond" : 416.6666666666667,
  "processedRowsPerSecond" : 5.63063063063063,
  "durationMs" : {
    "addBatch" : 691,
    "commitOffsets" : 98,
    "getBatch" : 0,
    "latestOffset" : 2,
    "queryPlanning" : 22,
    "triggerExecution" : 888,
    "walCommit" : 74
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 6594,
        "1" : 7287,
        "0" : 8633
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 6594,
        "1" : 7291,
        "0" : 8634
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 6594,
        "1" : 7291,
        "0" : 8634
      }
    },
    "numInputRows" : 5,
    "inputRowsPerSecond" : 416.6666666666667,
    "processedRowsPerSecond" : 5.63063063063063,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@7fe9b5de",
    "numOutputRows" : 0
  }
}
26/02/13 11:57:10 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/98 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.98.ff132b7f-dd61-4f5e-b532-660520d953bb.tmp
26/02/13 11:57:10 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.98.ff132b7f-dd61-4f5e-b532-660520d953bb.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/98
26/02/13 11:57:10 INFO MicroBatchExecution: Committed offsets for batch 98. Metadata OffsetSeqMetadata(0,1770983830557,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 11:57:10 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:57:10 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:57:10 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:57:10 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:57:10 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:57:10 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:57:10 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#85643 - origin_code.nullCount#85642) > 0)
26/02/13 11:57:10 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#85648 - destination_code.nullCount#85647) > 0)
26/02/13 11:57:10 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#85678 - callsign.nullCount#85677) > 0)
26/02/13 11:57:10 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:57:10 INFO DAGScheduler: Got job 205 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 11:57:10 INFO DAGScheduler: Final stage: ResultStage 311 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:57:10 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 310)
26/02/13 11:57:10 INFO DAGScheduler: Missing parents: List()
26/02/13 11:57:10 INFO DAGScheduler: Submitting ResultStage 311 (MapPartitionsRDD[1121] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:57:10 INFO MemoryStore: Block broadcast_307 stored as values in memory (estimated size 99.3 KiB, free 433.8 MiB)
26/02/13 11:57:10 INFO MemoryStore: Block broadcast_307_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 433.7 MiB)
26/02/13 11:57:10 INFO BlockManagerInfo: Added broadcast_307_piece0 in memory on spark-master:40291 (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 11:57:10 INFO SparkContext: Created broadcast 307 from broadcast at DAGScheduler.scala:1585
26/02/13 11:57:10 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 311 (MapPartitionsRDD[1121] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 11:57:10 INFO TaskSchedulerImpl: Adding task set 311.0 with 4 tasks resource profile 0
26/02/13 11:57:10 INFO TaskSetManager: Starting task 0.0 in stage 311.0 (TID 690) (172.18.0.15, executor 0, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:57:10 INFO TaskSetManager: Starting task 1.0 in stage 311.0 (TID 691) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:57:10 INFO BlockManagerInfo: Added broadcast_307_piece0 in memory on 172.18.0.15:34959 (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 11:57:10 INFO TaskSetManager: Starting task 2.0 in stage 311.0 (TID 692) (172.18.0.15, executor 0, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:57:10 INFO TaskSetManager: Finished task 1.0 in stage 311.0 (TID 691) in 14 ms on 172.18.0.15 (executor 0) (1/4)
26/02/13 11:57:10 INFO TaskSetManager: Starting task 3.0 in stage 311.0 (TID 693) (172.18.0.15, executor 0, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:57:10 INFO TaskSetManager: Finished task 0.0 in stage 311.0 (TID 690) in 20 ms on 172.18.0.15 (executor 0) (2/4)
26/02/13 11:57:10 INFO TaskSetManager: Finished task 2.0 in stage 311.0 (TID 692) in 13 ms on 172.18.0.15 (executor 0) (3/4)
26/02/13 11:57:10 INFO TaskSetManager: Finished task 3.0 in stage 311.0 (TID 693) in 15 ms on 172.18.0.15 (executor 0) (4/4)
26/02/13 11:57:10 INFO TaskSchedulerImpl: Removed TaskSet 311.0, whose tasks have all completed, from pool 
26/02/13 11:57:10 INFO DAGScheduler: ResultStage 311 (start at NativeMethodAccessorImpl.java:0) finished in 0.040 s
26/02/13 11:57:10 INFO DAGScheduler: Job 205 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:57:10 INFO TaskSchedulerImpl: Killing all running tasks in stage 311: Stage finished
26/02/13 11:57:10 INFO DAGScheduler: Job 205 finished: start at NativeMethodAccessorImpl.java:0, took 0.041463 s
26/02/13 11:57:10 INFO MemoryStore: Block broadcast_308_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.6 MiB)
26/02/13 11:57:10 INFO BlockManagerInfo: Added broadcast_308_piece0 in memory on spark-master:40291 (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 11:57:10 INFO SparkContext: Created broadcast 308 from start at NativeMethodAccessorImpl.java:0
26/02/13 11:57:10 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 98, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@58861ad0]. The input RDD has 3 partitions.
26/02/13 11:57:10 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:57:10 INFO DAGScheduler: Got job 206 (start at NativeMethodAccessorImpl.java:0) with 3 output partitions
26/02/13 11:57:10 INFO DAGScheduler: Final stage: ResultStage 312 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:57:10 INFO DAGScheduler: Parents of final stage: List()
26/02/13 11:57:10 INFO DAGScheduler: Missing parents: List()
26/02/13 11:57:10 INFO DAGScheduler: Submitting ResultStage 312 (MapPartitionsRDD[1127] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:57:10 INFO MemoryStore: Block broadcast_309 stored as values in memory (estimated size 49.9 KiB, free 433.6 MiB)
26/02/13 11:57:10 INFO MemoryStore: Block broadcast_309_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.6 MiB)
26/02/13 11:57:10 INFO BlockManagerInfo: Added broadcast_309_piece0 in memory on spark-master:40291 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 11:57:10 INFO SparkContext: Created broadcast 309 from broadcast at DAGScheduler.scala:1585
26/02/13 11:57:10 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 312 (MapPartitionsRDD[1127] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
26/02/13 11:57:10 INFO TaskSchedulerImpl: Adding task set 312.0 with 3 tasks resource profile 0
26/02/13 11:57:10 INFO TaskSetManager: Starting task 1.0 in stage 312.0 (TID 694) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:57:10 INFO TaskSetManager: Starting task 0.0 in stage 312.0 (TID 695) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:57:10 INFO TaskSetManager: Starting task 2.0 in stage 312.0 (TID 696) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:57:10 INFO BlockManagerInfo: Added broadcast_309_piece0 in memory on 172.18.0.14:40939 (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 11:57:10 INFO BlockManagerInfo: Added broadcast_309_piece0 in memory on 172.18.0.15:34959 (size: 19.5 KiB, free: 434.0 MiB)
26/02/13 11:57:10 INFO BlockManagerInfo: Added broadcast_308_piece0 in memory on 172.18.0.14:40939 (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 11:57:10 INFO BlockManagerInfo: Added broadcast_308_piece0 in memory on 172.18.0.15:34959 (size: 107.2 KiB, free: 433.9 MiB)
26/02/13 11:57:10 INFO TaskSetManager: Finished task 1.0 in stage 312.0 (TID 694) in 60 ms on 172.18.0.15 (executor 0) (1/3)
26/02/13 11:57:10 INFO TaskSetManager: Finished task 2.0 in stage 312.0 (TID 696) in 66 ms on 172.18.0.14 (executor 1) (2/3)
26/02/13 11:57:11 INFO TaskSetManager: Finished task 0.0 in stage 312.0 (TID 695) in 575 ms on 172.18.0.14 (executor 1) (3/3)
26/02/13 11:57:11 INFO TaskSchedulerImpl: Removed TaskSet 312.0, whose tasks have all completed, from pool 
26/02/13 11:57:11 INFO DAGScheduler: ResultStage 312 (start at NativeMethodAccessorImpl.java:0) finished in 0.580 s
26/02/13 11:57:11 INFO DAGScheduler: Job 206 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:57:11 INFO TaskSchedulerImpl: Killing all running tasks in stage 312: Stage finished
26/02/13 11:57:11 INFO DAGScheduler: Job 206 finished: start at NativeMethodAccessorImpl.java:0, took 0.580852 s
26/02/13 11:57:11 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 98, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@58861ad0] is committing.
26/02/13 11:57:11 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 98, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@58861ad0] committed.
26/02/13 11:57:11 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/98 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.98.22fa1477-88fa-4bb0-8904-21f6cdf49e0b.tmp
26/02/13 11:57:11 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.98.22fa1477-88fa-4bb0-8904-21f6cdf49e0b.tmp to file:/tmp/spark-checkpoint-enrichment/commits/98
26/02/13 11:57:11 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "fc5c6452-bd70-4c90-becf-772d3bebdb57",
  "name" : null,
  "timestamp" : "2026-02-13T11:57:10.556Z",
  "batchId" : 98,
  "numInputRows" : 245,
  "inputRowsPerSecond" : 275.59055118110234,
  "processedRowsPerSecond" : 265.4387865655471,
  "durationMs" : {
    "addBatch" : 712,
    "commitOffsets" : 65,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 27,
    "triggerExecution" : 923,
    "walCommit" : 118
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 6594,
        "1" : 7291,
        "0" : 8634
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 6673,
        "1" : 7383,
        "0" : 8708
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 6673,
        "1" : 7383,
        "0" : 8708
      }
    },
    "numInputRows" : 245,
    "inputRowsPerSecond" : 275.59055118110234,
    "processedRowsPerSecond" : 265.4387865655471,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@7fe9b5de",
    "numOutputRows" : 53
  }
}
26/02/13 11:57:11 INFO BlockManagerInfo: Removed broadcast_309_piece0 on spark-master:40291 in memory (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 11:57:11 INFO BlockManagerInfo: Removed broadcast_309_piece0 on 172.18.0.15:34959 in memory (size: 19.5 KiB, free: 433.9 MiB)
26/02/13 11:57:11 INFO BlockManagerInfo: Removed broadcast_309_piece0 on 172.18.0.14:40939 in memory (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 11:57:11 INFO BlockManagerInfo: Removed broadcast_306_piece0 on spark-master:40291 in memory (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 11:57:11 INFO BlockManagerInfo: Removed broadcast_306_piece0 on 172.18.0.14:40939 in memory (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 11:57:11 INFO BlockManagerInfo: Removed broadcast_306_piece0 on 172.18.0.15:34959 in memory (size: 19.5 KiB, free: 433.9 MiB)
26/02/13 11:57:11 INFO BlockManagerInfo: Removed broadcast_307_piece0 on spark-master:40291 in memory (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 11:57:11 INFO BlockManagerInfo: Removed broadcast_307_piece0 on 172.18.0.15:34959 in memory (size: 33.3 KiB, free: 433.9 MiB)
26/02/13 11:57:11 INFO BlockManagerInfo: Removed broadcast_304_piece0 on spark-master:40291 in memory (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 11:57:11 INFO BlockManagerInfo: Removed broadcast_304_piece0 on 172.18.0.15:34959 in memory (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 11:57:11 INFO BlockManagerInfo: Removed broadcast_305_piece0 on spark-master:40291 in memory (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 11:57:11 INFO BlockManagerInfo: Removed broadcast_305_piece0 on 172.18.0.15:34959 in memory (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 11:57:11 INFO BlockManagerInfo: Removed broadcast_305_piece0 on 172.18.0.14:40939 in memory (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 11:57:21 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 11:57:25 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/99 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.99.b08df744-b3f3-4e4d-a657-2d7b480a4073.tmp
26/02/13 11:57:25 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.99.b08df744-b3f3-4e4d-a657-2d7b480a4073.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/99
26/02/13 11:57:25 INFO MicroBatchExecution: Committed offsets for batch 99. Metadata OffsetSeqMetadata(0,1770983845857,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 11:57:26 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:57:26 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:57:26 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:57:26 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:57:26 INFO BlockManagerInfo: Removed broadcast_308_piece0 on spark-master:40291 in memory (size: 107.2 KiB, free: 434.4 MiB)
26/02/13 11:57:26 INFO BlockManagerInfo: Removed broadcast_308_piece0 on 172.18.0.14:40939 in memory (size: 107.2 KiB, free: 434.4 MiB)
26/02/13 11:57:26 INFO BlockManagerInfo: Removed broadcast_308_piece0 on 172.18.0.15:34959 in memory (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 11:57:26 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:57:26 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:57:26 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#86497 - origin_code.nullCount#86496) > 0)
26/02/13 11:57:26 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#86502 - destination_code.nullCount#86501) > 0)
26/02/13 11:57:26 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#86532 - callsign.nullCount#86531) > 0)
26/02/13 11:57:26 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:57:26 INFO DAGScheduler: Got job 207 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 11:57:26 INFO DAGScheduler: Final stage: ResultStage 314 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:57:26 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 313)
26/02/13 11:57:26 INFO DAGScheduler: Missing parents: List()
26/02/13 11:57:26 INFO DAGScheduler: Submitting ResultStage 314 (MapPartitionsRDD[1132] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:57:26 INFO MemoryStore: Block broadcast_310 stored as values in memory (estimated size 99.3 KiB, free 434.1 MiB)
26/02/13 11:57:26 INFO MemoryStore: Block broadcast_310_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 434.0 MiB)
26/02/13 11:57:26 INFO BlockManagerInfo: Added broadcast_310_piece0 in memory on spark-master:40291 (size: 33.3 KiB, free: 434.3 MiB)
26/02/13 11:57:26 INFO SparkContext: Created broadcast 310 from broadcast at DAGScheduler.scala:1585
26/02/13 11:57:26 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 314 (MapPartitionsRDD[1132] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 11:57:26 INFO TaskSchedulerImpl: Adding task set 314.0 with 4 tasks resource profile 0
26/02/13 11:57:26 INFO TaskSetManager: Starting task 0.0 in stage 314.0 (TID 697) (172.18.0.15, executor 0, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:57:26 INFO TaskSetManager: Starting task 1.0 in stage 314.0 (TID 698) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:57:26 INFO BlockManagerInfo: Added broadcast_310_piece0 in memory on 172.18.0.15:34959 (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 11:57:26 INFO TaskSetManager: Starting task 2.0 in stage 314.0 (TID 699) (172.18.0.15, executor 0, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:57:26 INFO TaskSetManager: Finished task 1.0 in stage 314.0 (TID 698) in 33 ms on 172.18.0.15 (executor 0) (1/4)
26/02/13 11:57:26 INFO TaskSetManager: Starting task 3.0 in stage 314.0 (TID 700) (172.18.0.15, executor 0, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:57:26 INFO TaskSetManager: Finished task 0.0 in stage 314.0 (TID 697) in 35 ms on 172.18.0.15 (executor 0) (2/4)
26/02/13 11:57:26 INFO TaskSetManager: Finished task 2.0 in stage 314.0 (TID 699) in 26 ms on 172.18.0.15 (executor 0) (3/4)
26/02/13 11:57:26 INFO TaskSetManager: Finished task 3.0 in stage 314.0 (TID 700) in 25 ms on 172.18.0.15 (executor 0) (4/4)
26/02/13 11:57:26 INFO TaskSchedulerImpl: Removed TaskSet 314.0, whose tasks have all completed, from pool 
26/02/13 11:57:26 INFO DAGScheduler: ResultStage 314 (start at NativeMethodAccessorImpl.java:0) finished in 0.068 s
26/02/13 11:57:26 INFO DAGScheduler: Job 207 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:57:26 INFO TaskSchedulerImpl: Killing all running tasks in stage 314: Stage finished
26/02/13 11:57:26 INFO DAGScheduler: Job 207 finished: start at NativeMethodAccessorImpl.java:0, took 0.071231 s
26/02/13 11:57:26 INFO MemoryStore: Block broadcast_311_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.9 MiB)
26/02/13 11:57:26 INFO BlockManagerInfo: Added broadcast_311_piece0 in memory on spark-master:40291 (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 11:57:26 INFO SparkContext: Created broadcast 311 from start at NativeMethodAccessorImpl.java:0
26/02/13 11:57:26 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 99, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@4f6e8975]. The input RDD has 3 partitions.
26/02/13 11:57:26 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:57:26 INFO DAGScheduler: Got job 208 (start at NativeMethodAccessorImpl.java:0) with 3 output partitions
26/02/13 11:57:26 INFO DAGScheduler: Final stage: ResultStage 315 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:57:26 INFO DAGScheduler: Parents of final stage: List()
26/02/13 11:57:26 INFO DAGScheduler: Missing parents: List()
26/02/13 11:57:26 INFO DAGScheduler: Submitting ResultStage 315 (MapPartitionsRDD[1138] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:57:26 INFO MemoryStore: Block broadcast_312 stored as values in memory (estimated size 49.9 KiB, free 433.9 MiB)
26/02/13 11:57:26 INFO MemoryStore: Block broadcast_312_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.9 MiB)
26/02/13 11:57:26 INFO BlockManagerInfo: Added broadcast_312_piece0 in memory on spark-master:40291 (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 11:57:26 INFO SparkContext: Created broadcast 312 from broadcast at DAGScheduler.scala:1585
26/02/13 11:57:26 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 315 (MapPartitionsRDD[1138] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
26/02/13 11:57:26 INFO TaskSchedulerImpl: Adding task set 315.0 with 3 tasks resource profile 0
26/02/13 11:57:26 INFO TaskSetManager: Starting task 0.0 in stage 315.0 (TID 701) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:57:26 INFO TaskSetManager: Starting task 1.0 in stage 315.0 (TID 702) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:57:26 INFO TaskSetManager: Starting task 2.0 in stage 315.0 (TID 703) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:57:26 INFO BlockManagerInfo: Added broadcast_312_piece0 in memory on 172.18.0.15:34959 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 11:57:26 INFO BlockManagerInfo: Added broadcast_312_piece0 in memory on 172.18.0.14:40939 (size: 19.5 KiB, free: 434.4 MiB)
26/02/13 11:57:26 INFO BlockManagerInfo: Added broadcast_311_piece0 in memory on 172.18.0.15:34959 (size: 107.2 KiB, free: 434.0 MiB)
26/02/13 11:57:26 INFO BlockManagerInfo: Added broadcast_311_piece0 in memory on 172.18.0.14:40939 (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 11:57:26 INFO TaskSetManager: Finished task 1.0 in stage 315.0 (TID 702) in 579 ms on 172.18.0.15 (executor 0) (1/3)
26/02/13 11:57:26 INFO TaskSetManager: Finished task 2.0 in stage 315.0 (TID 703) in 597 ms on 172.18.0.14 (executor 1) (2/3)
26/02/13 11:57:26 INFO TaskSetManager: Finished task 0.0 in stage 315.0 (TID 701) in 603 ms on 172.18.0.14 (executor 1) (3/3)
26/02/13 11:57:26 INFO TaskSchedulerImpl: Removed TaskSet 315.0, whose tasks have all completed, from pool 
26/02/13 11:57:26 INFO DAGScheduler: ResultStage 315 (start at NativeMethodAccessorImpl.java:0) finished in 0.611 s
26/02/13 11:57:26 INFO DAGScheduler: Job 208 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:57:26 INFO TaskSchedulerImpl: Killing all running tasks in stage 315: Stage finished
26/02/13 11:57:26 INFO DAGScheduler: Job 208 finished: start at NativeMethodAccessorImpl.java:0, took 0.613754 s
26/02/13 11:57:26 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 99, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@4f6e8975] is committing.
26/02/13 11:57:26 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 99, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@4f6e8975] committed.
26/02/13 11:57:26 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/99 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.99.ca9d5f00-603d-4170-8841-f4727edb17e9.tmp
26/02/13 11:57:27 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.99.ca9d5f00-603d-4170-8841-f4727edb17e9.tmp to file:/tmp/spark-checkpoint-enrichment/commits/99
26/02/13 11:57:27 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "fc5c6452-bd70-4c90-becf-772d3bebdb57",
  "name" : null,
  "timestamp" : "2026-02-13T11:57:25.854Z",
  "batchId" : 99,
  "numInputRows" : 6,
  "inputRowsPerSecond" : 428.57142857142856,
  "processedRowsPerSecond" : 5.199306759098787,
  "durationMs" : {
    "addBatch" : 905,
    "commitOffsets" : 76,
    "getBatch" : 0,
    "latestOffset" : 3,
    "queryPlanning" : 46,
    "triggerExecution" : 1154,
    "walCommit" : 122
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 6673,
        "1" : 7383,
        "0" : 8708
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 6674,
        "1" : 7387,
        "0" : 8709
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 6674,
        "1" : 7387,
        "0" : 8709
      }
    },
    "numInputRows" : 6,
    "inputRowsPerSecond" : 428.57142857142856,
    "processedRowsPerSecond" : 5.199306759098787,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@7fe9b5de",
    "numOutputRows" : 1
  }
}
26/02/13 11:57:27 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/100 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.100.da026d3f-54cf-4171-84aa-a34b9180959b.tmp
26/02/13 11:57:27 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.100.da026d3f-54cf-4171-84aa-a34b9180959b.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/100
26/02/13 11:57:27 INFO MicroBatchExecution: Committed offsets for batch 100. Metadata OffsetSeqMetadata(0,1770983847010,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 11:57:27 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:57:27 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:57:27 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:57:27 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:57:27 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:57:27 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:57:27 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#87351 - origin_code.nullCount#87350) > 0)
26/02/13 11:57:27 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#87356 - destination_code.nullCount#87355) > 0)
26/02/13 11:57:27 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#87386 - callsign.nullCount#87385) > 0)
26/02/13 11:57:27 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:57:27 INFO DAGScheduler: Got job 209 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 11:57:27 INFO DAGScheduler: Final stage: ResultStage 317 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:57:27 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 316)
26/02/13 11:57:27 INFO DAGScheduler: Missing parents: List()
26/02/13 11:57:27 INFO DAGScheduler: Submitting ResultStage 317 (MapPartitionsRDD[1143] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:57:27 INFO MemoryStore: Block broadcast_313 stored as values in memory (estimated size 99.3 KiB, free 433.8 MiB)
26/02/13 11:57:27 INFO MemoryStore: Block broadcast_313_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 433.7 MiB)
26/02/13 11:57:27 INFO BlockManagerInfo: Added broadcast_313_piece0 in memory on spark-master:40291 (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 11:57:27 INFO SparkContext: Created broadcast 313 from broadcast at DAGScheduler.scala:1585
26/02/13 11:57:27 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 317 (MapPartitionsRDD[1143] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 11:57:27 INFO TaskSchedulerImpl: Adding task set 317.0 with 4 tasks resource profile 0
26/02/13 11:57:27 INFO TaskSetManager: Starting task 0.0 in stage 317.0 (TID 704) (172.18.0.15, executor 0, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:57:27 INFO TaskSetManager: Starting task 1.0 in stage 317.0 (TID 705) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:57:27 INFO BlockManagerInfo: Added broadcast_313_piece0 in memory on 172.18.0.15:34959 (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 11:57:27 INFO TaskSetManager: Starting task 2.0 in stage 317.0 (TID 706) (172.18.0.15, executor 0, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:57:27 INFO TaskSetManager: Starting task 3.0 in stage 317.0 (TID 707) (172.18.0.15, executor 0, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:57:27 INFO TaskSetManager: Finished task 0.0 in stage 317.0 (TID 704) in 18 ms on 172.18.0.15 (executor 0) (1/4)
26/02/13 11:57:27 INFO TaskSetManager: Finished task 1.0 in stage 317.0 (TID 705) in 18 ms on 172.18.0.15 (executor 0) (2/4)
26/02/13 11:57:27 INFO TaskSetManager: Finished task 2.0 in stage 317.0 (TID 706) in 11 ms on 172.18.0.15 (executor 0) (3/4)
26/02/13 11:57:27 INFO TaskSetManager: Finished task 3.0 in stage 317.0 (TID 707) in 12 ms on 172.18.0.15 (executor 0) (4/4)
26/02/13 11:57:27 INFO TaskSchedulerImpl: Removed TaskSet 317.0, whose tasks have all completed, from pool 
26/02/13 11:57:27 INFO DAGScheduler: ResultStage 317 (start at NativeMethodAccessorImpl.java:0) finished in 0.036 s
26/02/13 11:57:27 INFO DAGScheduler: Job 209 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:57:27 INFO TaskSchedulerImpl: Killing all running tasks in stage 317: Stage finished
26/02/13 11:57:27 INFO DAGScheduler: Job 209 finished: start at NativeMethodAccessorImpl.java:0, took 0.037893 s
26/02/13 11:57:27 INFO MemoryStore: Block broadcast_314_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.6 MiB)
26/02/13 11:57:27 INFO BlockManagerInfo: Added broadcast_314_piece0 in memory on spark-master:40291 (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 11:57:27 INFO SparkContext: Created broadcast 314 from start at NativeMethodAccessorImpl.java:0
26/02/13 11:57:27 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 100, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@16cb94cd]. The input RDD has 3 partitions.
26/02/13 11:57:27 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:57:27 INFO DAGScheduler: Got job 210 (start at NativeMethodAccessorImpl.java:0) with 3 output partitions
26/02/13 11:57:27 INFO DAGScheduler: Final stage: ResultStage 318 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:57:27 INFO DAGScheduler: Parents of final stage: List()
26/02/13 11:57:27 INFO DAGScheduler: Missing parents: List()
26/02/13 11:57:27 INFO DAGScheduler: Submitting ResultStage 318 (MapPartitionsRDD[1149] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:57:27 INFO MemoryStore: Block broadcast_315 stored as values in memory (estimated size 49.9 KiB, free 433.6 MiB)
26/02/13 11:57:27 INFO BlockManagerInfo: Removed broadcast_311_piece0 on spark-master:40291 in memory (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 11:57:27 INFO MemoryStore: Block broadcast_315_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.7 MiB)
26/02/13 11:57:27 INFO BlockManagerInfo: Removed broadcast_311_piece0 on 172.18.0.15:34959 in memory (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 11:57:27 INFO BlockManagerInfo: Removed broadcast_311_piece0 on 172.18.0.14:40939 in memory (size: 107.2 KiB, free: 434.4 MiB)
26/02/13 11:57:27 INFO BlockManagerInfo: Added broadcast_315_piece0 in memory on spark-master:40291 (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 11:57:27 INFO SparkContext: Created broadcast 315 from broadcast at DAGScheduler.scala:1585
26/02/13 11:57:27 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 318 (MapPartitionsRDD[1149] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
26/02/13 11:57:27 INFO TaskSchedulerImpl: Adding task set 318.0 with 3 tasks resource profile 0
26/02/13 11:57:27 INFO TaskSetManager: Starting task 1.0 in stage 318.0 (TID 708) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:57:27 INFO TaskSetManager: Starting task 0.0 in stage 318.0 (TID 709) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:57:27 INFO TaskSetManager: Starting task 2.0 in stage 318.0 (TID 710) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:57:27 INFO BlockManagerInfo: Removed broadcast_313_piece0 on spark-master:40291 in memory (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 11:57:27 INFO BlockManagerInfo: Removed broadcast_313_piece0 on 172.18.0.15:34959 in memory (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 11:57:27 INFO BlockManagerInfo: Added broadcast_315_piece0 in memory on 172.18.0.15:34959 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 11:57:27 INFO BlockManagerInfo: Added broadcast_315_piece0 in memory on 172.18.0.14:40939 (size: 19.5 KiB, free: 434.4 MiB)
26/02/13 11:57:27 INFO BlockManagerInfo: Removed broadcast_310_piece0 on spark-master:40291 in memory (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 11:57:27 INFO BlockManagerInfo: Removed broadcast_310_piece0 on 172.18.0.15:34959 in memory (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 11:57:27 INFO BlockManagerInfo: Removed broadcast_312_piece0 on spark-master:40291 in memory (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 11:57:27 INFO BlockManagerInfo: Removed broadcast_312_piece0 on 172.18.0.15:34959 in memory (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 11:57:27 INFO BlockManagerInfo: Removed broadcast_312_piece0 on 172.18.0.14:40939 in memory (size: 19.5 KiB, free: 434.4 MiB)
26/02/13 11:57:27 INFO BlockManagerInfo: Added broadcast_314_piece0 in memory on 172.18.0.15:34959 (size: 107.2 KiB, free: 434.0 MiB)
26/02/13 11:57:27 INFO BlockManagerInfo: Added broadcast_314_piece0 in memory on 172.18.0.14:40939 (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 11:57:27 INFO TaskSetManager: Finished task 2.0 in stage 318.0 (TID 710) in 93 ms on 172.18.0.14 (executor 1) (1/3)
26/02/13 11:57:27 INFO TaskSetManager: Finished task 0.0 in stage 318.0 (TID 709) in 102 ms on 172.18.0.14 (executor 1) (2/3)
26/02/13 11:57:27 INFO TaskSetManager: Finished task 1.0 in stage 318.0 (TID 708) in 103 ms on 172.18.0.15 (executor 0) (3/3)
26/02/13 11:57:27 INFO TaskSchedulerImpl: Removed TaskSet 318.0, whose tasks have all completed, from pool 
26/02/13 11:57:27 INFO DAGScheduler: ResultStage 318 (start at NativeMethodAccessorImpl.java:0) finished in 0.118 s
26/02/13 11:57:27 INFO DAGScheduler: Job 210 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:57:27 INFO TaskSchedulerImpl: Killing all running tasks in stage 318: Stage finished
26/02/13 11:57:27 INFO DAGScheduler: Job 210 finished: start at NativeMethodAccessorImpl.java:0, took 0.122510 s
26/02/13 11:57:27 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 100, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@16cb94cd] is committing.
26/02/13 11:57:27 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 100, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@16cb94cd] committed.
26/02/13 11:57:27 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/100 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.100.e40a4918-4a97-42ca-8c63-14f6a471d116.tmp
26/02/13 11:57:27 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.100.e40a4918-4a97-42ca-8c63-14f6a471d116.tmp to file:/tmp/spark-checkpoint-enrichment/commits/100
26/02/13 11:57:27 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "fc5c6452-bd70-4c90-becf-772d3bebdb57",
  "name" : null,
  "timestamp" : "2026-02-13T11:57:27.009Z",
  "batchId" : 100,
  "numInputRows" : 244,
  "inputRowsPerSecond" : 211.25541125541125,
  "processedRowsPerSecond" : 552.0361990950227,
  "durationMs" : {
    "addBatch" : 247,
    "commitOffsets" : 108,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 23,
    "triggerExecution" : 442,
    "walCommit" : 63
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 6674,
        "1" : 7387,
        "0" : 8709
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 6752,
        "1" : 7478,
        "0" : 8784
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 6752,
        "1" : 7478,
        "0" : 8784
      }
    },
    "numInputRows" : 244,
    "inputRowsPerSecond" : 211.25541125541125,
    "processedRowsPerSecond" : 552.0361990950227,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@7fe9b5de",
    "numOutputRows" : 52
  }
}
26/02/13 11:57:28 INFO BlockManagerInfo: Removed broadcast_315_piece0 on spark-master:40291 in memory (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 11:57:28 INFO BlockManagerInfo: Removed broadcast_315_piece0 on 172.18.0.14:40939 in memory (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 11:57:28 INFO BlockManagerInfo: Removed broadcast_315_piece0 on 172.18.0.15:34959 in memory (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 11:57:37 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 11:57:42 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/101 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.101.c9cb9e55-98bd-4acd-a153-7ae1b410b878.tmp
26/02/13 11:57:42 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.101.c9cb9e55-98bd-4acd-a153-7ae1b410b878.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/101
26/02/13 11:57:42 INFO MicroBatchExecution: Committed offsets for batch 101. Metadata OffsetSeqMetadata(0,1770983862145,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 11:57:42 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:57:42 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:57:42 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:57:42 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:57:42 INFO BlockManagerInfo: Removed broadcast_314_piece0 on spark-master:40291 in memory (size: 107.2 KiB, free: 434.4 MiB)
26/02/13 11:57:42 INFO BlockManagerInfo: Removed broadcast_314_piece0 on 172.18.0.14:40939 in memory (size: 107.2 KiB, free: 434.4 MiB)
26/02/13 11:57:42 INFO BlockManagerInfo: Removed broadcast_314_piece0 on 172.18.0.15:34959 in memory (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 11:57:42 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:57:42 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:57:42 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#88205 - origin_code.nullCount#88204) > 0)
26/02/13 11:57:42 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#88210 - destination_code.nullCount#88209) > 0)
26/02/13 11:57:42 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#88240 - callsign.nullCount#88239) > 0)
26/02/13 11:57:42 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:57:42 INFO DAGScheduler: Got job 211 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 11:57:42 INFO DAGScheduler: Final stage: ResultStage 320 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:57:42 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 319)
26/02/13 11:57:42 INFO DAGScheduler: Missing parents: List()
26/02/13 11:57:42 INFO DAGScheduler: Submitting ResultStage 320 (MapPartitionsRDD[1154] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:57:42 INFO MemoryStore: Block broadcast_316 stored as values in memory (estimated size 99.3 KiB, free 434.1 MiB)
26/02/13 11:57:42 INFO MemoryStore: Block broadcast_316_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 434.0 MiB)
26/02/13 11:57:42 INFO BlockManagerInfo: Added broadcast_316_piece0 in memory on spark-master:40291 (size: 33.3 KiB, free: 434.3 MiB)
26/02/13 11:57:42 INFO SparkContext: Created broadcast 316 from broadcast at DAGScheduler.scala:1585
26/02/13 11:57:42 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 320 (MapPartitionsRDD[1154] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 11:57:42 INFO TaskSchedulerImpl: Adding task set 320.0 with 4 tasks resource profile 0
26/02/13 11:57:42 INFO TaskSetManager: Starting task 0.0 in stage 320.0 (TID 711) (172.18.0.15, executor 0, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:57:42 INFO TaskSetManager: Starting task 1.0 in stage 320.0 (TID 712) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:57:42 INFO BlockManagerInfo: Added broadcast_316_piece0 in memory on 172.18.0.15:34959 (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 11:57:42 INFO TaskSetManager: Starting task 2.0 in stage 320.0 (TID 713) (172.18.0.15, executor 0, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:57:42 INFO TaskSetManager: Finished task 0.0 in stage 320.0 (TID 711) in 14 ms on 172.18.0.15 (executor 0) (1/4)
26/02/13 11:57:42 INFO TaskSetManager: Starting task 3.0 in stage 320.0 (TID 714) (172.18.0.15, executor 0, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:57:42 INFO TaskSetManager: Finished task 1.0 in stage 320.0 (TID 712) in 15 ms on 172.18.0.15 (executor 0) (2/4)
26/02/13 11:57:42 INFO TaskSetManager: Finished task 3.0 in stage 320.0 (TID 714) in 8 ms on 172.18.0.15 (executor 0) (3/4)
26/02/13 11:57:42 INFO TaskSetManager: Finished task 2.0 in stage 320.0 (TID 713) in 14 ms on 172.18.0.15 (executor 0) (4/4)
26/02/13 11:57:42 INFO TaskSchedulerImpl: Removed TaskSet 320.0, whose tasks have all completed, from pool 
26/02/13 11:57:42 INFO DAGScheduler: ResultStage 320 (start at NativeMethodAccessorImpl.java:0) finished in 0.034 s
26/02/13 11:57:42 INFO DAGScheduler: Job 211 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:57:42 INFO TaskSchedulerImpl: Killing all running tasks in stage 320: Stage finished
26/02/13 11:57:42 INFO DAGScheduler: Job 211 finished: start at NativeMethodAccessorImpl.java:0, took 0.035486 s
26/02/13 11:57:42 INFO MemoryStore: Block broadcast_317_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.9 MiB)
26/02/13 11:57:42 INFO BlockManagerInfo: Added broadcast_317_piece0 in memory on spark-master:40291 (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 11:57:42 INFO SparkContext: Created broadcast 317 from start at NativeMethodAccessorImpl.java:0
26/02/13 11:57:42 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 101, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@22bc34c2]. The input RDD has 3 partitions.
26/02/13 11:57:42 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:57:42 INFO DAGScheduler: Got job 212 (start at NativeMethodAccessorImpl.java:0) with 3 output partitions
26/02/13 11:57:42 INFO DAGScheduler: Final stage: ResultStage 321 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:57:42 INFO DAGScheduler: Parents of final stage: List()
26/02/13 11:57:42 INFO DAGScheduler: Missing parents: List()
26/02/13 11:57:42 INFO DAGScheduler: Submitting ResultStage 321 (MapPartitionsRDD[1160] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:57:42 INFO MemoryStore: Block broadcast_318 stored as values in memory (estimated size 49.9 KiB, free 433.9 MiB)
26/02/13 11:57:42 INFO MemoryStore: Block broadcast_318_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.9 MiB)
26/02/13 11:57:42 INFO BlockManagerInfo: Added broadcast_318_piece0 in memory on spark-master:40291 (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 11:57:42 INFO SparkContext: Created broadcast 318 from broadcast at DAGScheduler.scala:1585
26/02/13 11:57:42 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 321 (MapPartitionsRDD[1160] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
26/02/13 11:57:42 INFO TaskSchedulerImpl: Adding task set 321.0 with 3 tasks resource profile 0
26/02/13 11:57:42 INFO TaskSetManager: Starting task 1.0 in stage 321.0 (TID 715) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:57:42 INFO TaskSetManager: Starting task 0.0 in stage 321.0 (TID 716) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:57:42 INFO TaskSetManager: Starting task 2.0 in stage 321.0 (TID 717) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:57:42 INFO BlockManagerInfo: Added broadcast_318_piece0 in memory on 172.18.0.15:34959 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 11:57:42 INFO BlockManagerInfo: Added broadcast_318_piece0 in memory on 172.18.0.14:40939 (size: 19.5 KiB, free: 434.4 MiB)
26/02/13 11:57:42 INFO BlockManagerInfo: Added broadcast_317_piece0 in memory on 172.18.0.14:40939 (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 11:57:42 INFO BlockManagerInfo: Added broadcast_317_piece0 in memory on 172.18.0.15:34959 (size: 107.2 KiB, free: 434.0 MiB)
26/02/13 11:57:42 INFO TaskSetManager: Finished task 2.0 in stage 321.0 (TID 717) in 547 ms on 172.18.0.14 (executor 1) (1/3)
26/02/13 11:57:42 INFO TaskSetManager: Finished task 0.0 in stage 321.0 (TID 716) in 547 ms on 172.18.0.14 (executor 1) (2/3)
26/02/13 11:57:42 INFO TaskSetManager: Finished task 1.0 in stage 321.0 (TID 715) in 551 ms on 172.18.0.15 (executor 0) (3/3)
26/02/13 11:57:42 INFO TaskSchedulerImpl: Removed TaskSet 321.0, whose tasks have all completed, from pool 
26/02/13 11:57:42 INFO DAGScheduler: ResultStage 321 (start at NativeMethodAccessorImpl.java:0) finished in 0.557 s
26/02/13 11:57:42 INFO DAGScheduler: Job 212 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:57:42 INFO TaskSchedulerImpl: Killing all running tasks in stage 321: Stage finished
26/02/13 11:57:42 INFO DAGScheduler: Job 212 finished: start at NativeMethodAccessorImpl.java:0, took 0.558230 s
26/02/13 11:57:42 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 101, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@22bc34c2] is committing.
26/02/13 11:57:42 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 101, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@22bc34c2] committed.
26/02/13 11:57:42 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/101 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.101.145a6c1d-76f4-479e-848b-728672ad9e4c.tmp
26/02/13 11:57:43 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.101.145a6c1d-76f4-479e-848b-728672ad9e4c.tmp to file:/tmp/spark-checkpoint-enrichment/commits/101
26/02/13 11:57:43 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "fc5c6452-bd70-4c90-becf-772d3bebdb57",
  "name" : null,
  "timestamp" : "2026-02-13T11:57:42.142Z",
  "batchId" : 101,
  "numInputRows" : 50,
  "inputRowsPerSecond" : 4545.454545454546,
  "processedRowsPerSecond" : 57.077625570776256,
  "durationMs" : {
    "addBatch" : 702,
    "commitOffsets" : 56,
    "getBatch" : 0,
    "latestOffset" : 3,
    "queryPlanning" : 36,
    "triggerExecution" : 876,
    "walCommit" : 78
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 6752,
        "1" : 7478,
        "0" : 8784
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 6767,
        "1" : 7501,
        "0" : 8796
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 6767,
        "1" : 7501,
        "0" : 8796
      }
    },
    "numInputRows" : 50,
    "inputRowsPerSecond" : 4545.454545454546,
    "processedRowsPerSecond" : 57.077625570776256,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@7fe9b5de",
    "numOutputRows" : 12
  }
}
26/02/13 11:57:43 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/102 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.102.8767308c-5a70-41ca-ac9b-c46a69db2bb8.tmp
26/02/13 11:57:43 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.102.8767308c-5a70-41ca-ac9b-c46a69db2bb8.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/102
26/02/13 11:57:43 INFO MicroBatchExecution: Committed offsets for batch 102. Metadata OffsetSeqMetadata(0,1770983863020,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 11:57:43 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:57:43 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:57:43 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:57:43 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:57:43 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:57:43 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:57:43 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#89059 - origin_code.nullCount#89058) > 0)
26/02/13 11:57:43 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#89064 - destination_code.nullCount#89063) > 0)
26/02/13 11:57:43 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#89094 - callsign.nullCount#89093) > 0)
26/02/13 11:57:43 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:57:43 INFO DAGScheduler: Got job 213 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 11:57:43 INFO DAGScheduler: Final stage: ResultStage 323 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:57:43 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 322)
26/02/13 11:57:43 INFO DAGScheduler: Missing parents: List()
26/02/13 11:57:43 INFO DAGScheduler: Submitting ResultStage 323 (MapPartitionsRDD[1165] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:57:43 INFO MemoryStore: Block broadcast_319 stored as values in memory (estimated size 99.3 KiB, free 433.8 MiB)
26/02/13 11:57:43 INFO MemoryStore: Block broadcast_319_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 433.7 MiB)
26/02/13 11:57:43 INFO BlockManagerInfo: Added broadcast_319_piece0 in memory on spark-master:40291 (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 11:57:43 INFO SparkContext: Created broadcast 319 from broadcast at DAGScheduler.scala:1585
26/02/13 11:57:43 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 323 (MapPartitionsRDD[1165] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 11:57:43 INFO TaskSchedulerImpl: Adding task set 323.0 with 4 tasks resource profile 0
26/02/13 11:57:43 INFO TaskSetManager: Starting task 0.0 in stage 323.0 (TID 718) (172.18.0.15, executor 0, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:57:43 INFO TaskSetManager: Starting task 1.0 in stage 323.0 (TID 719) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:57:43 INFO BlockManagerInfo: Added broadcast_319_piece0 in memory on 172.18.0.15:34959 (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 11:57:43 INFO TaskSetManager: Starting task 2.0 in stage 323.0 (TID 720) (172.18.0.15, executor 0, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:57:43 INFO TaskSetManager: Finished task 0.0 in stage 323.0 (TID 718) in 16 ms on 172.18.0.15 (executor 0) (1/4)
26/02/13 11:57:43 INFO TaskSetManager: Starting task 3.0 in stage 323.0 (TID 721) (172.18.0.15, executor 0, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:57:43 INFO TaskSetManager: Finished task 1.0 in stage 323.0 (TID 719) in 19 ms on 172.18.0.15 (executor 0) (2/4)
26/02/13 11:57:43 INFO TaskSetManager: Finished task 2.0 in stage 323.0 (TID 720) in 9 ms on 172.18.0.15 (executor 0) (3/4)
26/02/13 11:57:43 INFO TaskSetManager: Finished task 3.0 in stage 323.0 (TID 721) in 17 ms on 172.18.0.15 (executor 0) (4/4)
26/02/13 11:57:43 INFO TaskSchedulerImpl: Removed TaskSet 323.0, whose tasks have all completed, from pool 
26/02/13 11:57:43 INFO DAGScheduler: ResultStage 323 (start at NativeMethodAccessorImpl.java:0) finished in 0.041 s
26/02/13 11:57:43 INFO DAGScheduler: Job 213 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:57:43 INFO TaskSchedulerImpl: Killing all running tasks in stage 323: Stage finished
26/02/13 11:57:43 INFO DAGScheduler: Job 213 finished: start at NativeMethodAccessorImpl.java:0, took 0.043038 s
26/02/13 11:57:43 INFO MemoryStore: Block broadcast_320_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.6 MiB)
26/02/13 11:57:43 INFO BlockManagerInfo: Added broadcast_320_piece0 in memory on spark-master:40291 (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 11:57:43 INFO SparkContext: Created broadcast 320 from start at NativeMethodAccessorImpl.java:0
26/02/13 11:57:43 INFO BlockManagerInfo: Removed broadcast_316_piece0 on spark-master:40291 in memory (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 11:57:43 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 102, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@26b1a16e]. The input RDD has 3 partitions.
26/02/13 11:57:43 INFO BlockManagerInfo: Removed broadcast_316_piece0 on 172.18.0.15:34959 in memory (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 11:57:43 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:57:43 INFO DAGScheduler: Got job 214 (start at NativeMethodAccessorImpl.java:0) with 3 output partitions
26/02/13 11:57:43 INFO DAGScheduler: Final stage: ResultStage 324 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:57:43 INFO DAGScheduler: Parents of final stage: List()
26/02/13 11:57:43 INFO DAGScheduler: Missing parents: List()
26/02/13 11:57:43 INFO DAGScheduler: Submitting ResultStage 324 (MapPartitionsRDD[1171] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:57:43 INFO MemoryStore: Block broadcast_321 stored as values in memory (estimated size 49.9 KiB, free 433.7 MiB)
26/02/13 11:57:43 INFO MemoryStore: Block broadcast_321_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.7 MiB)
26/02/13 11:57:43 INFO BlockManagerInfo: Added broadcast_321_piece0 in memory on spark-master:40291 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 11:57:43 INFO BlockManagerInfo: Removed broadcast_318_piece0 on spark-master:40291 in memory (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 11:57:43 INFO SparkContext: Created broadcast 321 from broadcast at DAGScheduler.scala:1585
26/02/13 11:57:43 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 324 (MapPartitionsRDD[1171] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
26/02/13 11:57:43 INFO TaskSchedulerImpl: Adding task set 324.0 with 3 tasks resource profile 0
26/02/13 11:57:43 INFO BlockManagerInfo: Removed broadcast_318_piece0 on 172.18.0.15:34959 in memory (size: 19.5 KiB, free: 434.0 MiB)
26/02/13 11:57:43 INFO TaskSetManager: Starting task 1.0 in stage 324.0 (TID 722) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:57:43 INFO TaskSetManager: Starting task 0.0 in stage 324.0 (TID 723) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:57:43 INFO BlockManagerInfo: Removed broadcast_318_piece0 on 172.18.0.14:40939 in memory (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 11:57:43 INFO TaskSetManager: Starting task 2.0 in stage 324.0 (TID 724) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:57:43 INFO BlockManagerInfo: Removed broadcast_317_piece0 on spark-master:40291 in memory (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 11:57:43 INFO BlockManagerInfo: Removed broadcast_317_piece0 on 172.18.0.15:34959 in memory (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 11:57:43 INFO BlockManagerInfo: Removed broadcast_317_piece0 on 172.18.0.14:40939 in memory (size: 107.2 KiB, free: 434.4 MiB)
26/02/13 11:57:43 INFO BlockManagerInfo: Added broadcast_321_piece0 in memory on 172.18.0.15:34959 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 11:57:43 INFO BlockManagerInfo: Added broadcast_321_piece0 in memory on 172.18.0.14:40939 (size: 19.5 KiB, free: 434.4 MiB)
26/02/13 11:57:43 INFO BlockManagerInfo: Removed broadcast_319_piece0 on spark-master:40291 in memory (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 11:57:43 INFO BlockManagerInfo: Removed broadcast_319_piece0 on 172.18.0.15:34959 in memory (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 11:57:43 INFO BlockManagerInfo: Added broadcast_320_piece0 in memory on 172.18.0.14:40939 (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 11:57:43 INFO BlockManagerInfo: Added broadcast_320_piece0 in memory on 172.18.0.15:34959 (size: 107.2 KiB, free: 434.0 MiB)
26/02/13 11:57:43 INFO TaskSetManager: Finished task 1.0 in stage 324.0 (TID 722) in 81 ms on 172.18.0.15 (executor 0) (1/3)
26/02/13 11:57:43 INFO TaskSetManager: Finished task 0.0 in stage 324.0 (TID 723) in 79 ms on 172.18.0.14 (executor 1) (2/3)
26/02/13 11:57:43 INFO TaskSetManager: Finished task 2.0 in stage 324.0 (TID 724) in 79 ms on 172.18.0.14 (executor 1) (3/3)
26/02/13 11:57:43 INFO TaskSchedulerImpl: Removed TaskSet 324.0, whose tasks have all completed, from pool 
26/02/13 11:57:43 INFO DAGScheduler: ResultStage 324 (start at NativeMethodAccessorImpl.java:0) finished in 0.092 s
26/02/13 11:57:43 INFO DAGScheduler: Job 214 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:57:43 INFO TaskSchedulerImpl: Killing all running tasks in stage 324: Stage finished
26/02/13 11:57:43 INFO DAGScheduler: Job 214 finished: start at NativeMethodAccessorImpl.java:0, took 0.094362 s
26/02/13 11:57:43 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 102, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@26b1a16e] is committing.
26/02/13 11:57:43 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 102, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@26b1a16e] committed.
26/02/13 11:57:43 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/102 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.102.470a4469-f2a0-413f-bd41-f5470bdebaa1.tmp
26/02/13 11:57:43 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.102.470a4469-f2a0-413f-bd41-f5470bdebaa1.tmp to file:/tmp/spark-checkpoint-enrichment/commits/102
26/02/13 11:57:43 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "fc5c6452-bd70-4c90-becf-772d3bebdb57",
  "name" : null,
  "timestamp" : "2026-02-13T11:57:43.019Z",
  "batchId" : 102,
  "numInputRows" : 198,
  "inputRowsPerSecond" : 225.769669327252,
  "processedRowsPerSecond" : 539.5095367847412,
  "durationMs" : {
    "addBatch" : 224,
    "commitOffsets" : 59,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 22,
    "triggerExecution" : 367,
    "walCommit" : 61
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 6767,
        "1" : 7501,
        "0" : 8796
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 6831,
        "1" : 7571,
        "0" : 8860
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 6831,
        "1" : 7571,
        "0" : 8860
      }
    },
    "numInputRows" : 198,
    "inputRowsPerSecond" : 225.769669327252,
    "processedRowsPerSecond" : 539.5095367847412,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@7fe9b5de",
    "numOutputRows" : 41
  }
}
26/02/13 11:57:43 INFO BlockManagerInfo: Removed broadcast_321_piece0 on spark-master:40291 in memory (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 11:57:43 INFO BlockManagerInfo: Removed broadcast_321_piece0 on 172.18.0.14:40939 in memory (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 11:57:43 INFO BlockManagerInfo: Removed broadcast_321_piece0 on 172.18.0.15:34959 in memory (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 11:57:53 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 11:58:00 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/103 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.103.1d2c85a2-dcce-4790-9b4e-c568f09e75c0.tmp
26/02/13 11:58:00 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.103.1d2c85a2-dcce-4790-9b4e-c568f09e75c0.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/103
26/02/13 11:58:00 INFO MicroBatchExecution: Committed offsets for batch 103. Metadata OffsetSeqMetadata(0,1770983880779,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 11:58:00 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:58:00 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:58:00 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:58:00 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:58:00 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:58:00 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:58:00 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#89913 - origin_code.nullCount#89912) > 0)
26/02/13 11:58:00 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#89918 - destination_code.nullCount#89917) > 0)
26/02/13 11:58:00 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#89948 - callsign.nullCount#89947) > 0)
26/02/13 11:58:00 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:58:00 INFO DAGScheduler: Got job 215 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 11:58:00 INFO DAGScheduler: Final stage: ResultStage 326 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:58:00 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 325)
26/02/13 11:58:00 INFO DAGScheduler: Missing parents: List()
26/02/13 11:58:00 INFO DAGScheduler: Submitting ResultStage 326 (MapPartitionsRDD[1176] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:58:00 INFO MemoryStore: Block broadcast_322 stored as values in memory (estimated size 99.3 KiB, free 434.0 MiB)
26/02/13 11:58:00 INFO MemoryStore: Block broadcast_322_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 433.9 MiB)
26/02/13 11:58:00 INFO BlockManagerInfo: Added broadcast_322_piece0 in memory on spark-master:40291 (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 11:58:00 INFO SparkContext: Created broadcast 322 from broadcast at DAGScheduler.scala:1585
26/02/13 11:58:00 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 326 (MapPartitionsRDD[1176] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 11:58:00 INFO TaskSchedulerImpl: Adding task set 326.0 with 4 tasks resource profile 0
26/02/13 11:58:00 INFO TaskSetManager: Starting task 0.0 in stage 326.0 (TID 725) (172.18.0.15, executor 0, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:58:00 INFO TaskSetManager: Starting task 1.0 in stage 326.0 (TID 726) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:58:00 INFO BlockManagerInfo: Added broadcast_322_piece0 in memory on 172.18.0.15:34959 (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 11:58:00 INFO TaskSetManager: Starting task 2.0 in stage 326.0 (TID 727) (172.18.0.15, executor 0, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:58:00 INFO TaskSetManager: Finished task 1.0 in stage 326.0 (TID 726) in 14 ms on 172.18.0.15 (executor 0) (1/4)
26/02/13 11:58:00 INFO TaskSetManager: Starting task 3.0 in stage 326.0 (TID 728) (172.18.0.15, executor 0, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:58:00 INFO TaskSetManager: Finished task 0.0 in stage 326.0 (TID 725) in 19 ms on 172.18.0.15 (executor 0) (2/4)
26/02/13 11:58:00 INFO TaskSetManager: Finished task 2.0 in stage 326.0 (TID 727) in 9 ms on 172.18.0.15 (executor 0) (3/4)
26/02/13 11:58:00 INFO TaskSetManager: Finished task 3.0 in stage 326.0 (TID 728) in 13 ms on 172.18.0.15 (executor 0) (4/4)
26/02/13 11:58:00 INFO TaskSchedulerImpl: Removed TaskSet 326.0, whose tasks have all completed, from pool 
26/02/13 11:58:00 INFO DAGScheduler: ResultStage 326 (start at NativeMethodAccessorImpl.java:0) finished in 0.040 s
26/02/13 11:58:00 INFO DAGScheduler: Job 215 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:58:00 INFO TaskSchedulerImpl: Killing all running tasks in stage 326: Stage finished
26/02/13 11:58:00 INFO DAGScheduler: Job 215 finished: start at NativeMethodAccessorImpl.java:0, took 0.041169 s
26/02/13 11:58:01 INFO MemoryStore: Block broadcast_323_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.8 MiB)
26/02/13 11:58:01 INFO BlockManagerInfo: Added broadcast_323_piece0 in memory on spark-master:40291 (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 11:58:01 INFO SparkContext: Created broadcast 323 from start at NativeMethodAccessorImpl.java:0
26/02/13 11:58:01 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 103, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@5a8260a8]. The input RDD has 3 partitions.
26/02/13 11:58:01 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:58:01 INFO DAGScheduler: Got job 216 (start at NativeMethodAccessorImpl.java:0) with 3 output partitions
26/02/13 11:58:01 INFO DAGScheduler: Final stage: ResultStage 327 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:58:01 INFO DAGScheduler: Parents of final stage: List()
26/02/13 11:58:01 INFO DAGScheduler: Missing parents: List()
26/02/13 11:58:01 INFO DAGScheduler: Submitting ResultStage 327 (MapPartitionsRDD[1182] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:58:01 INFO MemoryStore: Block broadcast_324 stored as values in memory (estimated size 49.9 KiB, free 433.8 MiB)
26/02/13 11:58:01 INFO MemoryStore: Block broadcast_324_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.8 MiB)
26/02/13 11:58:01 INFO BlockManagerInfo: Added broadcast_324_piece0 in memory on spark-master:40291 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 11:58:01 INFO SparkContext: Created broadcast 324 from broadcast at DAGScheduler.scala:1585
26/02/13 11:58:01 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 327 (MapPartitionsRDD[1182] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
26/02/13 11:58:01 INFO TaskSchedulerImpl: Adding task set 327.0 with 3 tasks resource profile 0
26/02/13 11:58:01 INFO TaskSetManager: Starting task 0.0 in stage 327.0 (TID 729) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:58:01 INFO TaskSetManager: Starting task 1.0 in stage 327.0 (TID 730) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:58:01 INFO TaskSetManager: Starting task 2.0 in stage 327.0 (TID 731) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:58:01 INFO BlockManagerInfo: Added broadcast_324_piece0 in memory on 172.18.0.15:34959 (size: 19.5 KiB, free: 434.0 MiB)
26/02/13 11:58:01 INFO BlockManagerInfo: Added broadcast_324_piece0 in memory on 172.18.0.14:40939 (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 11:58:01 INFO BlockManagerInfo: Added broadcast_323_piece0 in memory on 172.18.0.15:34959 (size: 107.2 KiB, free: 433.9 MiB)
26/02/13 11:58:01 INFO BlockManagerInfo: Added broadcast_323_piece0 in memory on 172.18.0.14:40939 (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 11:58:01 INFO TaskSetManager: Finished task 2.0 in stage 327.0 (TID 731) in 554 ms on 172.18.0.14 (executor 1) (1/3)
26/02/13 11:58:01 INFO TaskSetManager: Finished task 0.0 in stage 327.0 (TID 729) in 556 ms on 172.18.0.14 (executor 1) (2/3)
26/02/13 11:58:01 INFO TaskSetManager: Finished task 1.0 in stage 327.0 (TID 730) in 557 ms on 172.18.0.15 (executor 0) (3/3)
26/02/13 11:58:01 INFO TaskSchedulerImpl: Removed TaskSet 327.0, whose tasks have all completed, from pool 
26/02/13 11:58:01 INFO DAGScheduler: ResultStage 327 (start at NativeMethodAccessorImpl.java:0) finished in 0.562 s
26/02/13 11:58:01 INFO DAGScheduler: Job 216 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:58:01 INFO TaskSchedulerImpl: Killing all running tasks in stage 327: Stage finished
26/02/13 11:58:01 INFO DAGScheduler: Job 216 finished: start at NativeMethodAccessorImpl.java:0, took 0.563906 s
26/02/13 11:58:01 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 103, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@5a8260a8] is committing.
26/02/13 11:58:01 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 103, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@5a8260a8] committed.
26/02/13 11:58:01 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/103 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.103.b93489dc-84ab-42d1-9096-b99d763a688f.tmp
26/02/13 11:58:01 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.103.b93489dc-84ab-42d1-9096-b99d763a688f.tmp to file:/tmp/spark-checkpoint-enrichment/commits/103
26/02/13 11:58:01 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "fc5c6452-bd70-4c90-becf-772d3bebdb57",
  "name" : null,
  "timestamp" : "2026-02-13T11:58:00.777Z",
  "batchId" : 103,
  "numInputRows" : 109,
  "inputRowsPerSecond" : 9083.333333333334,
  "processedRowsPerSecond" : 127.33644859813084,
  "durationMs" : {
    "addBatch" : 685,
    "commitOffsets" : 59,
    "getBatch" : 0,
    "latestOffset" : 2,
    "queryPlanning" : 21,
    "triggerExecution" : 856,
    "walCommit" : 88
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 6831,
        "1" : 7571,
        "0" : 8860
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 6868,
        "1" : 7608,
        "0" : 8895
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 6868,
        "1" : 7608,
        "0" : 8895
      }
    },
    "numInputRows" : 109,
    "inputRowsPerSecond" : 9083.333333333334,
    "processedRowsPerSecond" : 127.33644859813084,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@7fe9b5de",
    "numOutputRows" : 27
  }
}
26/02/13 11:58:01 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/104 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.104.6a1c2263-9b7e-46fa-acce-ee94323097f3.tmp
26/02/13 11:58:01 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.104.6a1c2263-9b7e-46fa-acce-ee94323097f3.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/104
26/02/13 11:58:01 INFO MicroBatchExecution: Committed offsets for batch 104. Metadata OffsetSeqMetadata(0,1770983881635,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 11:58:01 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:58:01 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:58:01 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:58:01 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:58:01 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:58:01 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:58:01 INFO BlockManagerInfo: Removed broadcast_323_piece0 on spark-master:40291 in memory (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 11:58:01 INFO BlockManagerInfo: Removed broadcast_323_piece0 on 172.18.0.15:34959 in memory (size: 107.2 KiB, free: 434.0 MiB)
26/02/13 11:58:01 INFO BlockManagerInfo: Removed broadcast_323_piece0 on 172.18.0.14:40939 in memory (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 11:58:01 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#90767 - origin_code.nullCount#90766) > 0)
26/02/13 11:58:01 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#90772 - destination_code.nullCount#90771) > 0)
26/02/13 11:58:01 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#90802 - callsign.nullCount#90801) > 0)
26/02/13 11:58:01 INFO BlockManagerInfo: Removed broadcast_324_piece0 on 172.18.0.14:40939 in memory (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 11:58:01 INFO BlockManagerInfo: Removed broadcast_324_piece0 on 172.18.0.15:34959 in memory (size: 19.5 KiB, free: 434.0 MiB)
26/02/13 11:58:01 INFO BlockManagerInfo: Removed broadcast_324_piece0 on spark-master:40291 in memory (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 11:58:01 INFO BlockManagerInfo: Removed broadcast_322_piece0 on spark-master:40291 in memory (size: 33.3 KiB, free: 434.3 MiB)
26/02/13 11:58:01 INFO BlockManagerInfo: Removed broadcast_322_piece0 on 172.18.0.15:34959 in memory (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 11:58:01 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:58:01 INFO DAGScheduler: Got job 217 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 11:58:01 INFO DAGScheduler: Final stage: ResultStage 329 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:58:01 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 328)
26/02/13 11:58:01 INFO DAGScheduler: Missing parents: List()
26/02/13 11:58:01 INFO DAGScheduler: Submitting ResultStage 329 (MapPartitionsRDD[1187] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:58:01 INFO MemoryStore: Block broadcast_325 stored as values in memory (estimated size 99.3 KiB, free 434.0 MiB)
26/02/13 11:58:01 INFO MemoryStore: Block broadcast_325_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 433.9 MiB)
26/02/13 11:58:01 INFO BlockManagerInfo: Added broadcast_325_piece0 in memory on spark-master:40291 (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 11:58:01 INFO SparkContext: Created broadcast 325 from broadcast at DAGScheduler.scala:1585
26/02/13 11:58:01 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 329 (MapPartitionsRDD[1187] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 11:58:01 INFO TaskSchedulerImpl: Adding task set 329.0 with 4 tasks resource profile 0
26/02/13 11:58:01 INFO TaskSetManager: Starting task 0.0 in stage 329.0 (TID 732) (172.18.0.15, executor 0, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:58:01 INFO TaskSetManager: Starting task 1.0 in stage 329.0 (TID 733) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:58:01 INFO BlockManagerInfo: Added broadcast_325_piece0 in memory on 172.18.0.15:34959 (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 11:58:01 INFO TaskSetManager: Starting task 2.0 in stage 329.0 (TID 734) (172.18.0.15, executor 0, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:58:01 INFO TaskSetManager: Finished task 0.0 in stage 329.0 (TID 732) in 25 ms on 172.18.0.15 (executor 0) (1/4)
26/02/13 11:58:01 INFO TaskSetManager: Starting task 3.0 in stage 329.0 (TID 735) (172.18.0.15, executor 0, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:58:01 INFO TaskSetManager: Finished task 1.0 in stage 329.0 (TID 733) in 30 ms on 172.18.0.15 (executor 0) (2/4)
26/02/13 11:58:01 INFO TaskSetManager: Finished task 2.0 in stage 329.0 (TID 734) in 14 ms on 172.18.0.15 (executor 0) (3/4)
26/02/13 11:58:01 INFO TaskSetManager: Finished task 3.0 in stage 329.0 (TID 735) in 13 ms on 172.18.0.15 (executor 0) (4/4)
26/02/13 11:58:01 INFO TaskSchedulerImpl: Removed TaskSet 329.0, whose tasks have all completed, from pool 
26/02/13 11:58:01 INFO DAGScheduler: ResultStage 329 (start at NativeMethodAccessorImpl.java:0) finished in 0.052 s
26/02/13 11:58:01 INFO DAGScheduler: Job 217 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:58:01 INFO TaskSchedulerImpl: Killing all running tasks in stage 329: Stage finished
26/02/13 11:58:01 INFO DAGScheduler: Job 217 finished: start at NativeMethodAccessorImpl.java:0, took 0.054018 s
26/02/13 11:58:01 INFO MemoryStore: Block broadcast_326_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.8 MiB)
26/02/13 11:58:01 INFO BlockManagerInfo: Added broadcast_326_piece0 in memory on spark-master:40291 (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 11:58:01 INFO SparkContext: Created broadcast 326 from start at NativeMethodAccessorImpl.java:0
26/02/13 11:58:01 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 104, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@13e58a61]. The input RDD has 3 partitions.
26/02/13 11:58:01 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:58:01 INFO DAGScheduler: Got job 218 (start at NativeMethodAccessorImpl.java:0) with 3 output partitions
26/02/13 11:58:01 INFO DAGScheduler: Final stage: ResultStage 330 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:58:01 INFO DAGScheduler: Parents of final stage: List()
26/02/13 11:58:01 INFO DAGScheduler: Missing parents: List()
26/02/13 11:58:01 INFO DAGScheduler: Submitting ResultStage 330 (MapPartitionsRDD[1193] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:58:01 INFO MemoryStore: Block broadcast_327 stored as values in memory (estimated size 49.9 KiB, free 433.8 MiB)
26/02/13 11:58:01 INFO MemoryStore: Block broadcast_327_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.8 MiB)
26/02/13 11:58:01 INFO BlockManagerInfo: Added broadcast_327_piece0 in memory on spark-master:40291 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 11:58:01 INFO BlockManagerInfo: Removed broadcast_320_piece0 on spark-master:40291 in memory (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 11:58:01 INFO SparkContext: Created broadcast 327 from broadcast at DAGScheduler.scala:1585
26/02/13 11:58:01 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 330 (MapPartitionsRDD[1193] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
26/02/13 11:58:01 INFO TaskSchedulerImpl: Adding task set 330.0 with 3 tasks resource profile 0
26/02/13 11:58:01 INFO BlockManagerInfo: Removed broadcast_320_piece0 on 172.18.0.14:40939 in memory (size: 107.2 KiB, free: 434.4 MiB)
26/02/13 11:58:01 INFO BlockManagerInfo: Removed broadcast_320_piece0 on 172.18.0.15:34959 in memory (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 11:58:01 INFO TaskSetManager: Starting task 1.0 in stage 330.0 (TID 736) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:58:01 INFO TaskSetManager: Starting task 0.0 in stage 330.0 (TID 737) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:58:01 INFO TaskSetManager: Starting task 2.0 in stage 330.0 (TID 738) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:58:01 INFO BlockManagerInfo: Added broadcast_327_piece0 in memory on 172.18.0.15:34959 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 11:58:01 INFO BlockManagerInfo: Added broadcast_327_piece0 in memory on 172.18.0.14:40939 (size: 19.5 KiB, free: 434.4 MiB)
26/02/13 11:58:01 INFO BlockManagerInfo: Added broadcast_326_piece0 in memory on 172.18.0.15:34959 (size: 107.2 KiB, free: 434.0 MiB)
26/02/13 11:58:01 INFO BlockManagerInfo: Added broadcast_326_piece0 in memory on 172.18.0.14:40939 (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 11:58:02 INFO TaskSetManager: Finished task 2.0 in stage 330.0 (TID 738) in 74 ms on 172.18.0.14 (executor 1) (1/3)
26/02/13 11:58:02 INFO TaskSetManager: Finished task 1.0 in stage 330.0 (TID 736) in 77 ms on 172.18.0.15 (executor 0) (2/3)
26/02/13 11:58:02 INFO TaskSetManager: Finished task 0.0 in stage 330.0 (TID 737) in 76 ms on 172.18.0.14 (executor 1) (3/3)
26/02/13 11:58:02 INFO TaskSchedulerImpl: Removed TaskSet 330.0, whose tasks have all completed, from pool 
26/02/13 11:58:02 INFO DAGScheduler: ResultStage 330 (start at NativeMethodAccessorImpl.java:0) finished in 0.096 s
26/02/13 11:58:02 INFO DAGScheduler: Job 218 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:58:02 INFO TaskSchedulerImpl: Killing all running tasks in stage 330: Stage finished
26/02/13 11:58:02 INFO DAGScheduler: Job 218 finished: start at NativeMethodAccessorImpl.java:0, took 0.098103 s
26/02/13 11:58:02 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 104, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@13e58a61] is committing.
26/02/13 11:58:02 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 104, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@13e58a61] committed.
26/02/13 11:58:02 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/104 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.104.267df736-f3d8-42d6-b902-4f089a641e50.tmp
26/02/13 11:58:02 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.104.267df736-f3d8-42d6-b902-4f089a641e50.tmp to file:/tmp/spark-checkpoint-enrichment/commits/104
26/02/13 11:58:02 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "fc5c6452-bd70-4c90-becf-772d3bebdb57",
  "name" : null,
  "timestamp" : "2026-02-13T11:58:01.634Z",
  "batchId" : 104,
  "numInputRows" : 144,
  "inputRowsPerSecond" : 168.02800466744458,
  "processedRowsPerSecond" : 331.0344827586207,
  "durationMs" : {
    "addBatch" : 273,
    "commitOffsets" : 66,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 26,
    "triggerExecution" : 435,
    "walCommit" : 69
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 6868,
        "1" : 7608,
        "0" : 8895
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 6912,
        "1" : 7666,
        "0" : 8937
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 6912,
        "1" : 7666,
        "0" : 8937
      }
    },
    "numInputRows" : 144,
    "inputRowsPerSecond" : 168.02800466744458,
    "processedRowsPerSecond" : 331.0344827586207,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@7fe9b5de",
    "numOutputRows" : 27
  }
}
26/02/13 11:58:12 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 11:58:14 INFO BlockManagerInfo: Removed broadcast_325_piece0 on spark-master:40291 in memory (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 11:58:14 INFO BlockManagerInfo: Removed broadcast_325_piece0 on 172.18.0.15:34959 in memory (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 11:58:14 INFO BlockManagerInfo: Removed broadcast_327_piece0 on spark-master:40291 in memory (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 11:58:14 INFO BlockManagerInfo: Removed broadcast_327_piece0 on 172.18.0.15:34959 in memory (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 11:58:14 INFO BlockManagerInfo: Removed broadcast_327_piece0 on 172.18.0.14:40939 in memory (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 11:58:17 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/105 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.105.7b3a230f-086c-4abf-9ae9-bbe4e1d3fcf3.tmp
26/02/13 11:58:17 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.105.7b3a230f-086c-4abf-9ae9-bbe4e1d3fcf3.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/105
26/02/13 11:58:17 INFO MicroBatchExecution: Committed offsets for batch 105. Metadata OffsetSeqMetadata(0,1770983897181,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 11:58:17 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:58:17 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:58:17 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:58:17 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:58:17 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:58:17 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:58:17 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#91621 - origin_code.nullCount#91620) > 0)
26/02/13 11:58:17 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#91626 - destination_code.nullCount#91625) > 0)
26/02/13 11:58:17 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#91656 - callsign.nullCount#91655) > 0)
26/02/13 11:58:17 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:58:17 INFO DAGScheduler: Got job 219 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 11:58:17 INFO DAGScheduler: Final stage: ResultStage 332 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:58:17 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 331)
26/02/13 11:58:17 INFO DAGScheduler: Missing parents: List()
26/02/13 11:58:17 INFO DAGScheduler: Submitting ResultStage 332 (MapPartitionsRDD[1198] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:58:17 INFO MemoryStore: Block broadcast_328 stored as values in memory (estimated size 99.3 KiB, free 434.0 MiB)
26/02/13 11:58:17 INFO MemoryStore: Block broadcast_328_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 433.9 MiB)
26/02/13 11:58:17 INFO BlockManagerInfo: Added broadcast_328_piece0 in memory on spark-master:40291 (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 11:58:17 INFO SparkContext: Created broadcast 328 from broadcast at DAGScheduler.scala:1585
26/02/13 11:58:17 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 332 (MapPartitionsRDD[1198] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 11:58:17 INFO TaskSchedulerImpl: Adding task set 332.0 with 4 tasks resource profile 0
26/02/13 11:58:17 INFO TaskSetManager: Starting task 0.0 in stage 332.0 (TID 739) (172.18.0.15, executor 0, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:58:17 INFO TaskSetManager: Starting task 1.0 in stage 332.0 (TID 740) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:58:17 INFO BlockManagerInfo: Added broadcast_328_piece0 in memory on 172.18.0.15:34959 (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 11:58:17 INFO TaskSetManager: Starting task 2.0 in stage 332.0 (TID 741) (172.18.0.15, executor 0, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:58:17 INFO TaskSetManager: Finished task 0.0 in stage 332.0 (TID 739) in 20 ms on 172.18.0.15 (executor 0) (1/4)
26/02/13 11:58:17 INFO TaskSetManager: Starting task 3.0 in stage 332.0 (TID 742) (172.18.0.15, executor 0, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:58:17 INFO TaskSetManager: Finished task 1.0 in stage 332.0 (TID 740) in 23 ms on 172.18.0.15 (executor 0) (2/4)
26/02/13 11:58:17 INFO TaskSetManager: Finished task 2.0 in stage 332.0 (TID 741) in 22 ms on 172.18.0.15 (executor 0) (3/4)
26/02/13 11:58:17 INFO TaskSetManager: Finished task 3.0 in stage 332.0 (TID 742) in 20 ms on 172.18.0.15 (executor 0) (4/4)
26/02/13 11:58:17 INFO TaskSchedulerImpl: Removed TaskSet 332.0, whose tasks have all completed, from pool 
26/02/13 11:58:17 INFO DAGScheduler: ResultStage 332 (start at NativeMethodAccessorImpl.java:0) finished in 0.050 s
26/02/13 11:58:17 INFO DAGScheduler: Job 219 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:58:17 INFO TaskSchedulerImpl: Killing all running tasks in stage 332: Stage finished
26/02/13 11:58:17 INFO DAGScheduler: Job 219 finished: start at NativeMethodAccessorImpl.java:0, took 0.050995 s
26/02/13 11:58:17 INFO MemoryStore: Block broadcast_329_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.8 MiB)
26/02/13 11:58:17 INFO BlockManagerInfo: Added broadcast_329_piece0 in memory on spark-master:40291 (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 11:58:17 INFO SparkContext: Created broadcast 329 from start at NativeMethodAccessorImpl.java:0
26/02/13 11:58:17 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 105, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@56ee657]. The input RDD has 3 partitions.
26/02/13 11:58:17 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:58:17 INFO DAGScheduler: Got job 220 (start at NativeMethodAccessorImpl.java:0) with 3 output partitions
26/02/13 11:58:17 INFO DAGScheduler: Final stage: ResultStage 333 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:58:17 INFO DAGScheduler: Parents of final stage: List()
26/02/13 11:58:17 INFO DAGScheduler: Missing parents: List()
26/02/13 11:58:17 INFO DAGScheduler: Submitting ResultStage 333 (MapPartitionsRDD[1204] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:58:17 INFO MemoryStore: Block broadcast_330 stored as values in memory (estimated size 49.9 KiB, free 433.8 MiB)
26/02/13 11:58:17 INFO MemoryStore: Block broadcast_330_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.8 MiB)
26/02/13 11:58:17 INFO BlockManagerInfo: Added broadcast_330_piece0 in memory on spark-master:40291 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 11:58:17 INFO SparkContext: Created broadcast 330 from broadcast at DAGScheduler.scala:1585
26/02/13 11:58:17 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 333 (MapPartitionsRDD[1204] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
26/02/13 11:58:17 INFO TaskSchedulerImpl: Adding task set 333.0 with 3 tasks resource profile 0
26/02/13 11:58:17 INFO TaskSetManager: Starting task 1.0 in stage 333.0 (TID 743) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:58:17 INFO TaskSetManager: Starting task 0.0 in stage 333.0 (TID 744) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:58:17 INFO TaskSetManager: Starting task 2.0 in stage 333.0 (TID 745) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:58:17 INFO BlockManagerInfo: Added broadcast_330_piece0 in memory on 172.18.0.14:40939 (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 11:58:17 INFO BlockManagerInfo: Added broadcast_330_piece0 in memory on 172.18.0.15:34959 (size: 19.5 KiB, free: 434.0 MiB)
26/02/13 11:58:17 INFO BlockManagerInfo: Added broadcast_329_piece0 in memory on 172.18.0.14:40939 (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 11:58:17 INFO BlockManagerInfo: Added broadcast_329_piece0 in memory on 172.18.0.15:34959 (size: 107.2 KiB, free: 433.9 MiB)
26/02/13 11:58:18 INFO TaskSetManager: Finished task 0.0 in stage 333.0 (TID 744) in 673 ms on 172.18.0.14 (executor 1) (1/3)
26/02/13 11:58:18 INFO TaskSetManager: Finished task 2.0 in stage 333.0 (TID 745) in 676 ms on 172.18.0.14 (executor 1) (2/3)
26/02/13 11:58:18 INFO TaskSetManager: Finished task 1.0 in stage 333.0 (TID 743) in 680 ms on 172.18.0.15 (executor 0) (3/3)
26/02/13 11:58:18 INFO TaskSchedulerImpl: Removed TaskSet 333.0, whose tasks have all completed, from pool 
26/02/13 11:58:18 INFO DAGScheduler: ResultStage 333 (start at NativeMethodAccessorImpl.java:0) finished in 0.688 s
26/02/13 11:58:18 INFO DAGScheduler: Job 220 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:58:18 INFO TaskSchedulerImpl: Killing all running tasks in stage 333: Stage finished
26/02/13 11:58:18 INFO DAGScheduler: Job 220 finished: start at NativeMethodAccessorImpl.java:0, took 0.689390 s
26/02/13 11:58:18 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 105, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@56ee657] is committing.
26/02/13 11:58:18 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 105, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@56ee657] committed.
26/02/13 11:58:18 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/105 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.105.160765fe-bd19-444c-8cb4-2f8c76b8b49a.tmp
26/02/13 11:58:18 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.105.160765fe-bd19-444c-8cb4-2f8c76b8b49a.tmp to file:/tmp/spark-checkpoint-enrichment/commits/105
26/02/13 11:58:18 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "fc5c6452-bd70-4c90-becf-772d3bebdb57",
  "name" : null,
  "timestamp" : "2026-02-13T11:58:17.180Z",
  "batchId" : 105,
  "numInputRows" : 55,
  "inputRowsPerSecond" : 4583.333333333333,
  "processedRowsPerSecond" : 39.53989935298346,
  "durationMs" : {
    "addBatch" : 1221,
    "commitOffsets" : 64,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 34,
    "triggerExecution" : 1391,
    "walCommit" : 71
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 6912,
        "1" : 7666,
        "0" : 8937
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 6930,
        "1" : 7691,
        "0" : 8949
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 6930,
        "1" : 7691,
        "0" : 8949
      }
    },
    "numInputRows" : 55,
    "inputRowsPerSecond" : 4583.333333333333,
    "processedRowsPerSecond" : 39.53989935298346,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@7fe9b5de",
    "numOutputRows" : 12
  }
}
26/02/13 11:58:18 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/106 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.106.7bb13030-7a00-4738-9431-8da5bc6fdb22.tmp
26/02/13 11:58:18 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.106.7bb13030-7a00-4738-9431-8da5bc6fdb22.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/106
26/02/13 11:58:18 INFO MicroBatchExecution: Committed offsets for batch 106. Metadata OffsetSeqMetadata(0,1770983898572,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 11:58:18 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:58:18 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:58:18 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:58:18 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:58:18 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:58:18 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:58:18 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#92475 - origin_code.nullCount#92474) > 0)
26/02/13 11:58:18 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#92480 - destination_code.nullCount#92479) > 0)
26/02/13 11:58:18 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#92510 - callsign.nullCount#92509) > 0)
26/02/13 11:58:18 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:58:18 INFO DAGScheduler: Got job 221 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 11:58:18 INFO DAGScheduler: Final stage: ResultStage 335 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:58:18 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 334)
26/02/13 11:58:18 INFO DAGScheduler: Missing parents: List()
26/02/13 11:58:18 INFO DAGScheduler: Submitting ResultStage 335 (MapPartitionsRDD[1209] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:58:18 INFO MemoryStore: Block broadcast_331 stored as values in memory (estimated size 99.3 KiB, free 433.7 MiB)
26/02/13 11:58:18 INFO MemoryStore: Block broadcast_331_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 433.6 MiB)
26/02/13 11:58:18 INFO BlockManagerInfo: Added broadcast_331_piece0 in memory on spark-master:40291 (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 11:58:18 INFO SparkContext: Created broadcast 331 from broadcast at DAGScheduler.scala:1585
26/02/13 11:58:18 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 335 (MapPartitionsRDD[1209] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 11:58:18 INFO TaskSchedulerImpl: Adding task set 335.0 with 4 tasks resource profile 0
26/02/13 11:58:18 INFO TaskSetManager: Starting task 0.0 in stage 335.0 (TID 746) (172.18.0.15, executor 0, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:58:18 INFO TaskSetManager: Starting task 1.0 in stage 335.0 (TID 747) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:58:18 INFO BlockManagerInfo: Added broadcast_331_piece0 in memory on 172.18.0.15:34959 (size: 33.3 KiB, free: 433.9 MiB)
26/02/13 11:58:18 INFO TaskSetManager: Starting task 2.0 in stage 335.0 (TID 748) (172.18.0.15, executor 0, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:58:18 INFO TaskSetManager: Finished task 0.0 in stage 335.0 (TID 746) in 30 ms on 172.18.0.15 (executor 0) (1/4)
26/02/13 11:58:18 INFO TaskSetManager: Starting task 3.0 in stage 335.0 (TID 749) (172.18.0.15, executor 0, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:58:18 INFO TaskSetManager: Finished task 1.0 in stage 335.0 (TID 747) in 45 ms on 172.18.0.15 (executor 0) (2/4)
26/02/13 11:58:18 INFO TaskSetManager: Finished task 2.0 in stage 335.0 (TID 748) in 20 ms on 172.18.0.15 (executor 0) (3/4)
26/02/13 11:58:18 INFO TaskSetManager: Finished task 3.0 in stage 335.0 (TID 749) in 19 ms on 172.18.0.15 (executor 0) (4/4)
26/02/13 11:58:18 INFO TaskSchedulerImpl: Removed TaskSet 335.0, whose tasks have all completed, from pool 
26/02/13 11:58:18 INFO DAGScheduler: ResultStage 335 (start at NativeMethodAccessorImpl.java:0) finished in 0.070 s
26/02/13 11:58:18 INFO DAGScheduler: Job 221 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:58:18 INFO TaskSchedulerImpl: Killing all running tasks in stage 335: Stage finished
26/02/13 11:58:18 INFO DAGScheduler: Job 221 finished: start at NativeMethodAccessorImpl.java:0, took 0.074338 s
26/02/13 11:58:18 INFO MemoryStore: Block broadcast_332_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.5 MiB)
26/02/13 11:58:18 INFO BlockManagerInfo: Added broadcast_332_piece0 in memory on spark-master:40291 (size: 107.2 KiB, free: 434.0 MiB)
26/02/13 11:58:18 INFO SparkContext: Created broadcast 332 from start at NativeMethodAccessorImpl.java:0
26/02/13 11:58:18 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 106, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@3e5af0d4]. The input RDD has 3 partitions.
26/02/13 11:58:18 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:58:18 INFO DAGScheduler: Got job 222 (start at NativeMethodAccessorImpl.java:0) with 3 output partitions
26/02/13 11:58:18 INFO DAGScheduler: Final stage: ResultStage 336 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:58:18 INFO DAGScheduler: Parents of final stage: List()
26/02/13 11:58:18 INFO DAGScheduler: Missing parents: List()
26/02/13 11:58:18 INFO DAGScheduler: Submitting ResultStage 336 (MapPartitionsRDD[1215] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:58:18 INFO MemoryStore: Block broadcast_333 stored as values in memory (estimated size 49.9 KiB, free 433.5 MiB)
26/02/13 11:58:18 INFO MemoryStore: Block broadcast_333_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.5 MiB)
26/02/13 11:58:18 INFO BlockManagerInfo: Added broadcast_333_piece0 in memory on spark-master:40291 (size: 19.5 KiB, free: 433.9 MiB)
26/02/13 11:58:18 INFO SparkContext: Created broadcast 333 from broadcast at DAGScheduler.scala:1585
26/02/13 11:58:18 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 336 (MapPartitionsRDD[1215] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
26/02/13 11:58:18 INFO TaskSchedulerImpl: Adding task set 336.0 with 3 tasks resource profile 0
26/02/13 11:58:18 INFO TaskSetManager: Starting task 1.0 in stage 336.0 (TID 750) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:58:18 INFO TaskSetManager: Starting task 0.0 in stage 336.0 (TID 751) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:58:18 INFO TaskSetManager: Starting task 2.0 in stage 336.0 (TID 752) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:58:18 INFO BlockManagerInfo: Added broadcast_333_piece0 in memory on 172.18.0.14:40939 (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 11:58:18 INFO BlockManagerInfo: Added broadcast_333_piece0 in memory on 172.18.0.15:34959 (size: 19.5 KiB, free: 433.9 MiB)
26/02/13 11:58:18 INFO BlockManagerInfo: Added broadcast_332_piece0 in memory on 172.18.0.14:40939 (size: 107.2 KiB, free: 434.0 MiB)
26/02/13 11:58:18 INFO BlockManagerInfo: Added broadcast_332_piece0 in memory on 172.18.0.15:34959 (size: 107.2 KiB, free: 433.7 MiB)
26/02/13 11:58:18 INFO TaskSetManager: Finished task 2.0 in stage 336.0 (TID 752) in 80 ms on 172.18.0.14 (executor 1) (1/3)
26/02/13 11:58:18 INFO TaskSetManager: Finished task 0.0 in stage 336.0 (TID 751) in 80 ms on 172.18.0.14 (executor 1) (2/3)
26/02/13 11:58:18 INFO TaskSetManager: Finished task 1.0 in stage 336.0 (TID 750) in 82 ms on 172.18.0.15 (executor 0) (3/3)
26/02/13 11:58:18 INFO TaskSchedulerImpl: Removed TaskSet 336.0, whose tasks have all completed, from pool 
26/02/13 11:58:18 INFO DAGScheduler: ResultStage 336 (start at NativeMethodAccessorImpl.java:0) finished in 0.087 s
26/02/13 11:58:18 INFO DAGScheduler: Job 222 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:58:18 INFO TaskSchedulerImpl: Killing all running tasks in stage 336: Stage finished
26/02/13 11:58:18 INFO DAGScheduler: Job 222 finished: start at NativeMethodAccessorImpl.java:0, took 0.090158 s
26/02/13 11:58:18 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 106, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@3e5af0d4] is committing.
26/02/13 11:58:18 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 106, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@3e5af0d4] committed.
26/02/13 11:58:18 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/106 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.106.657d726e-f016-43a5-902a-4d4c8825dce9.tmp
26/02/13 11:58:18 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.106.657d726e-f016-43a5-902a-4d4c8825dce9.tmp to file:/tmp/spark-checkpoint-enrichment/commits/106
26/02/13 11:58:19 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "fc5c6452-bd70-4c90-becf-772d3bebdb57",
  "name" : null,
  "timestamp" : "2026-02-13T11:58:18.571Z",
  "batchId" : 106,
  "numInputRows" : 200,
  "inputRowsPerSecond" : 143.78145219266713,
  "processedRowsPerSecond" : 467.2897196261682,
  "durationMs" : {
    "addBatch" : 252,
    "commitOffsets" : 100,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 20,
    "triggerExecution" : 428,
    "walCommit" : 55
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 6930,
        "1" : 7691,
        "0" : 8949
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 6993,
        "1" : 7763,
        "0" : 9014
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 6993,
        "1" : 7763,
        "0" : 9014
      }
    },
    "numInputRows" : 200,
    "inputRowsPerSecond" : 143.78145219266713,
    "processedRowsPerSecond" : 467.2897196261682,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@7fe9b5de",
    "numOutputRows" : 42
  }
}
26/02/13 11:58:22 INFO BlockManagerInfo: Removed broadcast_329_piece0 on spark-master:40291 in memory (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 11:58:22 INFO BlockManagerInfo: Removed broadcast_329_piece0 on 172.18.0.14:40939 in memory (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 11:58:22 INFO BlockManagerInfo: Removed broadcast_329_piece0 on 172.18.0.15:34959 in memory (size: 107.2 KiB, free: 433.9 MiB)
26/02/13 11:58:22 INFO BlockManagerInfo: Removed broadcast_331_piece0 on spark-master:40291 in memory (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 11:58:22 INFO BlockManagerInfo: Removed broadcast_331_piece0 on 172.18.0.15:34959 in memory (size: 33.3 KiB, free: 433.9 MiB)
26/02/13 11:58:22 INFO BlockManagerInfo: Removed broadcast_328_piece0 on spark-master:40291 in memory (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 11:58:22 INFO BlockManagerInfo: Removed broadcast_328_piece0 on 172.18.0.15:34959 in memory (size: 33.3 KiB, free: 433.9 MiB)
26/02/13 11:58:22 INFO BlockManagerInfo: Removed broadcast_330_piece0 on spark-master:40291 in memory (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 11:58:22 INFO BlockManagerInfo: Removed broadcast_330_piece0 on 172.18.0.15:34959 in memory (size: 19.5 KiB, free: 433.9 MiB)
26/02/13 11:58:22 INFO BlockManagerInfo: Removed broadcast_330_piece0 on 172.18.0.14:40939 in memory (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 11:58:22 INFO BlockManagerInfo: Removed broadcast_333_piece0 on spark-master:40291 in memory (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 11:58:22 INFO BlockManagerInfo: Removed broadcast_333_piece0 on 172.18.0.15:34959 in memory (size: 19.5 KiB, free: 434.0 MiB)
26/02/13 11:58:22 INFO BlockManagerInfo: Removed broadcast_333_piece0 on 172.18.0.14:40939 in memory (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 11:58:29 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 11:58:35 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/107 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.107.828de255-445b-41b0-a0e6-652f0b423b91.tmp
26/02/13 11:58:35 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.107.828de255-445b-41b0-a0e6-652f0b423b91.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/107
26/02/13 11:58:35 INFO MicroBatchExecution: Committed offsets for batch 107. Metadata OffsetSeqMetadata(0,1770983915515,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 11:58:35 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:58:35 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:58:35 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:58:35 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:58:35 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:58:35 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:58:35 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#93329 - origin_code.nullCount#93328) > 0)
26/02/13 11:58:35 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#93334 - destination_code.nullCount#93333) > 0)
26/02/13 11:58:35 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#93364 - callsign.nullCount#93363) > 0)
26/02/13 11:58:35 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:58:35 INFO DAGScheduler: Got job 223 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 11:58:35 INFO DAGScheduler: Final stage: ResultStage 338 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:58:35 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 337)
26/02/13 11:58:35 INFO DAGScheduler: Missing parents: List()
26/02/13 11:58:35 INFO DAGScheduler: Submitting ResultStage 338 (MapPartitionsRDD[1220] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:58:35 INFO MemoryStore: Block broadcast_334 stored as values in memory (estimated size 99.3 KiB, free 433.9 MiB)
26/02/13 11:58:35 INFO MemoryStore: Block broadcast_334_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 433.8 MiB)
26/02/13 11:58:35 INFO BlockManagerInfo: Added broadcast_334_piece0 in memory on spark-master:40291 (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 11:58:35 INFO SparkContext: Created broadcast 334 from broadcast at DAGScheduler.scala:1585
26/02/13 11:58:35 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 338 (MapPartitionsRDD[1220] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 11:58:35 INFO TaskSchedulerImpl: Adding task set 338.0 with 4 tasks resource profile 0
26/02/13 11:58:35 INFO TaskSetManager: Starting task 0.0 in stage 338.0 (TID 753) (172.18.0.15, executor 0, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:58:35 INFO TaskSetManager: Starting task 1.0 in stage 338.0 (TID 754) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:58:35 INFO BlockManagerInfo: Added broadcast_334_piece0 in memory on 172.18.0.15:34959 (size: 33.3 KiB, free: 433.9 MiB)
26/02/13 11:58:35 INFO TaskSetManager: Starting task 2.0 in stage 338.0 (TID 755) (172.18.0.15, executor 0, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:58:35 INFO BlockManagerInfo: Removed broadcast_332_piece0 on 172.18.0.14:40939 in memory (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 11:58:35 INFO TaskSetManager: Starting task 3.0 in stage 338.0 (TID 756) (172.18.0.15, executor 0, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:58:35 INFO BlockManagerInfo: Removed broadcast_332_piece0 on spark-master:40291 in memory (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 11:58:35 INFO TaskSetManager: Finished task 1.0 in stage 338.0 (TID 754) in 37 ms on 172.18.0.15 (executor 0) (1/4)
26/02/13 11:58:35 INFO BlockManagerInfo: Removed broadcast_332_piece0 on 172.18.0.15:34959 in memory (size: 107.2 KiB, free: 434.0 MiB)
26/02/13 11:58:35 INFO TaskSetManager: Finished task 0.0 in stage 338.0 (TID 753) in 40 ms on 172.18.0.15 (executor 0) (2/4)
26/02/13 11:58:35 INFO BlockManagerInfo: Removed broadcast_326_piece0 on spark-master:40291 in memory (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 11:58:35 INFO BlockManagerInfo: Removed broadcast_326_piece0 on 172.18.0.14:40939 in memory (size: 107.2 KiB, free: 434.4 MiB)
26/02/13 11:58:35 INFO BlockManagerInfo: Removed broadcast_326_piece0 on 172.18.0.15:34959 in memory (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 11:58:35 INFO TaskSetManager: Finished task 2.0 in stage 338.0 (TID 755) in 19 ms on 172.18.0.15 (executor 0) (3/4)
26/02/13 11:58:35 INFO TaskSetManager: Finished task 3.0 in stage 338.0 (TID 756) in 27 ms on 172.18.0.15 (executor 0) (4/4)
26/02/13 11:58:35 INFO TaskSchedulerImpl: Removed TaskSet 338.0, whose tasks have all completed, from pool 
26/02/13 11:58:35 INFO DAGScheduler: ResultStage 338 (start at NativeMethodAccessorImpl.java:0) finished in 0.074 s
26/02/13 11:58:35 INFO DAGScheduler: Job 223 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:58:35 INFO TaskSchedulerImpl: Killing all running tasks in stage 338: Stage finished
26/02/13 11:58:35 INFO DAGScheduler: Job 223 finished: start at NativeMethodAccessorImpl.java:0, took 0.077108 s
26/02/13 11:58:35 INFO MemoryStore: Block broadcast_335_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.9 MiB)
26/02/13 11:58:35 INFO BlockManagerInfo: Added broadcast_335_piece0 in memory on spark-master:40291 (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 11:58:35 INFO SparkContext: Created broadcast 335 from start at NativeMethodAccessorImpl.java:0
26/02/13 11:58:35 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 107, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@3c14b5f4]. The input RDD has 3 partitions.
26/02/13 11:58:35 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:58:35 INFO DAGScheduler: Got job 224 (start at NativeMethodAccessorImpl.java:0) with 3 output partitions
26/02/13 11:58:35 INFO DAGScheduler: Final stage: ResultStage 339 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:58:35 INFO DAGScheduler: Parents of final stage: List()
26/02/13 11:58:35 INFO DAGScheduler: Missing parents: List()
26/02/13 11:58:35 INFO DAGScheduler: Submitting ResultStage 339 (MapPartitionsRDD[1226] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:58:35 INFO MemoryStore: Block broadcast_336 stored as values in memory (estimated size 49.9 KiB, free 433.9 MiB)
26/02/13 11:58:35 INFO MemoryStore: Block broadcast_336_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.9 MiB)
26/02/13 11:58:35 INFO BlockManagerInfo: Added broadcast_336_piece0 in memory on spark-master:40291 (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 11:58:35 INFO SparkContext: Created broadcast 336 from broadcast at DAGScheduler.scala:1585
26/02/13 11:58:35 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 339 (MapPartitionsRDD[1226] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
26/02/13 11:58:35 INFO TaskSchedulerImpl: Adding task set 339.0 with 3 tasks resource profile 0
26/02/13 11:58:35 INFO TaskSetManager: Starting task 1.0 in stage 339.0 (TID 757) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:58:35 INFO TaskSetManager: Starting task 0.0 in stage 339.0 (TID 758) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:58:35 INFO TaskSetManager: Starting task 2.0 in stage 339.0 (TID 759) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:58:35 INFO BlockManagerInfo: Added broadcast_336_piece0 in memory on 172.18.0.15:34959 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 11:58:35 INFO BlockManagerInfo: Added broadcast_336_piece0 in memory on 172.18.0.14:40939 (size: 19.5 KiB, free: 434.4 MiB)
26/02/13 11:58:35 INFO BlockManagerInfo: Added broadcast_335_piece0 in memory on 172.18.0.15:34959 (size: 107.2 KiB, free: 434.0 MiB)
26/02/13 11:58:35 INFO BlockManagerInfo: Added broadcast_335_piece0 in memory on 172.18.0.14:40939 (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 11:58:36 INFO TaskSetManager: Finished task 1.0 in stage 339.0 (TID 757) in 558 ms on 172.18.0.15 (executor 0) (1/3)
26/02/13 11:58:36 INFO TaskSetManager: Finished task 2.0 in stage 339.0 (TID 759) in 558 ms on 172.18.0.14 (executor 1) (2/3)
26/02/13 11:58:36 INFO TaskSetManager: Finished task 0.0 in stage 339.0 (TID 758) in 559 ms on 172.18.0.14 (executor 1) (3/3)
26/02/13 11:58:36 INFO TaskSchedulerImpl: Removed TaskSet 339.0, whose tasks have all completed, from pool 
26/02/13 11:58:36 INFO DAGScheduler: ResultStage 339 (start at NativeMethodAccessorImpl.java:0) finished in 0.563 s
26/02/13 11:58:36 INFO DAGScheduler: Job 224 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:58:36 INFO TaskSchedulerImpl: Killing all running tasks in stage 339: Stage finished
26/02/13 11:58:36 INFO DAGScheduler: Job 224 finished: start at NativeMethodAccessorImpl.java:0, took 0.564898 s
26/02/13 11:58:36 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 107, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@3c14b5f4] is committing.
26/02/13 11:58:36 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 107, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@3c14b5f4] committed.
26/02/13 11:58:36 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/107 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.107.fc3d01b4-4374-4699-9a65-4cb63956c7d5.tmp
26/02/13 11:58:36 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.107.fc3d01b4-4374-4699-9a65-4cb63956c7d5.tmp to file:/tmp/spark-checkpoint-enrichment/commits/107
26/02/13 11:58:36 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "fc5c6452-bd70-4c90-becf-772d3bebdb57",
  "name" : null,
  "timestamp" : "2026-02-13T11:58:35.513Z",
  "batchId" : 107,
  "numInputRows" : 88,
  "inputRowsPerSecond" : 8000.000000000001,
  "processedRowsPerSecond" : 96.38554216867469,
  "durationMs" : {
    "addBatch" : 737,
    "commitOffsets" : 61,
    "getBatch" : 1,
    "latestOffset" : 1,
    "queryPlanning" : 22,
    "triggerExecution" : 913,
    "walCommit" : 89
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 6993,
        "1" : 7763,
        "0" : 9014
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 7023,
        "1" : 7797,
        "0" : 9038
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 7023,
        "1" : 7797,
        "0" : 9038
      }
    },
    "numInputRows" : 88,
    "inputRowsPerSecond" : 8000.000000000001,
    "processedRowsPerSecond" : 96.38554216867469,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@7fe9b5de",
    "numOutputRows" : 22
  }
}
26/02/13 11:58:36 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/108 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.108.da2845f2-578e-42e0-9002-238b22ed5ccb.tmp
26/02/13 11:58:36 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.108.da2845f2-578e-42e0-9002-238b22ed5ccb.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/108
26/02/13 11:58:36 INFO MicroBatchExecution: Committed offsets for batch 108. Metadata OffsetSeqMetadata(0,1770983916428,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 11:58:36 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:58:36 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:58:36 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:58:36 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:58:36 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:58:36 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:58:36 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#94183 - origin_code.nullCount#94182) > 0)
26/02/13 11:58:36 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#94188 - destination_code.nullCount#94187) > 0)
26/02/13 11:58:36 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#94218 - callsign.nullCount#94217) > 0)
26/02/13 11:58:36 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:58:36 INFO DAGScheduler: Got job 225 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 11:58:36 INFO DAGScheduler: Final stage: ResultStage 341 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:58:36 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 340)
26/02/13 11:58:36 INFO DAGScheduler: Missing parents: List()
26/02/13 11:58:36 INFO DAGScheduler: Submitting ResultStage 341 (MapPartitionsRDD[1231] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:58:36 INFO MemoryStore: Block broadcast_337 stored as values in memory (estimated size 99.3 KiB, free 433.8 MiB)
26/02/13 11:58:36 INFO MemoryStore: Block broadcast_337_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 433.7 MiB)
26/02/13 11:58:36 INFO BlockManagerInfo: Added broadcast_337_piece0 in memory on spark-master:40291 (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 11:58:36 INFO SparkContext: Created broadcast 337 from broadcast at DAGScheduler.scala:1585
26/02/13 11:58:36 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 341 (MapPartitionsRDD[1231] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 11:58:36 INFO TaskSchedulerImpl: Adding task set 341.0 with 4 tasks resource profile 0
26/02/13 11:58:36 INFO TaskSetManager: Starting task 0.0 in stage 341.0 (TID 760) (172.18.0.15, executor 0, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:58:36 INFO TaskSetManager: Starting task 1.0 in stage 341.0 (TID 761) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:58:36 INFO BlockManagerInfo: Added broadcast_337_piece0 in memory on 172.18.0.15:34959 (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 11:58:36 INFO TaskSetManager: Starting task 2.0 in stage 341.0 (TID 762) (172.18.0.15, executor 0, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:58:36 INFO TaskSetManager: Finished task 1.0 in stage 341.0 (TID 761) in 15 ms on 172.18.0.15 (executor 0) (1/4)
26/02/13 11:58:36 INFO TaskSetManager: Starting task 3.0 in stage 341.0 (TID 763) (172.18.0.15, executor 0, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:58:36 INFO TaskSetManager: Finished task 0.0 in stage 341.0 (TID 760) in 15 ms on 172.18.0.15 (executor 0) (2/4)
26/02/13 11:58:36 INFO TaskSetManager: Finished task 3.0 in stage 341.0 (TID 763) in 13 ms on 172.18.0.15 (executor 0) (3/4)
26/02/13 11:58:36 INFO TaskSetManager: Finished task 2.0 in stage 341.0 (TID 762) in 15 ms on 172.18.0.15 (executor 0) (4/4)
26/02/13 11:58:36 INFO TaskSchedulerImpl: Removed TaskSet 341.0, whose tasks have all completed, from pool 
26/02/13 11:58:36 INFO DAGScheduler: ResultStage 341 (start at NativeMethodAccessorImpl.java:0) finished in 0.035 s
26/02/13 11:58:36 INFO DAGScheduler: Job 225 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:58:36 INFO TaskSchedulerImpl: Killing all running tasks in stage 341: Stage finished
26/02/13 11:58:36 INFO DAGScheduler: Job 225 finished: start at NativeMethodAccessorImpl.java:0, took 0.036932 s
26/02/13 11:58:36 INFO MemoryStore: Block broadcast_338_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.6 MiB)
26/02/13 11:58:36 INFO BlockManagerInfo: Added broadcast_338_piece0 in memory on spark-master:40291 (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 11:58:36 INFO SparkContext: Created broadcast 338 from start at NativeMethodAccessorImpl.java:0
26/02/13 11:58:36 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 108, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@2efa0b4]. The input RDD has 3 partitions.
26/02/13 11:58:36 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:58:36 INFO DAGScheduler: Got job 226 (start at NativeMethodAccessorImpl.java:0) with 3 output partitions
26/02/13 11:58:36 INFO DAGScheduler: Final stage: ResultStage 342 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:58:36 INFO DAGScheduler: Parents of final stage: List()
26/02/13 11:58:36 INFO DAGScheduler: Missing parents: List()
26/02/13 11:58:36 INFO DAGScheduler: Submitting ResultStage 342 (MapPartitionsRDD[1237] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:58:36 INFO MemoryStore: Block broadcast_339 stored as values in memory (estimated size 49.9 KiB, free 433.6 MiB)
26/02/13 11:58:36 INFO MemoryStore: Block broadcast_339_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.6 MiB)
26/02/13 11:58:36 INFO BlockManagerInfo: Added broadcast_339_piece0 in memory on spark-master:40291 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 11:58:36 INFO SparkContext: Created broadcast 339 from broadcast at DAGScheduler.scala:1585
26/02/13 11:58:36 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 342 (MapPartitionsRDD[1237] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
26/02/13 11:58:36 INFO TaskSchedulerImpl: Adding task set 342.0 with 3 tasks resource profile 0
26/02/13 11:58:36 INFO TaskSetManager: Starting task 1.0 in stage 342.0 (TID 764) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:58:36 INFO TaskSetManager: Starting task 0.0 in stage 342.0 (TID 765) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:58:36 INFO TaskSetManager: Starting task 2.0 in stage 342.0 (TID 766) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:58:36 INFO BlockManagerInfo: Added broadcast_339_piece0 in memory on 172.18.0.15:34959 (size: 19.5 KiB, free: 434.0 MiB)
26/02/13 11:58:36 INFO BlockManagerInfo: Added broadcast_339_piece0 in memory on 172.18.0.14:40939 (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 11:58:36 INFO BlockManagerInfo: Added broadcast_338_piece0 in memory on 172.18.0.15:34959 (size: 107.2 KiB, free: 433.9 MiB)
26/02/13 11:58:36 INFO BlockManagerInfo: Added broadcast_338_piece0 in memory on 172.18.0.14:40939 (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 11:58:36 INFO TaskSetManager: Finished task 0.0 in stage 342.0 (TID 765) in 49 ms on 172.18.0.14 (executor 1) (1/3)
26/02/13 11:58:36 INFO TaskSetManager: Finished task 1.0 in stage 342.0 (TID 764) in 50 ms on 172.18.0.15 (executor 0) (2/3)
26/02/13 11:58:36 INFO TaskSetManager: Finished task 2.0 in stage 342.0 (TID 766) in 52 ms on 172.18.0.14 (executor 1) (3/3)
26/02/13 11:58:36 INFO TaskSchedulerImpl: Removed TaskSet 342.0, whose tasks have all completed, from pool 
26/02/13 11:58:36 INFO DAGScheduler: ResultStage 342 (start at NativeMethodAccessorImpl.java:0) finished in 0.057 s
26/02/13 11:58:36 INFO DAGScheduler: Job 226 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:58:36 INFO TaskSchedulerImpl: Killing all running tasks in stage 342: Stage finished
26/02/13 11:58:36 INFO DAGScheduler: Job 226 finished: start at NativeMethodAccessorImpl.java:0, took 0.058557 s
26/02/13 11:58:36 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 108, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@2efa0b4] is committing.
26/02/13 11:58:36 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 108, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@2efa0b4] committed.
26/02/13 11:58:36 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/108 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.108.a40d1860-54f5-4376-923a-ec234381cd61.tmp
26/02/13 11:58:36 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.108.a40d1860-54f5-4376-923a-ec234381cd61.tmp to file:/tmp/spark-checkpoint-enrichment/commits/108
26/02/13 11:58:36 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "fc5c6452-bd70-4c90-becf-772d3bebdb57",
  "name" : null,
  "timestamp" : "2026-02-13T11:58:36.427Z",
  "batchId" : 108,
  "numInputRows" : 169,
  "inputRowsPerSecond" : 184.9015317286652,
  "processedRowsPerSecond" : 487.0317002881845,
  "durationMs" : {
    "addBatch" : 174,
    "commitOffsets" : 62,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 32,
    "triggerExecution" : 347,
    "walCommit" : 78
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 7023,
        "1" : 7797,
        "0" : 9038
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 7073,
        "1" : 7860,
        "0" : 9094
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 7073,
        "1" : 7860,
        "0" : 9094
      }
    },
    "numInputRows" : 169,
    "inputRowsPerSecond" : 184.9015317286652,
    "processedRowsPerSecond" : 487.0317002881845,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@7fe9b5de",
    "numOutputRows" : 32
  }
}
26/02/13 11:58:37 INFO BlockManagerInfo: Removed broadcast_336_piece0 on spark-master:40291 in memory (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 11:58:37 INFO BlockManagerInfo: Removed broadcast_336_piece0 on 172.18.0.15:34959 in memory (size: 19.5 KiB, free: 433.9 MiB)
26/02/13 11:58:37 INFO BlockManagerInfo: Removed broadcast_336_piece0 on 172.18.0.14:40939 in memory (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 11:58:37 INFO BlockManagerInfo: Removed broadcast_335_piece0 on spark-master:40291 in memory (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 11:58:37 INFO BlockManagerInfo: Removed broadcast_335_piece0 on 172.18.0.15:34959 in memory (size: 107.2 KiB, free: 434.0 MiB)
26/02/13 11:58:37 INFO BlockManagerInfo: Removed broadcast_335_piece0 on 172.18.0.14:40939 in memory (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 11:58:37 INFO BlockManagerInfo: Removed broadcast_334_piece0 on spark-master:40291 in memory (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 11:58:37 INFO BlockManagerInfo: Removed broadcast_334_piece0 on 172.18.0.15:34959 in memory (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 11:58:37 INFO BlockManagerInfo: Removed broadcast_339_piece0 on spark-master:40291 in memory (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 11:58:37 INFO BlockManagerInfo: Removed broadcast_339_piece0 on 172.18.0.14:40939 in memory (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 11:58:37 INFO BlockManagerInfo: Removed broadcast_339_piece0 on 172.18.0.15:34959 in memory (size: 19.5 KiB, free: 434.0 MiB)
26/02/13 11:58:37 INFO BlockManagerInfo: Removed broadcast_337_piece0 on spark-master:40291 in memory (size: 33.3 KiB, free: 434.3 MiB)
26/02/13 11:58:37 INFO BlockManagerInfo: Removed broadcast_337_piece0 on 172.18.0.15:34959 in memory (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 11:58:46 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 11:58:52 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/109 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.109.e66fee35-849e-43fe-a5c2-cd83a4616c8a.tmp
26/02/13 11:58:52 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.109.e66fee35-849e-43fe-a5c2-cd83a4616c8a.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/109
26/02/13 11:58:52 INFO MicroBatchExecution: Committed offsets for batch 109. Metadata OffsetSeqMetadata(0,1770983932356,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 11:58:52 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:58:52 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:58:52 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:58:52 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:58:52 INFO BlockManagerInfo: Removed broadcast_338_piece0 on spark-master:40291 in memory (size: 107.2 KiB, free: 434.4 MiB)
26/02/13 11:58:52 INFO BlockManagerInfo: Removed broadcast_338_piece0 on 172.18.0.15:34959 in memory (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 11:58:52 INFO BlockManagerInfo: Removed broadcast_338_piece0 on 172.18.0.14:40939 in memory (size: 107.2 KiB, free: 434.4 MiB)
26/02/13 11:58:52 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:58:52 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:58:52 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#95037 - origin_code.nullCount#95036) > 0)
26/02/13 11:58:52 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#95042 - destination_code.nullCount#95041) > 0)
26/02/13 11:58:52 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#95072 - callsign.nullCount#95071) > 0)
26/02/13 11:58:52 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:58:52 INFO DAGScheduler: Got job 227 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 11:58:52 INFO DAGScheduler: Final stage: ResultStage 344 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:58:52 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 343)
26/02/13 11:58:52 INFO DAGScheduler: Missing parents: List()
26/02/13 11:58:52 INFO DAGScheduler: Submitting ResultStage 344 (MapPartitionsRDD[1242] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:58:52 INFO MemoryStore: Block broadcast_340 stored as values in memory (estimated size 99.3 KiB, free 434.1 MiB)
26/02/13 11:58:52 INFO MemoryStore: Block broadcast_340_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 434.0 MiB)
26/02/13 11:58:52 INFO BlockManagerInfo: Added broadcast_340_piece0 in memory on spark-master:40291 (size: 33.3 KiB, free: 434.3 MiB)
26/02/13 11:58:52 INFO SparkContext: Created broadcast 340 from broadcast at DAGScheduler.scala:1585
26/02/13 11:58:52 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 344 (MapPartitionsRDD[1242] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 11:58:52 INFO TaskSchedulerImpl: Adding task set 344.0 with 4 tasks resource profile 0
26/02/13 11:58:52 INFO TaskSetManager: Starting task 0.0 in stage 344.0 (TID 767) (172.18.0.15, executor 0, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:58:52 INFO TaskSetManager: Starting task 1.0 in stage 344.0 (TID 768) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:58:52 INFO BlockManagerInfo: Added broadcast_340_piece0 in memory on 172.18.0.15:34959 (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 11:58:52 INFO TaskSetManager: Starting task 2.0 in stage 344.0 (TID 769) (172.18.0.15, executor 0, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:58:52 INFO TaskSetManager: Finished task 1.0 in stage 344.0 (TID 768) in 13 ms on 172.18.0.15 (executor 0) (1/4)
26/02/13 11:58:52 INFO TaskSetManager: Starting task 3.0 in stage 344.0 (TID 770) (172.18.0.15, executor 0, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:58:52 INFO TaskSetManager: Finished task 0.0 in stage 344.0 (TID 767) in 14 ms on 172.18.0.15 (executor 0) (2/4)
26/02/13 11:58:52 INFO TaskSetManager: Finished task 2.0 in stage 344.0 (TID 769) in 10 ms on 172.18.0.15 (executor 0) (3/4)
26/02/13 11:58:52 INFO TaskSetManager: Finished task 3.0 in stage 344.0 (TID 770) in 10 ms on 172.18.0.15 (executor 0) (4/4)
26/02/13 11:58:52 INFO TaskSchedulerImpl: Removed TaskSet 344.0, whose tasks have all completed, from pool 
26/02/13 11:58:52 INFO DAGScheduler: ResultStage 344 (start at NativeMethodAccessorImpl.java:0) finished in 0.028 s
26/02/13 11:58:52 INFO DAGScheduler: Job 227 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:58:52 INFO TaskSchedulerImpl: Killing all running tasks in stage 344: Stage finished
26/02/13 11:58:52 INFO DAGScheduler: Job 227 finished: start at NativeMethodAccessorImpl.java:0, took 0.029713 s
26/02/13 11:58:52 INFO MemoryStore: Block broadcast_341_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.9 MiB)
26/02/13 11:58:52 INFO BlockManagerInfo: Added broadcast_341_piece0 in memory on spark-master:40291 (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 11:58:52 INFO SparkContext: Created broadcast 341 from start at NativeMethodAccessorImpl.java:0
26/02/13 11:58:52 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 109, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@5f124fe8]. The input RDD has 2 partitions.
26/02/13 11:58:52 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:58:52 INFO DAGScheduler: Got job 228 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/02/13 11:58:52 INFO DAGScheduler: Final stage: ResultStage 345 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:58:52 INFO DAGScheduler: Parents of final stage: List()
26/02/13 11:58:52 INFO DAGScheduler: Missing parents: List()
26/02/13 11:58:52 INFO DAGScheduler: Submitting ResultStage 345 (MapPartitionsRDD[1248] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:58:52 INFO MemoryStore: Block broadcast_342 stored as values in memory (estimated size 49.9 KiB, free 433.9 MiB)
26/02/13 11:58:52 INFO MemoryStore: Block broadcast_342_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.9 MiB)
26/02/13 11:58:52 INFO BlockManagerInfo: Added broadcast_342_piece0 in memory on spark-master:40291 (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 11:58:52 INFO SparkContext: Created broadcast 342 from broadcast at DAGScheduler.scala:1585
26/02/13 11:58:52 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 345 (MapPartitionsRDD[1248] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/02/13 11:58:52 INFO TaskSchedulerImpl: Adding task set 345.0 with 2 tasks resource profile 0
26/02/13 11:58:52 INFO TaskSetManager: Starting task 1.0 in stage 345.0 (TID 771) (172.18.0.14, executor 1, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:58:52 INFO TaskSetManager: Starting task 0.0 in stage 345.0 (TID 772) (172.18.0.15, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:58:52 INFO BlockManagerInfo: Added broadcast_342_piece0 in memory on 172.18.0.15:34959 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 11:58:52 INFO BlockManagerInfo: Added broadcast_342_piece0 in memory on 172.18.0.14:40939 (size: 19.5 KiB, free: 434.4 MiB)
26/02/13 11:58:52 INFO BlockManagerInfo: Added broadcast_341_piece0 in memory on 172.18.0.15:34959 (size: 107.2 KiB, free: 434.0 MiB)
26/02/13 11:58:52 INFO BlockManagerInfo: Added broadcast_341_piece0 in memory on 172.18.0.14:40939 (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 11:58:53 INFO TaskSetManager: Finished task 1.0 in stage 345.0 (TID 771) in 536 ms on 172.18.0.14 (executor 1) (1/2)
26/02/13 11:58:53 INFO TaskSetManager: Finished task 0.0 in stage 345.0 (TID 772) in 538 ms on 172.18.0.15 (executor 0) (2/2)
26/02/13 11:58:53 INFO TaskSchedulerImpl: Removed TaskSet 345.0, whose tasks have all completed, from pool 
26/02/13 11:58:53 INFO DAGScheduler: ResultStage 345 (start at NativeMethodAccessorImpl.java:0) finished in 0.542 s
26/02/13 11:58:53 INFO DAGScheduler: Job 228 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:58:53 INFO TaskSchedulerImpl: Killing all running tasks in stage 345: Stage finished
26/02/13 11:58:53 INFO DAGScheduler: Job 228 finished: start at NativeMethodAccessorImpl.java:0, took 0.543187 s
26/02/13 11:58:53 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 109, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@5f124fe8] is committing.
26/02/13 11:58:53 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 109, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@5f124fe8] committed.
26/02/13 11:58:53 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/109 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.109.4b4193be-c3d8-4047-8711-e2b9c6915479.tmp
26/02/13 11:58:53 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.109.4b4193be-c3d8-4047-8711-e2b9c6915479.tmp to file:/tmp/spark-checkpoint-enrichment/commits/109
26/02/13 11:58:53 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "fc5c6452-bd70-4c90-becf-772d3bebdb57",
  "name" : null,
  "timestamp" : "2026-02-13T11:58:52.354Z",
  "batchId" : 109,
  "numInputRows" : 3,
  "inputRowsPerSecond" : 250.0,
  "processedRowsPerSecond" : 3.409090909090909,
  "durationMs" : {
    "addBatch" : 683,
    "commitOffsets" : 60,
    "getBatch" : 0,
    "latestOffset" : 2,
    "queryPlanning" : 35,
    "triggerExecution" : 880,
    "walCommit" : 100
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 7073,
        "1" : 7860,
        "0" : 9094
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 7073,
        "1" : 7862,
        "0" : 9095
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 7073,
        "1" : 7862,
        "0" : 9095
      }
    },
    "numInputRows" : 3,
    "inputRowsPerSecond" : 250.0,
    "processedRowsPerSecond" : 3.409090909090909,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@7fe9b5de",
    "numOutputRows" : 0
  }
}
26/02/13 11:58:53 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/110 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.110.a9a48173-c7f0-45fd-ae2f-252f9b0eb55f.tmp
26/02/13 11:58:53 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.110.a9a48173-c7f0-45fd-ae2f-252f9b0eb55f.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/110
26/02/13 11:58:53 INFO MicroBatchExecution: Committed offsets for batch 110. Metadata OffsetSeqMetadata(0,1770983933236,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 11:58:53 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:58:53 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:58:53 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:58:53 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:58:53 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:58:53 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:58:53 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#95891 - origin_code.nullCount#95890) > 0)
26/02/13 11:58:53 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#95896 - destination_code.nullCount#95895) > 0)
26/02/13 11:58:53 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#95926 - callsign.nullCount#95925) > 0)
26/02/13 11:58:53 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:58:53 INFO DAGScheduler: Got job 229 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 11:58:53 INFO DAGScheduler: Final stage: ResultStage 347 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:58:53 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 346)
26/02/13 11:58:53 INFO DAGScheduler: Missing parents: List()
26/02/13 11:58:53 INFO DAGScheduler: Submitting ResultStage 347 (MapPartitionsRDD[1253] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:58:53 INFO MemoryStore: Block broadcast_343 stored as values in memory (estimated size 99.3 KiB, free 433.8 MiB)
26/02/13 11:58:53 INFO MemoryStore: Block broadcast_343_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 433.7 MiB)
26/02/13 11:58:53 INFO BlockManagerInfo: Added broadcast_343_piece0 in memory on spark-master:40291 (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 11:58:53 INFO SparkContext: Created broadcast 343 from broadcast at DAGScheduler.scala:1585
26/02/13 11:58:53 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 347 (MapPartitionsRDD[1253] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 11:58:53 INFO TaskSchedulerImpl: Adding task set 347.0 with 4 tasks resource profile 0
26/02/13 11:58:53 INFO TaskSetManager: Starting task 0.0 in stage 347.0 (TID 773) (172.18.0.15, executor 0, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:58:53 INFO TaskSetManager: Starting task 1.0 in stage 347.0 (TID 774) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:58:53 INFO BlockManagerInfo: Added broadcast_343_piece0 in memory on 172.18.0.15:34959 (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 11:58:53 INFO TaskSetManager: Starting task 2.0 in stage 347.0 (TID 775) (172.18.0.15, executor 0, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:58:53 INFO TaskSetManager: Finished task 1.0 in stage 347.0 (TID 774) in 16 ms on 172.18.0.15 (executor 0) (1/4)
26/02/13 11:58:53 INFO TaskSetManager: Starting task 3.0 in stage 347.0 (TID 776) (172.18.0.15, executor 0, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:58:53 INFO TaskSetManager: Finished task 0.0 in stage 347.0 (TID 773) in 21 ms on 172.18.0.15 (executor 0) (2/4)
26/02/13 11:58:53 INFO TaskSetManager: Finished task 2.0 in stage 347.0 (TID 775) in 10 ms on 172.18.0.15 (executor 0) (3/4)
26/02/13 11:58:53 INFO TaskSetManager: Finished task 3.0 in stage 347.0 (TID 776) in 14 ms on 172.18.0.15 (executor 0) (4/4)
26/02/13 11:58:53 INFO TaskSchedulerImpl: Removed TaskSet 347.0, whose tasks have all completed, from pool 
26/02/13 11:58:53 INFO DAGScheduler: ResultStage 347 (start at NativeMethodAccessorImpl.java:0) finished in 0.042 s
26/02/13 11:58:53 INFO DAGScheduler: Job 229 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:58:53 INFO TaskSchedulerImpl: Killing all running tasks in stage 347: Stage finished
26/02/13 11:58:53 INFO DAGScheduler: Job 229 finished: start at NativeMethodAccessorImpl.java:0, took 0.044702 s
26/02/13 11:58:53 INFO MemoryStore: Block broadcast_344_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.6 MiB)
26/02/13 11:58:53 INFO BlockManagerInfo: Added broadcast_344_piece0 in memory on spark-master:40291 (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 11:58:53 INFO SparkContext: Created broadcast 344 from start at NativeMethodAccessorImpl.java:0
26/02/13 11:58:53 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 110, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@15708454]. The input RDD has 3 partitions.
26/02/13 11:58:53 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:58:53 INFO DAGScheduler: Got job 230 (start at NativeMethodAccessorImpl.java:0) with 3 output partitions
26/02/13 11:58:53 INFO DAGScheduler: Final stage: ResultStage 348 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:58:53 INFO DAGScheduler: Parents of final stage: List()
26/02/13 11:58:53 INFO DAGScheduler: Missing parents: List()
26/02/13 11:58:53 INFO DAGScheduler: Submitting ResultStage 348 (MapPartitionsRDD[1259] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:58:53 INFO MemoryStore: Block broadcast_345 stored as values in memory (estimated size 49.9 KiB, free 433.6 MiB)
26/02/13 11:58:53 INFO MemoryStore: Block broadcast_345_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.6 MiB)
26/02/13 11:58:53 INFO BlockManagerInfo: Added broadcast_345_piece0 in memory on spark-master:40291 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 11:58:53 INFO SparkContext: Created broadcast 345 from broadcast at DAGScheduler.scala:1585
26/02/13 11:58:53 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 348 (MapPartitionsRDD[1259] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
26/02/13 11:58:53 INFO TaskSchedulerImpl: Adding task set 348.0 with 3 tasks resource profile 0
26/02/13 11:58:53 INFO TaskSetManager: Starting task 1.0 in stage 348.0 (TID 777) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:58:53 INFO TaskSetManager: Starting task 0.0 in stage 348.0 (TID 778) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:58:53 INFO TaskSetManager: Starting task 2.0 in stage 348.0 (TID 779) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:58:53 INFO BlockManagerInfo: Added broadcast_345_piece0 in memory on 172.18.0.15:34959 (size: 19.5 KiB, free: 434.0 MiB)
26/02/13 11:58:53 INFO BlockManagerInfo: Added broadcast_345_piece0 in memory on 172.18.0.14:40939 (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 11:58:53 INFO BlockManagerInfo: Added broadcast_344_piece0 in memory on 172.18.0.15:34959 (size: 107.2 KiB, free: 433.9 MiB)
26/02/13 11:58:53 INFO BlockManagerInfo: Added broadcast_344_piece0 in memory on 172.18.0.14:40939 (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 11:58:53 INFO TaskSetManager: Finished task 1.0 in stage 348.0 (TID 777) in 46 ms on 172.18.0.15 (executor 0) (1/3)
26/02/13 11:58:53 INFO TaskSetManager: Finished task 2.0 in stage 348.0 (TID 779) in 51 ms on 172.18.0.14 (executor 1) (2/3)
26/02/13 11:58:54 INFO TaskSetManager: Finished task 0.0 in stage 348.0 (TID 778) in 552 ms on 172.18.0.14 (executor 1) (3/3)
26/02/13 11:58:54 INFO TaskSchedulerImpl: Removed TaskSet 348.0, whose tasks have all completed, from pool 
26/02/13 11:58:54 INFO DAGScheduler: ResultStage 348 (start at NativeMethodAccessorImpl.java:0) finished in 0.561 s
26/02/13 11:58:54 INFO DAGScheduler: Job 230 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:58:54 INFO TaskSchedulerImpl: Killing all running tasks in stage 348: Stage finished
26/02/13 11:58:54 INFO DAGScheduler: Job 230 finished: start at NativeMethodAccessorImpl.java:0, took 0.562125 s
26/02/13 11:58:54 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 110, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@15708454] is committing.
26/02/13 11:58:54 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 110, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@15708454] committed.
26/02/13 11:58:54 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/110 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.110.d23cc565-cce6-456d-9bc1-48c5feadd02e.tmp
26/02/13 11:58:54 INFO BlockManagerInfo: Removed broadcast_343_piece0 on spark-master:40291 in memory (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 11:58:54 INFO BlockManagerInfo: Removed broadcast_343_piece0 on 172.18.0.15:34959 in memory (size: 33.3 KiB, free: 433.9 MiB)
26/02/13 11:58:54 INFO BlockManagerInfo: Removed broadcast_345_piece0 on spark-master:40291 in memory (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 11:58:54 INFO BlockManagerInfo: Removed broadcast_345_piece0 on 172.18.0.15:34959 in memory (size: 19.5 KiB, free: 433.9 MiB)
26/02/13 11:58:54 INFO BlockManagerInfo: Removed broadcast_345_piece0 on 172.18.0.14:40939 in memory (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 11:58:54 INFO BlockManagerInfo: Removed broadcast_340_piece0 on spark-master:40291 in memory (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 11:58:54 INFO BlockManagerInfo: Removed broadcast_340_piece0 on 172.18.0.15:34959 in memory (size: 33.3 KiB, free: 433.9 MiB)
26/02/13 11:58:54 INFO BlockManagerInfo: Removed broadcast_341_piece0 on spark-master:40291 in memory (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 11:58:54 INFO BlockManagerInfo: Removed broadcast_341_piece0 on 172.18.0.14:40939 in memory (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 11:58:54 INFO BlockManagerInfo: Removed broadcast_341_piece0 on 172.18.0.15:34959 in memory (size: 107.2 KiB, free: 434.0 MiB)
26/02/13 11:58:54 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.110.d23cc565-cce6-456d-9bc1-48c5feadd02e.tmp to file:/tmp/spark-checkpoint-enrichment/commits/110
26/02/13 11:58:54 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "fc5c6452-bd70-4c90-becf-772d3bebdb57",
  "name" : null,
  "timestamp" : "2026-02-13T11:58:53.235Z",
  "batchId" : 110,
  "numInputRows" : 253,
  "inputRowsPerSecond" : 287.17366628830877,
  "processedRowsPerSecond" : 281.73719376391983,
  "durationMs" : {
    "addBatch" : 692,
    "commitOffsets" : 61,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 40,
    "triggerExecution" : 898,
    "walCommit" : 104
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 7073,
        "1" : 7862,
        "0" : 9095
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 7152,
        "1" : 7959,
        "0" : 9172
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 7152,
        "1" : 7959,
        "0" : 9172
      }
    },
    "numInputRows" : 253,
    "inputRowsPerSecond" : 287.17366628830877,
    "processedRowsPerSecond" : 281.73719376391983,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@7fe9b5de",
    "numOutputRows" : 54
  }
}
26/02/13 11:58:54 INFO BlockManagerInfo: Removed broadcast_342_piece0 on spark-master:40291 in memory (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 11:58:54 INFO BlockManagerInfo: Removed broadcast_342_piece0 on 172.18.0.15:34959 in memory (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 11:58:54 INFO BlockManagerInfo: Removed broadcast_342_piece0 on 172.18.0.14:40939 in memory (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 11:59:04 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 11:59:08 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/111 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.111.598fa64d-c92e-4205-8446-f88c9ddd73a0.tmp
26/02/13 11:59:08 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.111.598fa64d-c92e-4205-8446-f88c9ddd73a0.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/111
26/02/13 11:59:08 INFO MicroBatchExecution: Committed offsets for batch 111. Metadata OffsetSeqMetadata(0,1770983948863,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 11:59:08 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:59:08 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:59:08 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:59:08 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:59:09 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:59:09 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:59:09 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#96745 - origin_code.nullCount#96744) > 0)
26/02/13 11:59:09 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#96750 - destination_code.nullCount#96749) > 0)
26/02/13 11:59:09 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#96780 - callsign.nullCount#96779) > 0)
26/02/13 11:59:09 INFO BlockManagerInfo: Removed broadcast_344_piece0 on spark-master:40291 in memory (size: 107.2 KiB, free: 434.4 MiB)
26/02/13 11:59:09 INFO BlockManagerInfo: Removed broadcast_344_piece0 on 172.18.0.14:40939 in memory (size: 107.2 KiB, free: 434.4 MiB)
26/02/13 11:59:09 INFO BlockManagerInfo: Removed broadcast_344_piece0 on 172.18.0.15:34959 in memory (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 11:59:09 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:59:09 INFO DAGScheduler: Got job 231 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 11:59:09 INFO DAGScheduler: Final stage: ResultStage 350 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:59:09 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 349)
26/02/13 11:59:09 INFO DAGScheduler: Missing parents: List()
26/02/13 11:59:09 INFO DAGScheduler: Submitting ResultStage 350 (MapPartitionsRDD[1264] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:59:09 INFO MemoryStore: Block broadcast_346 stored as values in memory (estimated size 99.3 KiB, free 434.1 MiB)
26/02/13 11:59:09 INFO MemoryStore: Block broadcast_346_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 434.0 MiB)
26/02/13 11:59:09 INFO BlockManagerInfo: Added broadcast_346_piece0 in memory on spark-master:40291 (size: 33.3 KiB, free: 434.3 MiB)
26/02/13 11:59:09 INFO SparkContext: Created broadcast 346 from broadcast at DAGScheduler.scala:1585
26/02/13 11:59:09 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 350 (MapPartitionsRDD[1264] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 11:59:09 INFO TaskSchedulerImpl: Adding task set 350.0 with 4 tasks resource profile 0
26/02/13 11:59:09 INFO TaskSetManager: Starting task 0.0 in stage 350.0 (TID 780) (172.18.0.15, executor 0, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:59:09 INFO TaskSetManager: Starting task 1.0 in stage 350.0 (TID 781) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:59:09 INFO BlockManagerInfo: Added broadcast_346_piece0 in memory on 172.18.0.15:34959 (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 11:59:09 INFO TaskSetManager: Starting task 2.0 in stage 350.0 (TID 782) (172.18.0.15, executor 0, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:59:09 INFO TaskSetManager: Starting task 3.0 in stage 350.0 (TID 783) (172.18.0.15, executor 0, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:59:09 INFO TaskSetManager: Finished task 1.0 in stage 350.0 (TID 781) in 21 ms on 172.18.0.15 (executor 0) (1/4)
26/02/13 11:59:09 INFO TaskSetManager: Finished task 0.0 in stage 350.0 (TID 780) in 22 ms on 172.18.0.15 (executor 0) (2/4)
26/02/13 11:59:09 INFO TaskSetManager: Finished task 2.0 in stage 350.0 (TID 782) in 11 ms on 172.18.0.15 (executor 0) (3/4)
26/02/13 11:59:09 INFO TaskSetManager: Finished task 3.0 in stage 350.0 (TID 783) in 18 ms on 172.18.0.15 (executor 0) (4/4)
26/02/13 11:59:09 INFO TaskSchedulerImpl: Removed TaskSet 350.0, whose tasks have all completed, from pool 
26/02/13 11:59:09 INFO DAGScheduler: ResultStage 350 (start at NativeMethodAccessorImpl.java:0) finished in 0.046 s
26/02/13 11:59:09 INFO DAGScheduler: Job 231 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:59:09 INFO TaskSchedulerImpl: Killing all running tasks in stage 350: Stage finished
26/02/13 11:59:09 INFO DAGScheduler: Job 231 finished: start at NativeMethodAccessorImpl.java:0, took 0.048553 s
26/02/13 11:59:09 INFO MemoryStore: Block broadcast_347_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.9 MiB)
26/02/13 11:59:09 INFO BlockManagerInfo: Added broadcast_347_piece0 in memory on spark-master:40291 (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 11:59:09 INFO SparkContext: Created broadcast 347 from start at NativeMethodAccessorImpl.java:0
26/02/13 11:59:09 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 111, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@7dda005f]. The input RDD has 3 partitions.
26/02/13 11:59:09 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:59:09 INFO DAGScheduler: Got job 232 (start at NativeMethodAccessorImpl.java:0) with 3 output partitions
26/02/13 11:59:09 INFO DAGScheduler: Final stage: ResultStage 351 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:59:09 INFO DAGScheduler: Parents of final stage: List()
26/02/13 11:59:09 INFO DAGScheduler: Missing parents: List()
26/02/13 11:59:09 INFO DAGScheduler: Submitting ResultStage 351 (MapPartitionsRDD[1270] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:59:09 INFO MemoryStore: Block broadcast_348 stored as values in memory (estimated size 49.9 KiB, free 433.9 MiB)
26/02/13 11:59:09 INFO MemoryStore: Block broadcast_348_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.9 MiB)
26/02/13 11:59:09 INFO BlockManagerInfo: Added broadcast_348_piece0 in memory on spark-master:40291 (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 11:59:09 INFO SparkContext: Created broadcast 348 from broadcast at DAGScheduler.scala:1585
26/02/13 11:59:09 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 351 (MapPartitionsRDD[1270] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
26/02/13 11:59:09 INFO TaskSchedulerImpl: Adding task set 351.0 with 3 tasks resource profile 0
26/02/13 11:59:09 INFO TaskSetManager: Starting task 1.0 in stage 351.0 (TID 784) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:59:09 INFO TaskSetManager: Starting task 0.0 in stage 351.0 (TID 785) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:59:09 INFO TaskSetManager: Starting task 2.0 in stage 351.0 (TID 786) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:59:09 INFO BlockManagerInfo: Added broadcast_348_piece0 in memory on 172.18.0.15:34959 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 11:59:09 INFO BlockManagerInfo: Added broadcast_348_piece0 in memory on 172.18.0.14:40939 (size: 19.5 KiB, free: 434.4 MiB)
26/02/13 11:59:09 INFO BlockManagerInfo: Added broadcast_347_piece0 in memory on 172.18.0.15:34959 (size: 107.2 KiB, free: 434.0 MiB)
26/02/13 11:59:09 INFO BlockManagerInfo: Added broadcast_347_piece0 in memory on 172.18.0.14:40939 (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 11:59:09 INFO TaskSetManager: Finished task 1.0 in stage 351.0 (TID 784) in 542 ms on 172.18.0.15 (executor 0) (1/3)
26/02/13 11:59:09 INFO TaskSetManager: Finished task 0.0 in stage 351.0 (TID 785) in 545 ms on 172.18.0.14 (executor 1) (2/3)
26/02/13 11:59:09 INFO TaskSetManager: Finished task 2.0 in stage 351.0 (TID 786) in 545 ms on 172.18.0.14 (executor 1) (3/3)
26/02/13 11:59:09 INFO TaskSchedulerImpl: Removed TaskSet 351.0, whose tasks have all completed, from pool 
26/02/13 11:59:09 INFO DAGScheduler: ResultStage 351 (start at NativeMethodAccessorImpl.java:0) finished in 0.549 s
26/02/13 11:59:09 INFO DAGScheduler: Job 232 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:59:09 INFO TaskSchedulerImpl: Killing all running tasks in stage 351: Stage finished
26/02/13 11:59:09 INFO DAGScheduler: Job 232 finished: start at NativeMethodAccessorImpl.java:0, took 0.550481 s
26/02/13 11:59:09 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 111, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@7dda005f] is committing.
26/02/13 11:59:09 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 111, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@7dda005f] committed.
26/02/13 11:59:09 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/111 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.111.f508b34d-4d0d-44bd-a1e5-483b6b6dc46c.tmp
26/02/13 11:59:09 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.111.f508b34d-4d0d-44bd-a1e5-483b6b6dc46c.tmp to file:/tmp/spark-checkpoint-enrichment/commits/111
26/02/13 11:59:09 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "fc5c6452-bd70-4c90-becf-772d3bebdb57",
  "name" : null,
  "timestamp" : "2026-02-13T11:59:08.862Z",
  "batchId" : 111,
  "numInputRows" : 11,
  "inputRowsPerSecond" : 916.6666666666666,
  "processedRowsPerSecond" : 12.542759407069555,
  "durationMs" : {
    "addBatch" : 712,
    "commitOffsets" : 73,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 25,
    "triggerExecution" : 877,
    "walCommit" : 66
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 7152,
        "1" : 7959,
        "0" : 9172
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 7154,
        "1" : 7966,
        "0" : 9174
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 7154,
        "1" : 7966,
        "0" : 9174
      }
    },
    "numInputRows" : 11,
    "inputRowsPerSecond" : 916.6666666666666,
    "processedRowsPerSecond" : 12.542759407069555,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@7fe9b5de",
    "numOutputRows" : 0
  }
}
26/02/13 11:59:09 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/112 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.112.0495ad92-203d-4cde-a013-68289085e6a1.tmp
26/02/13 11:59:09 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.112.0495ad92-203d-4cde-a013-68289085e6a1.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/112
26/02/13 11:59:09 INFO MicroBatchExecution: Committed offsets for batch 112. Metadata OffsetSeqMetadata(0,1770983949741,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 11:59:09 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:59:09 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:59:09 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:59:09 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:59:09 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:59:09 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:59:09 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#97599 - origin_code.nullCount#97598) > 0)
26/02/13 11:59:09 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#97604 - destination_code.nullCount#97603) > 0)
26/02/13 11:59:09 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#97634 - callsign.nullCount#97633) > 0)
26/02/13 11:59:09 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:59:09 INFO DAGScheduler: Got job 233 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 11:59:09 INFO DAGScheduler: Final stage: ResultStage 353 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:59:09 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 352)
26/02/13 11:59:09 INFO DAGScheduler: Missing parents: List()
26/02/13 11:59:09 INFO DAGScheduler: Submitting ResultStage 353 (MapPartitionsRDD[1275] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:59:09 INFO MemoryStore: Block broadcast_349 stored as values in memory (estimated size 99.3 KiB, free 433.8 MiB)
26/02/13 11:59:09 INFO MemoryStore: Block broadcast_349_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 433.7 MiB)
26/02/13 11:59:09 INFO BlockManagerInfo: Added broadcast_349_piece0 in memory on spark-master:40291 (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 11:59:09 INFO SparkContext: Created broadcast 349 from broadcast at DAGScheduler.scala:1585
26/02/13 11:59:09 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 353 (MapPartitionsRDD[1275] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 11:59:09 INFO TaskSchedulerImpl: Adding task set 353.0 with 4 tasks resource profile 0
26/02/13 11:59:09 INFO TaskSetManager: Starting task 0.0 in stage 353.0 (TID 787) (172.18.0.15, executor 0, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:59:09 INFO TaskSetManager: Starting task 1.0 in stage 353.0 (TID 788) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:59:09 INFO BlockManagerInfo: Added broadcast_349_piece0 in memory on 172.18.0.15:34959 (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 11:59:09 INFO TaskSetManager: Starting task 2.0 in stage 353.0 (TID 789) (172.18.0.15, executor 0, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:59:09 INFO TaskSetManager: Finished task 1.0 in stage 353.0 (TID 788) in 14 ms on 172.18.0.15 (executor 0) (1/4)
26/02/13 11:59:09 INFO TaskSetManager: Starting task 3.0 in stage 353.0 (TID 790) (172.18.0.15, executor 0, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:59:09 INFO TaskSetManager: Finished task 0.0 in stage 353.0 (TID 787) in 16 ms on 172.18.0.15 (executor 0) (2/4)
26/02/13 11:59:09 INFO TaskSetManager: Finished task 2.0 in stage 353.0 (TID 789) in 10 ms on 172.18.0.15 (executor 0) (3/4)
26/02/13 11:59:09 INFO TaskSetManager: Finished task 3.0 in stage 353.0 (TID 790) in 11 ms on 172.18.0.15 (executor 0) (4/4)
26/02/13 11:59:09 INFO TaskSchedulerImpl: Removed TaskSet 353.0, whose tasks have all completed, from pool 
26/02/13 11:59:09 INFO DAGScheduler: ResultStage 353 (start at NativeMethodAccessorImpl.java:0) finished in 0.033 s
26/02/13 11:59:09 INFO DAGScheduler: Job 233 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:59:09 INFO TaskSchedulerImpl: Killing all running tasks in stage 353: Stage finished
26/02/13 11:59:09 INFO DAGScheduler: Job 233 finished: start at NativeMethodAccessorImpl.java:0, took 0.034425 s
26/02/13 11:59:09 INFO MemoryStore: Block broadcast_350_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.6 MiB)
26/02/13 11:59:09 INFO BlockManagerInfo: Added broadcast_350_piece0 in memory on spark-master:40291 (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 11:59:09 INFO SparkContext: Created broadcast 350 from start at NativeMethodAccessorImpl.java:0
26/02/13 11:59:09 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 112, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@7c4b2212]. The input RDD has 3 partitions.
26/02/13 11:59:09 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:59:09 INFO DAGScheduler: Got job 234 (start at NativeMethodAccessorImpl.java:0) with 3 output partitions
26/02/13 11:59:09 INFO DAGScheduler: Final stage: ResultStage 354 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:59:09 INFO DAGScheduler: Parents of final stage: List()
26/02/13 11:59:09 INFO DAGScheduler: Missing parents: List()
26/02/13 11:59:09 INFO DAGScheduler: Submitting ResultStage 354 (MapPartitionsRDD[1281] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:59:09 INFO MemoryStore: Block broadcast_351 stored as values in memory (estimated size 49.9 KiB, free 433.6 MiB)
26/02/13 11:59:09 INFO MemoryStore: Block broadcast_351_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.6 MiB)
26/02/13 11:59:09 INFO BlockManagerInfo: Added broadcast_351_piece0 in memory on spark-master:40291 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 11:59:09 INFO SparkContext: Created broadcast 351 from broadcast at DAGScheduler.scala:1585
26/02/13 11:59:09 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 354 (MapPartitionsRDD[1281] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
26/02/13 11:59:09 INFO TaskSchedulerImpl: Adding task set 354.0 with 3 tasks resource profile 0
26/02/13 11:59:09 INFO TaskSetManager: Starting task 1.0 in stage 354.0 (TID 791) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:59:09 INFO TaskSetManager: Starting task 0.0 in stage 354.0 (TID 792) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:59:09 INFO TaskSetManager: Starting task 2.0 in stage 354.0 (TID 793) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:59:09 INFO BlockManagerInfo: Added broadcast_351_piece0 in memory on 172.18.0.15:34959 (size: 19.5 KiB, free: 434.0 MiB)
26/02/13 11:59:09 INFO BlockManagerInfo: Added broadcast_350_piece0 in memory on 172.18.0.15:34959 (size: 107.2 KiB, free: 433.9 MiB)
26/02/13 11:59:09 INFO TaskSetManager: Finished task 1.0 in stage 354.0 (TID 791) in 36 ms on 172.18.0.15 (executor 0) (1/3)
26/02/13 11:59:10 INFO BlockManagerInfo: Added broadcast_351_piece0 in memory on 172.18.0.14:40939 (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 11:59:10 INFO BlockManagerInfo: Added broadcast_350_piece0 in memory on 172.18.0.14:40939 (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 11:59:10 INFO TaskSetManager: Finished task 2.0 in stage 354.0 (TID 793) in 311 ms on 172.18.0.14 (executor 1) (2/3)
26/02/13 11:59:10 INFO TaskSetManager: Finished task 0.0 in stage 354.0 (TID 792) in 311 ms on 172.18.0.14 (executor 1) (3/3)
26/02/13 11:59:10 INFO TaskSchedulerImpl: Removed TaskSet 354.0, whose tasks have all completed, from pool 
26/02/13 11:59:10 INFO DAGScheduler: ResultStage 354 (start at NativeMethodAccessorImpl.java:0) finished in 0.315 s
26/02/13 11:59:10 INFO DAGScheduler: Job 234 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:59:10 INFO TaskSchedulerImpl: Killing all running tasks in stage 354: Stage finished
26/02/13 11:59:10 INFO DAGScheduler: Job 234 finished: start at NativeMethodAccessorImpl.java:0, took 0.316982 s
26/02/13 11:59:10 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 112, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@7c4b2212] is committing.
26/02/13 11:59:10 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 112, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@7c4b2212] committed.
26/02/13 11:59:10 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/112 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.112.349861d9-8a07-4f60-9173-2babcc5cb671.tmp
26/02/13 11:59:10 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.112.349861d9-8a07-4f60-9173-2babcc5cb671.tmp to file:/tmp/spark-checkpoint-enrichment/commits/112
26/02/13 11:59:10 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "fc5c6452-bd70-4c90-becf-772d3bebdb57",
  "name" : null,
  "timestamp" : "2026-02-13T11:59:09.740Z",
  "batchId" : 112,
  "numInputRows" : 241,
  "inputRowsPerSecond" : 274.4874715261959,
  "processedRowsPerSecond" : 420.5933682373473,
  "durationMs" : {
    "addBatch" : 432,
    "commitOffsets" : 58,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 22,
    "triggerExecution" : 573,
    "walCommit" : 60
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 7154,
        "1" : 7966,
        "0" : 9174
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 7230,
        "1" : 8058,
        "0" : 9247
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 7230,
        "1" : 8058,
        "0" : 9247
      }
    },
    "numInputRows" : 241,
    "inputRowsPerSecond" : 274.4874715261959,
    "processedRowsPerSecond" : 420.5933682373473,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@7fe9b5de",
    "numOutputRows" : 52
  }
}
26/02/13 11:59:11 INFO BlockManagerInfo: Removed broadcast_351_piece0 on spark-master:40291 in memory (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 11:59:11 INFO BlockManagerInfo: Removed broadcast_351_piece0 on 172.18.0.14:40939 in memory (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 11:59:11 INFO BlockManagerInfo: Removed broadcast_351_piece0 on 172.18.0.15:34959 in memory (size: 19.5 KiB, free: 433.9 MiB)
26/02/13 11:59:11 INFO BlockManagerInfo: Removed broadcast_348_piece0 on spark-master:40291 in memory (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 11:59:11 INFO BlockManagerInfo: Removed broadcast_348_piece0 on 172.18.0.14:40939 in memory (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 11:59:11 INFO BlockManagerInfo: Removed broadcast_348_piece0 on 172.18.0.15:34959 in memory (size: 19.5 KiB, free: 433.9 MiB)
26/02/13 11:59:11 INFO BlockManagerInfo: Removed broadcast_346_piece0 on spark-master:40291 in memory (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 11:59:11 INFO BlockManagerInfo: Removed broadcast_346_piece0 on 172.18.0.15:34959 in memory (size: 33.3 KiB, free: 433.9 MiB)
26/02/13 11:59:11 INFO BlockManagerInfo: Removed broadcast_347_piece0 on spark-master:40291 in memory (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 11:59:11 INFO BlockManagerInfo: Removed broadcast_347_piece0 on 172.18.0.14:40939 in memory (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 11:59:11 INFO BlockManagerInfo: Removed broadcast_347_piece0 on 172.18.0.15:34959 in memory (size: 107.2 KiB, free: 434.0 MiB)
26/02/13 11:59:11 INFO BlockManagerInfo: Removed broadcast_349_piece0 on spark-master:40291 in memory (size: 33.3 KiB, free: 434.3 MiB)
26/02/13 11:59:11 INFO BlockManagerInfo: Removed broadcast_349_piece0 on 172.18.0.15:34959 in memory (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 11:59:20 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 11:59:25 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/113 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.113.598daf26-9924-4e7f-b1c6-332396464cc7.tmp
26/02/13 11:59:25 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.113.598daf26-9924-4e7f-b1c6-332396464cc7.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/113
26/02/13 11:59:25 INFO MicroBatchExecution: Committed offsets for batch 113. Metadata OffsetSeqMetadata(0,1770983965437,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 11:59:25 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:59:25 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:59:25 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:59:25 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:59:25 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:59:25 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:59:25 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#98453 - origin_code.nullCount#98452) > 0)
26/02/13 11:59:25 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#98458 - destination_code.nullCount#98457) > 0)
26/02/13 11:59:25 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#98488 - callsign.nullCount#98487) > 0)
26/02/13 11:59:25 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:59:25 INFO DAGScheduler: Got job 235 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 11:59:25 INFO DAGScheduler: Final stage: ResultStage 356 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:59:25 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 355)
26/02/13 11:59:25 INFO DAGScheduler: Missing parents: List()
26/02/13 11:59:25 INFO DAGScheduler: Submitting ResultStage 356 (MapPartitionsRDD[1286] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:59:25 INFO MemoryStore: Block broadcast_352 stored as values in memory (estimated size 99.3 KiB, free 434.0 MiB)
26/02/13 11:59:25 INFO MemoryStore: Block broadcast_352_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 433.9 MiB)
26/02/13 11:59:25 INFO BlockManagerInfo: Added broadcast_352_piece0 in memory on spark-master:40291 (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 11:59:25 INFO SparkContext: Created broadcast 352 from broadcast at DAGScheduler.scala:1585
26/02/13 11:59:25 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 356 (MapPartitionsRDD[1286] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 11:59:25 INFO TaskSchedulerImpl: Adding task set 356.0 with 4 tasks resource profile 0
26/02/13 11:59:25 INFO TaskSetManager: Starting task 0.0 in stage 356.0 (TID 794) (172.18.0.15, executor 0, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:59:25 INFO TaskSetManager: Starting task 1.0 in stage 356.0 (TID 795) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:59:25 INFO BlockManagerInfo: Added broadcast_352_piece0 in memory on 172.18.0.15:34959 (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 11:59:25 INFO TaskSetManager: Starting task 2.0 in stage 356.0 (TID 796) (172.18.0.15, executor 0, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:59:25 INFO TaskSetManager: Finished task 0.0 in stage 356.0 (TID 794) in 23 ms on 172.18.0.15 (executor 0) (1/4)
26/02/13 11:59:25 INFO TaskSetManager: Finished task 1.0 in stage 356.0 (TID 795) in 23 ms on 172.18.0.15 (executor 0) (2/4)
26/02/13 11:59:25 INFO TaskSetManager: Starting task 3.0 in stage 356.0 (TID 797) (172.18.0.15, executor 0, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:59:25 INFO TaskSetManager: Finished task 2.0 in stage 356.0 (TID 796) in 22 ms on 172.18.0.15 (executor 0) (3/4)
26/02/13 11:59:25 INFO TaskSetManager: Finished task 3.0 in stage 356.0 (TID 797) in 22 ms on 172.18.0.15 (executor 0) (4/4)
26/02/13 11:59:25 INFO TaskSchedulerImpl: Removed TaskSet 356.0, whose tasks have all completed, from pool 
26/02/13 11:59:25 INFO DAGScheduler: ResultStage 356 (start at NativeMethodAccessorImpl.java:0) finished in 0.058 s
26/02/13 11:59:25 INFO DAGScheduler: Job 235 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:59:25 INFO TaskSchedulerImpl: Killing all running tasks in stage 356: Stage finished
26/02/13 11:59:25 INFO DAGScheduler: Job 235 finished: start at NativeMethodAccessorImpl.java:0, took 0.060824 s
26/02/13 11:59:25 INFO MemoryStore: Block broadcast_353_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.8 MiB)
26/02/13 11:59:25 INFO BlockManagerInfo: Added broadcast_353_piece0 in memory on spark-master:40291 (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 11:59:25 INFO SparkContext: Created broadcast 353 from start at NativeMethodAccessorImpl.java:0
26/02/13 11:59:25 INFO BlockManagerInfo: Removed broadcast_350_piece0 on spark-master:40291 in memory (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 11:59:25 INFO BlockManagerInfo: Removed broadcast_350_piece0 on 172.18.0.15:34959 in memory (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 11:59:25 INFO BlockManagerInfo: Removed broadcast_350_piece0 on 172.18.0.14:40939 in memory (size: 107.2 KiB, free: 434.4 MiB)
26/02/13 11:59:25 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 113, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@4ceda3e1]. The input RDD has 3 partitions.
26/02/13 11:59:25 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:59:25 INFO DAGScheduler: Got job 236 (start at NativeMethodAccessorImpl.java:0) with 3 output partitions
26/02/13 11:59:25 INFO DAGScheduler: Final stage: ResultStage 357 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:59:25 INFO DAGScheduler: Parents of final stage: List()
26/02/13 11:59:25 INFO DAGScheduler: Missing parents: List()
26/02/13 11:59:25 INFO DAGScheduler: Submitting ResultStage 357 (MapPartitionsRDD[1292] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:59:25 INFO MemoryStore: Block broadcast_354 stored as values in memory (estimated size 49.9 KiB, free 433.9 MiB)
26/02/13 11:59:25 INFO MemoryStore: Block broadcast_354_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.9 MiB)
26/02/13 11:59:25 INFO BlockManagerInfo: Added broadcast_354_piece0 in memory on spark-master:40291 (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 11:59:25 INFO SparkContext: Created broadcast 354 from broadcast at DAGScheduler.scala:1585
26/02/13 11:59:25 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 357 (MapPartitionsRDD[1292] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
26/02/13 11:59:25 INFO TaskSchedulerImpl: Adding task set 357.0 with 3 tasks resource profile 0
26/02/13 11:59:25 INFO TaskSetManager: Starting task 0.0 in stage 357.0 (TID 798) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:59:25 INFO TaskSetManager: Starting task 1.0 in stage 357.0 (TID 799) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:59:25 INFO TaskSetManager: Starting task 2.0 in stage 357.0 (TID 800) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:59:25 INFO BlockManagerInfo: Added broadcast_354_piece0 in memory on 172.18.0.14:40939 (size: 19.5 KiB, free: 434.4 MiB)
26/02/13 11:59:25 INFO BlockManagerInfo: Added broadcast_354_piece0 in memory on 172.18.0.15:34959 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 11:59:25 INFO BlockManagerInfo: Added broadcast_353_piece0 in memory on 172.18.0.14:40939 (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 11:59:25 INFO BlockManagerInfo: Added broadcast_353_piece0 in memory on 172.18.0.15:34959 (size: 107.2 KiB, free: 434.0 MiB)
26/02/13 11:59:26 INFO TaskSetManager: Finished task 2.0 in stage 357.0 (TID 800) in 571 ms on 172.18.0.14 (executor 1) (1/3)
26/02/13 11:59:26 INFO TaskSetManager: Finished task 0.0 in stage 357.0 (TID 798) in 580 ms on 172.18.0.14 (executor 1) (2/3)
26/02/13 11:59:26 INFO TaskSetManager: Finished task 1.0 in stage 357.0 (TID 799) in 584 ms on 172.18.0.15 (executor 0) (3/3)
26/02/13 11:59:26 INFO TaskSchedulerImpl: Removed TaskSet 357.0, whose tasks have all completed, from pool 
26/02/13 11:59:26 INFO DAGScheduler: ResultStage 357 (start at NativeMethodAccessorImpl.java:0) finished in 0.591 s
26/02/13 11:59:26 INFO DAGScheduler: Job 236 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:59:26 INFO TaskSchedulerImpl: Killing all running tasks in stage 357: Stage finished
26/02/13 11:59:26 INFO DAGScheduler: Job 236 finished: start at NativeMethodAccessorImpl.java:0, took 0.594361 s
26/02/13 11:59:26 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 113, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@4ceda3e1] is committing.
26/02/13 11:59:26 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 113, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@4ceda3e1] committed.
26/02/13 11:59:26 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/113 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.113.3d4e791f-ecd9-41ab-bb0f-bb4725d419d7.tmp
26/02/13 11:59:26 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.113.3d4e791f-ecd9-41ab-bb0f-bb4725d419d7.tmp to file:/tmp/spark-checkpoint-enrichment/commits/113
26/02/13 11:59:26 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "fc5c6452-bd70-4c90-becf-772d3bebdb57",
  "name" : null,
  "timestamp" : "2026-02-13T11:59:25.436Z",
  "batchId" : 113,
  "numInputRows" : 79,
  "inputRowsPerSecond" : 6076.923076923077,
  "processedRowsPerSecond" : 85.77633007600434,
  "durationMs" : {
    "addBatch" : 758,
    "commitOffsets" : 64,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 26,
    "triggerExecution" : 921,
    "walCommit" : 72
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 7230,
        "1" : 8058,
        "0" : 9247
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 7254,
        "1" : 8089,
        "0" : 9271
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 7254,
        "1" : 8089,
        "0" : 9271
      }
    },
    "numInputRows" : 79,
    "inputRowsPerSecond" : 6076.923076923077,
    "processedRowsPerSecond" : 85.77633007600434,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@7fe9b5de",
    "numOutputRows" : 19
  }
}
26/02/13 11:59:26 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/114 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.114.5aa119d0-a3ae-4a2e-b3b0-0dc529b502d2.tmp
26/02/13 11:59:26 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.114.5aa119d0-a3ae-4a2e-b3b0-0dc529b502d2.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/114
26/02/13 11:59:26 INFO MicroBatchExecution: Committed offsets for batch 114. Metadata OffsetSeqMetadata(0,1770983966359,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 11:59:26 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:59:26 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:59:26 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:59:26 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:59:26 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:59:26 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:59:26 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#99307 - origin_code.nullCount#99306) > 0)
26/02/13 11:59:26 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#99312 - destination_code.nullCount#99311) > 0)
26/02/13 11:59:26 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#99342 - callsign.nullCount#99341) > 0)
26/02/13 11:59:26 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:59:26 INFO DAGScheduler: Got job 237 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 11:59:26 INFO DAGScheduler: Final stage: ResultStage 359 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:59:26 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 358)
26/02/13 11:59:26 INFO DAGScheduler: Missing parents: List()
26/02/13 11:59:26 INFO DAGScheduler: Submitting ResultStage 359 (MapPartitionsRDD[1297] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:59:26 INFO MemoryStore: Block broadcast_355 stored as values in memory (estimated size 99.3 KiB, free 433.8 MiB)
26/02/13 11:59:26 INFO MemoryStore: Block broadcast_355_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 433.7 MiB)
26/02/13 11:59:26 INFO BlockManagerInfo: Added broadcast_355_piece0 in memory on spark-master:40291 (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 11:59:26 INFO SparkContext: Created broadcast 355 from broadcast at DAGScheduler.scala:1585
26/02/13 11:59:26 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 359 (MapPartitionsRDD[1297] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 11:59:26 INFO TaskSchedulerImpl: Adding task set 359.0 with 4 tasks resource profile 0
26/02/13 11:59:26 INFO TaskSetManager: Starting task 0.0 in stage 359.0 (TID 801) (172.18.0.15, executor 0, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:59:26 INFO TaskSetManager: Starting task 1.0 in stage 359.0 (TID 802) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:59:26 INFO BlockManagerInfo: Added broadcast_355_piece0 in memory on 172.18.0.15:34959 (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 11:59:26 INFO TaskSetManager: Starting task 2.0 in stage 359.0 (TID 803) (172.18.0.15, executor 0, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:59:26 INFO TaskSetManager: Starting task 3.0 in stage 359.0 (TID 804) (172.18.0.15, executor 0, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:59:26 INFO TaskSetManager: Finished task 0.0 in stage 359.0 (TID 801) in 15 ms on 172.18.0.15 (executor 0) (1/4)
26/02/13 11:59:26 INFO TaskSetManager: Finished task 1.0 in stage 359.0 (TID 802) in 15 ms on 172.18.0.15 (executor 0) (2/4)
26/02/13 11:59:26 INFO TaskSetManager: Finished task 2.0 in stage 359.0 (TID 803) in 8 ms on 172.18.0.15 (executor 0) (3/4)
26/02/13 11:59:26 INFO TaskSetManager: Finished task 3.0 in stage 359.0 (TID 804) in 10 ms on 172.18.0.15 (executor 0) (4/4)
26/02/13 11:59:26 INFO TaskSchedulerImpl: Removed TaskSet 359.0, whose tasks have all completed, from pool 
26/02/13 11:59:26 INFO DAGScheduler: ResultStage 359 (start at NativeMethodAccessorImpl.java:0) finished in 0.032 s
26/02/13 11:59:26 INFO DAGScheduler: Job 237 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:59:26 INFO TaskSchedulerImpl: Killing all running tasks in stage 359: Stage finished
26/02/13 11:59:26 INFO DAGScheduler: Job 237 finished: start at NativeMethodAccessorImpl.java:0, took 0.033275 s
26/02/13 11:59:26 INFO MemoryStore: Block broadcast_356_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.6 MiB)
26/02/13 11:59:26 INFO BlockManagerInfo: Added broadcast_356_piece0 in memory on spark-master:40291 (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 11:59:26 INFO SparkContext: Created broadcast 356 from start at NativeMethodAccessorImpl.java:0
26/02/13 11:59:26 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 114, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@619a2f26]. The input RDD has 3 partitions.
26/02/13 11:59:26 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:59:26 INFO DAGScheduler: Got job 238 (start at NativeMethodAccessorImpl.java:0) with 3 output partitions
26/02/13 11:59:26 INFO DAGScheduler: Final stage: ResultStage 360 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:59:26 INFO DAGScheduler: Parents of final stage: List()
26/02/13 11:59:26 INFO DAGScheduler: Missing parents: List()
26/02/13 11:59:26 INFO DAGScheduler: Submitting ResultStage 360 (MapPartitionsRDD[1303] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:59:26 INFO MemoryStore: Block broadcast_357 stored as values in memory (estimated size 49.9 KiB, free 433.6 MiB)
26/02/13 11:59:26 INFO MemoryStore: Block broadcast_357_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.6 MiB)
26/02/13 11:59:26 INFO BlockManagerInfo: Added broadcast_357_piece0 in memory on spark-master:40291 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 11:59:26 INFO SparkContext: Created broadcast 357 from broadcast at DAGScheduler.scala:1585
26/02/13 11:59:26 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 360 (MapPartitionsRDD[1303] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
26/02/13 11:59:26 INFO TaskSchedulerImpl: Adding task set 360.0 with 3 tasks resource profile 0
26/02/13 11:59:26 INFO TaskSetManager: Starting task 0.0 in stage 360.0 (TID 805) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:59:26 INFO TaskSetManager: Starting task 1.0 in stage 360.0 (TID 806) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:59:26 INFO TaskSetManager: Starting task 2.0 in stage 360.0 (TID 807) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:59:26 INFO BlockManagerInfo: Added broadcast_357_piece0 in memory on 172.18.0.15:34959 (size: 19.5 KiB, free: 434.0 MiB)
26/02/13 11:59:26 INFO BlockManagerInfo: Added broadcast_357_piece0 in memory on 172.18.0.14:40939 (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 11:59:26 INFO BlockManagerInfo: Added broadcast_356_piece0 in memory on 172.18.0.15:34959 (size: 107.2 KiB, free: 433.9 MiB)
26/02/13 11:59:26 INFO BlockManagerInfo: Added broadcast_356_piece0 in memory on 172.18.0.14:40939 (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 11:59:26 INFO TaskSetManager: Finished task 1.0 in stage 360.0 (TID 806) in 59 ms on 172.18.0.15 (executor 0) (1/3)
26/02/13 11:59:26 INFO TaskSetManager: Finished task 0.0 in stage 360.0 (TID 805) in 60 ms on 172.18.0.14 (executor 1) (2/3)
26/02/13 11:59:26 INFO TaskSetManager: Finished task 2.0 in stage 360.0 (TID 807) in 60 ms on 172.18.0.14 (executor 1) (3/3)
26/02/13 11:59:26 INFO TaskSchedulerImpl: Removed TaskSet 360.0, whose tasks have all completed, from pool 
26/02/13 11:59:26 INFO DAGScheduler: ResultStage 360 (start at NativeMethodAccessorImpl.java:0) finished in 0.068 s
26/02/13 11:59:26 INFO DAGScheduler: Job 238 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:59:26 INFO TaskSchedulerImpl: Killing all running tasks in stage 360: Stage finished
26/02/13 11:59:26 INFO DAGScheduler: Job 238 finished: start at NativeMethodAccessorImpl.java:0, took 0.069547 s
26/02/13 11:59:26 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 114, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@619a2f26] is committing.
26/02/13 11:59:26 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 114, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@619a2f26] committed.
26/02/13 11:59:26 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/114 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.114.f62d7abf-e618-4f35-acf4-7b33af552394.tmp
26/02/13 11:59:26 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.114.f62d7abf-e618-4f35-acf4-7b33af552394.tmp to file:/tmp/spark-checkpoint-enrichment/commits/114
26/02/13 11:59:26 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "fc5c6452-bd70-4c90-becf-772d3bebdb57",
  "name" : null,
  "timestamp" : "2026-02-13T11:59:26.358Z",
  "batchId" : 114,
  "numInputRows" : 172,
  "inputRowsPerSecond" : 186.55097613882862,
  "processedRowsPerSecond" : 535.8255451713395,
  "durationMs" : {
    "addBatch" : 178,
    "commitOffsets" : 64,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 21,
    "triggerExecution" : 321,
    "walCommit" : 56
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 7254,
        "1" : 8089,
        "0" : 9271
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 7307,
        "1" : 8156,
        "0" : 9323
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 7307,
        "1" : 8156,
        "0" : 9323
      }
    },
    "numInputRows" : 172,
    "inputRowsPerSecond" : 186.55097613882862,
    "processedRowsPerSecond" : 535.8255451713395,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@7fe9b5de",
    "numOutputRows" : 33
  }
}
26/02/13 11:59:31 INFO BlockManagerInfo: Removed broadcast_352_piece0 on spark-master:40291 in memory (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 11:59:31 INFO BlockManagerInfo: Removed broadcast_352_piece0 on 172.18.0.15:34959 in memory (size: 33.3 KiB, free: 433.9 MiB)
26/02/13 11:59:31 INFO BlockManagerInfo: Removed broadcast_354_piece0 on spark-master:40291 in memory (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 11:59:31 INFO BlockManagerInfo: Removed broadcast_354_piece0 on 172.18.0.15:34959 in memory (size: 19.5 KiB, free: 433.9 MiB)
26/02/13 11:59:31 INFO BlockManagerInfo: Removed broadcast_354_piece0 on 172.18.0.14:40939 in memory (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 11:59:31 INFO BlockManagerInfo: Removed broadcast_353_piece0 on spark-master:40291 in memory (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 11:59:31 INFO BlockManagerInfo: Removed broadcast_353_piece0 on 172.18.0.14:40939 in memory (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 11:59:31 INFO BlockManagerInfo: Removed broadcast_353_piece0 on 172.18.0.15:34959 in memory (size: 107.2 KiB, free: 434.0 MiB)
26/02/13 11:59:31 INFO BlockManagerInfo: Removed broadcast_357_piece0 on spark-master:40291 in memory (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 11:59:31 INFO BlockManagerInfo: Removed broadcast_357_piece0 on 172.18.0.15:34959 in memory (size: 19.5 KiB, free: 434.0 MiB)
26/02/13 11:59:31 INFO BlockManagerInfo: Removed broadcast_357_piece0 on 172.18.0.14:40939 in memory (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 11:59:31 INFO BlockManagerInfo: Removed broadcast_355_piece0 on spark-master:40291 in memory (size: 33.3 KiB, free: 434.3 MiB)
26/02/13 11:59:31 INFO BlockManagerInfo: Removed broadcast_355_piece0 on 172.18.0.15:34959 in memory (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 11:59:36 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 11:59:41 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/115 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.115.1a8c5449-881a-4691-a3a6-aea13e686717.tmp
26/02/13 11:59:41 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.115.1a8c5449-881a-4691-a3a6-aea13e686717.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/115
26/02/13 11:59:41 INFO MicroBatchExecution: Committed offsets for batch 115. Metadata OffsetSeqMetadata(0,1770983981666,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 11:59:41 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:59:41 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:59:41 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:59:41 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:59:41 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:59:41 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:59:41 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#100161 - origin_code.nullCount#100160) > 0)
26/02/13 11:59:41 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#100166 - destination_code.nullCount#100165) > 0)
26/02/13 11:59:41 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#100196 - callsign.nullCount#100195) > 0)
26/02/13 11:59:41 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:59:41 INFO DAGScheduler: Got job 239 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 11:59:41 INFO DAGScheduler: Final stage: ResultStage 362 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:59:41 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 361)
26/02/13 11:59:41 INFO DAGScheduler: Missing parents: List()
26/02/13 11:59:41 INFO DAGScheduler: Submitting ResultStage 362 (MapPartitionsRDD[1308] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:59:41 INFO MemoryStore: Block broadcast_358 stored as values in memory (estimated size 99.3 KiB, free 434.0 MiB)
26/02/13 11:59:41 INFO MemoryStore: Block broadcast_358_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 433.9 MiB)
26/02/13 11:59:41 INFO BlockManagerInfo: Added broadcast_358_piece0 in memory on spark-master:40291 (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 11:59:41 INFO SparkContext: Created broadcast 358 from broadcast at DAGScheduler.scala:1585
26/02/13 11:59:41 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 362 (MapPartitionsRDD[1308] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 11:59:41 INFO TaskSchedulerImpl: Adding task set 362.0 with 4 tasks resource profile 0
26/02/13 11:59:41 INFO TaskSetManager: Starting task 0.0 in stage 362.0 (TID 808) (172.18.0.15, executor 0, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:59:41 INFO TaskSetManager: Starting task 1.0 in stage 362.0 (TID 809) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:59:41 INFO BlockManagerInfo: Added broadcast_358_piece0 in memory on 172.18.0.15:34959 (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 11:59:41 INFO TaskSetManager: Starting task 2.0 in stage 362.0 (TID 810) (172.18.0.15, executor 0, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:59:41 INFO TaskSetManager: Starting task 3.0 in stage 362.0 (TID 811) (172.18.0.15, executor 0, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:59:41 INFO TaskSetManager: Finished task 0.0 in stage 362.0 (TID 808) in 24 ms on 172.18.0.15 (executor 0) (1/4)
26/02/13 11:59:41 INFO TaskSetManager: Finished task 1.0 in stage 362.0 (TID 809) in 24 ms on 172.18.0.15 (executor 0) (2/4)
26/02/13 11:59:41 INFO TaskSetManager: Finished task 2.0 in stage 362.0 (TID 810) in 18 ms on 172.18.0.15 (executor 0) (3/4)
26/02/13 11:59:41 INFO TaskSetManager: Finished task 3.0 in stage 362.0 (TID 811) in 18 ms on 172.18.0.15 (executor 0) (4/4)
26/02/13 11:59:41 INFO TaskSchedulerImpl: Removed TaskSet 362.0, whose tasks have all completed, from pool 
26/02/13 11:59:41 INFO DAGScheduler: ResultStage 362 (start at NativeMethodAccessorImpl.java:0) finished in 0.047 s
26/02/13 11:59:41 INFO DAGScheduler: Job 239 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:59:41 INFO TaskSchedulerImpl: Killing all running tasks in stage 362: Stage finished
26/02/13 11:59:41 INFO DAGScheduler: Job 239 finished: start at NativeMethodAccessorImpl.java:0, took 0.049763 s
26/02/13 11:59:41 INFO MemoryStore: Block broadcast_359_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.8 MiB)
26/02/13 11:59:41 INFO BlockManagerInfo: Added broadcast_359_piece0 in memory on spark-master:40291 (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 11:59:41 INFO SparkContext: Created broadcast 359 from start at NativeMethodAccessorImpl.java:0
26/02/13 11:59:41 INFO BlockManagerInfo: Removed broadcast_358_piece0 on spark-master:40291 in memory (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 11:59:41 INFO BlockManagerInfo: Removed broadcast_358_piece0 on 172.18.0.15:34959 in memory (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 11:59:41 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 115, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@4afabc98]. The input RDD has 3 partitions.
26/02/13 11:59:41 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:59:41 INFO DAGScheduler: Got job 240 (start at NativeMethodAccessorImpl.java:0) with 3 output partitions
26/02/13 11:59:41 INFO DAGScheduler: Final stage: ResultStage 363 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:59:41 INFO DAGScheduler: Parents of final stage: List()
26/02/13 11:59:41 INFO DAGScheduler: Missing parents: List()
26/02/13 11:59:41 INFO DAGScheduler: Submitting ResultStage 363 (MapPartitionsRDD[1314] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:59:41 INFO MemoryStore: Block broadcast_360 stored as values in memory (estimated size 49.9 KiB, free 433.9 MiB)
26/02/13 11:59:41 INFO MemoryStore: Block broadcast_360_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.9 MiB)
26/02/13 11:59:41 INFO BlockManagerInfo: Added broadcast_360_piece0 in memory on spark-master:40291 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 11:59:41 INFO SparkContext: Created broadcast 360 from broadcast at DAGScheduler.scala:1585
26/02/13 11:59:41 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 363 (MapPartitionsRDD[1314] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
26/02/13 11:59:41 INFO TaskSchedulerImpl: Adding task set 363.0 with 3 tasks resource profile 0
26/02/13 11:59:41 INFO TaskSetManager: Starting task 0.0 in stage 363.0 (TID 812) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:59:41 INFO TaskSetManager: Starting task 1.0 in stage 363.0 (TID 813) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:59:41 INFO TaskSetManager: Starting task 2.0 in stage 363.0 (TID 814) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:59:41 INFO BlockManagerInfo: Added broadcast_360_piece0 in memory on 172.18.0.15:34959 (size: 19.5 KiB, free: 434.0 MiB)
26/02/13 11:59:41 INFO BlockManagerInfo: Added broadcast_360_piece0 in memory on 172.18.0.14:40939 (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 11:59:42 INFO BlockManagerInfo: Added broadcast_359_piece0 in memory on 172.18.0.15:34959 (size: 107.2 KiB, free: 433.9 MiB)
26/02/13 11:59:42 INFO BlockManagerInfo: Added broadcast_359_piece0 in memory on 172.18.0.14:40939 (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 11:59:42 INFO BlockManagerInfo: Removed broadcast_356_piece0 on spark-master:40291 in memory (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 11:59:42 INFO BlockManagerInfo: Removed broadcast_356_piece0 on 172.18.0.15:34959 in memory (size: 107.2 KiB, free: 434.0 MiB)
26/02/13 11:59:42 INFO BlockManagerInfo: Removed broadcast_356_piece0 on 172.18.0.14:40939 in memory (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 11:59:42 INFO TaskSetManager: Finished task 0.0 in stage 363.0 (TID 812) in 579 ms on 172.18.0.14 (executor 1) (1/3)
26/02/13 11:59:42 INFO TaskSetManager: Finished task 1.0 in stage 363.0 (TID 813) in 583 ms on 172.18.0.15 (executor 0) (2/3)
26/02/13 11:59:42 INFO TaskSetManager: Finished task 2.0 in stage 363.0 (TID 814) in 584 ms on 172.18.0.14 (executor 1) (3/3)
26/02/13 11:59:42 INFO TaskSchedulerImpl: Removed TaskSet 363.0, whose tasks have all completed, from pool 
26/02/13 11:59:42 INFO DAGScheduler: ResultStage 363 (start at NativeMethodAccessorImpl.java:0) finished in 0.594 s
26/02/13 11:59:42 INFO DAGScheduler: Job 240 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:59:42 INFO TaskSchedulerImpl: Killing all running tasks in stage 363: Stage finished
26/02/13 11:59:42 INFO DAGScheduler: Job 240 finished: start at NativeMethodAccessorImpl.java:0, took 0.596093 s
26/02/13 11:59:42 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 115, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@4afabc98] is committing.
26/02/13 11:59:42 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 115, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@4afabc98] committed.
26/02/13 11:59:42 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/115 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.115.dfec371c-ae1c-42b5-a814-758987281601.tmp
26/02/13 11:59:42 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.115.dfec371c-ae1c-42b5-a814-758987281601.tmp to file:/tmp/spark-checkpoint-enrichment/commits/115
26/02/13 11:59:42 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "fc5c6452-bd70-4c90-becf-772d3bebdb57",
  "name" : null,
  "timestamp" : "2026-02-13T11:59:41.664Z",
  "batchId" : 115,
  "numInputRows" : 93,
  "inputRowsPerSecond" : 7750.0,
  "processedRowsPerSecond" : 96.875,
  "durationMs" : {
    "addBatch" : 765,
    "commitOffsets" : 67,
    "getBatch" : 0,
    "latestOffset" : 2,
    "queryPlanning" : 33,
    "triggerExecution" : 960,
    "walCommit" : 93
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 7307,
        "1" : 8156,
        "0" : 9323
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 7338,
        "1" : 8192,
        "0" : 9349
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 7338,
        "1" : 8192,
        "0" : 9349
      }
    },
    "numInputRows" : 93,
    "inputRowsPerSecond" : 7750.0,
    "processedRowsPerSecond" : 96.875,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@7fe9b5de",
    "numOutputRows" : 21
  }
}
26/02/13 11:59:42 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/116 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.116.6477fb86-d892-4496-a132-09ebffbec15f.tmp
26/02/13 11:59:42 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.116.6477fb86-d892-4496-a132-09ebffbec15f.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/116
26/02/13 11:59:42 INFO MicroBatchExecution: Committed offsets for batch 116. Metadata OffsetSeqMetadata(0,1770983982626,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 11:59:42 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:59:42 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:59:42 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:59:42 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:59:42 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:59:42 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:59:42 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#101015 - origin_code.nullCount#101014) > 0)
26/02/13 11:59:42 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#101020 - destination_code.nullCount#101019) > 0)
26/02/13 11:59:42 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#101050 - callsign.nullCount#101049) > 0)
26/02/13 11:59:42 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:59:42 INFO DAGScheduler: Got job 241 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 11:59:42 INFO DAGScheduler: Final stage: ResultStage 365 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:59:42 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 364)
26/02/13 11:59:42 INFO DAGScheduler: Missing parents: List()
26/02/13 11:59:42 INFO DAGScheduler: Submitting ResultStage 365 (MapPartitionsRDD[1319] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:59:42 INFO MemoryStore: Block broadcast_361 stored as values in memory (estimated size 99.3 KiB, free 433.9 MiB)
26/02/13 11:59:42 INFO MemoryStore: Block broadcast_361_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 433.9 MiB)
26/02/13 11:59:42 INFO BlockManagerInfo: Added broadcast_361_piece0 in memory on spark-master:40291 (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 11:59:42 INFO SparkContext: Created broadcast 361 from broadcast at DAGScheduler.scala:1585
26/02/13 11:59:42 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 365 (MapPartitionsRDD[1319] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 11:59:42 INFO TaskSchedulerImpl: Adding task set 365.0 with 4 tasks resource profile 0
26/02/13 11:59:42 INFO TaskSetManager: Starting task 0.0 in stage 365.0 (TID 815) (172.18.0.15, executor 0, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:59:42 INFO TaskSetManager: Starting task 1.0 in stage 365.0 (TID 816) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:59:42 INFO BlockManagerInfo: Added broadcast_361_piece0 in memory on 172.18.0.15:34959 (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 11:59:42 INFO TaskSetManager: Starting task 2.0 in stage 365.0 (TID 817) (172.18.0.15, executor 0, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:59:42 INFO TaskSetManager: Starting task 3.0 in stage 365.0 (TID 818) (172.18.0.15, executor 0, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:59:42 INFO TaskSetManager: Finished task 1.0 in stage 365.0 (TID 816) in 27 ms on 172.18.0.15 (executor 0) (1/4)
26/02/13 11:59:42 INFO TaskSetManager: Finished task 0.0 in stage 365.0 (TID 815) in 28 ms on 172.18.0.15 (executor 0) (2/4)
26/02/13 11:59:42 INFO TaskSetManager: Finished task 2.0 in stage 365.0 (TID 817) in 14 ms on 172.18.0.15 (executor 0) (3/4)
26/02/13 11:59:42 INFO TaskSetManager: Finished task 3.0 in stage 365.0 (TID 818) in 21 ms on 172.18.0.15 (executor 0) (4/4)
26/02/13 11:59:42 INFO TaskSchedulerImpl: Removed TaskSet 365.0, whose tasks have all completed, from pool 
26/02/13 11:59:42 INFO DAGScheduler: ResultStage 365 (start at NativeMethodAccessorImpl.java:0) finished in 0.055 s
26/02/13 11:59:42 INFO DAGScheduler: Job 241 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:59:42 INFO TaskSchedulerImpl: Killing all running tasks in stage 365: Stage finished
26/02/13 11:59:42 INFO DAGScheduler: Job 241 finished: start at NativeMethodAccessorImpl.java:0, took 0.056489 s
26/02/13 11:59:42 INFO MemoryStore: Block broadcast_362_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.8 MiB)
26/02/13 11:59:42 INFO BlockManagerInfo: Added broadcast_362_piece0 in memory on spark-master:40291 (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 11:59:42 INFO SparkContext: Created broadcast 362 from start at NativeMethodAccessorImpl.java:0
26/02/13 11:59:42 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 116, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@341f148d]. The input RDD has 3 partitions.
26/02/13 11:59:42 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:59:42 INFO DAGScheduler: Got job 242 (start at NativeMethodAccessorImpl.java:0) with 3 output partitions
26/02/13 11:59:42 INFO DAGScheduler: Final stage: ResultStage 366 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:59:42 INFO DAGScheduler: Parents of final stage: List()
26/02/13 11:59:42 INFO DAGScheduler: Missing parents: List()
26/02/13 11:59:42 INFO DAGScheduler: Submitting ResultStage 366 (MapPartitionsRDD[1325] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:59:42 INFO MemoryStore: Block broadcast_363 stored as values in memory (estimated size 49.9 KiB, free 433.7 MiB)
26/02/13 11:59:42 INFO MemoryStore: Block broadcast_363_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.7 MiB)
26/02/13 11:59:42 INFO BlockManagerInfo: Added broadcast_363_piece0 in memory on spark-master:40291 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 11:59:42 INFO SparkContext: Created broadcast 363 from broadcast at DAGScheduler.scala:1585
26/02/13 11:59:42 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 366 (MapPartitionsRDD[1325] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
26/02/13 11:59:42 INFO TaskSchedulerImpl: Adding task set 366.0 with 3 tasks resource profile 0
26/02/13 11:59:42 INFO TaskSetManager: Starting task 1.0 in stage 366.0 (TID 819) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:59:42 INFO TaskSetManager: Starting task 0.0 in stage 366.0 (TID 820) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:59:42 INFO TaskSetManager: Starting task 2.0 in stage 366.0 (TID 821) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:59:42 INFO BlockManagerInfo: Added broadcast_363_piece0 in memory on 172.18.0.15:34959 (size: 19.5 KiB, free: 434.0 MiB)
26/02/13 11:59:42 INFO BlockManagerInfo: Added broadcast_363_piece0 in memory on 172.18.0.14:40939 (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 11:59:42 INFO BlockManagerInfo: Added broadcast_362_piece0 in memory on 172.18.0.14:40939 (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 11:59:42 INFO BlockManagerInfo: Added broadcast_362_piece0 in memory on 172.18.0.15:34959 (size: 107.2 KiB, free: 433.9 MiB)
26/02/13 11:59:42 INFO TaskSetManager: Finished task 2.0 in stage 366.0 (TID 821) in 54 ms on 172.18.0.14 (executor 1) (1/3)
26/02/13 11:59:42 INFO TaskSetManager: Finished task 0.0 in stage 366.0 (TID 820) in 54 ms on 172.18.0.14 (executor 1) (2/3)
26/02/13 11:59:42 INFO TaskSetManager: Finished task 1.0 in stage 366.0 (TID 819) in 59 ms on 172.18.0.15 (executor 0) (3/3)
26/02/13 11:59:42 INFO TaskSchedulerImpl: Removed TaskSet 366.0, whose tasks have all completed, from pool 
26/02/13 11:59:42 INFO DAGScheduler: ResultStage 366 (start at NativeMethodAccessorImpl.java:0) finished in 0.064 s
26/02/13 11:59:42 INFO DAGScheduler: Job 242 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:59:42 INFO TaskSchedulerImpl: Killing all running tasks in stage 366: Stage finished
26/02/13 11:59:42 INFO DAGScheduler: Job 242 finished: start at NativeMethodAccessorImpl.java:0, took 0.065572 s
26/02/13 11:59:42 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 116, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@341f148d] is committing.
26/02/13 11:59:42 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 116, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@341f148d] committed.
26/02/13 11:59:42 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/116 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.116.282f38f8-417b-4968-bad1-a5dabd3b8df6.tmp
26/02/13 11:59:43 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.116.282f38f8-417b-4968-bad1-a5dabd3b8df6.tmp to file:/tmp/spark-checkpoint-enrichment/commits/116
26/02/13 11:59:43 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "fc5c6452-bd70-4c90-becf-772d3bebdb57",
  "name" : null,
  "timestamp" : "2026-02-13T11:59:42.625Z",
  "batchId" : 116,
  "numInputRows" : 156,
  "inputRowsPerSecond" : 162.3309053069719,
  "processedRowsPerSecond" : 403.1007751937984,
  "durationMs" : {
    "addBatch" : 195,
    "commitOffsets" : 80,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 45,
    "triggerExecution" : 387,
    "walCommit" : 65
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 7338,
        "1" : 8192,
        "0" : 9349
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 7383,
        "1" : 8254,
        "0" : 9398
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 7383,
        "1" : 8254,
        "0" : 9398
      }
    },
    "numInputRows" : 156,
    "inputRowsPerSecond" : 162.3309053069719,
    "processedRowsPerSecond" : 403.1007751937984,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@7fe9b5de",
    "numOutputRows" : 30
  }
}
26/02/13 11:59:50 INFO BlockManagerInfo: Removed broadcast_363_piece0 on spark-master:40291 in memory (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 11:59:50 INFO BlockManagerInfo: Removed broadcast_363_piece0 on 172.18.0.15:34959 in memory (size: 19.5 KiB, free: 433.9 MiB)
26/02/13 11:59:50 INFO BlockManagerInfo: Removed broadcast_363_piece0 on 172.18.0.14:40939 in memory (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 11:59:50 INFO BlockManagerInfo: Removed broadcast_359_piece0 on spark-master:40291 in memory (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 11:59:50 INFO BlockManagerInfo: Removed broadcast_359_piece0 on 172.18.0.15:34959 in memory (size: 107.2 KiB, free: 434.0 MiB)
26/02/13 11:59:50 INFO BlockManagerInfo: Removed broadcast_359_piece0 on 172.18.0.14:40939 in memory (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 11:59:50 INFO BlockManagerInfo: Removed broadcast_360_piece0 on spark-master:40291 in memory (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 11:59:50 INFO BlockManagerInfo: Removed broadcast_360_piece0 on 172.18.0.15:34959 in memory (size: 19.5 KiB, free: 434.0 MiB)
26/02/13 11:59:50 INFO BlockManagerInfo: Removed broadcast_360_piece0 on 172.18.0.14:40939 in memory (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 11:59:50 INFO BlockManagerInfo: Removed broadcast_361_piece0 on spark-master:40291 in memory (size: 33.3 KiB, free: 434.3 MiB)
26/02/13 11:59:50 INFO BlockManagerInfo: Removed broadcast_361_piece0 on 172.18.0.15:34959 in memory (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 11:59:53 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 11:59:58 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/117 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.117.980aa87a-e1d8-4ebd-b3e7-f9da5a249557.tmp
26/02/13 11:59:58 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.117.980aa87a-e1d8-4ebd-b3e7-f9da5a249557.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/117
26/02/13 11:59:58 INFO MicroBatchExecution: Committed offsets for batch 117. Metadata OffsetSeqMetadata(0,1770983998036,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 11:59:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:59:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:59:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:59:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:59:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:59:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:59:58 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#101869 - origin_code.nullCount#101868) > 0)
26/02/13 11:59:58 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#101874 - destination_code.nullCount#101873) > 0)
26/02/13 11:59:58 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#101904 - callsign.nullCount#101903) > 0)
26/02/13 11:59:58 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:59:58 INFO DAGScheduler: Got job 243 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 11:59:58 INFO DAGScheduler: Final stage: ResultStage 368 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:59:58 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 367)
26/02/13 11:59:58 INFO DAGScheduler: Missing parents: List()
26/02/13 11:59:58 INFO DAGScheduler: Submitting ResultStage 368 (MapPartitionsRDD[1330] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:59:58 INFO MemoryStore: Block broadcast_364 stored as values in memory (estimated size 99.3 KiB, free 434.0 MiB)
26/02/13 11:59:58 INFO MemoryStore: Block broadcast_364_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 433.9 MiB)
26/02/13 11:59:58 INFO BlockManagerInfo: Added broadcast_364_piece0 in memory on spark-master:40291 (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 11:59:58 INFO SparkContext: Created broadcast 364 from broadcast at DAGScheduler.scala:1585
26/02/13 11:59:58 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 368 (MapPartitionsRDD[1330] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 11:59:58 INFO TaskSchedulerImpl: Adding task set 368.0 with 4 tasks resource profile 0
26/02/13 11:59:58 INFO TaskSetManager: Starting task 0.0 in stage 368.0 (TID 822) (172.18.0.15, executor 0, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:59:58 INFO TaskSetManager: Starting task 1.0 in stage 368.0 (TID 823) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:59:58 INFO BlockManagerInfo: Added broadcast_364_piece0 in memory on 172.18.0.15:34959 (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 11:59:58 INFO TaskSetManager: Starting task 2.0 in stage 368.0 (TID 824) (172.18.0.15, executor 0, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:59:58 INFO TaskSetManager: Finished task 1.0 in stage 368.0 (TID 823) in 15 ms on 172.18.0.15 (executor 0) (1/4)
26/02/13 11:59:58 INFO TaskSetManager: Starting task 3.0 in stage 368.0 (TID 825) (172.18.0.15, executor 0, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:59:58 INFO TaskSetManager: Finished task 0.0 in stage 368.0 (TID 822) in 16 ms on 172.18.0.15 (executor 0) (2/4)
26/02/13 11:59:58 INFO TaskSetManager: Finished task 3.0 in stage 368.0 (TID 825) in 10 ms on 172.18.0.15 (executor 0) (3/4)
26/02/13 11:59:58 INFO TaskSetManager: Finished task 2.0 in stage 368.0 (TID 824) in 11 ms on 172.18.0.15 (executor 0) (4/4)
26/02/13 11:59:58 INFO TaskSchedulerImpl: Removed TaskSet 368.0, whose tasks have all completed, from pool 
26/02/13 11:59:58 INFO DAGScheduler: ResultStage 368 (start at NativeMethodAccessorImpl.java:0) finished in 0.031 s
26/02/13 11:59:58 INFO DAGScheduler: Job 243 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:59:58 INFO TaskSchedulerImpl: Killing all running tasks in stage 368: Stage finished
26/02/13 11:59:58 INFO DAGScheduler: Job 243 finished: start at NativeMethodAccessorImpl.java:0, took 0.033720 s
26/02/13 11:59:58 INFO MemoryStore: Block broadcast_365_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.8 MiB)
26/02/13 11:59:58 INFO BlockManagerInfo: Added broadcast_365_piece0 in memory on spark-master:40291 (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 11:59:58 INFO SparkContext: Created broadcast 365 from start at NativeMethodAccessorImpl.java:0
26/02/13 11:59:58 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 117, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@5b69e54a]. The input RDD has 3 partitions.
26/02/13 11:59:58 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:59:58 INFO DAGScheduler: Got job 244 (start at NativeMethodAccessorImpl.java:0) with 3 output partitions
26/02/13 11:59:58 INFO DAGScheduler: Final stage: ResultStage 369 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:59:58 INFO DAGScheduler: Parents of final stage: List()
26/02/13 11:59:58 INFO DAGScheduler: Missing parents: List()
26/02/13 11:59:58 INFO DAGScheduler: Submitting ResultStage 369 (MapPartitionsRDD[1336] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:59:58 INFO MemoryStore: Block broadcast_366 stored as values in memory (estimated size 49.9 KiB, free 433.8 MiB)
26/02/13 11:59:58 INFO MemoryStore: Block broadcast_366_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.8 MiB)
26/02/13 11:59:58 INFO BlockManagerInfo: Added broadcast_366_piece0 in memory on spark-master:40291 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 11:59:58 INFO SparkContext: Created broadcast 366 from broadcast at DAGScheduler.scala:1585
26/02/13 11:59:58 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 369 (MapPartitionsRDD[1336] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
26/02/13 11:59:58 INFO TaskSchedulerImpl: Adding task set 369.0 with 3 tasks resource profile 0
26/02/13 11:59:58 INFO TaskSetManager: Starting task 0.0 in stage 369.0 (TID 826) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:59:58 INFO TaskSetManager: Starting task 1.0 in stage 369.0 (TID 827) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:59:58 INFO TaskSetManager: Starting task 2.0 in stage 369.0 (TID 828) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:59:58 INFO BlockManagerInfo: Added broadcast_366_piece0 in memory on 172.18.0.15:34959 (size: 19.5 KiB, free: 434.0 MiB)
26/02/13 11:59:58 INFO BlockManagerInfo: Added broadcast_366_piece0 in memory on 172.18.0.14:40939 (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 11:59:58 INFO BlockManagerInfo: Added broadcast_365_piece0 in memory on 172.18.0.14:40939 (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 11:59:58 INFO BlockManagerInfo: Added broadcast_365_piece0 in memory on 172.18.0.15:34959 (size: 107.2 KiB, free: 433.9 MiB)
26/02/13 11:59:58 INFO TaskSetManager: Finished task 1.0 in stage 369.0 (TID 827) in 556 ms on 172.18.0.15 (executor 0) (1/3)
26/02/13 11:59:58 INFO TaskSetManager: Finished task 0.0 in stage 369.0 (TID 826) in 560 ms on 172.18.0.14 (executor 1) (2/3)
26/02/13 11:59:58 INFO TaskSetManager: Finished task 2.0 in stage 369.0 (TID 828) in 559 ms on 172.18.0.14 (executor 1) (3/3)
26/02/13 11:59:58 INFO TaskSchedulerImpl: Removed TaskSet 369.0, whose tasks have all completed, from pool 
26/02/13 11:59:58 INFO DAGScheduler: ResultStage 369 (start at NativeMethodAccessorImpl.java:0) finished in 0.566 s
26/02/13 11:59:58 INFO DAGScheduler: Job 244 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:59:58 INFO TaskSchedulerImpl: Killing all running tasks in stage 369: Stage finished
26/02/13 11:59:58 INFO DAGScheduler: Job 244 finished: start at NativeMethodAccessorImpl.java:0, took 0.567088 s
26/02/13 11:59:58 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 117, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@5b69e54a] is committing.
26/02/13 11:59:58 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 117, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@5b69e54a] committed.
26/02/13 11:59:58 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/117 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.117.bfcc8a1c-bf6c-40e2-bfa2-5c9e37d22e70.tmp
26/02/13 11:59:58 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.117.bfcc8a1c-bf6c-40e2-bfa2-5c9e37d22e70.tmp to file:/tmp/spark-checkpoint-enrichment/commits/117
26/02/13 11:59:58 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "fc5c6452-bd70-4c90-becf-772d3bebdb57",
  "name" : null,
  "timestamp" : "2026-02-13T11:59:58.034Z",
  "batchId" : 117,
  "numInputRows" : 47,
  "inputRowsPerSecond" : 3916.6666666666665,
  "processedRowsPerSecond" : 53.04740406320542,
  "durationMs" : {
    "addBatch" : 672,
    "commitOffsets" : 59,
    "getBatch" : 0,
    "latestOffset" : 2,
    "queryPlanning" : 48,
    "triggerExecution" : 886,
    "walCommit" : 104
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 7383,
        "1" : 8254,
        "0" : 9398
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 7396,
        "1" : 8277,
        "0" : 9409
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 7396,
        "1" : 8277,
        "0" : 9409
      }
    },
    "numInputRows" : 47,
    "inputRowsPerSecond" : 3916.6666666666665,
    "processedRowsPerSecond" : 53.04740406320542,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@7fe9b5de",
    "numOutputRows" : 10
  }
}
26/02/13 11:59:58 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/118 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.118.a98ee1bc-7e44-4c1b-bda9-3c1827042206.tmp
26/02/13 11:59:58 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.118.a98ee1bc-7e44-4c1b-bda9-3c1827042206.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/118
26/02/13 11:59:58 INFO MicroBatchExecution: Committed offsets for batch 118. Metadata OffsetSeqMetadata(0,1770983998922,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 11:59:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:59:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:59:59 INFO BlockManagerInfo: Removed broadcast_365_piece0 on spark-master:40291 in memory (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 11:59:59 INFO BlockManagerInfo: Removed broadcast_365_piece0 on 172.18.0.14:40939 in memory (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 11:59:59 INFO BlockManagerInfo: Removed broadcast_365_piece0 on 172.18.0.15:34959 in memory (size: 107.2 KiB, free: 434.0 MiB)
26/02/13 11:59:59 INFO BlockManagerInfo: Removed broadcast_364_piece0 on spark-master:40291 in memory (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 11:59:59 INFO BlockManagerInfo: Removed broadcast_364_piece0 on 172.18.0.15:34959 in memory (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 11:59:59 INFO BlockManagerInfo: Removed broadcast_362_piece0 on spark-master:40291 in memory (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 11:59:59 INFO BlockManagerInfo: Removed broadcast_362_piece0 on 172.18.0.14:40939 in memory (size: 107.2 KiB, free: 434.4 MiB)
26/02/13 11:59:59 INFO BlockManagerInfo: Removed broadcast_362_piece0 on 172.18.0.15:34959 in memory (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 11:59:59 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:59:59 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:59:59 INFO BlockManagerInfo: Removed broadcast_366_piece0 on spark-master:40291 in memory (size: 19.5 KiB, free: 434.4 MiB)
26/02/13 11:59:59 INFO BlockManagerInfo: Removed broadcast_366_piece0 on 172.18.0.15:34959 in memory (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 11:59:59 INFO BlockManagerInfo: Removed broadcast_366_piece0 on 172.18.0.14:40939 in memory (size: 19.5 KiB, free: 434.4 MiB)
26/02/13 11:59:59 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:59:59 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 11:59:59 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#102723 - origin_code.nullCount#102722) > 0)
26/02/13 11:59:59 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#102728 - destination_code.nullCount#102727) > 0)
26/02/13 11:59:59 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#102758 - callsign.nullCount#102757) > 0)
26/02/13 11:59:59 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:59:59 INFO DAGScheduler: Got job 245 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 11:59:59 INFO DAGScheduler: Final stage: ResultStage 371 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:59:59 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 370)
26/02/13 11:59:59 INFO DAGScheduler: Missing parents: List()
26/02/13 11:59:59 INFO DAGScheduler: Submitting ResultStage 371 (MapPartitionsRDD[1341] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:59:59 INFO MemoryStore: Block broadcast_367 stored as values in memory (estimated size 99.3 KiB, free 434.1 MiB)
26/02/13 11:59:59 INFO MemoryStore: Block broadcast_367_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 434.0 MiB)
26/02/13 11:59:59 INFO BlockManagerInfo: Added broadcast_367_piece0 in memory on spark-master:40291 (size: 33.3 KiB, free: 434.3 MiB)
26/02/13 11:59:59 INFO SparkContext: Created broadcast 367 from broadcast at DAGScheduler.scala:1585
26/02/13 11:59:59 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 371 (MapPartitionsRDD[1341] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 11:59:59 INFO TaskSchedulerImpl: Adding task set 371.0 with 4 tasks resource profile 0
26/02/13 11:59:59 INFO TaskSetManager: Starting task 0.0 in stage 371.0 (TID 829) (172.18.0.15, executor 0, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:59:59 INFO TaskSetManager: Starting task 1.0 in stage 371.0 (TID 830) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:59:59 INFO BlockManagerInfo: Added broadcast_367_piece0 in memory on 172.18.0.15:34959 (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 11:59:59 INFO TaskSetManager: Starting task 2.0 in stage 371.0 (TID 831) (172.18.0.15, executor 0, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:59:59 INFO TaskSetManager: Finished task 0.0 in stage 371.0 (TID 829) in 15 ms on 172.18.0.15 (executor 0) (1/4)
26/02/13 11:59:59 INFO TaskSetManager: Starting task 3.0 in stage 371.0 (TID 832) (172.18.0.15, executor 0, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 11:59:59 INFO TaskSetManager: Finished task 1.0 in stage 371.0 (TID 830) in 15 ms on 172.18.0.15 (executor 0) (2/4)
26/02/13 11:59:59 INFO TaskSetManager: Finished task 2.0 in stage 371.0 (TID 831) in 10 ms on 172.18.0.15 (executor 0) (3/4)
26/02/13 11:59:59 INFO TaskSetManager: Finished task 3.0 in stage 371.0 (TID 832) in 9 ms on 172.18.0.15 (executor 0) (4/4)
26/02/13 11:59:59 INFO TaskSchedulerImpl: Removed TaskSet 371.0, whose tasks have all completed, from pool 
26/02/13 11:59:59 INFO DAGScheduler: ResultStage 371 (start at NativeMethodAccessorImpl.java:0) finished in 0.032 s
26/02/13 11:59:59 INFO DAGScheduler: Job 245 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:59:59 INFO TaskSchedulerImpl: Killing all running tasks in stage 371: Stage finished
26/02/13 11:59:59 INFO DAGScheduler: Job 245 finished: start at NativeMethodAccessorImpl.java:0, took 0.033591 s
26/02/13 11:59:59 INFO MemoryStore: Block broadcast_368_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.9 MiB)
26/02/13 11:59:59 INFO BlockManagerInfo: Added broadcast_368_piece0 in memory on spark-master:40291 (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 11:59:59 INFO SparkContext: Created broadcast 368 from start at NativeMethodAccessorImpl.java:0
26/02/13 11:59:59 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 118, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@2bb7d1f9]. The input RDD has 3 partitions.
26/02/13 11:59:59 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 11:59:59 INFO DAGScheduler: Got job 246 (start at NativeMethodAccessorImpl.java:0) with 3 output partitions
26/02/13 11:59:59 INFO DAGScheduler: Final stage: ResultStage 372 (start at NativeMethodAccessorImpl.java:0)
26/02/13 11:59:59 INFO DAGScheduler: Parents of final stage: List()
26/02/13 11:59:59 INFO DAGScheduler: Missing parents: List()
26/02/13 11:59:59 INFO DAGScheduler: Submitting ResultStage 372 (MapPartitionsRDD[1347] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 11:59:59 INFO MemoryStore: Block broadcast_369 stored as values in memory (estimated size 49.9 KiB, free 433.9 MiB)
26/02/13 11:59:59 INFO MemoryStore: Block broadcast_369_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.9 MiB)
26/02/13 11:59:59 INFO BlockManagerInfo: Added broadcast_369_piece0 in memory on spark-master:40291 (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 11:59:59 INFO SparkContext: Created broadcast 369 from broadcast at DAGScheduler.scala:1585
26/02/13 11:59:59 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 372 (MapPartitionsRDD[1347] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
26/02/13 11:59:59 INFO TaskSchedulerImpl: Adding task set 372.0 with 3 tasks resource profile 0
26/02/13 11:59:59 INFO TaskSetManager: Starting task 0.0 in stage 372.0 (TID 833) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:59:59 INFO TaskSetManager: Starting task 1.0 in stage 372.0 (TID 834) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:59:59 INFO TaskSetManager: Starting task 2.0 in stage 372.0 (TID 835) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 8594 bytes) 
26/02/13 11:59:59 INFO BlockManagerInfo: Added broadcast_369_piece0 in memory on 172.18.0.15:34959 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 11:59:59 INFO BlockManagerInfo: Added broadcast_369_piece0 in memory on 172.18.0.14:40939 (size: 19.5 KiB, free: 434.4 MiB)
26/02/13 11:59:59 INFO BlockManagerInfo: Added broadcast_368_piece0 in memory on 172.18.0.14:40939 (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 11:59:59 INFO BlockManagerInfo: Added broadcast_368_piece0 in memory on 172.18.0.15:34959 (size: 107.2 KiB, free: 434.0 MiB)
26/02/13 11:59:59 INFO TaskSetManager: Finished task 2.0 in stage 372.0 (TID 835) in 60 ms on 172.18.0.14 (executor 1) (1/3)
26/02/13 11:59:59 INFO TaskSetManager: Finished task 0.0 in stage 372.0 (TID 833) in 61 ms on 172.18.0.14 (executor 1) (2/3)
26/02/13 11:59:59 INFO TaskSetManager: Finished task 1.0 in stage 372.0 (TID 834) in 86 ms on 172.18.0.15 (executor 0) (3/3)
26/02/13 11:59:59 INFO TaskSchedulerImpl: Removed TaskSet 372.0, whose tasks have all completed, from pool 
26/02/13 11:59:59 INFO DAGScheduler: ResultStage 372 (start at NativeMethodAccessorImpl.java:0) finished in 0.090 s
26/02/13 11:59:59 INFO DAGScheduler: Job 246 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 11:59:59 INFO TaskSchedulerImpl: Killing all running tasks in stage 372: Stage finished
26/02/13 11:59:59 INFO DAGScheduler: Job 246 finished: start at NativeMethodAccessorImpl.java:0, took 0.092456 s
26/02/13 11:59:59 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 118, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@2bb7d1f9] is committing.
26/02/13 11:59:59 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 118, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@2bb7d1f9] committed.
26/02/13 11:59:59 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/118 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.118.0a90e3a6-7660-4a48-971e-e3267dabb964.tmp
26/02/13 11:59:59 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.118.0a90e3a6-7660-4a48-971e-e3267dabb964.tmp to file:/tmp/spark-checkpoint-enrichment/commits/118
26/02/13 11:59:59 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "fc5c6452-bd70-4c90-becf-772d3bebdb57",
  "name" : null,
  "timestamp" : "2026-02-13T11:59:58.921Z",
  "batchId" : 118,
  "numInputRows" : 202,
  "inputRowsPerSecond" : 227.73393461104848,
  "processedRowsPerSecond" : 577.1428571428572,
  "durationMs" : {
    "addBatch" : 209,
    "commitOffsets" : 62,
    "getBatch" : 1,
    "latestOffset" : 1,
    "queryPlanning" : 22,
    "triggerExecution" : 350,
    "walCommit" : 55
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 7396,
        "1" : 8277,
        "0" : 9409
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 7459,
        "1" : 8352,
        "0" : 9473
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 7459,
        "1" : 8352,
        "0" : 9473
      }
    },
    "numInputRows" : 202,
    "inputRowsPerSecond" : 227.73393461104848,
    "processedRowsPerSecond" : 577.1428571428572,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@7fe9b5de",
    "numOutputRows" : 41
  }
}
26/02/13 12:00:09 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:00:14 INFO BlockManagerInfo: Removed broadcast_369_piece0 on spark-master:40291 in memory (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 12:00:14 INFO BlockManagerInfo: Removed broadcast_369_piece0 on 172.18.0.14:40939 in memory (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 12:00:14 INFO BlockManagerInfo: Removed broadcast_369_piece0 on 172.18.0.15:34959 in memory (size: 19.5 KiB, free: 434.0 MiB)
26/02/13 12:00:14 INFO BlockManagerInfo: Removed broadcast_367_piece0 on spark-master:40291 in memory (size: 33.3 KiB, free: 434.3 MiB)
26/02/13 12:00:14 INFO BlockManagerInfo: Removed broadcast_367_piece0 on 172.18.0.15:34959 in memory (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 12:00:14 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/119 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.119.f87122c2-0c49-44f9-8c14-aff3854160fc.tmp
26/02/13 12:00:14 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.119.f87122c2-0c49-44f9-8c14-aff3854160fc.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/119
26/02/13 12:00:14 INFO MicroBatchExecution: Committed offsets for batch 119. Metadata OffsetSeqMetadata(0,1770984014738,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 12:00:14 INFO BlockManagerInfo: Removed broadcast_368_piece0 on spark-master:40291 in memory (size: 107.2 KiB, free: 434.4 MiB)
26/02/13 12:00:14 INFO BlockManagerInfo: Removed broadcast_368_piece0 on 172.18.0.15:34959 in memory (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 12:00:14 INFO BlockManagerInfo: Removed broadcast_368_piece0 on 172.18.0.14:40939 in memory (size: 107.2 KiB, free: 434.4 MiB)
26/02/13 12:00:14 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:00:14 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:00:14 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:00:14 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:00:14 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:00:14 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:00:14 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#103577 - origin_code.nullCount#103576) > 0)
26/02/13 12:00:14 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#103582 - destination_code.nullCount#103581) > 0)
26/02/13 12:00:14 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#103612 - callsign.nullCount#103611) > 0)
26/02/13 12:00:14 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 12:00:14 INFO DAGScheduler: Got job 247 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 12:00:14 INFO DAGScheduler: Final stage: ResultStage 374 (start at NativeMethodAccessorImpl.java:0)
26/02/13 12:00:14 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 373)
26/02/13 12:00:14 INFO DAGScheduler: Missing parents: List()
26/02/13 12:00:14 INFO DAGScheduler: Submitting ResultStage 374 (MapPartitionsRDD[1352] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:00:14 INFO MemoryStore: Block broadcast_370 stored as values in memory (estimated size 99.3 KiB, free 434.1 MiB)
26/02/13 12:00:14 INFO MemoryStore: Block broadcast_370_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 434.0 MiB)
26/02/13 12:00:14 INFO BlockManagerInfo: Added broadcast_370_piece0 in memory on spark-master:40291 (size: 33.3 KiB, free: 434.3 MiB)
26/02/13 12:00:14 INFO SparkContext: Created broadcast 370 from broadcast at DAGScheduler.scala:1585
26/02/13 12:00:14 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 374 (MapPartitionsRDD[1352] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 12:00:14 INFO TaskSchedulerImpl: Adding task set 374.0 with 4 tasks resource profile 0
26/02/13 12:00:14 INFO TaskSetManager: Starting task 0.0 in stage 374.0 (TID 836) (172.18.0.15, executor 0, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:00:14 INFO TaskSetManager: Starting task 1.0 in stage 374.0 (TID 837) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:00:14 INFO BlockManagerInfo: Added broadcast_370_piece0 in memory on 172.18.0.15:34959 (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 12:00:15 INFO TaskSetManager: Starting task 2.0 in stage 374.0 (TID 838) (172.18.0.15, executor 0, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:00:15 INFO TaskSetManager: Starting task 3.0 in stage 374.0 (TID 839) (172.18.0.15, executor 0, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:00:15 INFO TaskSetManager: Finished task 0.0 in stage 374.0 (TID 836) in 38 ms on 172.18.0.15 (executor 0) (1/4)
26/02/13 12:00:15 INFO TaskSetManager: Finished task 1.0 in stage 374.0 (TID 837) in 39 ms on 172.18.0.15 (executor 0) (2/4)
26/02/13 12:00:15 INFO TaskSetManager: Finished task 3.0 in stage 374.0 (TID 839) in 16 ms on 172.18.0.15 (executor 0) (3/4)
26/02/13 12:00:15 INFO TaskSetManager: Finished task 2.0 in stage 374.0 (TID 838) in 21 ms on 172.18.0.15 (executor 0) (4/4)
26/02/13 12:00:15 INFO TaskSchedulerImpl: Removed TaskSet 374.0, whose tasks have all completed, from pool 
26/02/13 12:00:15 INFO DAGScheduler: ResultStage 374 (start at NativeMethodAccessorImpl.java:0) finished in 0.065 s
26/02/13 12:00:15 INFO DAGScheduler: Job 247 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 12:00:15 INFO TaskSchedulerImpl: Killing all running tasks in stage 374: Stage finished
26/02/13 12:00:15 INFO DAGScheduler: Job 247 finished: start at NativeMethodAccessorImpl.java:0, took 0.069148 s
26/02/13 12:00:15 INFO MemoryStore: Block broadcast_371_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.9 MiB)
26/02/13 12:00:15 INFO BlockManagerInfo: Added broadcast_371_piece0 in memory on spark-master:40291 (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 12:00:15 INFO SparkContext: Created broadcast 371 from start at NativeMethodAccessorImpl.java:0
26/02/13 12:00:15 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 119, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@1c6524e2]. The input RDD has 3 partitions.
26/02/13 12:00:15 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 12:00:15 INFO DAGScheduler: Got job 248 (start at NativeMethodAccessorImpl.java:0) with 3 output partitions
26/02/13 12:00:15 INFO DAGScheduler: Final stage: ResultStage 375 (start at NativeMethodAccessorImpl.java:0)
26/02/13 12:00:15 INFO DAGScheduler: Parents of final stage: List()
26/02/13 12:00:15 INFO DAGScheduler: Missing parents: List()
26/02/13 12:00:15 INFO DAGScheduler: Submitting ResultStage 375 (MapPartitionsRDD[1358] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:00:15 INFO MemoryStore: Block broadcast_372 stored as values in memory (estimated size 49.9 KiB, free 433.9 MiB)
26/02/13 12:00:15 INFO MemoryStore: Block broadcast_372_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.9 MiB)
26/02/13 12:00:15 INFO BlockManagerInfo: Added broadcast_372_piece0 in memory on spark-master:40291 (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 12:00:15 INFO SparkContext: Created broadcast 372 from broadcast at DAGScheduler.scala:1585
26/02/13 12:00:15 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 375 (MapPartitionsRDD[1358] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
26/02/13 12:00:15 INFO TaskSchedulerImpl: Adding task set 375.0 with 3 tasks resource profile 0
26/02/13 12:00:15 INFO TaskSetManager: Starting task 0.0 in stage 375.0 (TID 840) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:00:15 INFO TaskSetManager: Starting task 1.0 in stage 375.0 (TID 841) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:00:15 INFO TaskSetManager: Starting task 2.0 in stage 375.0 (TID 842) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:00:15 INFO BlockManagerInfo: Added broadcast_372_piece0 in memory on 172.18.0.15:34959 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 12:00:15 INFO BlockManagerInfo: Added broadcast_372_piece0 in memory on 172.18.0.14:40939 (size: 19.5 KiB, free: 434.4 MiB)
26/02/13 12:00:15 INFO BlockManagerInfo: Added broadcast_371_piece0 in memory on 172.18.0.15:34959 (size: 107.2 KiB, free: 434.0 MiB)
26/02/13 12:00:15 INFO BlockManagerInfo: Added broadcast_371_piece0 in memory on 172.18.0.14:40939 (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 12:00:15 INFO TaskSetManager: Finished task 1.0 in stage 375.0 (TID 841) in 602 ms on 172.18.0.15 (executor 0) (1/3)
26/02/13 12:00:15 INFO TaskSetManager: Finished task 0.0 in stage 375.0 (TID 840) in 635 ms on 172.18.0.14 (executor 1) (2/3)
26/02/13 12:00:15 INFO TaskSetManager: Finished task 2.0 in stage 375.0 (TID 842) in 634 ms on 172.18.0.14 (executor 1) (3/3)
26/02/13 12:00:15 INFO TaskSchedulerImpl: Removed TaskSet 375.0, whose tasks have all completed, from pool 
26/02/13 12:00:15 INFO DAGScheduler: ResultStage 375 (start at NativeMethodAccessorImpl.java:0) finished in 0.648 s
26/02/13 12:00:15 INFO DAGScheduler: Job 248 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 12:00:15 INFO TaskSchedulerImpl: Killing all running tasks in stage 375: Stage finished
26/02/13 12:00:15 INFO DAGScheduler: Job 248 finished: start at NativeMethodAccessorImpl.java:0, took 0.653059 s
26/02/13 12:00:15 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 119, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@1c6524e2] is committing.
26/02/13 12:00:15 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 119, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@1c6524e2] committed.
26/02/13 12:00:15 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/119 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.119.a7e0e7c7-12c4-4c18-b010-b1357cd78d39.tmp
26/02/13 12:00:15 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.119.a7e0e7c7-12c4-4c18-b010-b1357cd78d39.tmp to file:/tmp/spark-checkpoint-enrichment/commits/119
26/02/13 12:00:15 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "fc5c6452-bd70-4c90-becf-772d3bebdb57",
  "name" : null,
  "timestamp" : "2026-02-13T12:00:14.737Z",
  "batchId" : 119,
  "numInputRows" : 73,
  "inputRowsPerSecond" : 6083.333333333333,
  "processedRowsPerSecond" : 67.03397612488521,
  "durationMs" : {
    "addBatch" : 850,
    "commitOffsets" : 103,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 51,
    "triggerExecution" : 1089,
    "walCommit" : 83
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 7459,
        "1" : 8352,
        "0" : 9473
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 7480,
        "1" : 8383,
        "0" : 9494
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 7480,
        "1" : 8383,
        "0" : 9494
      }
    },
    "numInputRows" : 73,
    "inputRowsPerSecond" : 6083.333333333333,
    "processedRowsPerSecond" : 67.03397612488521,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@7fe9b5de",
    "numOutputRows" : 18
  }
}
26/02/13 12:00:15 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/120 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.120.5adae317-9d94-427b-a757-2ed839e09e5a.tmp
26/02/13 12:00:15 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.120.5adae317-9d94-427b-a757-2ed839e09e5a.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/120
26/02/13 12:00:15 INFO MicroBatchExecution: Committed offsets for batch 120. Metadata OffsetSeqMetadata(0,1770984015829,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 12:00:15 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:00:15 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:00:15 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:00:15 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:00:16 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:00:16 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:00:16 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#104431 - origin_code.nullCount#104430) > 0)
26/02/13 12:00:16 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#104436 - destination_code.nullCount#104435) > 0)
26/02/13 12:00:16 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#104466 - callsign.nullCount#104465) > 0)
26/02/13 12:00:16 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 12:00:16 INFO DAGScheduler: Got job 249 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 12:00:16 INFO DAGScheduler: Final stage: ResultStage 377 (start at NativeMethodAccessorImpl.java:0)
26/02/13 12:00:16 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 376)
26/02/13 12:00:16 INFO DAGScheduler: Missing parents: List()
26/02/13 12:00:16 INFO DAGScheduler: Submitting ResultStage 377 (MapPartitionsRDD[1363] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:00:16 INFO MemoryStore: Block broadcast_373 stored as values in memory (estimated size 99.3 KiB, free 433.8 MiB)
26/02/13 12:00:16 INFO MemoryStore: Block broadcast_373_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 433.7 MiB)
26/02/13 12:00:16 INFO BlockManagerInfo: Removed broadcast_372_piece0 on spark-master:40291 in memory (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 12:00:16 INFO BlockManagerInfo: Added broadcast_373_piece0 in memory on spark-master:40291 (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 12:00:16 INFO SparkContext: Created broadcast 373 from broadcast at DAGScheduler.scala:1585
26/02/13 12:00:16 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 377 (MapPartitionsRDD[1363] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 12:00:16 INFO TaskSchedulerImpl: Adding task set 377.0 with 4 tasks resource profile 0
26/02/13 12:00:16 INFO BlockManagerInfo: Removed broadcast_372_piece0 on 172.18.0.14:40939 in memory (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 12:00:16 INFO BlockManagerInfo: Removed broadcast_372_piece0 on 172.18.0.15:34959 in memory (size: 19.5 KiB, free: 434.0 MiB)
26/02/13 12:00:16 INFO TaskSetManager: Starting task 0.0 in stage 377.0 (TID 843) (172.18.0.15, executor 0, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:00:16 INFO TaskSetManager: Starting task 1.0 in stage 377.0 (TID 844) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:00:16 INFO BlockManagerInfo: Removed broadcast_371_piece0 on spark-master:40291 in memory (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 12:00:16 INFO BlockManagerInfo: Removed broadcast_371_piece0 on 172.18.0.15:34959 in memory (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 12:00:16 INFO BlockManagerInfo: Removed broadcast_371_piece0 on 172.18.0.14:40939 in memory (size: 107.2 KiB, free: 434.4 MiB)
26/02/13 12:00:16 INFO BlockManagerInfo: Added broadcast_373_piece0 in memory on 172.18.0.15:34959 (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 12:00:16 INFO BlockManagerInfo: Removed broadcast_370_piece0 on spark-master:40291 in memory (size: 33.3 KiB, free: 434.3 MiB)
26/02/13 12:00:16 INFO BlockManagerInfo: Removed broadcast_370_piece0 on 172.18.0.15:34959 in memory (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 12:00:16 INFO TaskSetManager: Starting task 2.0 in stage 377.0 (TID 845) (172.18.0.15, executor 0, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:00:16 INFO TaskSetManager: Starting task 3.0 in stage 377.0 (TID 846) (172.18.0.15, executor 0, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:00:16 INFO TaskSetManager: Finished task 0.0 in stage 377.0 (TID 843) in 34 ms on 172.18.0.15 (executor 0) (1/4)
26/02/13 12:00:16 INFO TaskSetManager: Finished task 1.0 in stage 377.0 (TID 844) in 35 ms on 172.18.0.15 (executor 0) (2/4)
26/02/13 12:00:16 INFO TaskSetManager: Finished task 3.0 in stage 377.0 (TID 846) in 19 ms on 172.18.0.15 (executor 0) (3/4)
26/02/13 12:00:16 INFO TaskSetManager: Finished task 2.0 in stage 377.0 (TID 845) in 21 ms on 172.18.0.15 (executor 0) (4/4)
26/02/13 12:00:16 INFO DAGScheduler: ResultStage 377 (start at NativeMethodAccessorImpl.java:0) finished in 0.068 s
26/02/13 12:00:16 INFO DAGScheduler: Job 249 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 12:00:16 INFO TaskSchedulerImpl: Removed TaskSet 377.0, whose tasks have all completed, from pool 
26/02/13 12:00:16 INFO TaskSchedulerImpl: Killing all running tasks in stage 377: Stage finished
26/02/13 12:00:16 INFO DAGScheduler: Job 249 finished: start at NativeMethodAccessorImpl.java:0, took 0.070774 s
26/02/13 12:00:16 INFO BlockManagerInfo: Removed broadcast_373_piece0 on spark-master:40291 in memory (size: 33.3 KiB, free: 434.4 MiB)
26/02/13 12:00:16 INFO BlockManagerInfo: Removed broadcast_373_piece0 on 172.18.0.15:34959 in memory (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 12:00:16 INFO MemoryStore: Block broadcast_374_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 434.1 MiB)
26/02/13 12:00:16 INFO BlockManagerInfo: Added broadcast_374_piece0 in memory on spark-master:40291 (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 12:00:16 INFO SparkContext: Created broadcast 374 from start at NativeMethodAccessorImpl.java:0
26/02/13 12:00:16 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 120, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@6cdf1d77]. The input RDD has 3 partitions.
26/02/13 12:00:16 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 12:00:16 INFO DAGScheduler: Got job 250 (start at NativeMethodAccessorImpl.java:0) with 3 output partitions
26/02/13 12:00:16 INFO DAGScheduler: Final stage: ResultStage 378 (start at NativeMethodAccessorImpl.java:0)
26/02/13 12:00:16 INFO DAGScheduler: Parents of final stage: List()
26/02/13 12:00:16 INFO DAGScheduler: Missing parents: List()
26/02/13 12:00:16 INFO DAGScheduler: Submitting ResultStage 378 (MapPartitionsRDD[1369] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:00:16 INFO MemoryStore: Block broadcast_375 stored as values in memory (estimated size 49.9 KiB, free 434.0 MiB)
26/02/13 12:00:16 INFO MemoryStore: Block broadcast_375_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 434.0 MiB)
26/02/13 12:00:16 INFO BlockManagerInfo: Added broadcast_375_piece0 in memory on spark-master:40291 (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 12:00:16 INFO SparkContext: Created broadcast 375 from broadcast at DAGScheduler.scala:1585
26/02/13 12:00:16 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 378 (MapPartitionsRDD[1369] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
26/02/13 12:00:16 INFO TaskSchedulerImpl: Adding task set 378.0 with 3 tasks resource profile 0
26/02/13 12:00:16 INFO TaskSetManager: Starting task 1.0 in stage 378.0 (TID 847) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:00:16 INFO TaskSetManager: Starting task 0.0 in stage 378.0 (TID 848) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:00:16 INFO TaskSetManager: Starting task 2.0 in stage 378.0 (TID 849) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:00:16 INFO BlockManagerInfo: Added broadcast_375_piece0 in memory on 172.18.0.15:34959 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 12:00:16 INFO BlockManagerInfo: Added broadcast_375_piece0 in memory on 172.18.0.14:40939 (size: 19.5 KiB, free: 434.4 MiB)
26/02/13 12:00:16 INFO BlockManagerInfo: Added broadcast_374_piece0 in memory on 172.18.0.15:34959 (size: 107.2 KiB, free: 434.0 MiB)
26/02/13 12:00:16 INFO BlockManagerInfo: Added broadcast_374_piece0 in memory on 172.18.0.14:40939 (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 12:00:16 INFO TaskSetManager: Finished task 1.0 in stage 378.0 (TID 847) in 106 ms on 172.18.0.15 (executor 0) (1/3)
26/02/13 12:00:16 INFO TaskSetManager: Finished task 2.0 in stage 378.0 (TID 849) in 135 ms on 172.18.0.14 (executor 1) (2/3)
26/02/13 12:00:16 INFO TaskSetManager: Finished task 0.0 in stage 378.0 (TID 848) in 137 ms on 172.18.0.14 (executor 1) (3/3)
26/02/13 12:00:16 INFO TaskSchedulerImpl: Removed TaskSet 378.0, whose tasks have all completed, from pool 
26/02/13 12:00:16 INFO DAGScheduler: ResultStage 378 (start at NativeMethodAccessorImpl.java:0) finished in 0.146 s
26/02/13 12:00:16 INFO DAGScheduler: Job 250 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 12:00:16 INFO TaskSchedulerImpl: Killing all running tasks in stage 378: Stage finished
26/02/13 12:00:16 INFO DAGScheduler: Job 250 finished: start at NativeMethodAccessorImpl.java:0, took 0.150456 s
26/02/13 12:00:16 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 120, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@6cdf1d77] is committing.
26/02/13 12:00:16 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 120, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@6cdf1d77] committed.
26/02/13 12:00:16 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/120 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.120.0f572344-1bbc-4bd6-8f7d-dd45038c82fa.tmp
26/02/13 12:00:16 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.120.0f572344-1bbc-4bd6-8f7d-dd45038c82fa.tmp to file:/tmp/spark-checkpoint-enrichment/commits/120
26/02/13 12:00:16 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "fc5c6452-bd70-4c90-becf-772d3bebdb57",
  "name" : null,
  "timestamp" : "2026-02-13T12:00:15.827Z",
  "batchId" : 120,
  "numInputRows" : 174,
  "inputRowsPerSecond" : 159.63302752293578,
  "processedRowsPerSecond" : 271.875,
  "durationMs" : {
    "addBatch" : 365,
    "commitOffsets" : 145,
    "getBatch" : 0,
    "latestOffset" : 2,
    "queryPlanning" : 33,
    "triggerExecution" : 640,
    "walCommit" : 93
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 7480,
        "1" : 8383,
        "0" : 9494
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 7534,
        "1" : 8450,
        "0" : 9547
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 7534,
        "1" : 8450,
        "0" : 9547
      }
    },
    "numInputRows" : 174,
    "inputRowsPerSecond" : 159.63302752293578,
    "processedRowsPerSecond" : 271.875,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@7fe9b5de",
    "numOutputRows" : 34
  }
}
26/02/13 12:00:26 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:00:31 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/121 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.121.f8f318a2-abb7-47c7-8539-ce69ec7e5bce.tmp
26/02/13 12:00:31 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.121.f8f318a2-abb7-47c7-8539-ce69ec7e5bce.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/121
26/02/13 12:00:31 INFO MicroBatchExecution: Committed offsets for batch 121. Metadata OffsetSeqMetadata(0,1770984031108,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 12:00:31 INFO BlockManagerInfo: Removed broadcast_374_piece0 on spark-master:40291 in memory (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 12:00:31 INFO BlockManagerInfo: Removed broadcast_374_piece0 on 172.18.0.14:40939 in memory (size: 107.2 KiB, free: 434.4 MiB)
26/02/13 12:00:31 INFO BlockManagerInfo: Removed broadcast_374_piece0 on 172.18.0.15:34959 in memory (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 12:00:31 INFO BlockManagerInfo: Removed broadcast_375_piece0 on spark-master:40291 in memory (size: 19.5 KiB, free: 434.4 MiB)
26/02/13 12:00:31 INFO BlockManagerInfo: Removed broadcast_375_piece0 on 172.18.0.14:40939 in memory (size: 19.5 KiB, free: 434.4 MiB)
26/02/13 12:00:31 INFO BlockManagerInfo: Removed broadcast_375_piece0 on 172.18.0.15:34959 in memory (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 12:00:31 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:00:31 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:00:31 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:00:31 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:00:31 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:00:31 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:00:31 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#105285 - origin_code.nullCount#105284) > 0)
26/02/13 12:00:31 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#105290 - destination_code.nullCount#105289) > 0)
26/02/13 12:00:31 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#105320 - callsign.nullCount#105319) > 0)
26/02/13 12:00:31 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 12:00:31 INFO DAGScheduler: Got job 251 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 12:00:31 INFO DAGScheduler: Final stage: ResultStage 380 (start at NativeMethodAccessorImpl.java:0)
26/02/13 12:00:31 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 379)
26/02/13 12:00:31 INFO DAGScheduler: Missing parents: List()
26/02/13 12:00:31 INFO DAGScheduler: Submitting ResultStage 380 (MapPartitionsRDD[1374] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:00:31 INFO MemoryStore: Block broadcast_376 stored as values in memory (estimated size 99.3 KiB, free 434.1 MiB)
26/02/13 12:00:31 INFO MemoryStore: Block broadcast_376_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 434.0 MiB)
26/02/13 12:00:31 INFO BlockManagerInfo: Added broadcast_376_piece0 in memory on spark-master:40291 (size: 33.3 KiB, free: 434.3 MiB)
26/02/13 12:00:31 INFO SparkContext: Created broadcast 376 from broadcast at DAGScheduler.scala:1585
26/02/13 12:00:31 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 380 (MapPartitionsRDD[1374] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 12:00:31 INFO TaskSchedulerImpl: Adding task set 380.0 with 4 tasks resource profile 0
26/02/13 12:00:31 INFO TaskSetManager: Starting task 0.0 in stage 380.0 (TID 850) (172.18.0.15, executor 0, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:00:31 INFO TaskSetManager: Starting task 1.0 in stage 380.0 (TID 851) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:00:31 INFO BlockManagerInfo: Added broadcast_376_piece0 in memory on 172.18.0.15:34959 (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 12:00:31 INFO TaskSetManager: Starting task 2.0 in stage 380.0 (TID 852) (172.18.0.15, executor 0, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:00:31 INFO TaskSetManager: Finished task 1.0 in stage 380.0 (TID 851) in 39 ms on 172.18.0.15 (executor 0) (1/4)
26/02/13 12:00:31 INFO TaskSetManager: Starting task 3.0 in stage 380.0 (TID 853) (172.18.0.15, executor 0, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:00:31 INFO TaskSetManager: Finished task 0.0 in stage 380.0 (TID 850) in 43 ms on 172.18.0.15 (executor 0) (2/4)
26/02/13 12:00:31 INFO TaskSetManager: Finished task 2.0 in stage 380.0 (TID 852) in 19 ms on 172.18.0.15 (executor 0) (3/4)
26/02/13 12:00:31 INFO TaskSetManager: Finished task 3.0 in stage 380.0 (TID 853) in 20 ms on 172.18.0.15 (executor 0) (4/4)
26/02/13 12:00:31 INFO TaskSchedulerImpl: Removed TaskSet 380.0, whose tasks have all completed, from pool 
26/02/13 12:00:31 INFO DAGScheduler: ResultStage 380 (start at NativeMethodAccessorImpl.java:0) finished in 0.073 s
26/02/13 12:00:31 INFO DAGScheduler: Job 251 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 12:00:31 INFO TaskSchedulerImpl: Killing all running tasks in stage 380: Stage finished
26/02/13 12:00:31 INFO DAGScheduler: Job 251 finished: start at NativeMethodAccessorImpl.java:0, took 0.074390 s
26/02/13 12:00:31 INFO MemoryStore: Block broadcast_377_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.9 MiB)
26/02/13 12:00:31 INFO BlockManagerInfo: Added broadcast_377_piece0 in memory on spark-master:40291 (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 12:00:31 INFO SparkContext: Created broadcast 377 from start at NativeMethodAccessorImpl.java:0
26/02/13 12:00:31 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 121, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@785ff948]. The input RDD has 3 partitions.
26/02/13 12:00:31 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 12:00:31 INFO DAGScheduler: Got job 252 (start at NativeMethodAccessorImpl.java:0) with 3 output partitions
26/02/13 12:00:31 INFO DAGScheduler: Final stage: ResultStage 381 (start at NativeMethodAccessorImpl.java:0)
26/02/13 12:00:31 INFO DAGScheduler: Parents of final stage: List()
26/02/13 12:00:31 INFO DAGScheduler: Missing parents: List()
26/02/13 12:00:31 INFO DAGScheduler: Submitting ResultStage 381 (MapPartitionsRDD[1380] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:00:31 INFO MemoryStore: Block broadcast_378 stored as values in memory (estimated size 49.9 KiB, free 433.9 MiB)
26/02/13 12:00:31 INFO MemoryStore: Block broadcast_378_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.9 MiB)
26/02/13 12:00:31 INFO BlockManagerInfo: Added broadcast_378_piece0 in memory on spark-master:40291 (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 12:00:31 INFO SparkContext: Created broadcast 378 from broadcast at DAGScheduler.scala:1585
26/02/13 12:00:31 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 381 (MapPartitionsRDD[1380] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
26/02/13 12:00:31 INFO TaskSchedulerImpl: Adding task set 381.0 with 3 tasks resource profile 0
26/02/13 12:00:31 INFO TaskSetManager: Starting task 1.0 in stage 381.0 (TID 854) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:00:31 INFO TaskSetManager: Starting task 0.0 in stage 381.0 (TID 855) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:00:31 INFO TaskSetManager: Starting task 2.0 in stage 381.0 (TID 856) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:00:31 INFO BlockManagerInfo: Added broadcast_378_piece0 in memory on 172.18.0.15:34959 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 12:00:31 INFO BlockManagerInfo: Added broadcast_378_piece0 in memory on 172.18.0.14:40939 (size: 19.5 KiB, free: 434.4 MiB)
26/02/13 12:00:31 INFO BlockManagerInfo: Added broadcast_377_piece0 in memory on 172.18.0.15:34959 (size: 107.2 KiB, free: 434.0 MiB)
26/02/13 12:00:31 INFO BlockManagerInfo: Added broadcast_377_piece0 in memory on 172.18.0.14:40939 (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 12:00:32 INFO TaskSetManager: Finished task 0.0 in stage 381.0 (TID 855) in 565 ms on 172.18.0.14 (executor 1) (1/3)
26/02/13 12:00:32 INFO TaskSetManager: Finished task 2.0 in stage 381.0 (TID 856) in 567 ms on 172.18.0.14 (executor 1) (2/3)
26/02/13 12:00:32 INFO TaskSetManager: Finished task 1.0 in stage 381.0 (TID 854) in 610 ms on 172.18.0.15 (executor 0) (3/3)
26/02/13 12:00:32 INFO TaskSchedulerImpl: Removed TaskSet 381.0, whose tasks have all completed, from pool 
26/02/13 12:00:32 INFO DAGScheduler: ResultStage 381 (start at NativeMethodAccessorImpl.java:0) finished in 0.618 s
26/02/13 12:00:32 INFO DAGScheduler: Job 252 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 12:00:32 INFO TaskSchedulerImpl: Killing all running tasks in stage 381: Stage finished
26/02/13 12:00:32 INFO DAGScheduler: Job 252 finished: start at NativeMethodAccessorImpl.java:0, took 0.621684 s
26/02/13 12:00:32 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 121, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@785ff948] is committing.
26/02/13 12:00:32 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 121, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@785ff948] committed.
26/02/13 12:00:32 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/121 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.121.3e79f5bc-3bee-4d28-8d33-cfcb7d1e98d8.tmp
26/02/13 12:00:32 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.121.3e79f5bc-3bee-4d28-8d33-cfcb7d1e98d8.tmp to file:/tmp/spark-checkpoint-enrichment/commits/121
26/02/13 12:00:32 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "fc5c6452-bd70-4c90-becf-772d3bebdb57",
  "name" : null,
  "timestamp" : "2026-02-13T12:00:31.098Z",
  "batchId" : 121,
  "numInputRows" : 11,
  "inputRowsPerSecond" : 916.6666666666666,
  "processedRowsPerSecond" : 10.101010101010102,
  "durationMs" : {
    "addBatch" : 848,
    "commitOffsets" : 108,
    "getBatch" : 0,
    "latestOffset" : 10,
    "queryPlanning" : 49,
    "triggerExecution" : 1089,
    "walCommit" : 71
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 7534,
        "1" : 8450,
        "0" : 9547
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 7535,
        "1" : 8457,
        "0" : 9550
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 7535,
        "1" : 8457,
        "0" : 9550
      }
    },
    "numInputRows" : 11,
    "inputRowsPerSecond" : 916.6666666666666,
    "processedRowsPerSecond" : 10.101010101010102,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@7fe9b5de",
    "numOutputRows" : 1
  }
}
26/02/13 12:00:32 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/122 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.122.fbc4ed80-edf8-4ffa-959d-a9bd8abe4f8e.tmp
26/02/13 12:00:32 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.122.fbc4ed80-edf8-4ffa-959d-a9bd8abe4f8e.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/122
26/02/13 12:00:32 INFO MicroBatchExecution: Committed offsets for batch 122. Metadata OffsetSeqMetadata(0,1770984032191,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 12:00:32 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:00:32 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:00:32 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:00:32 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:00:32 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:00:32 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:00:32 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#106139 - origin_code.nullCount#106138) > 0)
26/02/13 12:00:32 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#106144 - destination_code.nullCount#106143) > 0)
26/02/13 12:00:32 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#106174 - callsign.nullCount#106173) > 0)
26/02/13 12:00:32 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 12:00:32 INFO DAGScheduler: Got job 253 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 12:00:32 INFO DAGScheduler: Final stage: ResultStage 383 (start at NativeMethodAccessorImpl.java:0)
26/02/13 12:00:32 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 382)
26/02/13 12:00:32 INFO DAGScheduler: Missing parents: List()
26/02/13 12:00:32 INFO DAGScheduler: Submitting ResultStage 383 (MapPartitionsRDD[1385] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:00:32 INFO MemoryStore: Block broadcast_379 stored as values in memory (estimated size 99.3 KiB, free 433.8 MiB)
26/02/13 12:00:32 INFO MemoryStore: Block broadcast_379_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 433.7 MiB)
26/02/13 12:00:32 INFO BlockManagerInfo: Removed broadcast_376_piece0 on spark-master:40291 in memory (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 12:00:32 INFO BlockManagerInfo: Added broadcast_379_piece0 in memory on spark-master:40291 (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 12:00:32 INFO BlockManagerInfo: Removed broadcast_376_piece0 on 172.18.0.15:34959 in memory (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 12:00:32 INFO SparkContext: Created broadcast 379 from broadcast at DAGScheduler.scala:1585
26/02/13 12:00:32 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 383 (MapPartitionsRDD[1385] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 12:00:32 INFO TaskSchedulerImpl: Adding task set 383.0 with 4 tasks resource profile 0
26/02/13 12:00:32 INFO TaskSetManager: Starting task 0.0 in stage 383.0 (TID 857) (172.18.0.15, executor 0, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:00:32 INFO TaskSetManager: Starting task 1.0 in stage 383.0 (TID 858) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:00:32 INFO BlockManagerInfo: Removed broadcast_377_piece0 on spark-master:40291 in memory (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 12:00:32 INFO BlockManagerInfo: Removed broadcast_377_piece0 on 172.18.0.14:40939 in memory (size: 107.2 KiB, free: 434.4 MiB)
26/02/13 12:00:32 INFO BlockManagerInfo: Removed broadcast_377_piece0 on 172.18.0.15:34959 in memory (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 12:00:32 INFO BlockManagerInfo: Removed broadcast_378_piece0 on spark-master:40291 in memory (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 12:00:32 INFO BlockManagerInfo: Added broadcast_379_piece0 in memory on 172.18.0.15:34959 (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 12:00:32 INFO BlockManagerInfo: Removed broadcast_378_piece0 on 172.18.0.15:34959 in memory (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 12:00:32 INFO BlockManagerInfo: Removed broadcast_378_piece0 on 172.18.0.14:40939 in memory (size: 19.5 KiB, free: 434.4 MiB)
26/02/13 12:00:32 INFO TaskSetManager: Starting task 2.0 in stage 383.0 (TID 859) (172.18.0.15, executor 0, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:00:32 INFO TaskSetManager: Starting task 3.0 in stage 383.0 (TID 860) (172.18.0.15, executor 0, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:00:32 INFO TaskSetManager: Finished task 1.0 in stage 383.0 (TID 858) in 48 ms on 172.18.0.15 (executor 0) (1/4)
26/02/13 12:00:32 INFO TaskSetManager: Finished task 0.0 in stage 383.0 (TID 857) in 49 ms on 172.18.0.15 (executor 0) (2/4)
26/02/13 12:00:32 INFO TaskSetManager: Finished task 2.0 in stage 383.0 (TID 859) in 33 ms on 172.18.0.15 (executor 0) (3/4)
26/02/13 12:00:32 INFO TaskSetManager: Finished task 3.0 in stage 383.0 (TID 860) in 33 ms on 172.18.0.15 (executor 0) (4/4)
26/02/13 12:00:32 INFO TaskSchedulerImpl: Removed TaskSet 383.0, whose tasks have all completed, from pool 
26/02/13 12:00:32 INFO DAGScheduler: ResultStage 383 (start at NativeMethodAccessorImpl.java:0) finished in 0.102 s
26/02/13 12:00:32 INFO DAGScheduler: Job 253 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 12:00:32 INFO TaskSchedulerImpl: Killing all running tasks in stage 383: Stage finished
26/02/13 12:00:32 INFO DAGScheduler: Job 253 finished: start at NativeMethodAccessorImpl.java:0, took 0.108547 s
26/02/13 12:00:32 INFO BlockManagerInfo: Removed broadcast_379_piece0 on spark-master:40291 in memory (size: 33.3 KiB, free: 434.4 MiB)
26/02/13 12:00:32 INFO BlockManagerInfo: Removed broadcast_379_piece0 on 172.18.0.15:34959 in memory (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 12:00:32 INFO MemoryStore: Block broadcast_380_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 434.1 MiB)
26/02/13 12:00:32 INFO BlockManagerInfo: Added broadcast_380_piece0 in memory on spark-master:40291 (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 12:00:32 INFO SparkContext: Created broadcast 380 from start at NativeMethodAccessorImpl.java:0
26/02/13 12:00:32 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 122, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@11a3939d]. The input RDD has 3 partitions.
26/02/13 12:00:32 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 12:00:32 INFO DAGScheduler: Got job 254 (start at NativeMethodAccessorImpl.java:0) with 3 output partitions
26/02/13 12:00:32 INFO DAGScheduler: Final stage: ResultStage 384 (start at NativeMethodAccessorImpl.java:0)
26/02/13 12:00:32 INFO DAGScheduler: Parents of final stage: List()
26/02/13 12:00:32 INFO DAGScheduler: Missing parents: List()
26/02/13 12:00:32 INFO DAGScheduler: Submitting ResultStage 384 (MapPartitionsRDD[1391] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:00:32 INFO MemoryStore: Block broadcast_381 stored as values in memory (estimated size 49.9 KiB, free 434.0 MiB)
26/02/13 12:00:32 INFO MemoryStore: Block broadcast_381_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 434.0 MiB)
26/02/13 12:00:32 INFO BlockManagerInfo: Added broadcast_381_piece0 in memory on spark-master:40291 (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 12:00:32 INFO SparkContext: Created broadcast 381 from broadcast at DAGScheduler.scala:1585
26/02/13 12:00:32 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 384 (MapPartitionsRDD[1391] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
26/02/13 12:00:32 INFO TaskSchedulerImpl: Adding task set 384.0 with 3 tasks resource profile 0
26/02/13 12:00:32 INFO TaskSetManager: Starting task 0.0 in stage 384.0 (TID 861) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:00:32 INFO TaskSetManager: Starting task 1.0 in stage 384.0 (TID 862) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:00:32 INFO TaskSetManager: Starting task 2.0 in stage 384.0 (TID 863) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:00:32 INFO BlockManagerInfo: Added broadcast_381_piece0 in memory on 172.18.0.15:34959 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 12:00:32 INFO BlockManagerInfo: Added broadcast_381_piece0 in memory on 172.18.0.14:40939 (size: 19.5 KiB, free: 434.4 MiB)
26/02/13 12:00:32 INFO BlockManagerInfo: Added broadcast_380_piece0 in memory on 172.18.0.14:40939 (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 12:00:32 INFO BlockManagerInfo: Added broadcast_380_piece0 in memory on 172.18.0.15:34959 (size: 107.2 KiB, free: 434.0 MiB)
26/02/13 12:00:32 INFO TaskSetManager: Finished task 2.0 in stage 384.0 (TID 863) in 121 ms on 172.18.0.14 (executor 1) (1/3)
26/02/13 12:00:32 INFO TaskSetManager: Finished task 0.0 in stage 384.0 (TID 861) in 122 ms on 172.18.0.14 (executor 1) (2/3)
26/02/13 12:00:32 INFO TaskSetManager: Finished task 1.0 in stage 384.0 (TID 862) in 156 ms on 172.18.0.15 (executor 0) (3/3)
26/02/13 12:00:32 INFO TaskSchedulerImpl: Removed TaskSet 384.0, whose tasks have all completed, from pool 
26/02/13 12:00:32 INFO DAGScheduler: ResultStage 384 (start at NativeMethodAccessorImpl.java:0) finished in 0.163 s
26/02/13 12:00:32 INFO DAGScheduler: Job 254 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 12:00:32 INFO TaskSchedulerImpl: Killing all running tasks in stage 384: Stage finished
26/02/13 12:00:32 INFO DAGScheduler: Job 254 finished: start at NativeMethodAccessorImpl.java:0, took 0.166768 s
26/02/13 12:00:32 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 122, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@11a3939d] is committing.
26/02/13 12:00:32 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 122, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@11a3939d] committed.
26/02/13 12:00:32 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/122 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.122.cf501d04-7b4b-4546-957e-b9273d88b31d.tmp
26/02/13 12:00:32 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.122.cf501d04-7b4b-4546-957e-b9273d88b31d.tmp to file:/tmp/spark-checkpoint-enrichment/commits/122
26/02/13 12:00:32 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "fc5c6452-bd70-4c90-becf-772d3bebdb57",
  "name" : null,
  "timestamp" : "2026-02-13T12:00:32.189Z",
  "batchId" : 122,
  "numInputRows" : 240,
  "inputRowsPerSecond" : 219.98166819431714,
  "processedRowsPerSecond" : 344.82758620689657,
  "durationMs" : {
    "addBatch" : 415,
    "commitOffsets" : 87,
    "getBatch" : 0,
    "latestOffset" : 2,
    "queryPlanning" : 55,
    "triggerExecution" : 696,
    "walCommit" : 136
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 7535,
        "1" : 8457,
        "0" : 9550
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 7613,
        "1" : 8550,
        "0" : 9619
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 7613,
        "1" : 8550,
        "0" : 9619
      }
    },
    "numInputRows" : 240,
    "inputRowsPerSecond" : 219.98166819431714,
    "processedRowsPerSecond" : 344.82758620689657,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@7fe9b5de",
    "numOutputRows" : 50
  }
}
26/02/13 12:00:39 WARN KafkaOffsetReaderAdmin: Error in attempt 1 getting Kafka offsets: 
java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
	at java.base/java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:396)
	at java.base/java.util.concurrent.CompletableFuture.get(CompletableFuture.java:2073)
	at org.apache.kafka.common.internals.KafkaFutureImpl.get(KafkaFutureImpl.java:165)
	at org.apache.spark.sql.kafka010.ConsumerStrategy.retrieveAllPartitions(ConsumerStrategy.scala:66)
	at org.apache.spark.sql.kafka010.ConsumerStrategy.retrieveAllPartitions$(ConsumerStrategy.scala:65)
	at org.apache.spark.sql.kafka010.SubscribeStrategy.retrieveAllPartitions(ConsumerStrategy.scala:102)
	at org.apache.spark.sql.kafka010.SubscribeStrategy.assignedTopicPartitions(ConsumerStrategy.scala:113)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.$anonfun$partitionsAssignedToAdmin$1(KafkaOffsetReaderAdmin.scala:499)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.withRetries(KafkaOffsetReaderAdmin.scala:518)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.partitionsAssignedToAdmin(KafkaOffsetReaderAdmin.scala:498)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.fetchLatestOffsets(KafkaOffsetReaderAdmin.scala:297)
	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.latestOffset(KafkaMicroBatchStream.scala:130)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at scala.collection.TraversableLike.map(TraversableLike.scala:286)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:279)
	at scala.collection.AbstractTraversable.map(Traversable.scala:108)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)
	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)
	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)
Caused by: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
26/02/13 12:00:40 INFO AppInfoParser: App info kafka.admin.client for adminclient-1 unregistered
26/02/13 12:00:40 INFO Metrics: Metrics scheduler closed
26/02/13 12:00:40 INFO Metrics: Closing reporter org.apache.kafka.common.metrics.JmxReporter
26/02/13 12:00:40 INFO Metrics: Metrics reporters closed
26/02/13 12:00:40 INFO AdminClientConfig: AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.servers = [kafka:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

26/02/13 12:00:40 INFO AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
26/02/13 12:00:40 INFO AppInfoParser: Kafka version: 3.5.1
26/02/13 12:00:40 INFO AppInfoParser: Kafka commitId: 2c6fb6c54472e90a
26/02/13 12:00:40 INFO AppInfoParser: Kafka startTimeMs: 1770984040901
26/02/13 12:00:40 WARN KafkaOffsetReaderAdmin: Error in attempt 2 getting Kafka offsets: 
java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
	at java.base/java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:396)
	at java.base/java.util.concurrent.CompletableFuture.get(CompletableFuture.java:2073)
	at org.apache.kafka.common.internals.KafkaFutureImpl.get(KafkaFutureImpl.java:165)
	at org.apache.spark.sql.kafka010.ConsumerStrategy.retrieveAllPartitions(ConsumerStrategy.scala:66)
	at org.apache.spark.sql.kafka010.ConsumerStrategy.retrieveAllPartitions$(ConsumerStrategy.scala:65)
	at org.apache.spark.sql.kafka010.SubscribeStrategy.retrieveAllPartitions(ConsumerStrategy.scala:102)
	at org.apache.spark.sql.kafka010.SubscribeStrategy.assignedTopicPartitions(ConsumerStrategy.scala:113)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.$anonfun$partitionsAssignedToAdmin$1(KafkaOffsetReaderAdmin.scala:499)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.withRetries(KafkaOffsetReaderAdmin.scala:518)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.partitionsAssignedToAdmin(KafkaOffsetReaderAdmin.scala:498)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.fetchLatestOffsets(KafkaOffsetReaderAdmin.scala:297)
	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.latestOffset(KafkaMicroBatchStream.scala:130)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at scala.collection.TraversableLike.map(TraversableLike.scala:286)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:279)
	at scala.collection.AbstractTraversable.map(Traversable.scala:108)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)
	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)
	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)
Caused by: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
26/02/13 12:00:41 INFO AppInfoParser: App info kafka.admin.client for adminclient-2 unregistered
26/02/13 12:00:41 INFO Metrics: Metrics scheduler closed
26/02/13 12:00:41 INFO Metrics: Closing reporter org.apache.kafka.common.metrics.JmxReporter
26/02/13 12:00:41 INFO Metrics: Metrics reporters closed
26/02/13 12:00:41 INFO AdminClientConfig: AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.servers = [kafka:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

26/02/13 12:00:41 INFO AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
26/02/13 12:00:41 INFO AppInfoParser: Kafka version: 3.5.1
26/02/13 12:00:41 INFO AppInfoParser: Kafka commitId: 2c6fb6c54472e90a
26/02/13 12:00:41 INFO AppInfoParser: Kafka startTimeMs: 1770984041933
26/02/13 12:00:41 WARN KafkaOffsetReaderAdmin: Error in attempt 3 getting Kafka offsets: 
java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
	at java.base/java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:396)
	at java.base/java.util.concurrent.CompletableFuture.get(CompletableFuture.java:2073)
	at org.apache.kafka.common.internals.KafkaFutureImpl.get(KafkaFutureImpl.java:165)
	at org.apache.spark.sql.kafka010.ConsumerStrategy.retrieveAllPartitions(ConsumerStrategy.scala:66)
	at org.apache.spark.sql.kafka010.ConsumerStrategy.retrieveAllPartitions$(ConsumerStrategy.scala:65)
	at org.apache.spark.sql.kafka010.SubscribeStrategy.retrieveAllPartitions(ConsumerStrategy.scala:102)
	at org.apache.spark.sql.kafka010.SubscribeStrategy.assignedTopicPartitions(ConsumerStrategy.scala:113)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.$anonfun$partitionsAssignedToAdmin$1(KafkaOffsetReaderAdmin.scala:499)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.withRetries(KafkaOffsetReaderAdmin.scala:518)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.partitionsAssignedToAdmin(KafkaOffsetReaderAdmin.scala:498)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.fetchLatestOffsets(KafkaOffsetReaderAdmin.scala:297)
	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.latestOffset(KafkaMicroBatchStream.scala:130)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at scala.collection.TraversableLike.map(TraversableLike.scala:286)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:279)
	at scala.collection.AbstractTraversable.map(Traversable.scala:108)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)
	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)
	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)
Caused by: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
26/02/13 12:00:42 INFO AppInfoParser: App info kafka.admin.client for adminclient-3 unregistered
26/02/13 12:00:42 INFO Metrics: Metrics scheduler closed
26/02/13 12:00:42 INFO Metrics: Closing reporter org.apache.kafka.common.metrics.JmxReporter
26/02/13 12:00:42 INFO Metrics: Metrics reporters closed
26/02/13 12:00:42 ERROR MicroBatchExecution: Query [id = ebfab33f-0465-4e53-8f7a-961711cffb90, runId = fc5c6452-bd70-4c90-becf-772d3bebdb57] terminated with error
java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
	at java.base/java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:396)
	at java.base/java.util.concurrent.CompletableFuture.get(CompletableFuture.java:2073)
	at org.apache.kafka.common.internals.KafkaFutureImpl.get(KafkaFutureImpl.java:165)
	at org.apache.spark.sql.kafka010.ConsumerStrategy.retrieveAllPartitions(ConsumerStrategy.scala:66)
	at org.apache.spark.sql.kafka010.ConsumerStrategy.retrieveAllPartitions$(ConsumerStrategy.scala:65)
	at org.apache.spark.sql.kafka010.SubscribeStrategy.retrieveAllPartitions(ConsumerStrategy.scala:102)
	at org.apache.spark.sql.kafka010.SubscribeStrategy.assignedTopicPartitions(ConsumerStrategy.scala:113)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.$anonfun$partitionsAssignedToAdmin$1(KafkaOffsetReaderAdmin.scala:499)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.withRetries(KafkaOffsetReaderAdmin.scala:518)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.partitionsAssignedToAdmin(KafkaOffsetReaderAdmin.scala:498)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.fetchLatestOffsets(KafkaOffsetReaderAdmin.scala:297)
	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.latestOffset(KafkaMicroBatchStream.scala:130)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at scala.collection.TraversableLike.map(TraversableLike.scala:286)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:279)
	at scala.collection.AbstractTraversable.map(Traversable.scala:108)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)
	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)
	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)
Caused by: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
26/02/13 12:00:42 INFO MicroBatchExecution: Async log purge executor pool for query [id = ebfab33f-0465-4e53-8f7a-961711cffb90, runId = fc5c6452-bd70-4c90-becf-772d3bebdb57] has been shutdown

❌ FATAL ERROR: [STREAM_FAILED] Query [id = ebfab33f-0465-4e53-8f7a-961711cffb90, runId = fc5c6452-bd70-4c90-becf-772d3bebdb57] terminated with exception: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
Traceback (most recent call last):
  File "/opt/airflow/src/speed/spark_streaming_enrichment_sql.py", line 420, in <module>
    run_enrichment_stream()
  File "/opt/airflow/src/speed/spark_streaming_enrichment_sql.py", line 415, in run_enrichment_stream
    output_stream.awaitTermination()
  File "/opt/spark/python/lib/pyspark.zip/pyspark/sql/streaming/query.py", line 221, in awaitTermination
    return self._jsq.awaitTermination()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
                   ^^^^^^^^^^^^^^^^^
  File "/opt/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 185, in deco
    raise converted from None
pyspark.errors.exceptions.captured.StreamingQueryException: [STREAM_FAILED] Query [id = ebfab33f-0465-4e53-8f7a-961711cffb90, runId = fc5c6452-bd70-4c90-becf-772d3bebdb57] terminated with exception: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
Traceback (most recent call last):
  File "/opt/airflow/src/speed/spark_streaming_enrichment_sql.py", line 420, in <module>
    run_enrichment_stream()
  File "/opt/airflow/src/speed/spark_streaming_enrichment_sql.py", line 415, in run_enrichment_stream
    output_stream.awaitTermination()
  File "/opt/spark/python/lib/pyspark.zip/pyspark/sql/streaming/query.py", line 221, in awaitTermination
  File "/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
  File "/opt/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 185, in deco
pyspark.errors.exceptions.captured.StreamingQueryException: [STREAM_FAILED] Query [id = ebfab33f-0465-4e53-8f7a-961711cffb90, runId = fc5c6452-bd70-4c90-becf-772d3bebdb57] terminated with exception: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
26/02/13 12:00:44 INFO SparkContext: Invoking stop() from shutdown hook
26/02/13 12:00:44 INFO SparkContext: SparkContext is stopping with exitCode 0.
26/02/13 12:00:44 INFO SparkUI: Stopped Spark web UI at http://spark-master:4040
26/02/13 12:00:44 INFO StandaloneSchedulerBackend: Shutting down all executors
26/02/13 12:00:44 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Asking each executor to shut down
26/02/13 12:00:44 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
26/02/13 12:00:44 INFO MemoryStore: MemoryStore cleared
26/02/13 12:00:44 INFO BlockManager: BlockManager stopped
26/02/13 12:00:44 INFO BlockManagerMaster: BlockManagerMaster stopped
26/02/13 12:00:44 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
26/02/13 12:00:44 INFO SparkContext: Successfully stopped SparkContext
26/02/13 12:00:44 INFO ShutdownHookManager: Shutdown hook called
26/02/13 12:00:44 INFO ShutdownHookManager: Deleting directory /tmp/spark-e17c22f0-7b3d-4088-9205-777831d69a82
26/02/13 12:00:44 INFO ShutdownHookManager: Deleting directory /tmp/spark-e17c22f0-7b3d-4088-9205-777831d69a82/pyspark-bd0422f8-8817-4a58-949a-22c63bd56699
26/02/13 12:00:44 INFO ShutdownHookManager: Deleting directory /tmp/spark-02bdbd2e-ebe1-4b37-bcc2-80d6f2f5be69
[Fri Feb 13 12:00:45 UTC 2026] Spark Streaming job exited with code 0
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)
Caused by: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
26/02/13 12:00:42 INFO AppInfoParser: App info kafka.admin.client for adminclient-3 unregistered
26/02/13 12:00:42 INFO Metrics: Metrics scheduler closed
26/02/13 12:00:42 INFO Metrics: Closing reporter org.apache.kafka.common.metrics.JmxReporter
26/02/13 12:00:42 INFO Metrics: Metrics reporters closed
26/02/13 12:00:42 ERROR MicroBatchExecution: Query [id = ebfab33f-0465-4e53-8f7a-961711cffb90, runId = fc5c6452-bd70-4c90-becf-772d3bebdb57] terminated with error
java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
	at java.base/java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:396)
	at java.base/java.util.concurrent.CompletableFuture.get(CompletableFuture.java:2073)
	at org.apache.kafka.common.internals.KafkaFutureImpl.get(KafkaFutureImpl.java:165)
	at org.apache.spark.sql.kafka010.ConsumerStrategy.retrieveAllPartitions(ConsumerStrategy.scala:66)
	at org.apache.spark.sql.kafka010.ConsumerStrategy.retrieveAllPartitions$(ConsumerStrategy.scala:65)
	at org.apache.spark.sql.kafka010.SubscribeStrategy.retrieveAllPartitions(ConsumerStrategy.scala:102)
	at org.apache.spark.sql.kafka010.SubscribeStrategy.assignedTopicPartitions(ConsumerStrategy.scala:113)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.$anonfun$partitionsAssignedToAdmin$1(KafkaOffsetReaderAdmin.scala:499)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.withRetries(KafkaOffsetReaderAdmin.scala:518)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.partitionsAssignedToAdmin(KafkaOffsetReaderAdmin.scala:498)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.fetchLatestOffsets(KafkaOffsetReaderAdmin.scala:297)
	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.latestOffset(KafkaMicroBatchStream.scala:130)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at scala.collection.TraversableLike.map(TraversableLike.scala:286)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:279)
	at scala.collection.AbstractTraversable.map(Traversable.scala:108)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)
	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)
	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)
Caused by: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
26/02/13 12:00:42 INFO MicroBatchExecution: Async log purge executor pool for query [id = ebfab33f-0465-4e53-8f7a-961711cffb90, runId = fc5c6452-bd70-4c90-becf-772d3bebdb57] has been shutdown

❌ FATAL ERROR: [STREAM_FAILED] Query [id = ebfab33f-0465-4e53-8f7a-961711cffb90, runId = fc5c6452-bd70-4c90-becf-772d3bebdb57] terminated with exception: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
Traceback (most recent call last):
  File "/opt/airflow/src/speed/spark_streaming_enrichment_sql.py", line 420, in <module>
    run_enrichment_stream()
  File "/opt/airflow/src/speed/spark_streaming_enrichment_sql.py", line 415, in run_enrichment_stream
    output_stream.awaitTermination()
  File "/opt/spark/python/lib/pyspark.zip/pyspark/sql/streaming/query.py", line 221, in awaitTermination
    return self._jsq.awaitTermination()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
                   ^^^^^^^^^^^^^^^^^
  File "/opt/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 185, in deco
    raise converted from None
pyspark.errors.exceptions.captured.StreamingQueryException: [STREAM_FAILED] Query [id = ebfab33f-0465-4e53-8f7a-961711cffb90, runId = fc5c6452-bd70-4c90-becf-772d3bebdb57] terminated with exception: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
Traceback (most recent call last):
  File "/opt/airflow/src/speed/spark_streaming_enrichment_sql.py", line 420, in <module>
    run_enrichment_stream()
  File "/opt/airflow/src/speed/spark_streaming_enrichment_sql.py", line 415, in run_enrichment_stream
    output_stream.awaitTermination()
  File "/opt/spark/python/lib/pyspark.zip/pyspark/sql/streaming/query.py", line 221, in awaitTermination
  File "/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
  File "/opt/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 185, in deco
pyspark.errors.exceptions.captured.StreamingQueryException: [STREAM_FAILED] Query [id = ebfab33f-0465-4e53-8f7a-961711cffb90, runId = fc5c6452-bd70-4c90-becf-772d3bebdb57] terminated with exception: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
26/02/13 12:00:44 INFO SparkContext: Invoking stop() from shutdown hook
26/02/13 12:00:44 INFO SparkContext: SparkContext is stopping with exitCode 0.
26/02/13 12:00:44 INFO SparkUI: Stopped Spark web UI at http://spark-master:4040
26/02/13 12:00:44 INFO StandaloneSchedulerBackend: Shutting down all executors
26/02/13 12:00:44 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Asking each executor to shut down
26/02/13 12:00:44 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
26/02/13 12:00:44 INFO MemoryStore: MemoryStore cleared
26/02/13 12:00:44 INFO BlockManager: BlockManager stopped
26/02/13 12:00:44 INFO BlockManagerMaster: BlockManagerMaster stopped
26/02/13 12:00:44 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
26/02/13 12:00:44 INFO SparkContext: Successfully stopped SparkContext
26/02/13 12:00:44 INFO ShutdownHookManager: Shutdown hook called
26/02/13 12:00:44 INFO ShutdownHookManager: Deleting directory /tmp/spark-e17c22f0-7b3d-4088-9205-777831d69a82
26/02/13 12:00:44 INFO ShutdownHookManager: Deleting directory /tmp/spark-e17c22f0-7b3d-4088-9205-777831d69a82/pyspark-bd0422f8-8817-4a58-949a-22c63bd56699
26/02/13 12:00:44 INFO ShutdownHookManager: Deleting directory /tmp/spark-02bdbd2e-ebe1-4b37-bcc2-80d6f2f5be69
[Fri Feb 13 12:00:45 UTC 2026] Spark Streaming job exited with code 0
[Fri Feb 13 12:00:45 UTC 2026] Spark Streaming exited with code 0
Clean exit - restarting in 30s...
[Fri Feb 13 12:01:15 UTC 2026] Starting Spark Streaming attempt...
============================================
Starting Spark Streaming Enrichment Service
============================================
[Fri Feb 13 12:01:15 UTC 2026] Waiting for Kafka and topics to be ready...
  Waiting 40 seconds for Kafka initialization...
✓ Kafka should be ready now
[Fri Feb 13 12:01:55 UTC 2026] Submitting Spark Streaming job...
============================================================
Starting Spark Streaming Enrichment
============================================================
26/02/13 12:02:03 INFO SparkContext: Running Spark version 3.5.1
26/02/13 12:02:03 INFO SparkContext: OS info Linux, 6.17.0-14-generic, amd64
26/02/13 12:02:03 INFO SparkContext: Java version 17.0.17
26/02/13 12:02:03 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
26/02/13 12:02:04 INFO ResourceUtils: ==============================================================
26/02/13 12:02:04 INFO ResourceUtils: No custom resources configured for spark.driver.
26/02/13 12:02:04 INFO ResourceUtils: ==============================================================
26/02/13 12:02:04 INFO SparkContext: Submitted application: FlightStreamEnrichment
26/02/13 12:02:04 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
26/02/13 12:02:04 INFO ResourceProfile: Limiting resource is cpu
26/02/13 12:02:04 INFO ResourceProfileManager: Added ResourceProfile id: 0
26/02/13 12:02:04 INFO SecurityManager: Changing view acls to: root
26/02/13 12:02:04 INFO SecurityManager: Changing modify acls to: root
26/02/13 12:02:04 INFO SecurityManager: Changing view acls groups to: 
26/02/13 12:02:04 INFO SecurityManager: Changing modify acls groups to: 
26/02/13 12:02:04 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY
26/02/13 12:02:05 INFO Utils: Successfully started service 'sparkDriver' on port 34315.
26/02/13 12:02:05 INFO SparkEnv: Registering MapOutputTracker
26/02/13 12:02:05 INFO SparkEnv: Registering BlockManagerMaster
26/02/13 12:02:05 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
26/02/13 12:02:05 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
26/02/13 12:02:05 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
26/02/13 12:02:06 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-a34d3e76-4399-4334-9bf8-2002c941bac6
26/02/13 12:02:06 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
26/02/13 12:02:06 INFO SparkEnv: Registering OutputCommitCoordinator
26/02/13 12:02:06 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
26/02/13 12:02:06 INFO Utils: Successfully started service 'SparkUI' on port 4040.
26/02/13 12:02:07 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...
26/02/13 12:02:07 INFO TransportClientFactory: Successfully created connection to spark-master/172.18.0.10:7077 after 72 ms (0 ms spent in bootstraps)
26/02/13 12:02:07 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20260213120207-0001
26/02/13 12:02:07 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20260213120207-0001/0 on worker-20260213114143-172.18.0.15-35515 (172.18.0.15:35515) with 2 core(s)
26/02/13 12:02:07 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 42073.
26/02/13 12:02:07 INFO NettyBlockTransferService: Server created on spark-master 0.0.0.0:42073
26/02/13 12:02:07 INFO StandaloneSchedulerBackend: Granted executor ID app-20260213120207-0001/0 on hostPort 172.18.0.15:35515 with 2 core(s), 1024.0 MiB RAM
26/02/13 12:02:07 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20260213120207-0001/1 on worker-20260213114143-172.18.0.14-33335 (172.18.0.14:33335) with 2 core(s)
26/02/13 12:02:07 INFO StandaloneSchedulerBackend: Granted executor ID app-20260213120207-0001/1 on hostPort 172.18.0.14:33335 with 2 core(s), 1024.0 MiB RAM
26/02/13 12:02:07 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
26/02/13 12:02:07 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, spark-master, 42073, None)
26/02/13 12:02:07 INFO BlockManagerMasterEndpoint: Registering block manager spark-master:42073 with 434.4 MiB RAM, BlockManagerId(driver, spark-master, 42073, None)
26/02/13 12:02:07 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, spark-master, 42073, None)
26/02/13 12:02:07 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, spark-master, 42073, None)
26/02/13 12:02:08 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
26/02/13 12:02:08 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20260213120207-0001/0 is now RUNNING
26/02/13 12:02:08 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20260213120207-0001/1 is now RUNNING
✓ Spark session created
✓ Reading from: aviation-india-states
✓ Writing to: aviation-enriched-states
[DEBUG] Creating route mapping table...
Loading routes from CSV: /opt/airflow/data/routes.csv
26/02/13 12:02:09 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
26/02/13 12:02:09 INFO SharedState: Warehouse path is 'file:/opt/airflow/spark-warehouse'.
26/02/13 12:02:12 INFO InMemoryFileIndex: It took 108 ms to list leaf files for 1 paths.
26/02/13 12:02:12 INFO InMemoryFileIndex: It took 5 ms to list leaf files for 1 paths.
26/02/13 12:02:16 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.14:40320) with ID 1,  ResourceProfileId 0
26/02/13 12:02:17 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.15:48682) with ID 0,  ResourceProfileId 0
26/02/13 12:02:17 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.14:37571 with 434.4 MiB RAM, BlockManagerId(1, 172.18.0.14, 37571, None)
26/02/13 12:02:17 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.15:37717 with 434.4 MiB RAM, BlockManagerId(0, 172.18.0.15, 37717, None)
26/02/13 12:02:19 INFO FileSourceStrategy: Pushed Filters: 
26/02/13 12:02:19 INFO FileSourceStrategy: Post-Scan Filters: (length(trim(value#0, None)) > 0)
26/02/13 12:02:20 INFO CodeGenerator: Code generated in 447.022027 ms
26/02/13 12:02:21 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 200.4 KiB, free 434.2 MiB)
26/02/13 12:02:21 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 434.2 MiB)
26/02/13 12:02:21 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on spark-master:42073 (size: 34.5 KiB, free: 434.4 MiB)
26/02/13 12:02:21 INFO SparkContext: Created broadcast 0 from csv at NativeMethodAccessorImpl.java:0
26/02/13 12:02:21 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
26/02/13 12:02:21 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0
26/02/13 12:02:21 INFO DAGScheduler: Got job 0 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/02/13 12:02:21 INFO DAGScheduler: Final stage: ResultStage 0 (csv at NativeMethodAccessorImpl.java:0)
26/02/13 12:02:21 INFO DAGScheduler: Parents of final stage: List()
26/02/13 12:02:21 INFO DAGScheduler: Missing parents: List()
26/02/13 12:02:21 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:02:21 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 13.5 KiB, free 434.2 MiB)
26/02/13 12:02:21 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.4 KiB, free 434.2 MiB)
26/02/13 12:02:21 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on spark-master:42073 (size: 6.4 KiB, free: 434.4 MiB)
26/02/13 12:02:21 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1585
26/02/13 12:02:21 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/02/13 12:02:21 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
26/02/13 12:02:21 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8217 bytes) 
26/02/13 12:02:22 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.18.0.14:37571 (size: 6.4 KiB, free: 434.4 MiB)
26/02/13 12:02:23 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.18.0.14:37571 (size: 34.5 KiB, free: 434.4 MiB)
26/02/13 12:02:23 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 2238 ms on 172.18.0.14 (executor 1) (1/1)
26/02/13 12:02:23 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
26/02/13 12:02:23 INFO DAGScheduler: ResultStage 0 (csv at NativeMethodAccessorImpl.java:0) finished in 2.454 s
26/02/13 12:02:23 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 12:02:23 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
26/02/13 12:02:23 INFO DAGScheduler: Job 0 finished: csv at NativeMethodAccessorImpl.java:0, took 2.533778 s
26/02/13 12:02:23 INFO CodeGenerator: Code generated in 21.136175 ms
26/02/13 12:02:24 INFO FileSourceStrategy: Pushed Filters: 
26/02/13 12:02:24 INFO FileSourceStrategy: Post-Scan Filters: 
26/02/13 12:02:24 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 200.4 KiB, free 434.0 MiB)
26/02/13 12:02:24 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.9 MiB)
26/02/13 12:02:24 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 172.18.0.14:37571 in memory (size: 6.4 KiB, free: 434.4 MiB)
26/02/13 12:02:24 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on spark-master:42073 (size: 34.5 KiB, free: 434.3 MiB)
26/02/13 12:02:24 INFO BlockManagerInfo: Removed broadcast_1_piece0 on spark-master:42073 in memory (size: 6.4 KiB, free: 434.3 MiB)
26/02/13 12:02:24 INFO SparkContext: Created broadcast 2 from csv at NativeMethodAccessorImpl.java:0
26/02/13 12:02:24 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
26/02/13 12:02:24 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0
26/02/13 12:02:24 INFO DAGScheduler: Got job 1 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/02/13 12:02:24 INFO DAGScheduler: Final stage: ResultStage 1 (csv at NativeMethodAccessorImpl.java:0)
26/02/13 12:02:24 INFO DAGScheduler: Parents of final stage: List()
26/02/13 12:02:24 INFO DAGScheduler: Missing parents: List()
26/02/13 12:02:24 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[9] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:02:24 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 27.8 KiB, free 433.9 MiB)
26/02/13 12:02:24 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 12.8 KiB, free 433.9 MiB)
26/02/13 12:02:24 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on spark-master:42073 (size: 12.8 KiB, free: 434.3 MiB)
26/02/13 12:02:24 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1585
26/02/13 12:02:24 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[9] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/02/13 12:02:24 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
26/02/13 12:02:24 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8217 bytes) 
26/02/13 12:02:24 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 172.18.0.14:37571 (size: 12.8 KiB, free: 434.4 MiB)
26/02/13 12:02:26 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.18.0.14:37571 (size: 34.5 KiB, free: 434.3 MiB)
26/02/13 12:02:26 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 2172 ms on 172.18.0.14 (executor 1) (1/1)
26/02/13 12:02:26 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
26/02/13 12:02:26 INFO DAGScheduler: ResultStage 1 (csv at NativeMethodAccessorImpl.java:0) finished in 2.251 s
26/02/13 12:02:26 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 12:02:26 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
26/02/13 12:02:26 INFO DAGScheduler: Job 1 finished: csv at NativeMethodAccessorImpl.java:0, took 2.273979 s
26/02/13 12:02:27 INFO BlockManagerInfo: Removed broadcast_3_piece0 on spark-master:42073 in memory (size: 12.8 KiB, free: 434.3 MiB)
26/02/13 12:02:27 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 172.18.0.14:37571 in memory (size: 12.8 KiB, free: 434.3 MiB)
26/02/13 12:02:27 INFO FileSourceStrategy: Pushed Filters: IsNotNull(FlightNo),IsNotNull(Origin),IsNotNull(Destination)
26/02/13 12:02:27 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(FlightNo#17),isnotnull(Origin#18),isnotnull(Destination#19),NOT (Origin#18 = Destination#19)
26/02/13 12:02:28 INFO CodeGenerator: Code generated in 190.546998 ms
26/02/13 12:02:28 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 200.3 KiB, free 433.7 MiB)
26/02/13 12:02:28 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.7 MiB)
26/02/13 12:02:28 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on spark-master:42073 (size: 34.5 KiB, free: 434.3 MiB)
26/02/13 12:02:28 INFO SparkContext: Created broadcast 4 from count at NativeMethodAccessorImpl.java:0
26/02/13 12:02:28 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
26/02/13 12:02:28 INFO BlockManagerInfo: Removed broadcast_0_piece0 on spark-master:42073 in memory (size: 34.5 KiB, free: 434.3 MiB)
26/02/13 12:02:28 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 172.18.0.14:37571 in memory (size: 34.5 KiB, free: 434.4 MiB)
26/02/13 12:02:28 INFO DAGScheduler: Registering RDD 13 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 0
26/02/13 12:02:28 INFO DAGScheduler: Got map stage job 2 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/02/13 12:02:28 INFO DAGScheduler: Final stage: ShuffleMapStage 2 (count at NativeMethodAccessorImpl.java:0)
26/02/13 12:02:28 INFO DAGScheduler: Parents of final stage: List()
26/02/13 12:02:28 INFO DAGScheduler: Missing parents: List()
26/02/13 12:02:28 INFO DAGScheduler: Submitting ShuffleMapStage 2 (MapPartitionsRDD[13] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:02:28 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 51.6 KiB, free 433.9 MiB)
26/02/13 12:02:28 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 21.5 KiB, free 433.9 MiB)
26/02/13 12:02:28 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on spark-master:42073 (size: 21.5 KiB, free: 434.3 MiB)
26/02/13 12:02:28 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1585
26/02/13 12:02:28 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[13] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/02/13 12:02:28 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
26/02/13 12:02:28 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8206 bytes) 
26/02/13 12:02:28 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 172.18.0.14:37571 (size: 21.5 KiB, free: 434.3 MiB)
26/02/13 12:02:28 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 172.18.0.14:37571 (size: 34.5 KiB, free: 434.3 MiB)
26/02/13 12:02:29 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 868 ms on 172.18.0.14 (executor 1) (1/1)
26/02/13 12:02:29 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool 
26/02/13 12:02:29 INFO DAGScheduler: ShuffleMapStage 2 (count at NativeMethodAccessorImpl.java:0) finished in 0.907 s
26/02/13 12:02:29 INFO DAGScheduler: looking for newly runnable stages
26/02/13 12:02:29 INFO DAGScheduler: running: Set()
26/02/13 12:02:29 INFO DAGScheduler: waiting: Set()
26/02/13 12:02:29 INFO DAGScheduler: failed: Set()
26/02/13 12:02:29 INFO ShufflePartitionsUtil: For shuffle(0), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
26/02/13 12:02:29 INFO CodeGenerator: Code generated in 47.58471 ms
26/02/13 12:02:29 INFO DAGScheduler: Registering RDD 16 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 1
26/02/13 12:02:29 INFO DAGScheduler: Got map stage job 3 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/02/13 12:02:29 INFO DAGScheduler: Final stage: ShuffleMapStage 4 (count at NativeMethodAccessorImpl.java:0)
26/02/13 12:02:29 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 3)
26/02/13 12:02:29 INFO DAGScheduler: Missing parents: List()
26/02/13 12:02:29 INFO DAGScheduler: Submitting ShuffleMapStage 4 (MapPartitionsRDD[16] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:02:29 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 54.7 KiB, free 433.8 MiB)
26/02/13 12:02:29 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 24.2 KiB, free 433.8 MiB)
26/02/13 12:02:29 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on spark-master:42073 (size: 24.2 KiB, free: 434.3 MiB)
26/02/13 12:02:29 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1585
26/02/13 12:02:29 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 4 (MapPartitionsRDD[16] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/02/13 12:02:29 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0
26/02/13 12:02:29 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 3) (172.18.0.14, executor 1, partition 0, NODE_LOCAL, 7608 bytes) 
26/02/13 12:02:29 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 172.18.0.14:37571 (size: 24.2 KiB, free: 434.3 MiB)
26/02/13 12:02:29 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 172.18.0.14:40320
26/02/13 12:02:29 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 3) in 349 ms on 172.18.0.14 (executor 1) (1/1)
26/02/13 12:02:29 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool 
26/02/13 12:02:29 INFO DAGScheduler: ShuffleMapStage 4 (count at NativeMethodAccessorImpl.java:0) finished in 0.371 s
26/02/13 12:02:29 INFO DAGScheduler: looking for newly runnable stages
26/02/13 12:02:29 INFO DAGScheduler: running: Set()
26/02/13 12:02:29 INFO DAGScheduler: waiting: Set()
26/02/13 12:02:29 INFO DAGScheduler: failed: Set()
26/02/13 12:02:29 INFO CodeGenerator: Code generated in 10.878602 ms
26/02/13 12:02:29 INFO SparkContext: Starting job: count at NativeMethodAccessorImpl.java:0
26/02/13 12:02:29 INFO DAGScheduler: Got job 4 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/02/13 12:02:29 INFO DAGScheduler: Final stage: ResultStage 7 (count at NativeMethodAccessorImpl.java:0)
26/02/13 12:02:29 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 6)
26/02/13 12:02:29 INFO DAGScheduler: Missing parents: List()
26/02/13 12:02:29 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[19] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:02:29 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 12.5 KiB, free 433.8 MiB)
26/02/13 12:02:29 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 5.9 KiB, free 433.8 MiB)
26/02/13 12:02:29 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on spark-master:42073 (size: 5.9 KiB, free: 434.3 MiB)
26/02/13 12:02:29 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1585
26/02/13 12:02:29 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[19] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/02/13 12:02:29 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks resource profile 0
26/02/13 12:02:29 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 4) (172.18.0.14, executor 1, partition 0, NODE_LOCAL, 7619 bytes) 
26/02/13 12:02:29 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 172.18.0.14:37571 (size: 5.9 KiB, free: 434.3 MiB)
26/02/13 12:02:29 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 172.18.0.14:40320
26/02/13 12:02:29 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 4) in 77 ms on 172.18.0.14 (executor 1) (1/1)
26/02/13 12:02:29 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool 
26/02/13 12:02:29 INFO DAGScheduler: ResultStage 7 (count at NativeMethodAccessorImpl.java:0) finished in 0.089 s
26/02/13 12:02:29 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 12:02:29 INFO TaskSchedulerImpl: Killing all running tasks in stage 7: Stage finished
26/02/13 12:02:29 INFO DAGScheduler: Job 4 finished: count at NativeMethodAccessorImpl.java:0, took 0.095420 s
✓ Loaded 2773 routes from CSV: /opt/airflow/data/routes.csv
✓ Using flight number-based route matching
✓ Sample callsigns: IGO102, AIC176, etc.
26/02/13 12:02:29 INFO FileSourceStrategy: Pushed Filters: IsNotNull(FlightNo),IsNotNull(Origin),IsNotNull(Destination)
26/02/13 12:02:29 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(FlightNo#17),isnotnull(Origin#18),isnotnull(Destination#19),NOT (Origin#18 = Destination#19)
26/02/13 12:02:30 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
26/02/13 12:02:30 INFO BlockManagerInfo: Removed broadcast_6_piece0 on spark-master:42073 in memory (size: 24.2 KiB, free: 434.3 MiB)
26/02/13 12:02:30 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 172.18.0.14:37571 in memory (size: 24.2 KiB, free: 434.3 MiB)
26/02/13 12:02:30 INFO BlockManagerInfo: Removed broadcast_7_piece0 on spark-master:42073 in memory (size: 5.9 KiB, free: 434.3 MiB)
26/02/13 12:02:30 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 172.18.0.14:37571 in memory (size: 5.9 KiB, free: 434.3 MiB)
26/02/13 12:02:30 INFO BlockManagerInfo: Removed broadcast_5_piece0 on spark-master:42073 in memory (size: 21.5 KiB, free: 434.3 MiB)
26/02/13 12:02:30 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 172.18.0.14:37571 in memory (size: 21.5 KiB, free: 434.3 MiB)
26/02/13 12:02:30 INFO BlockManagerInfo: Removed broadcast_4_piece0 on spark-master:42073 in memory (size: 34.5 KiB, free: 434.4 MiB)
26/02/13 12:02:30 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 172.18.0.14:37571 in memory (size: 34.5 KiB, free: 434.4 MiB)
26/02/13 12:02:30 INFO BlockManagerInfo: Removed broadcast_2_piece0 on spark-master:42073 in memory (size: 34.5 KiB, free: 434.4 MiB)
26/02/13 12:02:30 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 172.18.0.14:37571 in memory (size: 34.5 KiB, free: 434.4 MiB)
26/02/13 12:02:30 INFO CodeGenerator: Code generated in 211.68615 ms
26/02/13 12:02:30 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 200.3 KiB, free 434.2 MiB)
26/02/13 12:02:30 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 434.2 MiB)
26/02/13 12:02:30 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on spark-master:42073 (size: 34.5 KiB, free: 434.4 MiB)
26/02/13 12:02:30 INFO SparkContext: Created broadcast 8 from count at NativeMethodAccessorImpl.java:0
26/02/13 12:02:30 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
26/02/13 12:02:30 INFO DAGScheduler: Registering RDD 24 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 2
26/02/13 12:02:30 INFO DAGScheduler: Got map stage job 5 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/02/13 12:02:30 INFO DAGScheduler: Final stage: ShuffleMapStage 8 (count at NativeMethodAccessorImpl.java:0)
26/02/13 12:02:30 INFO DAGScheduler: Parents of final stage: List()
26/02/13 12:02:30 INFO DAGScheduler: Missing parents: List()
26/02/13 12:02:30 INFO DAGScheduler: Submitting ShuffleMapStage 8 (MapPartitionsRDD[24] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:02:30 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 111.9 KiB, free 434.1 MiB)
26/02/13 12:02:30 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 434.0 MiB)
26/02/13 12:02:30 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on spark-master:42073 (size: 34.0 KiB, free: 434.3 MiB)
26/02/13 12:02:30 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1585
26/02/13 12:02:30 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 8 (MapPartitionsRDD[24] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/02/13 12:02:30 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks resource profile 0
26/02/13 12:02:30 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 5) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8206 bytes) 
26/02/13 12:02:30 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 172.18.0.14:37571 (size: 34.0 KiB, free: 434.4 MiB)
26/02/13 12:02:30 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 172.18.0.14:37571 (size: 34.5 KiB, free: 434.3 MiB)
26/02/13 12:02:31 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 5) in 813 ms on 172.18.0.14 (executor 1) (1/1)
26/02/13 12:02:31 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool 
26/02/13 12:02:31 INFO DAGScheduler: ShuffleMapStage 8 (count at NativeMethodAccessorImpl.java:0) finished in 0.831 s
26/02/13 12:02:31 INFO DAGScheduler: looking for newly runnable stages
26/02/13 12:02:31 INFO DAGScheduler: running: Set()
26/02/13 12:02:31 INFO DAGScheduler: waiting: Set()
26/02/13 12:02:31 INFO DAGScheduler: failed: Set()
26/02/13 12:02:31 INFO CodeGenerator: Code generated in 16.514236 ms
26/02/13 12:02:31 INFO DAGScheduler: Got job 6 (count at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 12:02:31 INFO DAGScheduler: Final stage: ResultStage 10 (count at NativeMethodAccessorImpl.java:0)
26/02/13 12:02:31 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 9)
26/02/13 12:02:31 INFO DAGScheduler: Missing parents: List()
26/02/13 12:02:31 INFO DAGScheduler: Submitting ResultStage 10 (AdaptiveSparkPlan isFinalPlan=false
+- SortAggregate(key=[callsign#67], functions=[first(flight_number#25, false), first(origin_code#26, false), first(destination_code#27, false), first(airline_iata#32, false), first(airline_name#37, false), first(airline_prefix#43, false), first(airline_icao#50, false), first(flight_num_only#58, false), first(origin_city#77, false), first(origin_airport#88, false), first(origin_lat#100, false), first(origin_lon#113, false), first(destination_city#127, false), first(destination_airport#142, false), first(destination_lat#158, false), first(destination_lon#175, false)], output=[flight_number#251, origin_code#253, destination_code#255, airline_iata#257, airline_name#259, airline_prefix#261, airline_icao#263, flight_num_only#265, callsign#67, origin_city#267, origin_airport#269, origin_lat#271, origin_lon#273, destination_city#275, destination_airport#277, destination_lat#279, destination_lon#281])
   +- Sort [callsign#67 ASC NULLS FIRST], false, 0
      +- Exchange hashpartit... MapPartitionsRDD[29] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:02:31 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 89.6 KiB, free 433.9 MiB)
26/02/13 12:02:31 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 29.7 KiB, free 433.9 MiB)
26/02/13 12:02:31 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on spark-master:42073 (size: 29.7 KiB, free: 434.3 MiB)
26/02/13 12:02:31 INFO SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:1585
26/02/13 12:02:31 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 10 (AdaptiveSparkPlan isFinalPlan=false
+- SortAggregate(key=[callsign#67], functions=[first(flight_number#25, false), first(origin_code#26, false), first(destination_code#27, false), first(airline_iata#32, false), first(airline_name#37, false), first(airline_prefix#43, false), first(airline_icao#50, false), first(flight_num_only#58, false), first(origin_city#77, false), first(origin_airport#88, false), first(origin_lat#100, false), first(origin_lon#113, false), first(destination_city#127, false), first(destination_airport#142, false), first(destination_lat#158, false), first(destination_lon#175, false)], output=[flight_number#251, origin_code#253, destination_code#255, airline_iata#257, airline_name#259, airline_prefix#261, airline_icao#263, flight_num_only#265, callsign#67, origin_city#267, origin_airport#269, origin_lat#271, origin_lon#273, destination_city#275, destination_airport#277, destination_lat#279, destination_lon#281])
   +- Sort [callsign#67 ASC NULLS FIRST], false, 0
      +- Exchange hashpartit... MapPartitionsRDD[29] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 12:02:31 INFO TaskSchedulerImpl: Adding task set 10.0 with 4 tasks resource profile 0
26/02/13 12:02:31 INFO TaskSetManager: Starting task 0.0 in stage 10.0 (TID 6) (172.18.0.14, executor 1, partition 0, NODE_LOCAL, 7619 bytes) 
26/02/13 12:02:31 INFO TaskSetManager: Starting task 1.0 in stage 10.0 (TID 7) (172.18.0.14, executor 1, partition 1, NODE_LOCAL, 7619 bytes) 
26/02/13 12:02:31 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 172.18.0.14:37571 (size: 29.7 KiB, free: 434.3 MiB)
26/02/13 12:02:31 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 2 to 172.18.0.14:40320
26/02/13 12:02:32 INFO BlockManagerInfo: Added rdd_29_1 in memory on 172.18.0.14:37571 (size: 52.1 KiB, free: 434.3 MiB)
26/02/13 12:02:32 INFO BlockManagerInfo: Added rdd_29_0 in memory on 172.18.0.14:37571 (size: 51.7 KiB, free: 434.2 MiB)
26/02/13 12:02:32 INFO TaskSetManager: Starting task 2.0 in stage 10.0 (TID 8) (172.18.0.14, executor 1, partition 2, NODE_LOCAL, 7619 bytes) 
26/02/13 12:02:32 INFO TaskSetManager: Starting task 3.0 in stage 10.0 (TID 9) (172.18.0.14, executor 1, partition 3, NODE_LOCAL, 7619 bytes) 
26/02/13 12:02:32 INFO TaskSetManager: Finished task 1.0 in stage 10.0 (TID 7) in 847 ms on 172.18.0.14 (executor 1) (1/4)
26/02/13 12:02:32 INFO TaskSetManager: Finished task 0.0 in stage 10.0 (TID 6) in 850 ms on 172.18.0.14 (executor 1) (2/4)
26/02/13 12:02:32 INFO BlockManagerInfo: Added rdd_29_3 in memory on 172.18.0.14:37571 (size: 51.3 KiB, free: 434.2 MiB)
26/02/13 12:02:32 INFO BlockManagerInfo: Added rdd_29_2 in memory on 172.18.0.14:37571 (size: 49.7 KiB, free: 434.1 MiB)
26/02/13 12:02:32 INFO TaskSetManager: Finished task 3.0 in stage 10.0 (TID 9) in 245 ms on 172.18.0.14 (executor 1) (3/4)
26/02/13 12:02:32 INFO TaskSetManager: Finished task 2.0 in stage 10.0 (TID 8) in 268 ms on 172.18.0.14 (executor 1) (4/4)
26/02/13 12:02:32 INFO DAGScheduler: ResultStage 10 (count at NativeMethodAccessorImpl.java:0) finished in 1.130 s
26/02/13 12:02:32 INFO TaskSchedulerImpl: Removed TaskSet 10.0, whose tasks have all completed, from pool 
26/02/13 12:02:32 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 12:02:32 INFO TaskSchedulerImpl: Killing all running tasks in stage 10: Stage finished
26/02/13 12:02:32 INFO CodeGenerator: Code generated in 45.666914 ms
26/02/13 12:02:32 INFO DAGScheduler: Registering RDD 34 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 3
26/02/13 12:02:32 INFO DAGScheduler: Got map stage job 7 (count at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 12:02:32 INFO DAGScheduler: Final stage: ShuffleMapStage 12 (count at NativeMethodAccessorImpl.java:0)
26/02/13 12:02:32 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 11)
26/02/13 12:02:32 INFO DAGScheduler: Missing parents: List()
26/02/13 12:02:32 INFO DAGScheduler: Submitting ShuffleMapStage 12 (MapPartitionsRDD[34] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:02:32 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 95.1 KiB, free 433.8 MiB)
26/02/13 12:02:32 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 32.0 KiB, free 433.8 MiB)
26/02/13 12:02:32 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on spark-master:42073 (size: 32.0 KiB, free: 434.3 MiB)
26/02/13 12:02:32 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1585
26/02/13 12:02:32 INFO DAGScheduler: Submitting 4 missing tasks from ShuffleMapStage 12 (MapPartitionsRDD[34] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 12:02:32 INFO TaskSchedulerImpl: Adding task set 12.0 with 4 tasks resource profile 0
26/02/13 12:02:32 INFO TaskSetManager: Starting task 0.0 in stage 12.0 (TID 10) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 7608 bytes) 
26/02/13 12:02:32 INFO TaskSetManager: Starting task 1.0 in stage 12.0 (TID 11) (172.18.0.14, executor 1, partition 1, PROCESS_LOCAL, 7608 bytes) 
26/02/13 12:02:32 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 172.18.0.14:37571 (size: 32.0 KiB, free: 434.1 MiB)
26/02/13 12:02:33 INFO TaskSetManager: Starting task 2.0 in stage 12.0 (TID 12) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 7608 bytes) 
26/02/13 12:02:33 INFO TaskSetManager: Starting task 3.0 in stage 12.0 (TID 13) (172.18.0.14, executor 1, partition 3, PROCESS_LOCAL, 7608 bytes) 
26/02/13 12:02:33 INFO TaskSetManager: Finished task 1.0 in stage 12.0 (TID 11) in 216 ms on 172.18.0.14 (executor 1) (1/4)
26/02/13 12:02:33 INFO TaskSetManager: Finished task 0.0 in stage 12.0 (TID 10) in 220 ms on 172.18.0.14 (executor 1) (2/4)
26/02/13 12:02:33 INFO TaskSetManager: Finished task 2.0 in stage 12.0 (TID 12) in 49 ms on 172.18.0.14 (executor 1) (3/4)
26/02/13 12:02:33 INFO TaskSetManager: Finished task 3.0 in stage 12.0 (TID 13) in 59 ms on 172.18.0.14 (executor 1) (4/4)
26/02/13 12:02:33 INFO TaskSchedulerImpl: Removed TaskSet 12.0, whose tasks have all completed, from pool 
26/02/13 12:02:33 INFO DAGScheduler: ShuffleMapStage 12 (count at NativeMethodAccessorImpl.java:0) finished in 0.298 s
26/02/13 12:02:33 INFO DAGScheduler: looking for newly runnable stages
26/02/13 12:02:33 INFO DAGScheduler: running: Set()
26/02/13 12:02:33 INFO DAGScheduler: waiting: Set()
26/02/13 12:02:33 INFO DAGScheduler: failed: Set()
26/02/13 12:02:33 INFO CodeGenerator: Code generated in 21.090851 ms
26/02/13 12:02:33 INFO SparkContext: Starting job: count at NativeMethodAccessorImpl.java:0
26/02/13 12:02:33 INFO DAGScheduler: Got job 8 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/02/13 12:02:33 INFO DAGScheduler: Final stage: ResultStage 15 (count at NativeMethodAccessorImpl.java:0)
26/02/13 12:02:33 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 14)
26/02/13 12:02:33 INFO DAGScheduler: Missing parents: List()
26/02/13 12:02:33 INFO DAGScheduler: Submitting ResultStage 15 (MapPartitionsRDD[37] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:02:33 INFO BlockManagerInfo: Removed broadcast_11_piece0 on spark-master:42073 in memory (size: 32.0 KiB, free: 434.3 MiB)
26/02/13 12:02:33 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 12.5 KiB, free 433.9 MiB)
26/02/13 12:02:33 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 5.9 KiB, free 433.9 MiB)
26/02/13 12:02:33 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on spark-master:42073 (size: 5.9 KiB, free: 434.3 MiB)
26/02/13 12:02:33 INFO SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:1585
26/02/13 12:02:33 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 15 (MapPartitionsRDD[37] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/02/13 12:02:33 INFO TaskSchedulerImpl: Adding task set 15.0 with 1 tasks resource profile 0
26/02/13 12:02:33 INFO BlockManagerInfo: Removed broadcast_11_piece0 on 172.18.0.14:37571 in memory (size: 32.0 KiB, free: 434.1 MiB)
26/02/13 12:02:33 INFO TaskSetManager: Starting task 0.0 in stage 15.0 (TID 14) (172.18.0.14, executor 1, partition 0, NODE_LOCAL, 7619 bytes) 
26/02/13 12:02:33 INFO BlockManagerInfo: Removed broadcast_10_piece0 on spark-master:42073 in memory (size: 29.7 KiB, free: 434.3 MiB)
26/02/13 12:02:33 INFO BlockManagerInfo: Removed broadcast_10_piece0 on 172.18.0.14:37571 in memory (size: 29.7 KiB, free: 434.1 MiB)
26/02/13 12:02:33 INFO BlockManagerInfo: Removed broadcast_9_piece0 on 172.18.0.14:37571 in memory (size: 34.0 KiB, free: 434.2 MiB)
26/02/13 12:02:33 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 172.18.0.14:37571 (size: 5.9 KiB, free: 434.2 MiB)
26/02/13 12:02:33 INFO BlockManagerInfo: Removed broadcast_9_piece0 on spark-master:42073 in memory (size: 34.0 KiB, free: 434.4 MiB)
26/02/13 12:02:33 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 3 to 172.18.0.14:40320
26/02/13 12:02:33 INFO TaskSetManager: Finished task 0.0 in stage 15.0 (TID 14) in 124 ms on 172.18.0.14 (executor 1) (1/1)
26/02/13 12:02:33 INFO TaskSchedulerImpl: Removed TaskSet 15.0, whose tasks have all completed, from pool 
26/02/13 12:02:33 INFO DAGScheduler: ResultStage 15 (count at NativeMethodAccessorImpl.java:0) finished in 0.150 s
26/02/13 12:02:33 INFO DAGScheduler: Job 8 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 12:02:33 INFO TaskSchedulerImpl: Killing all running tasks in stage 15: Stage finished
26/02/13 12:02:33 INFO DAGScheduler: Job 8 finished: count at NativeMethodAccessorImpl.java:0, took 0.178666 s
✓ Route mapping table created (2773 routes)
[DEBUG] Setting up Kafka input stream...
✓ Connected to Kafka input stream
26/02/13 12:02:33 INFO BlockManagerInfo: Removed broadcast_12_piece0 on spark-master:42073 in memory (size: 5.9 KiB, free: 434.4 MiB)
[DEBUG] Parsing JSON and extracting fields...
26/02/13 12:02:33 INFO BlockManagerInfo: Removed broadcast_12_piece0 on 172.18.0.14:37571 in memory (size: 5.9 KiB, free: 434.2 MiB)
[DEBUG] Joining with route mapping...
  Using CSV-based exact callsign matching
[DEBUG] Filtering records with valid routes...
✓ Filtering enabled: Only flights with routes will be sent to next topic
[DEBUG] Setting up output stream to Kafka...
26/02/13 12:02:34 INFO StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint
26/02/13 12:02:34 INFO ResolveWriteToStream: Checkpoint root /tmp/spark-checkpoint-enrichment resolved to file:/tmp/spark-checkpoint-enrichment.
26/02/13 12:02:34 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
26/02/13 12:02:34 INFO MicroBatchExecution: Starting [id = ebfab33f-0465-4e53-8f7a-961711cffb90, runId = 4a161d66-225b-4b29-812d-ddec3c05364e]. Use file:/tmp/spark-checkpoint-enrichment to store the query checkpoint.
============================================================
✓ Streaming query started successfully!
✓ Enriching flights with route information...
✓ Query ID: ebfab33f-0465-4e53-8f7a-961711cffb90
============================================================
[DEBUG] Waiting for stream termination...
26/02/13 12:02:34 INFO MicroBatchExecution: Reading table [org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@67ef27c4] from DataSourceV2 named 'kafka' [org.apache.spark.sql.kafka010.KafkaSourceProvider@3ee8c8b0]
26/02/13 12:02:34 INFO OffsetSeqLog: BatchIds found from listing: 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122
26/02/13 12:02:34 INFO OffsetSeqLog: Getting latest batch 122
26/02/13 12:02:34 INFO OffsetSeqLog: BatchIds found from listing: 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122
26/02/13 12:02:34 INFO OffsetSeqLog: Getting latest batch 122
26/02/13 12:02:34 INFO CommitLog: BatchIds found from listing: 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122
26/02/13 12:02:34 INFO CommitLog: Getting latest batch 122
26/02/13 12:02:34 INFO MicroBatchExecution: Resuming at batch 123 with committed offsets {KafkaV2[Subscribe[aviation-india-states]]: {"aviation-india-states":{"2":7613,"1":8550,"0":9619}}} and available offsets {KafkaV2[Subscribe[aviation-india-states]]: {"aviation-india-states":{"2":7613,"1":8550,"0":9619}}}
26/02/13 12:02:34 INFO MicroBatchExecution: Stream started from {KafkaV2[Subscribe[aviation-india-states]]: {"aviation-india-states":{"2":7613,"1":8550,"0":9619}}}
26/02/13 12:02:34 INFO AdminClientConfig: AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.servers = [kafka:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

26/02/13 12:02:34 INFO AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
26/02/13 12:02:34 INFO AppInfoParser: Kafka version: 3.5.1
26/02/13 12:02:34 INFO AppInfoParser: Kafka commitId: 2c6fb6c54472e90a
26/02/13 12:02:34 INFO AppInfoParser: Kafka startTimeMs: 1770984154694
26/02/13 12:02:35 WARN KafkaOffsetReaderAdmin: Found incorrect offsets in some partitions (partition, previous offset, fetched offset): ArrayBuffer((aviation-india-states-2,7613,560), (aviation-india-states-1,8550,695), (aviation-india-states-0,9619,502))
26/02/13 12:02:35 WARN KafkaOffsetReaderAdmin: Retrying to fetch latest offsets because of incorrect offsets
26/02/13 12:02:36 WARN KafkaOffsetReaderAdmin: Found incorrect offsets in some partitions (partition, previous offset, fetched offset): ArrayBuffer((aviation-india-states-2,7613,560), (aviation-india-states-1,8550,695), (aviation-india-states-0,9619,502))
26/02/13 12:02:36 WARN KafkaOffsetReaderAdmin: Retrying to fetch latest offsets because of incorrect offsets
26/02/13 12:02:37 WARN KafkaOffsetReaderAdmin: Found incorrect offsets in some partitions (partition, previous offset, fetched offset): ArrayBuffer((aviation-india-states-2,7613,560), (aviation-india-states-1,8550,695), (aviation-india-states-0,9619,502))
26/02/13 12:02:37 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/123 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.123.2e1d93c1-4de2-4741-b61b-4bc1723a2893.tmp
26/02/13 12:02:37 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.123.2e1d93c1-4de2-4741-b61b-4bc1723a2893.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/123
26/02/13 12:02:37 INFO MicroBatchExecution: Committed offsets for batch 123. Metadata OffsetSeqMetadata(0,1770984157099,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 12:02:37 INFO OffsetSeqLog: BatchIds found from listing: 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 123
26/02/13 12:02:37 INFO CommitLog: BatchIds found from listing: 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122
26/02/13 12:02:37 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:02:37 WARN KafkaMicroBatchStream: Partition aviation-india-states-2's offset was changed from 7613 to 560, some data may have been missed. 
Some data may have been lost because they are not available in Kafka any more; either the
 data was aged out by Kafka or the topic may have been deleted before all the data in the
 topic was processed. If you want your streaming query to fail on such cases, set the source
 option "failOnDataLoss" to "true".
    
26/02/13 12:02:37 WARN KafkaMicroBatchStream: Partition aviation-india-states-1's offset was changed from 8550 to 695, some data may have been missed. 
Some data may have been lost because they are not available in Kafka any more; either the
 data was aged out by Kafka or the topic may have been deleted before all the data in the
 topic was processed. If you want your streaming query to fail on such cases, set the source
 option "failOnDataLoss" to "true".
    
26/02/13 12:02:37 WARN KafkaMicroBatchStream: Partition aviation-india-states-0's offset was changed from 9619 to 502, some data may have been missed. 
Some data may have been lost because they are not available in Kafka any more; either the
 data was aged out by Kafka or the topic may have been deleted before all the data in the
 topic was processed. If you want your streaming query to fail on such cases, set the source
 option "failOnDataLoss" to "true".
    
26/02/13 12:02:37 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:02:37 WARN KafkaMicroBatchStream: Partition aviation-india-states-2's offset was changed from 7613 to 560, some data may have been missed. 
Some data may have been lost because they are not available in Kafka any more; either the
 data was aged out by Kafka or the topic may have been deleted before all the data in the
 topic was processed. If you want your streaming query to fail on such cases, set the source
 option "failOnDataLoss" to "true".
    
26/02/13 12:02:37 WARN KafkaMicroBatchStream: Partition aviation-india-states-1's offset was changed from 8550 to 695, some data may have been missed. 
Some data may have been lost because they are not available in Kafka any more; either the
 data was aged out by Kafka or the topic may have been deleted before all the data in the
 topic was processed. If you want your streaming query to fail on such cases, set the source
 option "failOnDataLoss" to "true".
    
26/02/13 12:02:37 WARN KafkaMicroBatchStream: Partition aviation-india-states-0's offset was changed from 9619 to 502, some data may have been missed. 
Some data may have been lost because they are not available in Kafka any more; either the
 data was aged out by Kafka or the topic may have been deleted before all the data in the
 topic was processed. If you want your streaming query to fail on such cases, set the source
 option "failOnDataLoss" to "true".
    
26/02/13 12:02:37 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:02:37 WARN KafkaMicroBatchStream: Partition aviation-india-states-2's offset was changed from 7613 to 560, some data may have been missed. 
Some data may have been lost because they are not available in Kafka any more; either the
 data was aged out by Kafka or the topic may have been deleted before all the data in the
 topic was processed. If you want your streaming query to fail on such cases, set the source
 option "failOnDataLoss" to "true".
    
26/02/13 12:02:37 WARN KafkaMicroBatchStream: Partition aviation-india-states-1's offset was changed from 8550 to 695, some data may have been missed. 
Some data may have been lost because they are not available in Kafka any more; either the
 data was aged out by Kafka or the topic may have been deleted before all the data in the
 topic was processed. If you want your streaming query to fail on such cases, set the source
 option "failOnDataLoss" to "true".
    
26/02/13 12:02:37 WARN KafkaMicroBatchStream: Partition aviation-india-states-0's offset was changed from 9619 to 502, some data may have been missed. 
Some data may have been lost because they are not available in Kafka any more; either the
 data was aged out by Kafka or the topic may have been deleted before all the data in the
 topic was processed. If you want your streaming query to fail on such cases, set the source
 option "failOnDataLoss" to "true".
    
26/02/13 12:02:37 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:02:37 WARN KafkaMicroBatchStream: Partition aviation-india-states-2's offset was changed from 7613 to 560, some data may have been missed. 
Some data may have been lost because they are not available in Kafka any more; either the
 data was aged out by Kafka or the topic may have been deleted before all the data in the
 topic was processed. If you want your streaming query to fail on such cases, set the source
 option "failOnDataLoss" to "true".
    
26/02/13 12:02:37 WARN KafkaMicroBatchStream: Partition aviation-india-states-1's offset was changed from 8550 to 695, some data may have been missed. 
Some data may have been lost because they are not available in Kafka any more; either the
 data was aged out by Kafka or the topic may have been deleted before all the data in the
 topic was processed. If you want your streaming query to fail on such cases, set the source
 option "failOnDataLoss" to "true".
    
26/02/13 12:02:37 WARN KafkaMicroBatchStream: Partition aviation-india-states-0's offset was changed from 9619 to 502, some data may have been missed. 
Some data may have been lost because they are not available in Kafka any more; either the
 data was aged out by Kafka or the topic may have been deleted before all the data in the
 topic was processed. If you want your streaming query to fail on such cases, set the source
 option "failOnDataLoss" to "true".
    
26/02/13 12:02:37 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:02:37 WARN KafkaMicroBatchStream: Partition aviation-india-states-2's offset was changed from 7613 to 560, some data may have been missed. 
Some data may have been lost because they are not available in Kafka any more; either the
 data was aged out by Kafka or the topic may have been deleted before all the data in the
 topic was processed. If you want your streaming query to fail on such cases, set the source
 option "failOnDataLoss" to "true".
    
26/02/13 12:02:37 WARN KafkaMicroBatchStream: Partition aviation-india-states-1's offset was changed from 8550 to 695, some data may have been missed. 
Some data may have been lost because they are not available in Kafka any more; either the
 data was aged out by Kafka or the topic may have been deleted before all the data in the
 topic was processed. If you want your streaming query to fail on such cases, set the source
 option "failOnDataLoss" to "true".
    
26/02/13 12:02:37 WARN KafkaMicroBatchStream: Partition aviation-india-states-0's offset was changed from 9619 to 502, some data may have been missed. 
Some data may have been lost because they are not available in Kafka any more; either the
 data was aged out by Kafka or the topic may have been deleted before all the data in the
 topic was processed. If you want your streaming query to fail on such cases, set the source
 option "failOnDataLoss" to "true".
    
26/02/13 12:02:37 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:02:37 WARN KafkaMicroBatchStream: Partition aviation-india-states-2's offset was changed from 7613 to 560, some data may have been missed. 
Some data may have been lost because they are not available in Kafka any more; either the
 data was aged out by Kafka or the topic may have been deleted before all the data in the
 topic was processed. If you want your streaming query to fail on such cases, set the source
 option "failOnDataLoss" to "true".
    
26/02/13 12:02:37 WARN KafkaMicroBatchStream: Partition aviation-india-states-1's offset was changed from 8550 to 695, some data may have been missed. 
Some data may have been lost because they are not available in Kafka any more; either the
 data was aged out by Kafka or the topic may have been deleted before all the data in the
 topic was processed. If you want your streaming query to fail on such cases, set the source
 option "failOnDataLoss" to "true".
    
26/02/13 12:02:37 WARN KafkaMicroBatchStream: Partition aviation-india-states-0's offset was changed from 9619 to 502, some data may have been missed. 
Some data may have been lost because they are not available in Kafka any more; either the
 data was aged out by Kafka or the topic may have been deleted before all the data in the
 topic was processed. If you want your streaming query to fail on such cases, set the source
 option "failOnDataLoss" to "true".
    
26/02/13 12:02:37 INFO CodeGenerator: Code generated in 16.709744 ms
26/02/13 12:02:37 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#1951 - origin_code.nullCount#1950) > 0)
26/02/13 12:02:37 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#1956 - destination_code.nullCount#1955) > 0)
26/02/13 12:02:37 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#1986 - callsign.nullCount#1985) > 0)
26/02/13 12:02:37 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 12:02:37 INFO DAGScheduler: Got job 9 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 12:02:37 INFO DAGScheduler: Final stage: ResultStage 17 (start at NativeMethodAccessorImpl.java:0)
26/02/13 12:02:37 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 16)
26/02/13 12:02:37 INFO DAGScheduler: Missing parents: List()
26/02/13 12:02:37 INFO DAGScheduler: Submitting ResultStage 17 (MapPartitionsRDD[42] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:02:37 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 99.3 KiB, free 434.1 MiB)
26/02/13 12:02:37 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 434.0 MiB)
26/02/13 12:02:37 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on spark-master:42073 (size: 33.3 KiB, free: 434.3 MiB)
26/02/13 12:02:37 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1585
26/02/13 12:02:37 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 17 (MapPartitionsRDD[42] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 12:02:37 INFO TaskSchedulerImpl: Adding task set 17.0 with 4 tasks resource profile 0
26/02/13 12:02:37 INFO TaskSetManager: Starting task 0.0 in stage 17.0 (TID 15) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:02:37 INFO TaskSetManager: Starting task 1.0 in stage 17.0 (TID 16) (172.18.0.14, executor 1, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:02:37 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on 172.18.0.14:37571 (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 12:02:37 INFO TaskSetManager: Starting task 2.0 in stage 17.0 (TID 17) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:02:37 INFO TaskSetManager: Starting task 3.0 in stage 17.0 (TID 18) (172.18.0.14, executor 1, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:02:37 INFO TaskSetManager: Finished task 1.0 in stage 17.0 (TID 16) in 145 ms on 172.18.0.14 (executor 1) (1/4)
26/02/13 12:02:37 INFO TaskSetManager: Finished task 0.0 in stage 17.0 (TID 15) in 147 ms on 172.18.0.14 (executor 1) (2/4)
26/02/13 12:02:37 INFO TaskSetManager: Finished task 2.0 in stage 17.0 (TID 17) in 63 ms on 172.18.0.14 (executor 1) (3/4)
26/02/13 12:02:37 INFO TaskSetManager: Finished task 3.0 in stage 17.0 (TID 18) in 64 ms on 172.18.0.14 (executor 1) (4/4)
26/02/13 12:02:37 INFO TaskSchedulerImpl: Removed TaskSet 17.0, whose tasks have all completed, from pool 
26/02/13 12:02:37 INFO DAGScheduler: ResultStage 17 (start at NativeMethodAccessorImpl.java:0) finished in 0.224 s
26/02/13 12:02:37 INFO DAGScheduler: Job 9 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 12:02:37 INFO TaskSchedulerImpl: Killing all running tasks in stage 17: Stage finished
26/02/13 12:02:37 INFO DAGScheduler: Job 9 finished: start at NativeMethodAccessorImpl.java:0, took 0.232045 s
26/02/13 12:02:37 INFO CodeGenerator: Code generated in 13.912175 ms
26/02/13 12:02:38 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.9 MiB)
26/02/13 12:02:38 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on spark-master:42073 (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 12:02:38 INFO SparkContext: Created broadcast 14 from start at NativeMethodAccessorImpl.java:0
26/02/13 12:02:38 INFO CodeGenerator: Code generated in 72.533244 ms
26/02/13 12:02:38 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 123, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@2c04f7]. The input RDD has 1 partitions.
26/02/13 12:02:38 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 12:02:38 INFO DAGScheduler: Got job 10 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/02/13 12:02:38 INFO DAGScheduler: Final stage: ResultStage 18 (start at NativeMethodAccessorImpl.java:0)
26/02/13 12:02:38 INFO DAGScheduler: Parents of final stage: List()
26/02/13 12:02:38 INFO DAGScheduler: Missing parents: List()
26/02/13 12:02:38 INFO DAGScheduler: Submitting ResultStage 18 (ParallelCollectionRDD[49] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:02:38 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 4.4 KiB, free 433.9 MiB)
26/02/13 12:02:38 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 2.6 KiB, free 433.9 MiB)
26/02/13 12:02:38 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on spark-master:42073 (size: 2.6 KiB, free: 434.2 MiB)
26/02/13 12:02:38 INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1585
26/02/13 12:02:38 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 18 (ParallelCollectionRDD[49] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/02/13 12:02:38 INFO TaskSchedulerImpl: Adding task set 18.0 with 1 tasks resource profile 0
26/02/13 12:02:38 INFO TaskSetManager: Starting task 0.0 in stage 18.0 (TID 19) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 7640 bytes) 
26/02/13 12:02:38 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on 172.18.0.14:37571 (size: 2.6 KiB, free: 434.1 MiB)
26/02/13 12:02:38 INFO TaskSetManager: Finished task 0.0 in stage 18.0 (TID 19) in 191 ms on 172.18.0.14 (executor 1) (1/1)
26/02/13 12:02:38 INFO TaskSchedulerImpl: Removed TaskSet 18.0, whose tasks have all completed, from pool 
26/02/13 12:02:38 INFO DAGScheduler: ResultStage 18 (start at NativeMethodAccessorImpl.java:0) finished in 0.204 s
26/02/13 12:02:38 INFO DAGScheduler: Job 10 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 12:02:38 INFO TaskSchedulerImpl: Killing all running tasks in stage 18: Stage finished
26/02/13 12:02:38 INFO DAGScheduler: Job 10 finished: start at NativeMethodAccessorImpl.java:0, took 0.217129 s
26/02/13 12:02:38 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 123, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@2c04f7] is committing.
26/02/13 12:02:38 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 123, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@2c04f7] committed.
26/02/13 12:02:38 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/123 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.123.e822e7e8-92af-4eef-ad68-f81fe84d6ff6.tmp
26/02/13 12:02:38 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.123.e822e7e8-92af-4eef-ad68-f81fe84d6ff6.tmp to file:/tmp/spark-checkpoint-enrichment/commits/123
26/02/13 12:02:38 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "4a161d66-225b-4b29-812d-ddec3c05364e",
  "name" : null,
  "timestamp" : "2026-02-13T12:02:34.385Z",
  "batchId" : 123,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "addBatch" : 1000,
    "commitOffsets" : 97,
    "getBatch" : 3,
    "latestOffset" : 2587,
    "queryPlanning" : 240,
    "triggerExecution" : 4144,
    "walCommit" : 77
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 7613,
        "1" : 8550,
        "0" : 9619
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 560,
        "1" : 695,
        "0" : 502
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 560,
        "1" : 695,
        "0" : 502
      }
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@1a6a42c2",
    "numOutputRows" : 0
  }
}
26/02/13 12:02:42 INFO BlockManagerInfo: Removed broadcast_13_piece0 on spark-master:42073 in memory (size: 33.3 KiB, free: 434.3 MiB)
26/02/13 12:02:42 INFO BlockManagerInfo: Removed broadcast_13_piece0 on 172.18.0.14:37571 in memory (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 12:02:42 INFO BlockManagerInfo: Removed broadcast_15_piece0 on spark-master:42073 in memory (size: 2.6 KiB, free: 434.3 MiB)
26/02/13 12:02:42 INFO BlockManagerInfo: Removed broadcast_15_piece0 on 172.18.0.14:37571 in memory (size: 2.6 KiB, free: 434.2 MiB)
26/02/13 12:02:45 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/124 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.124.0c35f4b1-a74c-4e30-bf0f-20f6f590d1ce.tmp
26/02/13 12:02:45 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.124.0c35f4b1-a74c-4e30-bf0f-20f6f590d1ce.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/124
26/02/13 12:02:45 INFO MicroBatchExecution: Committed offsets for batch 124. Metadata OffsetSeqMetadata(0,1770984165635,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 12:02:45 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:02:45 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:02:46 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:02:46 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:02:46 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:02:46 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:02:46 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#2805 - origin_code.nullCount#2804) > 0)
26/02/13 12:02:46 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#2810 - destination_code.nullCount#2809) > 0)
26/02/13 12:02:46 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#2840 - callsign.nullCount#2839) > 0)
26/02/13 12:02:46 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 12:02:46 INFO DAGScheduler: Got job 11 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 12:02:46 INFO DAGScheduler: Final stage: ResultStage 20 (start at NativeMethodAccessorImpl.java:0)
26/02/13 12:02:46 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 19)
26/02/13 12:02:46 INFO DAGScheduler: Missing parents: List()
26/02/13 12:02:46 INFO DAGScheduler: Submitting ResultStage 20 (MapPartitionsRDD[54] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:02:46 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 99.3 KiB, free 434.0 MiB)
26/02/13 12:02:46 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 433.9 MiB)
26/02/13 12:02:46 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on spark-master:42073 (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 12:02:46 INFO SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1585
26/02/13 12:02:46 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 20 (MapPartitionsRDD[54] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 12:02:46 INFO TaskSchedulerImpl: Adding task set 20.0 with 4 tasks resource profile 0
26/02/13 12:02:46 INFO TaskSetManager: Starting task 0.0 in stage 20.0 (TID 20) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:02:46 INFO TaskSetManager: Starting task 1.0 in stage 20.0 (TID 21) (172.18.0.14, executor 1, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:02:46 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on 172.18.0.14:37571 (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 12:02:46 INFO TaskSetManager: Starting task 2.0 in stage 20.0 (TID 22) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:02:46 INFO BlockManagerInfo: Removed broadcast_14_piece0 on spark-master:42073 in memory (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 12:02:46 INFO TaskSetManager: Starting task 3.0 in stage 20.0 (TID 23) (172.18.0.14, executor 1, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:02:46 INFO TaskSetManager: Finished task 1.0 in stage 20.0 (TID 21) in 105 ms on 172.18.0.14 (executor 1) (1/4)
26/02/13 12:02:46 INFO TaskSetManager: Finished task 0.0 in stage 20.0 (TID 20) in 110 ms on 172.18.0.14 (executor 1) (2/4)
26/02/13 12:02:46 INFO TaskSetManager: Finished task 3.0 in stage 20.0 (TID 23) in 56 ms on 172.18.0.14 (executor 1) (3/4)
26/02/13 12:02:46 INFO TaskSetManager: Finished task 2.0 in stage 20.0 (TID 22) in 62 ms on 172.18.0.14 (executor 1) (4/4)
26/02/13 12:02:46 INFO TaskSchedulerImpl: Removed TaskSet 20.0, whose tasks have all completed, from pool 
26/02/13 12:02:46 INFO DAGScheduler: ResultStage 20 (start at NativeMethodAccessorImpl.java:0) finished in 0.181 s
26/02/13 12:02:46 INFO DAGScheduler: Job 11 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 12:02:46 INFO TaskSchedulerImpl: Killing all running tasks in stage 20: Stage finished
26/02/13 12:02:46 INFO DAGScheduler: Job 11 finished: start at NativeMethodAccessorImpl.java:0, took 0.189795 s
26/02/13 12:02:46 INFO MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.9 MiB)
26/02/13 12:02:46 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on spark-master:42073 (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 12:02:46 INFO SparkContext: Created broadcast 17 from start at NativeMethodAccessorImpl.java:0
26/02/13 12:02:46 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 124, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@60a8d7cc]. The input RDD has 3 partitions.
26/02/13 12:02:46 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 12:02:46 INFO DAGScheduler: Got job 12 (start at NativeMethodAccessorImpl.java:0) with 3 output partitions
26/02/13 12:02:46 INFO DAGScheduler: Final stage: ResultStage 21 (start at NativeMethodAccessorImpl.java:0)
26/02/13 12:02:46 INFO DAGScheduler: Parents of final stage: List()
26/02/13 12:02:46 INFO DAGScheduler: Missing parents: List()
26/02/13 12:02:46 INFO DAGScheduler: Submitting ResultStage 21 (MapPartitionsRDD[60] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:02:46 INFO MemoryStore: Block broadcast_18 stored as values in memory (estimated size 49.9 KiB, free 433.9 MiB)
26/02/13 12:02:46 INFO MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.9 MiB)
26/02/13 12:02:46 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on spark-master:42073 (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 12:02:46 INFO SparkContext: Created broadcast 18 from broadcast at DAGScheduler.scala:1585
26/02/13 12:02:46 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 21 (MapPartitionsRDD[60] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
26/02/13 12:02:46 INFO TaskSchedulerImpl: Adding task set 21.0 with 3 tasks resource profile 0
26/02/13 12:02:46 INFO TaskSetManager: Starting task 1.0 in stage 21.0 (TID 24) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:02:46 INFO TaskSetManager: Starting task 0.0 in stage 21.0 (TID 25) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:02:46 INFO TaskSetManager: Starting task 2.0 in stage 21.0 (TID 26) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:02:46 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on 172.18.0.14:37571 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 12:02:46 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on 172.18.0.14:37571 (size: 107.2 KiB, free: 434.0 MiB)
26/02/13 12:02:46 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on 172.18.0.15:37717 (size: 19.5 KiB, free: 434.4 MiB)
26/02/13 12:02:48 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on 172.18.0.15:37717 (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 12:02:49 INFO TaskSetManager: Finished task 2.0 in stage 21.0 (TID 26) in 2515 ms on 172.18.0.14 (executor 1) (1/3)
26/02/13 12:02:49 INFO TaskSetManager: Finished task 0.0 in stage 21.0 (TID 25) in 2531 ms on 172.18.0.14 (executor 1) (2/3)
26/02/13 12:02:50 INFO TaskSetManager: Finished task 1.0 in stage 21.0 (TID 24) in 4210 ms on 172.18.0.15 (executor 0) (3/3)
26/02/13 12:02:50 INFO TaskSchedulerImpl: Removed TaskSet 21.0, whose tasks have all completed, from pool 
26/02/13 12:02:50 INFO DAGScheduler: ResultStage 21 (start at NativeMethodAccessorImpl.java:0) finished in 4.236 s
26/02/13 12:02:50 INFO DAGScheduler: Job 12 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 12:02:50 INFO TaskSchedulerImpl: Killing all running tasks in stage 21: Stage finished
26/02/13 12:02:50 INFO DAGScheduler: Job 12 finished: start at NativeMethodAccessorImpl.java:0, took 4.242215 s
26/02/13 12:02:50 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 124, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@60a8d7cc] is committing.
26/02/13 12:02:50 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 124, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@60a8d7cc] committed.
26/02/13 12:02:50 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/124 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.124.34e05788-282f-448b-bb65-754ead52d189.tmp
26/02/13 12:02:50 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.124.34e05788-282f-448b-bb65-754ead52d189.tmp to file:/tmp/spark-checkpoint-enrichment/commits/124
26/02/13 12:02:50 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "4a161d66-225b-4b29-812d-ddec3c05364e",
  "name" : null,
  "timestamp" : "2026-02-13T12:02:45.622Z",
  "batchId" : 124,
  "numInputRows" : 67,
  "inputRowsPerSecond" : 2913.0434782608695,
  "processedRowsPerSecond" : 13.024883359253499,
  "durationMs" : {
    "addBatch" : 4736,
    "commitOffsets" : 62,
    "getBatch" : 0,
    "latestOffset" : 12,
    "queryPlanning" : 219,
    "triggerExecution" : 5144,
    "walCommit" : 111
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 560,
        "1" : 695,
        "0" : 502
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 582,
        "1" : 722,
        "0" : 520
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 582,
        "1" : 722,
        "0" : 520
      }
    },
    "numInputRows" : 67,
    "inputRowsPerSecond" : 2913.0434782608695,
    "processedRowsPerSecond" : 13.024883359253499,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@1a6a42c2",
    "numOutputRows" : 17
  }
}
26/02/13 12:02:50 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/125 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.125.2f529519-8262-4bd9-9ad7-0876c387d95a.tmp
26/02/13 12:02:50 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.125.2f529519-8262-4bd9-9ad7-0876c387d95a.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/125
26/02/13 12:02:50 INFO MicroBatchExecution: Committed offsets for batch 125. Metadata OffsetSeqMetadata(0,1770984170771,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 12:02:50 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:02:50 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:02:50 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:02:50 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:02:50 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:02:50 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:02:50 INFO BlockManagerInfo: Removed broadcast_16_piece0 on spark-master:42073 in memory (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 12:02:51 INFO BlockManagerInfo: Removed broadcast_16_piece0 on 172.18.0.14:37571 in memory (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 12:02:51 INFO BlockManagerInfo: Removed broadcast_18_piece0 on spark-master:42073 in memory (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 12:02:51 INFO BlockManagerInfo: Removed broadcast_18_piece0 on 172.18.0.14:37571 in memory (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 12:02:51 INFO BlockManagerInfo: Removed broadcast_18_piece0 on 172.18.0.15:37717 in memory (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 12:02:51 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#3659 - origin_code.nullCount#3658) > 0)
26/02/13 12:02:51 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#3664 - destination_code.nullCount#3663) > 0)
26/02/13 12:02:51 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#3694 - callsign.nullCount#3693) > 0)
26/02/13 12:02:51 INFO BlockManagerInfo: Removed broadcast_17_piece0 on spark-master:42073 in memory (size: 107.2 KiB, free: 434.4 MiB)
26/02/13 12:02:51 INFO BlockManagerInfo: Removed broadcast_17_piece0 on 172.18.0.14:37571 in memory (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 12:02:51 INFO BlockManagerInfo: Removed broadcast_17_piece0 on 172.18.0.15:37717 in memory (size: 107.2 KiB, free: 434.4 MiB)
26/02/13 12:02:51 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 12:02:51 INFO DAGScheduler: Got job 13 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 12:02:51 INFO DAGScheduler: Final stage: ResultStage 23 (start at NativeMethodAccessorImpl.java:0)
26/02/13 12:02:51 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 22)
26/02/13 12:02:51 INFO DAGScheduler: Missing parents: List()
26/02/13 12:02:51 INFO DAGScheduler: Submitting ResultStage 23 (MapPartitionsRDD[65] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:02:51 INFO MemoryStore: Block broadcast_19 stored as values in memory (estimated size 99.3 KiB, free 434.1 MiB)
26/02/13 12:02:51 INFO MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 434.0 MiB)
26/02/13 12:02:51 INFO BlockManagerInfo: Added broadcast_19_piece0 in memory on spark-master:42073 (size: 33.3 KiB, free: 434.3 MiB)
26/02/13 12:02:51 INFO SparkContext: Created broadcast 19 from broadcast at DAGScheduler.scala:1585
26/02/13 12:02:51 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 23 (MapPartitionsRDD[65] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 12:02:51 INFO TaskSchedulerImpl: Adding task set 23.0 with 4 tasks resource profile 0
26/02/13 12:02:51 INFO TaskSetManager: Starting task 0.0 in stage 23.0 (TID 27) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:02:51 INFO TaskSetManager: Starting task 1.0 in stage 23.0 (TID 28) (172.18.0.14, executor 1, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:02:51 INFO BlockManagerInfo: Added broadcast_19_piece0 in memory on 172.18.0.14:37571 (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 12:02:51 INFO TaskSetManager: Starting task 2.0 in stage 23.0 (TID 29) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:02:51 INFO TaskSetManager: Finished task 0.0 in stage 23.0 (TID 27) in 50 ms on 172.18.0.14 (executor 1) (1/4)
26/02/13 12:02:51 INFO TaskSetManager: Starting task 3.0 in stage 23.0 (TID 30) (172.18.0.14, executor 1, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:02:51 INFO TaskSetManager: Finished task 1.0 in stage 23.0 (TID 28) in 56 ms on 172.18.0.14 (executor 1) (2/4)
26/02/13 12:02:51 INFO TaskSetManager: Finished task 2.0 in stage 23.0 (TID 29) in 51 ms on 172.18.0.14 (executor 1) (3/4)
26/02/13 12:02:51 INFO TaskSetManager: Finished task 3.0 in stage 23.0 (TID 30) in 55 ms on 172.18.0.14 (executor 1) (4/4)
26/02/13 12:02:51 INFO TaskSchedulerImpl: Removed TaskSet 23.0, whose tasks have all completed, from pool 
26/02/13 12:02:51 INFO DAGScheduler: ResultStage 23 (start at NativeMethodAccessorImpl.java:0) finished in 0.123 s
26/02/13 12:02:51 INFO DAGScheduler: Job 13 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 12:02:51 INFO TaskSchedulerImpl: Killing all running tasks in stage 23: Stage finished
26/02/13 12:02:51 INFO DAGScheduler: Job 13 finished: start at NativeMethodAccessorImpl.java:0, took 0.130037 s
26/02/13 12:02:51 INFO MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.9 MiB)
26/02/13 12:02:51 INFO BlockManagerInfo: Added broadcast_20_piece0 in memory on spark-master:42073 (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 12:02:51 INFO SparkContext: Created broadcast 20 from start at NativeMethodAccessorImpl.java:0
26/02/13 12:02:51 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 125, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@2b164f88]. The input RDD has 3 partitions.
26/02/13 12:02:51 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 12:02:51 INFO DAGScheduler: Got job 14 (start at NativeMethodAccessorImpl.java:0) with 3 output partitions
26/02/13 12:02:51 INFO DAGScheduler: Final stage: ResultStage 24 (start at NativeMethodAccessorImpl.java:0)
26/02/13 12:02:51 INFO DAGScheduler: Parents of final stage: List()
26/02/13 12:02:51 INFO DAGScheduler: Missing parents: List()
26/02/13 12:02:51 INFO DAGScheduler: Submitting ResultStage 24 (MapPartitionsRDD[71] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:02:51 INFO MemoryStore: Block broadcast_21 stored as values in memory (estimated size 49.9 KiB, free 433.9 MiB)
26/02/13 12:02:51 INFO MemoryStore: Block broadcast_21_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.9 MiB)
26/02/13 12:02:51 INFO BlockManagerInfo: Added broadcast_21_piece0 in memory on spark-master:42073 (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 12:02:51 INFO SparkContext: Created broadcast 21 from broadcast at DAGScheduler.scala:1585
26/02/13 12:02:51 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 24 (MapPartitionsRDD[71] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
26/02/13 12:02:51 INFO TaskSchedulerImpl: Adding task set 24.0 with 3 tasks resource profile 0
26/02/13 12:02:51 INFO TaskSetManager: Starting task 0.0 in stage 24.0 (TID 31) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:02:51 INFO TaskSetManager: Starting task 1.0 in stage 24.0 (TID 32) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:02:51 INFO TaskSetManager: Starting task 2.0 in stage 24.0 (TID 33) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:02:51 INFO BlockManagerInfo: Added broadcast_21_piece0 in memory on 172.18.0.14:37571 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 12:02:51 INFO BlockManagerInfo: Added broadcast_21_piece0 in memory on 172.18.0.15:37717 (size: 19.5 KiB, free: 434.4 MiB)
26/02/13 12:02:51 INFO BlockManagerInfo: Added broadcast_20_piece0 in memory on 172.18.0.14:37571 (size: 107.2 KiB, free: 434.0 MiB)
26/02/13 12:02:51 INFO BlockManagerInfo: Added broadcast_20_piece0 in memory on 172.18.0.15:37717 (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 12:02:51 INFO TaskSetManager: Finished task 2.0 in stage 24.0 (TID 33) in 280 ms on 172.18.0.14 (executor 1) (1/3)
26/02/13 12:02:51 INFO TaskSetManager: Finished task 0.0 in stage 24.0 (TID 31) in 333 ms on 172.18.0.14 (executor 1) (2/3)
26/02/13 12:02:51 INFO TaskSetManager: Finished task 1.0 in stage 24.0 (TID 32) in 505 ms on 172.18.0.15 (executor 0) (3/3)
26/02/13 12:02:51 INFO TaskSchedulerImpl: Removed TaskSet 24.0, whose tasks have all completed, from pool 
26/02/13 12:02:51 INFO DAGScheduler: ResultStage 24 (start at NativeMethodAccessorImpl.java:0) finished in 0.514 s
26/02/13 12:02:51 INFO DAGScheduler: Job 14 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 12:02:51 INFO TaskSchedulerImpl: Killing all running tasks in stage 24: Stage finished
26/02/13 12:02:51 INFO DAGScheduler: Job 14 finished: start at NativeMethodAccessorImpl.java:0, took 0.517818 s
26/02/13 12:02:51 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 125, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@2b164f88] is committing.
26/02/13 12:02:51 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 125, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@2b164f88] committed.
26/02/13 12:02:51 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/125 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.125.3f52c63d-647d-4931-ac74-45d3be27d957.tmp
26/02/13 12:02:51 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.125.3f52c63d-647d-4931-ac74-45d3be27d957.tmp to file:/tmp/spark-checkpoint-enrichment/commits/125
26/02/13 12:02:51 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "4a161d66-225b-4b29-812d-ddec3c05364e",
  "name" : null,
  "timestamp" : "2026-02-13T12:02:50.768Z",
  "batchId" : 125,
  "numInputRows" : 181,
  "inputRowsPerSecond" : 35.172949863972015,
  "processedRowsPerSecond" : 169.9530516431925,
  "durationMs" : {
    "addBatch" : 853,
    "commitOffsets" : 93,
    "getBatch" : 0,
    "latestOffset" : 3,
    "queryPlanning" : 50,
    "triggerExecution" : 1065,
    "walCommit" : 65
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 582,
        "1" : 722,
        "0" : 520
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 640,
        "1" : 790,
        "0" : 575
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 640,
        "1" : 790,
        "0" : 575
      }
    },
    "numInputRows" : 181,
    "inputRowsPerSecond" : 35.172949863972015,
    "processedRowsPerSecond" : 169.9530516431925,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@1a6a42c2",
    "numOutputRows" : 34
  }
}
26/02/13 12:02:56 INFO BlockManagerInfo: Removed broadcast_21_piece0 on spark-master:42073 in memory (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 12:02:56 INFO BlockManagerInfo: Removed broadcast_21_piece0 on 172.18.0.14:37571 in memory (size: 19.5 KiB, free: 434.0 MiB)
26/02/13 12:02:56 INFO BlockManagerInfo: Removed broadcast_21_piece0 on 172.18.0.15:37717 in memory (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 12:02:56 INFO BlockManagerInfo: Removed broadcast_19_piece0 on spark-master:42073 in memory (size: 33.3 KiB, free: 434.3 MiB)
26/02/13 12:02:56 INFO BlockManagerInfo: Removed broadcast_19_piece0 on 172.18.0.14:37571 in memory (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 12:03:01 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:03:02 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/126 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.126.b021f94a-a0c1-4ffc-bf90-afe4cf56b053.tmp
26/02/13 12:03:02 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.126.b021f94a-a0c1-4ffc-bf90-afe4cf56b053.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/126
26/02/13 12:03:02 INFO MicroBatchExecution: Committed offsets for batch 126. Metadata OffsetSeqMetadata(0,1770984182280,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 12:03:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:03:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:03:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:03:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:03:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:03:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:03:02 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#4513 - origin_code.nullCount#4512) > 0)
26/02/13 12:03:02 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#4518 - destination_code.nullCount#4517) > 0)
26/02/13 12:03:02 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#4548 - callsign.nullCount#4547) > 0)
26/02/13 12:03:02 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 12:03:02 INFO DAGScheduler: Got job 15 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 12:03:02 INFO DAGScheduler: Final stage: ResultStage 26 (start at NativeMethodAccessorImpl.java:0)
26/02/13 12:03:02 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 25)
26/02/13 12:03:02 INFO DAGScheduler: Missing parents: List()
26/02/13 12:03:02 INFO DAGScheduler: Submitting ResultStage 26 (MapPartitionsRDD[76] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:03:02 INFO MemoryStore: Block broadcast_22 stored as values in memory (estimated size 99.3 KiB, free 434.0 MiB)
26/02/13 12:03:02 INFO MemoryStore: Block broadcast_22_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 433.9 MiB)
26/02/13 12:03:02 INFO BlockManagerInfo: Added broadcast_22_piece0 in memory on spark-master:42073 (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 12:03:02 INFO SparkContext: Created broadcast 22 from broadcast at DAGScheduler.scala:1585
26/02/13 12:03:02 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 26 (MapPartitionsRDD[76] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 12:03:02 INFO TaskSchedulerImpl: Adding task set 26.0 with 4 tasks resource profile 0
26/02/13 12:03:02 INFO TaskSetManager: Starting task 0.0 in stage 26.0 (TID 34) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:03:02 INFO TaskSetManager: Starting task 1.0 in stage 26.0 (TID 35) (172.18.0.14, executor 1, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:03:02 INFO BlockManagerInfo: Added broadcast_22_piece0 in memory on 172.18.0.14:37571 (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 12:03:02 INFO TaskSetManager: Starting task 2.0 in stage 26.0 (TID 36) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:03:02 INFO TaskSetManager: Finished task 1.0 in stage 26.0 (TID 35) in 79 ms on 172.18.0.14 (executor 1) (1/4)
26/02/13 12:03:02 INFO TaskSetManager: Starting task 3.0 in stage 26.0 (TID 37) (172.18.0.14, executor 1, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:03:02 INFO TaskSetManager: Finished task 0.0 in stage 26.0 (TID 34) in 104 ms on 172.18.0.14 (executor 1) (2/4)
26/02/13 12:03:02 INFO TaskSetManager: Finished task 2.0 in stage 26.0 (TID 36) in 74 ms on 172.18.0.14 (executor 1) (3/4)
26/02/13 12:03:02 INFO TaskSetManager: Finished task 3.0 in stage 26.0 (TID 37) in 53 ms on 172.18.0.14 (executor 1) (4/4)
26/02/13 12:03:02 INFO TaskSchedulerImpl: Removed TaskSet 26.0, whose tasks have all completed, from pool 
26/02/13 12:03:02 INFO DAGScheduler: ResultStage 26 (start at NativeMethodAccessorImpl.java:0) finished in 0.174 s
26/02/13 12:03:02 INFO DAGScheduler: Job 15 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 12:03:02 INFO TaskSchedulerImpl: Killing all running tasks in stage 26: Stage finished
26/02/13 12:03:02 INFO DAGScheduler: Job 15 finished: start at NativeMethodAccessorImpl.java:0, took 0.180202 s
26/02/13 12:03:02 INFO MemoryStore: Block broadcast_23_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.8 MiB)
26/02/13 12:03:02 INFO BlockManagerInfo: Added broadcast_23_piece0 in memory on spark-master:42073 (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 12:03:02 INFO SparkContext: Created broadcast 23 from start at NativeMethodAccessorImpl.java:0
26/02/13 12:03:02 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 126, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@33c3a72f]. The input RDD has 3 partitions.
26/02/13 12:03:02 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 12:03:02 INFO DAGScheduler: Got job 16 (start at NativeMethodAccessorImpl.java:0) with 3 output partitions
26/02/13 12:03:02 INFO DAGScheduler: Final stage: ResultStage 27 (start at NativeMethodAccessorImpl.java:0)
26/02/13 12:03:02 INFO DAGScheduler: Parents of final stage: List()
26/02/13 12:03:02 INFO DAGScheduler: Missing parents: List()
26/02/13 12:03:02 INFO DAGScheduler: Submitting ResultStage 27 (MapPartitionsRDD[82] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:03:02 INFO MemoryStore: Block broadcast_24 stored as values in memory (estimated size 49.9 KiB, free 433.8 MiB)
26/02/13 12:03:02 INFO MemoryStore: Block broadcast_24_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.8 MiB)
26/02/13 12:03:02 INFO BlockManagerInfo: Added broadcast_24_piece0 in memory on spark-master:42073 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 12:03:02 INFO SparkContext: Created broadcast 24 from broadcast at DAGScheduler.scala:1585
26/02/13 12:03:02 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 27 (MapPartitionsRDD[82] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
26/02/13 12:03:02 INFO TaskSchedulerImpl: Adding task set 27.0 with 3 tasks resource profile 0
26/02/13 12:03:02 INFO TaskSetManager: Starting task 1.0 in stage 27.0 (TID 38) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:03:02 INFO TaskSetManager: Starting task 0.0 in stage 27.0 (TID 39) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:03:02 INFO TaskSetManager: Starting task 2.0 in stage 27.0 (TID 40) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:03:02 INFO BlockManagerInfo: Added broadcast_24_piece0 in memory on 172.18.0.14:37571 (size: 19.5 KiB, free: 434.0 MiB)
26/02/13 12:03:02 INFO BlockManagerInfo: Added broadcast_24_piece0 in memory on 172.18.0.15:37717 (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 12:03:02 INFO BlockManagerInfo: Added broadcast_23_piece0 in memory on 172.18.0.14:37571 (size: 107.2 KiB, free: 433.9 MiB)
26/02/13 12:03:03 INFO BlockManagerInfo: Added broadcast_23_piece0 in memory on 172.18.0.15:37717 (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 12:03:03 INFO TaskSetManager: Finished task 0.0 in stage 27.0 (TID 39) in 722 ms on 172.18.0.14 (executor 1) (1/3)
26/02/13 12:03:03 INFO TaskSetManager: Finished task 2.0 in stage 27.0 (TID 40) in 771 ms on 172.18.0.14 (executor 1) (2/3)
26/02/13 12:03:03 INFO TaskSetManager: Finished task 1.0 in stage 27.0 (TID 38) in 840 ms on 172.18.0.15 (executor 0) (3/3)
26/02/13 12:03:03 INFO TaskSchedulerImpl: Removed TaskSet 27.0, whose tasks have all completed, from pool 
26/02/13 12:03:03 INFO DAGScheduler: ResultStage 27 (start at NativeMethodAccessorImpl.java:0) finished in 0.863 s
26/02/13 12:03:03 INFO DAGScheduler: Job 16 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 12:03:03 INFO TaskSchedulerImpl: Killing all running tasks in stage 27: Stage finished
26/02/13 12:03:03 INFO DAGScheduler: Job 16 finished: start at NativeMethodAccessorImpl.java:0, took 0.870887 s
26/02/13 12:03:03 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 126, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@33c3a72f] is committing.
26/02/13 12:03:03 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 126, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@33c3a72f] committed.
26/02/13 12:03:03 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/126 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.126.3adbd414-a257-4914-a1ea-a12324404f3c.tmp
26/02/13 12:03:03 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.126.3adbd414-a257-4914-a1ea-a12324404f3c.tmp to file:/tmp/spark-checkpoint-enrichment/commits/126
26/02/13 12:03:03 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "4a161d66-225b-4b29-812d-ddec3c05364e",
  "name" : null,
  "timestamp" : "2026-02-13T12:03:02.278Z",
  "batchId" : 126,
  "numInputRows" : 92,
  "inputRowsPerSecond" : 7076.923076923077,
  "processedRowsPerSecond" : 61.37424949966644,
  "durationMs" : {
    "addBatch" : 1297,
    "commitOffsets" : 58,
    "getBatch" : 0,
    "latestOffset" : 2,
    "queryPlanning" : 67,
    "triggerExecution" : 1499,
    "walCommit" : 74
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 640,
        "1" : 790,
        "0" : 575
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 668,
        "1" : 827,
        "0" : 602
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 668,
        "1" : 827,
        "0" : 602
      }
    },
    "numInputRows" : 92,
    "inputRowsPerSecond" : 7076.923076923077,
    "processedRowsPerSecond" : 61.37424949966644,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@1a6a42c2",
    "numOutputRows" : 24
  }
}
26/02/13 12:03:03 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/127 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.127.592b0e0a-d6a4-46dc-a002-2d2c3860fb3c.tmp
26/02/13 12:03:03 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.127.592b0e0a-d6a4-46dc-a002-2d2c3860fb3c.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/127
26/02/13 12:03:03 INFO MicroBatchExecution: Committed offsets for batch 127. Metadata OffsetSeqMetadata(0,1770984183781,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 12:03:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:03:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:03:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:03:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:03:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:03:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:03:04 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#5367 - origin_code.nullCount#5366) > 0)
26/02/13 12:03:04 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#5372 - destination_code.nullCount#5371) > 0)
26/02/13 12:03:04 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#5402 - callsign.nullCount#5401) > 0)
26/02/13 12:03:04 INFO BlockManagerInfo: Removed broadcast_24_piece0 on spark-master:42073 in memory (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 12:03:04 INFO BlockManagerInfo: Removed broadcast_24_piece0 on 172.18.0.14:37571 in memory (size: 19.5 KiB, free: 433.9 MiB)
26/02/13 12:03:04 INFO BlockManagerInfo: Removed broadcast_24_piece0 on 172.18.0.15:37717 in memory (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 12:03:04 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 12:03:04 INFO DAGScheduler: Got job 17 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 12:03:04 INFO BlockManagerInfo: Removed broadcast_23_piece0 on spark-master:42073 in memory (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 12:03:04 INFO DAGScheduler: Final stage: ResultStage 29 (start at NativeMethodAccessorImpl.java:0)
26/02/13 12:03:04 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 28)
26/02/13 12:03:04 INFO DAGScheduler: Missing parents: List()
26/02/13 12:03:04 INFO BlockManagerInfo: Removed broadcast_23_piece0 on 172.18.0.14:37571 in memory (size: 107.2 KiB, free: 434.0 MiB)
26/02/13 12:03:04 INFO BlockManagerInfo: Removed broadcast_23_piece0 on 172.18.0.15:37717 in memory (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 12:03:04 INFO DAGScheduler: Submitting ResultStage 29 (MapPartitionsRDD[87] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:03:04 INFO MemoryStore: Block broadcast_25 stored as values in memory (estimated size 99.3 KiB, free 433.8 MiB)
26/02/13 12:03:04 INFO BlockManagerInfo: Removed broadcast_22_piece0 on spark-master:42073 in memory (size: 33.3 KiB, free: 434.3 MiB)
26/02/13 12:03:04 INFO MemoryStore: Block broadcast_25_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 433.9 MiB)
26/02/13 12:03:04 INFO BlockManagerInfo: Added broadcast_25_piece0 in memory on spark-master:42073 (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 12:03:04 INFO SparkContext: Created broadcast 25 from broadcast at DAGScheduler.scala:1585
26/02/13 12:03:04 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 29 (MapPartitionsRDD[87] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 12:03:04 INFO TaskSchedulerImpl: Adding task set 29.0 with 4 tasks resource profile 0
26/02/13 12:03:04 INFO BlockManagerInfo: Removed broadcast_22_piece0 on 172.18.0.14:37571 in memory (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 12:03:04 INFO TaskSetManager: Starting task 0.0 in stage 29.0 (TID 41) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:03:04 INFO TaskSetManager: Starting task 1.0 in stage 29.0 (TID 42) (172.18.0.14, executor 1, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:03:04 INFO BlockManagerInfo: Added broadcast_25_piece0 in memory on 172.18.0.14:37571 (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 12:03:04 INFO TaskSetManager: Starting task 2.0 in stage 29.0 (TID 43) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:03:04 INFO TaskSetManager: Starting task 3.0 in stage 29.0 (TID 44) (172.18.0.14, executor 1, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:03:04 INFO TaskSetManager: Finished task 0.0 in stage 29.0 (TID 41) in 38 ms on 172.18.0.14 (executor 1) (1/4)
26/02/13 12:03:04 INFO TaskSetManager: Finished task 1.0 in stage 29.0 (TID 42) in 38 ms on 172.18.0.14 (executor 1) (2/4)
26/02/13 12:03:04 INFO TaskSetManager: Finished task 3.0 in stage 29.0 (TID 44) in 28 ms on 172.18.0.14 (executor 1) (3/4)
26/02/13 12:03:04 INFO TaskSetManager: Finished task 2.0 in stage 29.0 (TID 43) in 34 ms on 172.18.0.14 (executor 1) (4/4)
26/02/13 12:03:04 INFO TaskSchedulerImpl: Removed TaskSet 29.0, whose tasks have all completed, from pool 
26/02/13 12:03:04 INFO DAGScheduler: ResultStage 29 (start at NativeMethodAccessorImpl.java:0) finished in 0.091 s
26/02/13 12:03:04 INFO DAGScheduler: Job 17 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 12:03:04 INFO TaskSchedulerImpl: Killing all running tasks in stage 29: Stage finished
26/02/13 12:03:04 INFO DAGScheduler: Job 17 finished: start at NativeMethodAccessorImpl.java:0, took 0.106390 s
26/02/13 12:03:04 INFO MemoryStore: Block broadcast_26_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.8 MiB)
26/02/13 12:03:04 INFO BlockManagerInfo: Added broadcast_26_piece0 in memory on spark-master:42073 (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 12:03:04 INFO SparkContext: Created broadcast 26 from start at NativeMethodAccessorImpl.java:0
26/02/13 12:03:04 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 127, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@3e0fc296]. The input RDD has 3 partitions.
26/02/13 12:03:04 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 12:03:04 INFO DAGScheduler: Got job 18 (start at NativeMethodAccessorImpl.java:0) with 3 output partitions
26/02/13 12:03:04 INFO DAGScheduler: Final stage: ResultStage 30 (start at NativeMethodAccessorImpl.java:0)
26/02/13 12:03:04 INFO DAGScheduler: Parents of final stage: List()
26/02/13 12:03:04 INFO DAGScheduler: Missing parents: List()
26/02/13 12:03:04 INFO DAGScheduler: Submitting ResultStage 30 (MapPartitionsRDD[93] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:03:04 INFO MemoryStore: Block broadcast_27 stored as values in memory (estimated size 49.9 KiB, free 433.8 MiB)
26/02/13 12:03:04 INFO MemoryStore: Block broadcast_27_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.8 MiB)
26/02/13 12:03:04 INFO BlockManagerInfo: Added broadcast_27_piece0 in memory on spark-master:42073 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 12:03:04 INFO SparkContext: Created broadcast 27 from broadcast at DAGScheduler.scala:1585
26/02/13 12:03:04 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 30 (MapPartitionsRDD[93] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
26/02/13 12:03:04 INFO TaskSchedulerImpl: Adding task set 30.0 with 3 tasks resource profile 0
26/02/13 12:03:04 INFO TaskSetManager: Starting task 1.0 in stage 30.0 (TID 45) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:03:04 INFO TaskSetManager: Starting task 0.0 in stage 30.0 (TID 46) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:03:04 INFO TaskSetManager: Starting task 2.0 in stage 30.0 (TID 47) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:03:04 INFO BlockManagerInfo: Added broadcast_27_piece0 in memory on 172.18.0.14:37571 (size: 19.5 KiB, free: 434.0 MiB)
26/02/13 12:03:04 INFO BlockManagerInfo: Added broadcast_27_piece0 in memory on 172.18.0.15:37717 (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 12:03:04 INFO BlockManagerInfo: Added broadcast_26_piece0 in memory on 172.18.0.14:37571 (size: 107.2 KiB, free: 433.9 MiB)
26/02/13 12:03:04 INFO BlockManagerInfo: Added broadcast_26_piece0 in memory on 172.18.0.15:37717 (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 12:03:04 INFO TaskSetManager: Finished task 0.0 in stage 30.0 (TID 46) in 294 ms on 172.18.0.14 (executor 1) (1/3)
26/02/13 12:03:04 INFO TaskSetManager: Finished task 2.0 in stage 30.0 (TID 47) in 294 ms on 172.18.0.14 (executor 1) (2/3)
26/02/13 12:03:04 INFO TaskSetManager: Finished task 1.0 in stage 30.0 (TID 45) in 410 ms on 172.18.0.15 (executor 0) (3/3)
26/02/13 12:03:04 INFO TaskSchedulerImpl: Removed TaskSet 30.0, whose tasks have all completed, from pool 
26/02/13 12:03:04 INFO DAGScheduler: ResultStage 30 (start at NativeMethodAccessorImpl.java:0) finished in 0.423 s
26/02/13 12:03:04 INFO DAGScheduler: Job 18 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 12:03:04 INFO TaskSchedulerImpl: Killing all running tasks in stage 30: Stage finished
26/02/13 12:03:04 INFO DAGScheduler: Job 18 finished: start at NativeMethodAccessorImpl.java:0, took 0.430318 s
26/02/13 12:03:04 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 127, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@3e0fc296] is committing.
26/02/13 12:03:04 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 127, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@3e0fc296] committed.
26/02/13 12:03:04 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/127 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.127.09af33dc-3526-497e-aa43-50265210b0ec.tmp
26/02/13 12:03:04 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.127.09af33dc-3526-497e-aa43-50265210b0ec.tmp to file:/tmp/spark-checkpoint-enrichment/commits/127
26/02/13 12:03:04 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "4a161d66-225b-4b29-812d-ddec3c05364e",
  "name" : null,
  "timestamp" : "2026-02-13T12:03:03.779Z",
  "batchId" : 127,
  "numInputRows" : 155,
  "inputRowsPerSecond" : 103.26449033977349,
  "processedRowsPerSecond" : 166.13076098606643,
  "durationMs" : {
    "addBatch" : 730,
    "commitOffsets" : 90,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 54,
    "triggerExecution" : 933,
    "walCommit" : 57
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 668,
        "1" : 827,
        "0" : 602
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 719,
        "1" : 884,
        "0" : 649
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 719,
        "1" : 884,
        "0" : 649
      }
    },
    "numInputRows" : 155,
    "inputRowsPerSecond" : 103.26449033977349,
    "processedRowsPerSecond" : 166.13076098606643,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@1a6a42c2",
    "numOutputRows" : 27
  }
}
26/02/13 12:03:11 INFO BlockManagerInfo: Removed broadcast_27_piece0 on spark-master:42073 in memory (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 12:03:11 INFO BlockManagerInfo: Removed broadcast_27_piece0 on 172.18.0.14:37571 in memory (size: 19.5 KiB, free: 433.9 MiB)
26/02/13 12:03:11 INFO BlockManagerInfo: Removed broadcast_27_piece0 on 172.18.0.15:37717 in memory (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 12:03:11 INFO BlockManagerInfo: Removed broadcast_25_piece0 on spark-master:42073 in memory (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 12:03:11 INFO BlockManagerInfo: Removed broadcast_25_piece0 on 172.18.0.14:37571 in memory (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 12:03:14 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:03:22 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/128 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.128.4596734c-4530-401a-aaa2-d00635b37f1a.tmp
26/02/13 12:03:22 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.128.4596734c-4530-401a-aaa2-d00635b37f1a.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/128
26/02/13 12:03:22 INFO MicroBatchExecution: Committed offsets for batch 128. Metadata OffsetSeqMetadata(0,1770984202786,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 12:03:22 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:03:22 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:03:22 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:03:22 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:03:23 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:03:23 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:03:23 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#6221 - origin_code.nullCount#6220) > 0)
26/02/13 12:03:23 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#6226 - destination_code.nullCount#6225) > 0)
26/02/13 12:03:23 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#6256 - callsign.nullCount#6255) > 0)
26/02/13 12:03:23 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 12:03:23 INFO DAGScheduler: Got job 19 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 12:03:23 INFO DAGScheduler: Final stage: ResultStage 32 (start at NativeMethodAccessorImpl.java:0)
26/02/13 12:03:23 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 31)
26/02/13 12:03:23 INFO DAGScheduler: Missing parents: List()
26/02/13 12:03:23 INFO DAGScheduler: Submitting ResultStage 32 (MapPartitionsRDD[98] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:03:23 INFO MemoryStore: Block broadcast_28 stored as values in memory (estimated size 99.3 KiB, free 433.9 MiB)
26/02/13 12:03:23 INFO MemoryStore: Block broadcast_28_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 433.8 MiB)
26/02/13 12:03:23 INFO BlockManagerInfo: Added broadcast_28_piece0 in memory on spark-master:42073 (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 12:03:23 INFO BlockManagerInfo: Removed broadcast_20_piece0 on spark-master:42073 in memory (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 12:03:23 INFO BlockManagerInfo: Removed broadcast_20_piece0 on 172.18.0.15:37717 in memory (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 12:03:23 INFO SparkContext: Created broadcast 28 from broadcast at DAGScheduler.scala:1585
26/02/13 12:03:23 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 32 (MapPartitionsRDD[98] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 12:03:23 INFO BlockManagerInfo: Removed broadcast_20_piece0 on 172.18.0.14:37571 in memory (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 12:03:23 INFO TaskSchedulerImpl: Adding task set 32.0 with 4 tasks resource profile 0
26/02/13 12:03:23 INFO TaskSetManager: Starting task 0.0 in stage 32.0 (TID 48) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:03:23 INFO TaskSetManager: Starting task 1.0 in stage 32.0 (TID 49) (172.18.0.14, executor 1, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:03:23 INFO BlockManagerInfo: Added broadcast_28_piece0 in memory on 172.18.0.14:37571 (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 12:03:23 INFO TaskSetManager: Starting task 2.0 in stage 32.0 (TID 50) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:03:23 INFO TaskSetManager: Finished task 0.0 in stage 32.0 (TID 48) in 35 ms on 172.18.0.14 (executor 1) (1/4)
26/02/13 12:03:23 INFO TaskSetManager: Starting task 3.0 in stage 32.0 (TID 51) (172.18.0.14, executor 1, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:03:23 INFO TaskSetManager: Finished task 1.0 in stage 32.0 (TID 49) in 38 ms on 172.18.0.14 (executor 1) (2/4)
26/02/13 12:03:23 INFO TaskSetManager: Finished task 2.0 in stage 32.0 (TID 50) in 31 ms on 172.18.0.14 (executor 1) (3/4)
26/02/13 12:03:23 INFO TaskSetManager: Finished task 3.0 in stage 32.0 (TID 51) in 34 ms on 172.18.0.14 (executor 1) (4/4)
26/02/13 12:03:23 INFO TaskSchedulerImpl: Removed TaskSet 32.0, whose tasks have all completed, from pool 
26/02/13 12:03:23 INFO DAGScheduler: ResultStage 32 (start at NativeMethodAccessorImpl.java:0) finished in 0.094 s
26/02/13 12:03:23 INFO DAGScheduler: Job 19 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 12:03:23 INFO TaskSchedulerImpl: Killing all running tasks in stage 32: Stage finished
26/02/13 12:03:23 INFO DAGScheduler: Job 19 finished: start at NativeMethodAccessorImpl.java:0, took 0.099901 s
26/02/13 12:03:23 INFO MemoryStore: Block broadcast_29_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.8 MiB)
26/02/13 12:03:23 INFO BlockManagerInfo: Added broadcast_29_piece0 in memory on spark-master:42073 (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 12:03:23 INFO SparkContext: Created broadcast 29 from start at NativeMethodAccessorImpl.java:0
26/02/13 12:03:23 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 128, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@6d78afe3]. The input RDD has 2 partitions.
26/02/13 12:03:23 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 12:03:23 INFO DAGScheduler: Got job 20 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/02/13 12:03:23 INFO DAGScheduler: Final stage: ResultStage 33 (start at NativeMethodAccessorImpl.java:0)
26/02/13 12:03:23 INFO DAGScheduler: Parents of final stage: List()
26/02/13 12:03:23 INFO DAGScheduler: Missing parents: List()
26/02/13 12:03:23 INFO DAGScheduler: Submitting ResultStage 33 (MapPartitionsRDD[104] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:03:23 INFO MemoryStore: Block broadcast_30 stored as values in memory (estimated size 49.9 KiB, free 433.8 MiB)
26/02/13 12:03:23 INFO MemoryStore: Block broadcast_30_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.8 MiB)
26/02/13 12:03:23 INFO BlockManagerInfo: Added broadcast_30_piece0 in memory on spark-master:42073 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 12:03:23 INFO SparkContext: Created broadcast 30 from broadcast at DAGScheduler.scala:1585
26/02/13 12:03:23 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 33 (MapPartitionsRDD[104] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/02/13 12:03:23 INFO TaskSchedulerImpl: Adding task set 33.0 with 2 tasks resource profile 0
26/02/13 12:03:23 INFO TaskSetManager: Starting task 1.0 in stage 33.0 (TID 52) (172.18.0.14, executor 1, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:03:23 INFO TaskSetManager: Starting task 0.0 in stage 33.0 (TID 53) (172.18.0.15, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:03:23 INFO BlockManagerInfo: Added broadcast_30_piece0 in memory on 172.18.0.14:37571 (size: 19.5 KiB, free: 434.0 MiB)
26/02/13 12:03:23 INFO BlockManagerInfo: Added broadcast_30_piece0 in memory on 172.18.0.15:37717 (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 12:03:23 INFO BlockManagerInfo: Added broadcast_29_piece0 in memory on 172.18.0.14:37571 (size: 107.2 KiB, free: 433.9 MiB)
26/02/13 12:03:23 INFO BlockManagerInfo: Added broadcast_29_piece0 in memory on 172.18.0.15:37717 (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 12:03:23 INFO TaskSetManager: Finished task 1.0 in stage 33.0 (TID 52) in 598 ms on 172.18.0.14 (executor 1) (1/2)
26/02/13 12:03:23 INFO TaskSetManager: Finished task 0.0 in stage 33.0 (TID 53) in 636 ms on 172.18.0.15 (executor 0) (2/2)
26/02/13 12:03:23 INFO TaskSchedulerImpl: Removed TaskSet 33.0, whose tasks have all completed, from pool 
26/02/13 12:03:23 INFO DAGScheduler: ResultStage 33 (start at NativeMethodAccessorImpl.java:0) finished in 0.644 s
26/02/13 12:03:23 INFO DAGScheduler: Job 20 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 12:03:23 INFO TaskSchedulerImpl: Killing all running tasks in stage 33: Stage finished
26/02/13 12:03:23 INFO DAGScheduler: Job 20 finished: start at NativeMethodAccessorImpl.java:0, took 0.651158 s
26/02/13 12:03:23 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 128, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@6d78afe3] is committing.
26/02/13 12:03:23 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 128, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@6d78afe3] committed.
26/02/13 12:03:23 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/128 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.128.f4b741cf-f48e-4094-98ce-f53c541e3632.tmp
26/02/13 12:03:23 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.128.f4b741cf-f48e-4094-98ce-f53c541e3632.tmp to file:/tmp/spark-checkpoint-enrichment/commits/128
26/02/13 12:03:23 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "4a161d66-225b-4b29-812d-ddec3c05364e",
  "name" : null,
  "timestamp" : "2026-02-13T12:03:22.784Z",
  "batchId" : 128,
  "numInputRows" : 3,
  "inputRowsPerSecond" : 230.76923076923077,
  "processedRowsPerSecond" : 2.6785714285714284,
  "durationMs" : {
    "addBatch" : 897,
    "commitOffsets" : 65,
    "getBatch" : 0,
    "latestOffset" : 2,
    "queryPlanning" : 61,
    "triggerExecution" : 1120,
    "walCommit" : 94
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 719,
        "1" : 884,
        "0" : 649
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 719,
        "1" : 886,
        "0" : 650
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 719,
        "1" : 886,
        "0" : 650
      }
    },
    "numInputRows" : 3,
    "inputRowsPerSecond" : 230.76923076923077,
    "processedRowsPerSecond" : 2.6785714285714284,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@1a6a42c2",
    "numOutputRows" : 0
  }
}
26/02/13 12:03:23 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/129 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.129.49ef9758-b8ee-4296-8494-936528032cfe.tmp
26/02/13 12:03:23 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.129.49ef9758-b8ee-4296-8494-936528032cfe.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/129
26/02/13 12:03:23 INFO MicroBatchExecution: Committed offsets for batch 129. Metadata OffsetSeqMetadata(0,1770984203908,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 12:03:24 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:03:24 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:03:24 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:03:24 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:03:24 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:03:24 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:03:24 INFO BlockManagerInfo: Removed broadcast_26_piece0 on spark-master:42073 in memory (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 12:03:24 INFO BlockManagerInfo: Removed broadcast_26_piece0 on 172.18.0.15:37717 in memory (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 12:03:24 INFO BlockManagerInfo: Removed broadcast_26_piece0 on 172.18.0.14:37571 in memory (size: 107.2 KiB, free: 434.0 MiB)
26/02/13 12:03:24 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#7075 - origin_code.nullCount#7074) > 0)
26/02/13 12:03:24 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#7080 - destination_code.nullCount#7079) > 0)
26/02/13 12:03:24 INFO BlockManagerInfo: Removed broadcast_28_piece0 on spark-master:42073 in memory (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 12:03:24 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#7110 - callsign.nullCount#7109) > 0)
26/02/13 12:03:24 INFO BlockManagerInfo: Removed broadcast_28_piece0 on 172.18.0.14:37571 in memory (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 12:03:24 INFO BlockManagerInfo: Removed broadcast_30_piece0 on spark-master:42073 in memory (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 12:03:24 INFO BlockManagerInfo: Removed broadcast_30_piece0 on 172.18.0.14:37571 in memory (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 12:03:24 INFO BlockManagerInfo: Removed broadcast_30_piece0 on 172.18.0.15:37717 in memory (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 12:03:24 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 12:03:24 INFO BlockManagerInfo: Removed broadcast_29_piece0 on spark-master:42073 in memory (size: 107.2 KiB, free: 434.4 MiB)
26/02/13 12:03:24 INFO DAGScheduler: Got job 21 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 12:03:24 INFO DAGScheduler: Final stage: ResultStage 35 (start at NativeMethodAccessorImpl.java:0)
26/02/13 12:03:24 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 34)
26/02/13 12:03:24 INFO DAGScheduler: Missing parents: List()
26/02/13 12:03:24 INFO BlockManagerInfo: Removed broadcast_29_piece0 on 172.18.0.14:37571 in memory (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 12:03:24 INFO DAGScheduler: Submitting ResultStage 35 (MapPartitionsRDD[109] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:03:24 INFO BlockManagerInfo: Removed broadcast_29_piece0 on 172.18.0.15:37717 in memory (size: 107.2 KiB, free: 434.4 MiB)
26/02/13 12:03:24 INFO MemoryStore: Block broadcast_31 stored as values in memory (estimated size 99.3 KiB, free 434.1 MiB)
26/02/13 12:03:24 INFO MemoryStore: Block broadcast_31_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 434.0 MiB)
26/02/13 12:03:24 INFO BlockManagerInfo: Added broadcast_31_piece0 in memory on spark-master:42073 (size: 33.3 KiB, free: 434.3 MiB)
26/02/13 12:03:24 INFO SparkContext: Created broadcast 31 from broadcast at DAGScheduler.scala:1585
26/02/13 12:03:24 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 35 (MapPartitionsRDD[109] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 12:03:24 INFO TaskSchedulerImpl: Adding task set 35.0 with 4 tasks resource profile 0
26/02/13 12:03:24 INFO TaskSetManager: Starting task 0.0 in stage 35.0 (TID 54) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:03:24 INFO TaskSetManager: Starting task 1.0 in stage 35.0 (TID 55) (172.18.0.14, executor 1, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:03:24 INFO BlockManagerInfo: Added broadcast_31_piece0 in memory on 172.18.0.14:37571 (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 12:03:24 INFO TaskSetManager: Starting task 2.0 in stage 35.0 (TID 56) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:03:24 INFO TaskSetManager: Starting task 3.0 in stage 35.0 (TID 57) (172.18.0.14, executor 1, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:03:24 INFO TaskSetManager: Finished task 0.0 in stage 35.0 (TID 54) in 43 ms on 172.18.0.14 (executor 1) (1/4)
26/02/13 12:03:24 INFO TaskSetManager: Finished task 1.0 in stage 35.0 (TID 55) in 44 ms on 172.18.0.14 (executor 1) (2/4)
26/02/13 12:03:24 INFO TaskSetManager: Finished task 2.0 in stage 35.0 (TID 56) in 51 ms on 172.18.0.14 (executor 1) (3/4)
26/02/13 12:03:24 INFO TaskSetManager: Finished task 3.0 in stage 35.0 (TID 57) in 51 ms on 172.18.0.14 (executor 1) (4/4)
26/02/13 12:03:24 INFO TaskSchedulerImpl: Removed TaskSet 35.0, whose tasks have all completed, from pool 
26/02/13 12:03:24 INFO DAGScheduler: ResultStage 35 (start at NativeMethodAccessorImpl.java:0) finished in 0.105 s
26/02/13 12:03:24 INFO DAGScheduler: Job 21 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 12:03:24 INFO TaskSchedulerImpl: Killing all running tasks in stage 35: Stage finished
26/02/13 12:03:24 INFO DAGScheduler: Job 21 finished: start at NativeMethodAccessorImpl.java:0, took 0.113237 s
26/02/13 12:03:24 INFO MemoryStore: Block broadcast_32_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.9 MiB)
26/02/13 12:03:24 INFO BlockManagerInfo: Added broadcast_32_piece0 in memory on spark-master:42073 (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 12:03:24 INFO SparkContext: Created broadcast 32 from start at NativeMethodAccessorImpl.java:0
26/02/13 12:03:24 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 129, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@38c24c61]. The input RDD has 3 partitions.
26/02/13 12:03:24 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 12:03:24 INFO DAGScheduler: Got job 22 (start at NativeMethodAccessorImpl.java:0) with 3 output partitions
26/02/13 12:03:24 INFO DAGScheduler: Final stage: ResultStage 36 (start at NativeMethodAccessorImpl.java:0)
26/02/13 12:03:24 INFO DAGScheduler: Parents of final stage: List()
26/02/13 12:03:24 INFO DAGScheduler: Missing parents: List()
26/02/13 12:03:24 INFO DAGScheduler: Submitting ResultStage 36 (MapPartitionsRDD[115] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:03:24 INFO MemoryStore: Block broadcast_33 stored as values in memory (estimated size 49.9 KiB, free 433.9 MiB)
26/02/13 12:03:24 INFO MemoryStore: Block broadcast_33_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.9 MiB)
26/02/13 12:03:24 INFO BlockManagerInfo: Added broadcast_33_piece0 in memory on spark-master:42073 (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 12:03:24 INFO SparkContext: Created broadcast 33 from broadcast at DAGScheduler.scala:1585
26/02/13 12:03:24 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 36 (MapPartitionsRDD[115] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
26/02/13 12:03:24 INFO TaskSchedulerImpl: Adding task set 36.0 with 3 tasks resource profile 0
26/02/13 12:03:24 INFO TaskSetManager: Starting task 0.0 in stage 36.0 (TID 58) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:03:24 INFO TaskSetManager: Starting task 1.0 in stage 36.0 (TID 59) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:03:24 INFO TaskSetManager: Starting task 2.0 in stage 36.0 (TID 60) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:03:24 INFO BlockManagerInfo: Added broadcast_33_piece0 in memory on 172.18.0.14:37571 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 12:03:24 INFO BlockManagerInfo: Added broadcast_33_piece0 in memory on 172.18.0.15:37717 (size: 19.5 KiB, free: 434.4 MiB)
26/02/13 12:03:24 INFO BlockManagerInfo: Added broadcast_32_piece0 in memory on 172.18.0.14:37571 (size: 107.2 KiB, free: 434.0 MiB)
26/02/13 12:03:24 INFO BlockManagerInfo: Added broadcast_32_piece0 in memory on 172.18.0.15:37717 (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 12:03:24 INFO TaskSetManager: Finished task 2.0 in stage 36.0 (TID 60) in 191 ms on 172.18.0.14 (executor 1) (1/3)
26/02/13 12:03:24 INFO TaskSetManager: Finished task 1.0 in stage 36.0 (TID 59) in 251 ms on 172.18.0.15 (executor 0) (2/3)
26/02/13 12:03:24 INFO TaskSetManager: Finished task 0.0 in stage 36.0 (TID 58) in 661 ms on 172.18.0.14 (executor 1) (3/3)
26/02/13 12:03:24 INFO TaskSchedulerImpl: Removed TaskSet 36.0, whose tasks have all completed, from pool 
26/02/13 12:03:24 INFO DAGScheduler: ResultStage 36 (start at NativeMethodAccessorImpl.java:0) finished in 0.670 s
26/02/13 12:03:24 INFO DAGScheduler: Job 22 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 12:03:24 INFO TaskSchedulerImpl: Killing all running tasks in stage 36: Stage finished
26/02/13 12:03:24 INFO DAGScheduler: Job 22 finished: start at NativeMethodAccessorImpl.java:0, took 0.673729 s
26/02/13 12:03:24 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 129, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@38c24c61] is committing.
26/02/13 12:03:24 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 129, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@38c24c61] committed.
26/02/13 12:03:24 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/129 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.129.30f3e7ac-d50c-43c4-8883-60a45e96b5cc.tmp
26/02/13 12:03:25 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.129.30f3e7ac-d50c-43c4-8883-60a45e96b5cc.tmp to file:/tmp/spark-checkpoint-enrichment/commits/129
26/02/13 12:03:25 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "4a161d66-225b-4b29-812d-ddec3c05364e",
  "name" : null,
  "timestamp" : "2026-02-13T12:03:23.906Z",
  "batchId" : 129,
  "numInputRows" : 243,
  "inputRowsPerSecond" : 216.57754010695186,
  "processedRowsPerSecond" : 219.11632100991883,
  "durationMs" : {
    "addBatch" : 945,
    "commitOffsets" : 60,
    "getBatch" : 0,
    "latestOffset" : 2,
    "queryPlanning" : 45,
    "triggerExecution" : 1109,
    "walCommit" : 56
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 719,
        "1" : 886,
        "0" : 650
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 798,
        "1" : 978,
        "0" : 722
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 798,
        "1" : 978,
        "0" : 722
      }
    },
    "numInputRows" : 243,
    "inputRowsPerSecond" : 216.57754010695186,
    "processedRowsPerSecond" : 219.11632100991883,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@1a6a42c2",
    "numOutputRows" : 51
  }
}
26/02/13 12:03:31 INFO BlockManagerInfo: Removed broadcast_31_piece0 on spark-master:42073 in memory (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 12:03:31 INFO BlockManagerInfo: Removed broadcast_31_piece0 on 172.18.0.14:37571 in memory (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 12:03:31 INFO BlockManagerInfo: Removed broadcast_33_piece0 on spark-master:42073 in memory (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 12:03:31 INFO BlockManagerInfo: Removed broadcast_33_piece0 on 172.18.0.14:37571 in memory (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 12:03:31 INFO BlockManagerInfo: Removed broadcast_33_piece0 on 172.18.0.15:37717 in memory (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 12:03:35 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:03:39 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/130 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.130.57411147-dd0c-46d9-8d32-ecad8d46b58a.tmp
26/02/13 12:03:39 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.130.57411147-dd0c-46d9-8d32-ecad8d46b58a.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/130
26/02/13 12:03:39 INFO MicroBatchExecution: Committed offsets for batch 130. Metadata OffsetSeqMetadata(0,1770984219345,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 12:03:39 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:03:39 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:03:39 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:03:39 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:03:39 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:03:39 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:03:39 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#7929 - origin_code.nullCount#7928) > 0)
26/02/13 12:03:39 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#7934 - destination_code.nullCount#7933) > 0)
26/02/13 12:03:39 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#7964 - callsign.nullCount#7963) > 0)
26/02/13 12:03:39 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 12:03:39 INFO DAGScheduler: Got job 23 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 12:03:39 INFO DAGScheduler: Final stage: ResultStage 38 (start at NativeMethodAccessorImpl.java:0)
26/02/13 12:03:39 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 37)
26/02/13 12:03:39 INFO DAGScheduler: Missing parents: List()
26/02/13 12:03:39 INFO DAGScheduler: Submitting ResultStage 38 (MapPartitionsRDD[120] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:03:39 INFO MemoryStore: Block broadcast_34 stored as values in memory (estimated size 99.3 KiB, free 434.0 MiB)
26/02/13 12:03:39 INFO MemoryStore: Block broadcast_34_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 433.9 MiB)
26/02/13 12:03:39 INFO BlockManagerInfo: Added broadcast_34_piece0 in memory on spark-master:42073 (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 12:03:39 INFO SparkContext: Created broadcast 34 from broadcast at DAGScheduler.scala:1585
26/02/13 12:03:39 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 38 (MapPartitionsRDD[120] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 12:03:39 INFO TaskSchedulerImpl: Adding task set 38.0 with 4 tasks resource profile 0
26/02/13 12:03:39 INFO TaskSetManager: Starting task 0.0 in stage 38.0 (TID 61) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:03:39 INFO TaskSetManager: Starting task 1.0 in stage 38.0 (TID 62) (172.18.0.14, executor 1, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:03:39 INFO BlockManagerInfo: Added broadcast_34_piece0 in memory on 172.18.0.14:37571 (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 12:03:39 INFO TaskSetManager: Starting task 2.0 in stage 38.0 (TID 63) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:03:39 INFO TaskSetManager: Finished task 0.0 in stage 38.0 (TID 61) in 38 ms on 172.18.0.14 (executor 1) (1/4)
26/02/13 12:03:39 INFO TaskSetManager: Starting task 3.0 in stage 38.0 (TID 64) (172.18.0.14, executor 1, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:03:39 INFO TaskSetManager: Finished task 1.0 in stage 38.0 (TID 62) in 43 ms on 172.18.0.14 (executor 1) (2/4)
26/02/13 12:03:39 INFO TaskSetManager: Finished task 2.0 in stage 38.0 (TID 63) in 32 ms on 172.18.0.14 (executor 1) (3/4)
26/02/13 12:03:39 INFO TaskSetManager: Finished task 3.0 in stage 38.0 (TID 64) in 38 ms on 172.18.0.14 (executor 1) (4/4)
26/02/13 12:03:39 INFO TaskSchedulerImpl: Removed TaskSet 38.0, whose tasks have all completed, from pool 
26/02/13 12:03:39 INFO DAGScheduler: ResultStage 38 (start at NativeMethodAccessorImpl.java:0) finished in 0.089 s
26/02/13 12:03:39 INFO DAGScheduler: Job 23 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 12:03:39 INFO TaskSchedulerImpl: Killing all running tasks in stage 38: Stage finished
26/02/13 12:03:39 INFO DAGScheduler: Job 23 finished: start at NativeMethodAccessorImpl.java:0, took 0.093127 s
26/02/13 12:03:39 INFO MemoryStore: Block broadcast_35_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.8 MiB)
26/02/13 12:03:39 INFO BlockManagerInfo: Added broadcast_35_piece0 in memory on spark-master:42073 (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 12:03:39 INFO SparkContext: Created broadcast 35 from start at NativeMethodAccessorImpl.java:0
26/02/13 12:03:39 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 130, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@2d4bd70c]. The input RDD has 3 partitions.
26/02/13 12:03:39 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 12:03:39 INFO DAGScheduler: Got job 24 (start at NativeMethodAccessorImpl.java:0) with 3 output partitions
26/02/13 12:03:39 INFO DAGScheduler: Final stage: ResultStage 39 (start at NativeMethodAccessorImpl.java:0)
26/02/13 12:03:39 INFO DAGScheduler: Parents of final stage: List()
26/02/13 12:03:39 INFO DAGScheduler: Missing parents: List()
26/02/13 12:03:39 INFO DAGScheduler: Submitting ResultStage 39 (MapPartitionsRDD[126] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:03:39 INFO MemoryStore: Block broadcast_36 stored as values in memory (estimated size 49.9 KiB, free 433.8 MiB)
26/02/13 12:03:39 INFO MemoryStore: Block broadcast_36_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.8 MiB)
26/02/13 12:03:39 INFO BlockManagerInfo: Added broadcast_36_piece0 in memory on spark-master:42073 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 12:03:39 INFO SparkContext: Created broadcast 36 from broadcast at DAGScheduler.scala:1585
26/02/13 12:03:39 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 39 (MapPartitionsRDD[126] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
26/02/13 12:03:39 INFO TaskSchedulerImpl: Adding task set 39.0 with 3 tasks resource profile 0
26/02/13 12:03:39 INFO TaskSetManager: Starting task 1.0 in stage 39.0 (TID 65) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:03:39 INFO TaskSetManager: Starting task 0.0 in stage 39.0 (TID 66) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:03:39 INFO TaskSetManager: Starting task 2.0 in stage 39.0 (TID 67) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:03:39 INFO BlockManagerInfo: Added broadcast_36_piece0 in memory on 172.18.0.14:37571 (size: 19.5 KiB, free: 434.0 MiB)
26/02/13 12:03:39 INFO BlockManagerInfo: Added broadcast_36_piece0 in memory on 172.18.0.15:37717 (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 12:03:39 INFO BlockManagerInfo: Added broadcast_35_piece0 in memory on 172.18.0.14:37571 (size: 107.2 KiB, free: 433.9 MiB)
26/02/13 12:03:39 INFO BlockManagerInfo: Added broadcast_35_piece0 in memory on 172.18.0.15:37717 (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 12:03:40 INFO TaskSetManager: Finished task 0.0 in stage 39.0 (TID 66) in 613 ms on 172.18.0.14 (executor 1) (1/3)
26/02/13 12:03:40 INFO TaskSetManager: Finished task 2.0 in stage 39.0 (TID 67) in 615 ms on 172.18.0.14 (executor 1) (2/3)
26/02/13 12:03:40 INFO TaskSetManager: Finished task 1.0 in stage 39.0 (TID 65) in 661 ms on 172.18.0.15 (executor 0) (3/3)
26/02/13 12:03:40 INFO TaskSchedulerImpl: Removed TaskSet 39.0, whose tasks have all completed, from pool 
26/02/13 12:03:40 INFO DAGScheduler: ResultStage 39 (start at NativeMethodAccessorImpl.java:0) finished in 0.667 s
26/02/13 12:03:40 INFO DAGScheduler: Job 24 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 12:03:40 INFO TaskSchedulerImpl: Killing all running tasks in stage 39: Stage finished
26/02/13 12:03:40 INFO DAGScheduler: Job 24 finished: start at NativeMethodAccessorImpl.java:0, took 0.669798 s
26/02/13 12:03:40 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 130, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@2d4bd70c] is committing.
26/02/13 12:03:40 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 130, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@2d4bd70c] committed.
26/02/13 12:03:40 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/130 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.130.ced3af03-e13a-4c80-98de-719bbc6ae5f9.tmp
26/02/13 12:03:40 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.130.ced3af03-e13a-4c80-98de-719bbc6ae5f9.tmp to file:/tmp/spark-checkpoint-enrichment/commits/130
26/02/13 12:03:40 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "4a161d66-225b-4b29-812d-ddec3c05364e",
  "name" : null,
  "timestamp" : "2026-02-13T12:03:39.343Z",
  "batchId" : 130,
  "numInputRows" : 92,
  "inputRowsPerSecond" : 7666.666666666666,
  "processedRowsPerSecond" : 81.9964349376114,
  "durationMs" : {
    "addBatch" : 919,
    "commitOffsets" : 60,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 50,
    "triggerExecution" : 1122,
    "walCommit" : 89
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 798,
        "1" : 978,
        "0" : 722
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 826,
        "1" : 1015,
        "0" : 749
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 826,
        "1" : 1015,
        "0" : 749
      }
    },
    "numInputRows" : 92,
    "inputRowsPerSecond" : 7666.666666666666,
    "processedRowsPerSecond" : 81.9964349376114,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@1a6a42c2",
    "numOutputRows" : 24
  }
}
26/02/13 12:03:40 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/131 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.131.90d851bb-7b71-49be-a020-051a5ea363d4.tmp
26/02/13 12:03:40 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.131.90d851bb-7b71-49be-a020-051a5ea363d4.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/131
26/02/13 12:03:40 INFO MicroBatchExecution: Committed offsets for batch 131. Metadata OffsetSeqMetadata(0,1770984220467,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 12:03:40 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:03:40 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:03:40 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:03:40 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:03:40 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:03:40 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:03:40 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#8783 - origin_code.nullCount#8782) > 0)
26/02/13 12:03:40 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#8788 - destination_code.nullCount#8787) > 0)
26/02/13 12:03:40 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#8818 - callsign.nullCount#8817) > 0)
26/02/13 12:03:40 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 12:03:40 INFO DAGScheduler: Got job 25 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 12:03:40 INFO DAGScheduler: Final stage: ResultStage 41 (start at NativeMethodAccessorImpl.java:0)
26/02/13 12:03:40 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 40)
26/02/13 12:03:40 INFO DAGScheduler: Missing parents: List()
26/02/13 12:03:40 INFO DAGScheduler: Submitting ResultStage 41 (MapPartitionsRDD[131] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:03:40 INFO MemoryStore: Block broadcast_37 stored as values in memory (estimated size 99.3 KiB, free 433.7 MiB)
26/02/13 12:03:40 INFO MemoryStore: Block broadcast_37_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 433.6 MiB)
26/02/13 12:03:40 INFO BlockManagerInfo: Added broadcast_37_piece0 in memory on spark-master:42073 (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 12:03:40 INFO SparkContext: Created broadcast 37 from broadcast at DAGScheduler.scala:1585
26/02/13 12:03:40 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 41 (MapPartitionsRDD[131] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 12:03:40 INFO TaskSchedulerImpl: Adding task set 41.0 with 4 tasks resource profile 0
26/02/13 12:03:40 INFO TaskSetManager: Starting task 0.0 in stage 41.0 (TID 68) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:03:40 INFO TaskSetManager: Starting task 1.0 in stage 41.0 (TID 69) (172.18.0.14, executor 1, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:03:40 INFO BlockManagerInfo: Added broadcast_37_piece0 in memory on 172.18.0.14:37571 (size: 33.3 KiB, free: 433.9 MiB)
26/02/13 12:03:40 INFO TaskSetManager: Starting task 2.0 in stage 41.0 (TID 70) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:03:40 INFO TaskSetManager: Starting task 3.0 in stage 41.0 (TID 71) (172.18.0.14, executor 1, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:03:40 INFO TaskSetManager: Finished task 0.0 in stage 41.0 (TID 68) in 36 ms on 172.18.0.14 (executor 1) (1/4)
26/02/13 12:03:40 INFO TaskSetManager: Finished task 1.0 in stage 41.0 (TID 69) in 35 ms on 172.18.0.14 (executor 1) (2/4)
26/02/13 12:03:40 INFO TaskSetManager: Finished task 2.0 in stage 41.0 (TID 70) in 29 ms on 172.18.0.14 (executor 1) (3/4)
26/02/13 12:03:40 INFO TaskSetManager: Finished task 3.0 in stage 41.0 (TID 71) in 36 ms on 172.18.0.14 (executor 1) (4/4)
26/02/13 12:03:40 INFO TaskSchedulerImpl: Removed TaskSet 41.0, whose tasks have all completed, from pool 
26/02/13 12:03:40 INFO DAGScheduler: ResultStage 41 (start at NativeMethodAccessorImpl.java:0) finished in 0.081 s
26/02/13 12:03:40 INFO DAGScheduler: Job 25 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 12:03:40 INFO TaskSchedulerImpl: Killing all running tasks in stage 41: Stage finished
26/02/13 12:03:40 INFO DAGScheduler: Job 25 finished: start at NativeMethodAccessorImpl.java:0, took 0.086412 s
26/02/13 12:03:40 INFO BlockManagerInfo: Removed broadcast_36_piece0 on spark-master:42073 in memory (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 12:03:40 INFO BlockManagerInfo: Removed broadcast_36_piece0 on 172.18.0.15:37717 in memory (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 12:03:40 INFO BlockManagerInfo: Removed broadcast_36_piece0 on 172.18.0.14:37571 in memory (size: 19.5 KiB, free: 433.9 MiB)
26/02/13 12:03:40 INFO MemoryStore: Block broadcast_38_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.6 MiB)
26/02/13 12:03:40 INFO BlockManagerInfo: Added broadcast_38_piece0 in memory on spark-master:42073 (size: 107.2 KiB, free: 434.0 MiB)
26/02/13 12:03:40 INFO SparkContext: Created broadcast 38 from start at NativeMethodAccessorImpl.java:0
26/02/13 12:03:40 INFO BlockManagerInfo: Removed broadcast_35_piece0 on spark-master:42073 in memory (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 12:03:40 INFO BlockManagerInfo: Removed broadcast_35_piece0 on 172.18.0.14:37571 in memory (size: 107.2 KiB, free: 434.0 MiB)
26/02/13 12:03:40 INFO BlockManagerInfo: Removed broadcast_35_piece0 on 172.18.0.15:37717 in memory (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 12:03:40 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 131, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@13edfac4]. The input RDD has 3 partitions.
26/02/13 12:03:40 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 12:03:40 INFO DAGScheduler: Got job 26 (start at NativeMethodAccessorImpl.java:0) with 3 output partitions
26/02/13 12:03:40 INFO DAGScheduler: Final stage: ResultStage 42 (start at NativeMethodAccessorImpl.java:0)
26/02/13 12:03:40 INFO DAGScheduler: Parents of final stage: List()
26/02/13 12:03:40 INFO DAGScheduler: Missing parents: List()
26/02/13 12:03:40 INFO DAGScheduler: Submitting ResultStage 42 (MapPartitionsRDD[137] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:03:40 INFO MemoryStore: Block broadcast_39 stored as values in memory (estimated size 49.9 KiB, free 433.7 MiB)
26/02/13 12:03:40 INFO BlockManagerInfo: Removed broadcast_37_piece0 on spark-master:42073 in memory (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 12:03:40 INFO BlockManagerInfo: Removed broadcast_37_piece0 on 172.18.0.14:37571 in memory (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 12:03:40 INFO MemoryStore: Block broadcast_39_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.7 MiB)
26/02/13 12:03:40 INFO BlockManagerInfo: Added broadcast_39_piece0 in memory on spark-master:42073 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 12:03:40 INFO SparkContext: Created broadcast 39 from broadcast at DAGScheduler.scala:1585
26/02/13 12:03:40 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 42 (MapPartitionsRDD[137] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
26/02/13 12:03:40 INFO TaskSchedulerImpl: Adding task set 42.0 with 3 tasks resource profile 0
26/02/13 12:03:40 INFO TaskSetManager: Starting task 0.0 in stage 42.0 (TID 72) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:03:40 INFO TaskSetManager: Starting task 1.0 in stage 42.0 (TID 73) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:03:40 INFO TaskSetManager: Starting task 2.0 in stage 42.0 (TID 74) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:03:40 INFO BlockManagerInfo: Removed broadcast_34_piece0 on spark-master:42073 in memory (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 12:03:40 INFO BlockManagerInfo: Removed broadcast_34_piece0 on 172.18.0.14:37571 in memory (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 12:03:40 INFO BlockManagerInfo: Added broadcast_39_piece0 in memory on 172.18.0.14:37571 (size: 19.5 KiB, free: 434.0 MiB)
26/02/13 12:03:40 INFO BlockManagerInfo: Added broadcast_39_piece0 in memory on 172.18.0.15:37717 (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 12:03:40 INFO BlockManagerInfo: Added broadcast_38_piece0 in memory on 172.18.0.14:37571 (size: 107.2 KiB, free: 433.9 MiB)
26/02/13 12:03:40 INFO BlockManagerInfo: Added broadcast_38_piece0 in memory on 172.18.0.15:37717 (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 12:03:41 INFO TaskSetManager: Finished task 0.0 in stage 42.0 (TID 72) in 216 ms on 172.18.0.14 (executor 1) (1/3)
26/02/13 12:03:41 INFO TaskSetManager: Finished task 2.0 in stage 42.0 (TID 74) in 230 ms on 172.18.0.14 (executor 1) (2/3)
26/02/13 12:03:41 INFO TaskSetManager: Finished task 1.0 in stage 42.0 (TID 73) in 343 ms on 172.18.0.15 (executor 0) (3/3)
26/02/13 12:03:41 INFO TaskSchedulerImpl: Removed TaskSet 42.0, whose tasks have all completed, from pool 
26/02/13 12:03:41 INFO DAGScheduler: ResultStage 42 (start at NativeMethodAccessorImpl.java:0) finished in 0.364 s
26/02/13 12:03:41 INFO DAGScheduler: Job 26 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 12:03:41 INFO TaskSchedulerImpl: Killing all running tasks in stage 42: Stage finished
26/02/13 12:03:41 INFO DAGScheduler: Job 26 finished: start at NativeMethodAccessorImpl.java:0, took 0.379009 s
26/02/13 12:03:41 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 131, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@13edfac4] is committing.
26/02/13 12:03:41 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 131, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@13edfac4] committed.
26/02/13 12:03:41 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/131 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.131.2a4a0e2b-0fe1-482f-a073-dc7126e071bc.tmp
26/02/13 12:03:41 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.131.2a4a0e2b-0fe1-482f-a073-dc7126e071bc.tmp to file:/tmp/spark-checkpoint-enrichment/commits/131
26/02/13 12:03:41 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "4a161d66-225b-4b29-812d-ddec3c05364e",
  "name" : null,
  "timestamp" : "2026-02-13T12:03:40.466Z",
  "batchId" : 131,
  "numInputRows" : 154,
  "inputRowsPerSecond" : 137.1326803205699,
  "processedRowsPerSecond" : 183.7708830548926,
  "durationMs" : {
    "addBatch" : 635,
    "commitOffsets" : 85,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 58,
    "triggerExecution" : 838,
    "walCommit" : 57
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 826,
        "1" : 1015,
        "0" : 749
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 878,
        "1" : 1071,
        "0" : 795
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 878,
        "1" : 1071,
        "0" : 795
      }
    },
    "numInputRows" : 154,
    "inputRowsPerSecond" : 137.1326803205699,
    "processedRowsPerSecond" : 183.7708830548926,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@1a6a42c2",
    "numOutputRows" : 28
  }
}
26/02/13 12:03:50 INFO BlockManagerInfo: Removed broadcast_39_piece0 on spark-master:42073 in memory (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 12:03:50 INFO BlockManagerInfo: Removed broadcast_39_piece0 on 172.18.0.14:37571 in memory (size: 19.5 KiB, free: 434.0 MiB)
26/02/13 12:03:50 INFO BlockManagerInfo: Removed broadcast_39_piece0 on 172.18.0.15:37717 in memory (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 12:03:51 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:03:57 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/132 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.132.b59efa9f-2a99-4da7-b754-87a4b502119b.tmp
26/02/13 12:03:57 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.132.b59efa9f-2a99-4da7-b754-87a4b502119b.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/132
26/02/13 12:03:57 INFO MicroBatchExecution: Committed offsets for batch 132. Metadata OffsetSeqMetadata(0,1770984237566,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 12:03:57 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:03:57 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:03:57 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:03:57 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:03:57 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:03:57 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:03:57 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#9637 - origin_code.nullCount#9636) > 0)
26/02/13 12:03:57 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#9642 - destination_code.nullCount#9641) > 0)
26/02/13 12:03:57 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#9672 - callsign.nullCount#9671) > 0)
26/02/13 12:03:57 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 12:03:57 INFO DAGScheduler: Got job 27 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 12:03:57 INFO DAGScheduler: Final stage: ResultStage 44 (start at NativeMethodAccessorImpl.java:0)
26/02/13 12:03:57 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 43)
26/02/13 12:03:57 INFO DAGScheduler: Missing parents: List()
26/02/13 12:03:57 INFO DAGScheduler: Submitting ResultStage 44 (MapPartitionsRDD[142] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:03:57 INFO MemoryStore: Block broadcast_40 stored as values in memory (estimated size 99.3 KiB, free 433.9 MiB)
26/02/13 12:03:57 INFO MemoryStore: Block broadcast_40_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 433.8 MiB)
26/02/13 12:03:57 INFO BlockManagerInfo: Added broadcast_40_piece0 in memory on spark-master:42073 (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 12:03:57 INFO SparkContext: Created broadcast 40 from broadcast at DAGScheduler.scala:1585
26/02/13 12:03:57 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 44 (MapPartitionsRDD[142] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 12:03:57 INFO TaskSchedulerImpl: Adding task set 44.0 with 4 tasks resource profile 0
26/02/13 12:03:57 INFO TaskSetManager: Starting task 0.0 in stage 44.0 (TID 75) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:03:57 INFO TaskSetManager: Starting task 1.0 in stage 44.0 (TID 76) (172.18.0.14, executor 1, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:03:57 INFO BlockManagerInfo: Added broadcast_40_piece0 in memory on 172.18.0.14:37571 (size: 33.3 KiB, free: 433.9 MiB)
26/02/13 12:03:57 INFO TaskSetManager: Starting task 2.0 in stage 44.0 (TID 77) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:03:57 INFO TaskSetManager: Finished task 0.0 in stage 44.0 (TID 75) in 30 ms on 172.18.0.14 (executor 1) (1/4)
26/02/13 12:03:57 INFO TaskSetManager: Finished task 1.0 in stage 44.0 (TID 76) in 30 ms on 172.18.0.14 (executor 1) (2/4)
26/02/13 12:03:57 INFO TaskSetManager: Starting task 3.0 in stage 44.0 (TID 78) (172.18.0.14, executor 1, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:03:57 INFO TaskSetManager: Finished task 2.0 in stage 44.0 (TID 77) in 24 ms on 172.18.0.14 (executor 1) (3/4)
26/02/13 12:03:57 INFO TaskSetManager: Finished task 3.0 in stage 44.0 (TID 78) in 24 ms on 172.18.0.14 (executor 1) (4/4)
26/02/13 12:03:57 INFO TaskSchedulerImpl: Removed TaskSet 44.0, whose tasks have all completed, from pool 
26/02/13 12:03:57 INFO DAGScheduler: ResultStage 44 (start at NativeMethodAccessorImpl.java:0) finished in 0.066 s
26/02/13 12:03:57 INFO DAGScheduler: Job 27 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 12:03:57 INFO TaskSchedulerImpl: Killing all running tasks in stage 44: Stage finished
26/02/13 12:03:57 INFO DAGScheduler: Job 27 finished: start at NativeMethodAccessorImpl.java:0, took 0.069007 s
26/02/13 12:03:57 INFO MemoryStore: Block broadcast_41_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.7 MiB)
26/02/13 12:03:57 INFO BlockManagerInfo: Added broadcast_41_piece0 in memory on spark-master:42073 (size: 107.2 KiB, free: 434.0 MiB)
26/02/13 12:03:57 INFO SparkContext: Created broadcast 41 from start at NativeMethodAccessorImpl.java:0
26/02/13 12:03:57 INFO BlockManagerInfo: Removed broadcast_32_piece0 on spark-master:42073 in memory (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 12:03:57 INFO BlockManagerInfo: Removed broadcast_32_piece0 on 172.18.0.14:37571 in memory (size: 107.2 KiB, free: 434.0 MiB)
26/02/13 12:03:57 INFO BlockManagerInfo: Removed broadcast_32_piece0 on 172.18.0.15:37717 in memory (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 12:03:57 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 132, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@16fdcb56]. The input RDD has 3 partitions.
26/02/13 12:03:57 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 12:03:57 INFO DAGScheduler: Got job 28 (start at NativeMethodAccessorImpl.java:0) with 3 output partitions
26/02/13 12:03:57 INFO DAGScheduler: Final stage: ResultStage 45 (start at NativeMethodAccessorImpl.java:0)
26/02/13 12:03:57 INFO DAGScheduler: Parents of final stage: List()
26/02/13 12:03:57 INFO DAGScheduler: Missing parents: List()
26/02/13 12:03:57 INFO DAGScheduler: Submitting ResultStage 45 (MapPartitionsRDD[148] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:03:57 INFO MemoryStore: Block broadcast_42 stored as values in memory (estimated size 49.9 KiB, free 433.8 MiB)
26/02/13 12:03:57 INFO MemoryStore: Block broadcast_42_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.8 MiB)
26/02/13 12:03:57 INFO BlockManagerInfo: Added broadcast_42_piece0 in memory on spark-master:42073 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 12:03:57 INFO SparkContext: Created broadcast 42 from broadcast at DAGScheduler.scala:1585
26/02/13 12:03:57 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 45 (MapPartitionsRDD[148] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
26/02/13 12:03:57 INFO TaskSchedulerImpl: Adding task set 45.0 with 3 tasks resource profile 0
26/02/13 12:03:57 INFO TaskSetManager: Starting task 0.0 in stage 45.0 (TID 79) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:03:57 INFO TaskSetManager: Starting task 1.0 in stage 45.0 (TID 80) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:03:57 INFO TaskSetManager: Starting task 2.0 in stage 45.0 (TID 81) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:03:57 INFO BlockManagerInfo: Added broadcast_42_piece0 in memory on 172.18.0.15:37717 (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 12:03:57 INFO BlockManagerInfo: Added broadcast_42_piece0 in memory on 172.18.0.14:37571 (size: 19.5 KiB, free: 434.0 MiB)
26/02/13 12:03:57 INFO BlockManagerInfo: Added broadcast_41_piece0 in memory on 172.18.0.14:37571 (size: 107.2 KiB, free: 433.9 MiB)
26/02/13 12:03:58 INFO BlockManagerInfo: Added broadcast_41_piece0 in memory on 172.18.0.15:37717 (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 12:03:58 INFO TaskSetManager: Finished task 2.0 in stage 45.0 (TID 81) in 604 ms on 172.18.0.14 (executor 1) (1/3)
26/02/13 12:03:58 INFO TaskSetManager: Finished task 0.0 in stage 45.0 (TID 79) in 605 ms on 172.18.0.14 (executor 1) (2/3)
26/02/13 12:03:58 INFO TaskSetManager: Finished task 1.0 in stage 45.0 (TID 80) in 640 ms on 172.18.0.15 (executor 0) (3/3)
26/02/13 12:03:58 INFO TaskSchedulerImpl: Removed TaskSet 45.0, whose tasks have all completed, from pool 
26/02/13 12:03:58 INFO DAGScheduler: ResultStage 45 (start at NativeMethodAccessorImpl.java:0) finished in 0.648 s
26/02/13 12:03:58 INFO DAGScheduler: Job 28 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 12:03:58 INFO TaskSchedulerImpl: Killing all running tasks in stage 45: Stage finished
26/02/13 12:03:58 INFO DAGScheduler: Job 28 finished: start at NativeMethodAccessorImpl.java:0, took 0.650136 s
26/02/13 12:03:58 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 132, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@16fdcb56] is committing.
26/02/13 12:03:58 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 132, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@16fdcb56] committed.
26/02/13 12:03:58 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/132 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.132.3a24ea6f-d82c-4caa-bd45-bd05f0446efe.tmp
26/02/13 12:03:58 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.132.3a24ea6f-d82c-4caa-bd45-bd05f0446efe.tmp to file:/tmp/spark-checkpoint-enrichment/commits/132
26/02/13 12:03:58 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "4a161d66-225b-4b29-812d-ddec3c05364e",
  "name" : null,
  "timestamp" : "2026-02-13T12:03:57.564Z",
  "batchId" : 132,
  "numInputRows" : 111,
  "inputRowsPerSecond" : 10090.909090909092,
  "processedRowsPerSecond" : 102.58780036968577,
  "durationMs" : {
    "addBatch" : 863,
    "commitOffsets" : 53,
    "getBatch" : 0,
    "latestOffset" : 2,
    "queryPlanning" : 58,
    "triggerExecution" : 1082,
    "walCommit" : 106
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 878,
        "1" : 1071,
        "0" : 795
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 915,
        "1" : 1108,
        "0" : 832
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 915,
        "1" : 1108,
        "0" : 832
      }
    },
    "numInputRows" : 111,
    "inputRowsPerSecond" : 10090.909090909092,
    "processedRowsPerSecond" : 102.58780036968577,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@1a6a42c2",
    "numOutputRows" : 27
  }
}
26/02/13 12:03:58 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/133 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.133.cb6186e3-80b1-411b-b7f7-7ec7ffa26cea.tmp
26/02/13 12:03:58 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.133.cb6186e3-80b1-411b-b7f7-7ec7ffa26cea.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/133
26/02/13 12:03:58 INFO MicroBatchExecution: Committed offsets for batch 133. Metadata OffsetSeqMetadata(0,1770984238648,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 12:03:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:03:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:03:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:03:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:03:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:03:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:03:58 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#10491 - origin_code.nullCount#10490) > 0)
26/02/13 12:03:58 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#10496 - destination_code.nullCount#10495) > 0)
26/02/13 12:03:58 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#10526 - callsign.nullCount#10525) > 0)
26/02/13 12:03:58 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 12:03:58 INFO DAGScheduler: Got job 29 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 12:03:58 INFO DAGScheduler: Final stage: ResultStage 47 (start at NativeMethodAccessorImpl.java:0)
26/02/13 12:03:58 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 46)
26/02/13 12:03:58 INFO DAGScheduler: Missing parents: List()
26/02/13 12:03:58 INFO DAGScheduler: Submitting ResultStage 47 (MapPartitionsRDD[153] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:03:58 INFO MemoryStore: Block broadcast_43 stored as values in memory (estimated size 99.3 KiB, free 433.7 MiB)
26/02/13 12:03:58 INFO MemoryStore: Block broadcast_43_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 433.6 MiB)
26/02/13 12:03:58 INFO BlockManagerInfo: Added broadcast_43_piece0 in memory on spark-master:42073 (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 12:03:58 INFO SparkContext: Created broadcast 43 from broadcast at DAGScheduler.scala:1585
26/02/13 12:03:58 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 47 (MapPartitionsRDD[153] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 12:03:58 INFO TaskSchedulerImpl: Adding task set 47.0 with 4 tasks resource profile 0
26/02/13 12:03:58 INFO TaskSetManager: Starting task 0.0 in stage 47.0 (TID 82) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:03:58 INFO TaskSetManager: Starting task 1.0 in stage 47.0 (TID 83) (172.18.0.14, executor 1, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:03:58 INFO BlockManagerInfo: Added broadcast_43_piece0 in memory on 172.18.0.14:37571 (size: 33.3 KiB, free: 433.9 MiB)
26/02/13 12:03:58 INFO TaskSetManager: Starting task 2.0 in stage 47.0 (TID 84) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:03:58 INFO TaskSetManager: Finished task 1.0 in stage 47.0 (TID 83) in 26 ms on 172.18.0.14 (executor 1) (1/4)
26/02/13 12:03:58 INFO TaskSetManager: Starting task 3.0 in stage 47.0 (TID 85) (172.18.0.14, executor 1, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:03:58 INFO TaskSetManager: Finished task 0.0 in stage 47.0 (TID 82) in 30 ms on 172.18.0.14 (executor 1) (2/4)
26/02/13 12:03:58 INFO TaskSetManager: Finished task 2.0 in stage 47.0 (TID 84) in 17 ms on 172.18.0.14 (executor 1) (3/4)
26/02/13 12:03:58 INFO TaskSetManager: Finished task 3.0 in stage 47.0 (TID 85) in 17 ms on 172.18.0.14 (executor 1) (4/4)
26/02/13 12:03:58 INFO TaskSchedulerImpl: Removed TaskSet 47.0, whose tasks have all completed, from pool 
26/02/13 12:03:58 INFO DAGScheduler: ResultStage 47 (start at NativeMethodAccessorImpl.java:0) finished in 0.055 s
26/02/13 12:03:58 INFO DAGScheduler: Job 29 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 12:03:58 INFO TaskSchedulerImpl: Killing all running tasks in stage 47: Stage finished
26/02/13 12:03:58 INFO DAGScheduler: Job 29 finished: start at NativeMethodAccessorImpl.java:0, took 0.058999 s
26/02/13 12:03:58 INFO MemoryStore: Block broadcast_44_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.5 MiB)
26/02/13 12:03:58 INFO BlockManagerInfo: Added broadcast_44_piece0 in memory on spark-master:42073 (size: 107.2 KiB, free: 434.0 MiB)
26/02/13 12:03:58 INFO SparkContext: Created broadcast 44 from start at NativeMethodAccessorImpl.java:0
26/02/13 12:03:58 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 133, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@17ad2efe]. The input RDD has 3 partitions.
26/02/13 12:03:58 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 12:03:58 INFO DAGScheduler: Got job 30 (start at NativeMethodAccessorImpl.java:0) with 3 output partitions
26/02/13 12:03:58 INFO DAGScheduler: Final stage: ResultStage 48 (start at NativeMethodAccessorImpl.java:0)
26/02/13 12:03:58 INFO DAGScheduler: Parents of final stage: List()
26/02/13 12:03:58 INFO DAGScheduler: Missing parents: List()
26/02/13 12:03:58 INFO BlockManagerInfo: Removed broadcast_38_piece0 on spark-master:42073 in memory (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 12:03:58 INFO DAGScheduler: Submitting ResultStage 48 (MapPartitionsRDD[159] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:03:58 INFO BlockManagerInfo: Removed broadcast_38_piece0 on 172.18.0.14:37571 in memory (size: 107.2 KiB, free: 434.0 MiB)
26/02/13 12:03:58 INFO MemoryStore: Block broadcast_45 stored as values in memory (estimated size 49.9 KiB, free 433.6 MiB)
26/02/13 12:03:58 INFO BlockManagerInfo: Removed broadcast_38_piece0 on 172.18.0.15:37717 in memory (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 12:03:58 INFO MemoryStore: Block broadcast_45_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.6 MiB)
26/02/13 12:03:58 INFO BlockManagerInfo: Added broadcast_45_piece0 in memory on spark-master:42073 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 12:03:58 INFO SparkContext: Created broadcast 45 from broadcast at DAGScheduler.scala:1585
26/02/13 12:03:58 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 48 (MapPartitionsRDD[159] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
26/02/13 12:03:58 INFO TaskSchedulerImpl: Adding task set 48.0 with 3 tasks resource profile 0
26/02/13 12:03:58 INFO TaskSetManager: Starting task 1.0 in stage 48.0 (TID 86) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:03:58 INFO TaskSetManager: Starting task 0.0 in stage 48.0 (TID 87) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:03:58 INFO TaskSetManager: Starting task 2.0 in stage 48.0 (TID 88) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:03:58 INFO BlockManagerInfo: Removed broadcast_41_piece0 on spark-master:42073 in memory (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 12:03:58 INFO BlockManagerInfo: Removed broadcast_41_piece0 on 172.18.0.15:37717 in memory (size: 107.2 KiB, free: 434.4 MiB)
26/02/13 12:03:58 INFO BlockManagerInfo: Removed broadcast_41_piece0 on 172.18.0.14:37571 in memory (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 12:03:58 INFO BlockManagerInfo: Removed broadcast_43_piece0 on spark-master:42073 in memory (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 12:03:58 INFO BlockManagerInfo: Removed broadcast_43_piece0 on 172.18.0.14:37571 in memory (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 12:03:58 INFO BlockManagerInfo: Added broadcast_45_piece0 in memory on 172.18.0.15:37717 (size: 19.5 KiB, free: 434.4 MiB)
26/02/13 12:03:58 INFO BlockManagerInfo: Added broadcast_45_piece0 in memory on 172.18.0.14:37571 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 12:03:58 INFO BlockManagerInfo: Removed broadcast_40_piece0 on spark-master:42073 in memory (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 12:03:58 INFO BlockManagerInfo: Removed broadcast_40_piece0 on 172.18.0.14:37571 in memory (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 12:03:58 INFO BlockManagerInfo: Removed broadcast_42_piece0 on spark-master:42073 in memory (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 12:03:58 INFO BlockManagerInfo: Removed broadcast_42_piece0 on 172.18.0.15:37717 in memory (size: 19.5 KiB, free: 434.4 MiB)
26/02/13 12:03:58 INFO BlockManagerInfo: Removed broadcast_42_piece0 on 172.18.0.14:37571 in memory (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 12:03:58 INFO BlockManagerInfo: Added broadcast_44_piece0 in memory on 172.18.0.14:37571 (size: 107.2 KiB, free: 434.0 MiB)
26/02/13 12:03:58 INFO BlockManagerInfo: Added broadcast_44_piece0 in memory on 172.18.0.15:37717 (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 12:03:59 INFO TaskSetManager: Finished task 2.0 in stage 48.0 (TID 88) in 123 ms on 172.18.0.14 (executor 1) (1/3)
26/02/13 12:03:59 INFO TaskSetManager: Finished task 0.0 in stage 48.0 (TID 87) in 135 ms on 172.18.0.14 (executor 1) (2/3)
26/02/13 12:03:59 INFO TaskSetManager: Finished task 1.0 in stage 48.0 (TID 86) in 165 ms on 172.18.0.15 (executor 0) (3/3)
26/02/13 12:03:59 INFO TaskSchedulerImpl: Removed TaskSet 48.0, whose tasks have all completed, from pool 
26/02/13 12:03:59 INFO DAGScheduler: ResultStage 48 (start at NativeMethodAccessorImpl.java:0) finished in 0.176 s
26/02/13 12:03:59 INFO DAGScheduler: Job 30 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 12:03:59 INFO TaskSchedulerImpl: Killing all running tasks in stage 48: Stage finished
26/02/13 12:03:59 INFO DAGScheduler: Job 30 finished: start at NativeMethodAccessorImpl.java:0, took 0.179267 s
26/02/13 12:03:59 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 133, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@17ad2efe] is committing.
26/02/13 12:03:59 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 133, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@17ad2efe] committed.
26/02/13 12:03:59 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/133 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.133.8a120e6e-a345-4eb6-88bf-d06c3a32cfc4.tmp
26/02/13 12:03:59 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.133.8a120e6e-a345-4eb6-88bf-d06c3a32cfc4.tmp to file:/tmp/spark-checkpoint-enrichment/commits/133
26/02/13 12:03:59 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "4a161d66-225b-4b29-812d-ddec3c05364e",
  "name" : null,
  "timestamp" : "2026-02-13T12:03:58.647Z",
  "batchId" : 133,
  "numInputRows" : 133,
  "inputRowsPerSecond" : 122.80701754385966,
  "processedRowsPerSecond" : 264.9402390438247,
  "durationMs" : {
    "addBatch" : 349,
    "commitOffsets" : 64,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 33,
    "triggerExecution" : 502,
    "walCommit" : 54
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 915,
        "1" : 1108,
        "0" : 832
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 955,
        "1" : 1164,
        "0" : 869
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 955,
        "1" : 1164,
        "0" : 869
      }
    },
    "numInputRows" : 133,
    "inputRowsPerSecond" : 122.80701754385966,
    "processedRowsPerSecond" : 264.9402390438247,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@1a6a42c2",
    "numOutputRows" : 27
  }
}
26/02/13 12:03:59 INFO BlockManagerInfo: Removed broadcast_45_piece0 on spark-master:42073 in memory (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 12:03:59 INFO BlockManagerInfo: Removed broadcast_45_piece0 on 172.18.0.14:37571 in memory (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 12:03:59 INFO BlockManagerInfo: Removed broadcast_45_piece0 on 172.18.0.15:37717 in memory (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 12:04:09 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:04:13 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/134 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.134.5da774eb-91b1-4c0f-8fe2-03395c272df2.tmp
26/02/13 12:04:13 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.134.5da774eb-91b1-4c0f-8fe2-03395c272df2.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/134
26/02/13 12:04:13 INFO MicroBatchExecution: Committed offsets for batch 134. Metadata OffsetSeqMetadata(0,1770984253552,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 12:04:13 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:04:13 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:04:13 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:04:13 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:04:13 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:04:13 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:04:13 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#11345 - origin_code.nullCount#11344) > 0)
26/02/13 12:04:13 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#11350 - destination_code.nullCount#11349) > 0)
26/02/13 12:04:13 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#11380 - callsign.nullCount#11379) > 0)
26/02/13 12:04:13 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 12:04:13 INFO DAGScheduler: Got job 31 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 12:04:13 INFO DAGScheduler: Final stage: ResultStage 50 (start at NativeMethodAccessorImpl.java:0)
26/02/13 12:04:13 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 49)
26/02/13 12:04:13 INFO DAGScheduler: Missing parents: List()
26/02/13 12:04:13 INFO DAGScheduler: Submitting ResultStage 50 (MapPartitionsRDD[164] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:04:13 INFO MemoryStore: Block broadcast_46 stored as values in memory (estimated size 99.3 KiB, free 434.0 MiB)
26/02/13 12:04:13 INFO MemoryStore: Block broadcast_46_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 433.9 MiB)
26/02/13 12:04:13 INFO BlockManagerInfo: Added broadcast_46_piece0 in memory on spark-master:42073 (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 12:04:13 INFO SparkContext: Created broadcast 46 from broadcast at DAGScheduler.scala:1585
26/02/13 12:04:13 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 50 (MapPartitionsRDD[164] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 12:04:13 INFO TaskSchedulerImpl: Adding task set 50.0 with 4 tasks resource profile 0
26/02/13 12:04:13 INFO TaskSetManager: Starting task 0.0 in stage 50.0 (TID 89) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:04:13 INFO TaskSetManager: Starting task 1.0 in stage 50.0 (TID 90) (172.18.0.14, executor 1, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:04:13 INFO BlockManagerInfo: Added broadcast_46_piece0 in memory on 172.18.0.14:37571 (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 12:04:13 INFO TaskSetManager: Starting task 2.0 in stage 50.0 (TID 91) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:04:13 INFO TaskSetManager: Finished task 0.0 in stage 50.0 (TID 89) in 31 ms on 172.18.0.14 (executor 1) (1/4)
26/02/13 12:04:13 INFO TaskSetManager: Starting task 3.0 in stage 50.0 (TID 92) (172.18.0.14, executor 1, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:04:13 INFO TaskSetManager: Finished task 1.0 in stage 50.0 (TID 90) in 33 ms on 172.18.0.14 (executor 1) (2/4)
26/02/13 12:04:13 INFO TaskSetManager: Finished task 2.0 in stage 50.0 (TID 91) in 26 ms on 172.18.0.14 (executor 1) (3/4)
26/02/13 12:04:13 INFO TaskSetManager: Finished task 3.0 in stage 50.0 (TID 92) in 35 ms on 172.18.0.14 (executor 1) (4/4)
26/02/13 12:04:13 INFO TaskSchedulerImpl: Removed TaskSet 50.0, whose tasks have all completed, from pool 
26/02/13 12:04:13 INFO DAGScheduler: ResultStage 50 (start at NativeMethodAccessorImpl.java:0) finished in 0.080 s
26/02/13 12:04:13 INFO DAGScheduler: Job 31 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 12:04:13 INFO TaskSchedulerImpl: Killing all running tasks in stage 50: Stage finished
26/02/13 12:04:13 INFO BlockManagerInfo: Removed broadcast_44_piece0 on spark-master:42073 in memory (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 12:04:13 INFO DAGScheduler: Job 31 finished: start at NativeMethodAccessorImpl.java:0, took 0.085533 s
26/02/13 12:04:13 INFO BlockManagerInfo: Removed broadcast_44_piece0 on 172.18.0.14:37571 in memory (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 12:04:13 INFO BlockManagerInfo: Removed broadcast_44_piece0 on 172.18.0.15:37717 in memory (size: 107.2 KiB, free: 434.4 MiB)
26/02/13 12:04:13 INFO MemoryStore: Block broadcast_47_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.9 MiB)
26/02/13 12:04:13 INFO BlockManagerInfo: Added broadcast_47_piece0 in memory on spark-master:42073 (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 12:04:13 INFO SparkContext: Created broadcast 47 from start at NativeMethodAccessorImpl.java:0
26/02/13 12:04:13 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 134, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@509aef70]. The input RDD has 2 partitions.
26/02/13 12:04:13 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 12:04:13 INFO DAGScheduler: Got job 32 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/02/13 12:04:13 INFO DAGScheduler: Final stage: ResultStage 51 (start at NativeMethodAccessorImpl.java:0)
26/02/13 12:04:13 INFO DAGScheduler: Parents of final stage: List()
26/02/13 12:04:13 INFO DAGScheduler: Missing parents: List()
26/02/13 12:04:13 INFO DAGScheduler: Submitting ResultStage 51 (MapPartitionsRDD[170] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:04:13 INFO MemoryStore: Block broadcast_48 stored as values in memory (estimated size 49.9 KiB, free 433.9 MiB)
26/02/13 12:04:13 INFO MemoryStore: Block broadcast_48_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.9 MiB)
26/02/13 12:04:13 INFO BlockManagerInfo: Added broadcast_48_piece0 in memory on spark-master:42073 (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 12:04:13 INFO SparkContext: Created broadcast 48 from broadcast at DAGScheduler.scala:1585
26/02/13 12:04:13 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 51 (MapPartitionsRDD[170] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/02/13 12:04:13 INFO TaskSchedulerImpl: Adding task set 51.0 with 2 tasks resource profile 0
26/02/13 12:04:13 INFO TaskSetManager: Starting task 0.0 in stage 51.0 (TID 93) (172.18.0.15, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:04:13 INFO TaskSetManager: Starting task 1.0 in stage 51.0 (TID 94) (172.18.0.14, executor 1, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:04:13 INFO BlockManagerInfo: Added broadcast_48_piece0 in memory on 172.18.0.14:37571 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 12:04:13 INFO BlockManagerInfo: Added broadcast_48_piece0 in memory on 172.18.0.15:37717 (size: 19.5 KiB, free: 434.4 MiB)
26/02/13 12:04:13 INFO BlockManagerInfo: Added broadcast_47_piece0 in memory on 172.18.0.14:37571 (size: 107.2 KiB, free: 434.0 MiB)
26/02/13 12:04:13 INFO BlockManagerInfo: Added broadcast_47_piece0 in memory on 172.18.0.15:37717 (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 12:04:14 INFO TaskSetManager: Finished task 1.0 in stage 51.0 (TID 94) in 587 ms on 172.18.0.14 (executor 1) (1/2)
26/02/13 12:04:14 INFO TaskSetManager: Finished task 0.0 in stage 51.0 (TID 93) in 616 ms on 172.18.0.15 (executor 0) (2/2)
26/02/13 12:04:14 INFO TaskSchedulerImpl: Removed TaskSet 51.0, whose tasks have all completed, from pool 
26/02/13 12:04:14 INFO DAGScheduler: ResultStage 51 (start at NativeMethodAccessorImpl.java:0) finished in 0.624 s
26/02/13 12:04:14 INFO DAGScheduler: Job 32 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 12:04:14 INFO TaskSchedulerImpl: Killing all running tasks in stage 51: Stage finished
26/02/13 12:04:14 INFO DAGScheduler: Job 32 finished: start at NativeMethodAccessorImpl.java:0, took 0.630107 s
26/02/13 12:04:14 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 134, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@509aef70] is committing.
26/02/13 12:04:14 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 134, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@509aef70] committed.
26/02/13 12:04:14 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/134 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.134.7e1739ce-81c7-41b9-83d6-7c8f6e8c07f2.tmp
26/02/13 12:04:14 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.134.7e1739ce-81c7-41b9-83d6-7c8f6e8c07f2.tmp to file:/tmp/spark-checkpoint-enrichment/commits/134
26/02/13 12:04:14 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "4a161d66-225b-4b29-812d-ddec3c05364e",
  "name" : null,
  "timestamp" : "2026-02-13T12:04:13.550Z",
  "batchId" : 134,
  "numInputRows" : 7,
  "inputRowsPerSecond" : 583.3333333333334,
  "processedRowsPerSecond" : 6.673021925643471,
  "durationMs" : {
    "addBatch" : 844,
    "commitOffsets" : 61,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 58,
    "triggerExecution" : 1049,
    "walCommit" : 83
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 955,
        "1" : 1164,
        "0" : 869
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 955,
        "1" : 1169,
        "0" : 871
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 955,
        "1" : 1169,
        "0" : 871
      }
    },
    "numInputRows" : 7,
    "inputRowsPerSecond" : 583.3333333333334,
    "processedRowsPerSecond" : 6.673021925643471,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@1a6a42c2",
    "numOutputRows" : 1
  }
}
26/02/13 12:04:14 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/135 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.135.4d33cd74-4de0-4fcb-b9f6-064be5a58c1e.tmp
26/02/13 12:04:14 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.135.4d33cd74-4de0-4fcb-b9f6-064be5a58c1e.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/135
26/02/13 12:04:14 INFO MicroBatchExecution: Committed offsets for batch 135. Metadata OffsetSeqMetadata(0,1770984254602,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 12:04:14 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:04:14 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:04:14 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:04:14 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:04:14 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:04:14 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:04:14 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#12199 - origin_code.nullCount#12198) > 0)
26/02/13 12:04:14 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#12204 - destination_code.nullCount#12203) > 0)
26/02/13 12:04:14 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#12234 - callsign.nullCount#12233) > 0)
26/02/13 12:04:14 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 12:04:14 INFO DAGScheduler: Got job 33 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 12:04:14 INFO DAGScheduler: Final stage: ResultStage 53 (start at NativeMethodAccessorImpl.java:0)
26/02/13 12:04:14 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 52)
26/02/13 12:04:14 INFO DAGScheduler: Missing parents: List()
26/02/13 12:04:14 INFO DAGScheduler: Submitting ResultStage 53 (MapPartitionsRDD[175] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:04:14 INFO MemoryStore: Block broadcast_49 stored as values in memory (estimated size 99.3 KiB, free 433.8 MiB)
26/02/13 12:04:14 INFO MemoryStore: Block broadcast_49_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 433.7 MiB)
26/02/13 12:04:14 INFO BlockManagerInfo: Added broadcast_49_piece0 in memory on spark-master:42073 (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 12:04:14 INFO SparkContext: Created broadcast 49 from broadcast at DAGScheduler.scala:1585
26/02/13 12:04:14 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 53 (MapPartitionsRDD[175] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 12:04:14 INFO TaskSchedulerImpl: Adding task set 53.0 with 4 tasks resource profile 0
26/02/13 12:04:14 INFO TaskSetManager: Starting task 0.0 in stage 53.0 (TID 95) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:04:14 INFO TaskSetManager: Starting task 1.0 in stage 53.0 (TID 96) (172.18.0.14, executor 1, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:04:14 INFO BlockManagerInfo: Added broadcast_49_piece0 in memory on 172.18.0.14:37571 (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 12:04:14 INFO TaskSetManager: Starting task 2.0 in stage 53.0 (TID 97) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:04:14 INFO TaskSetManager: Finished task 0.0 in stage 53.0 (TID 95) in 31 ms on 172.18.0.14 (executor 1) (1/4)
26/02/13 12:04:14 INFO TaskSetManager: Starting task 3.0 in stage 53.0 (TID 98) (172.18.0.14, executor 1, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:04:14 INFO TaskSetManager: Finished task 1.0 in stage 53.0 (TID 96) in 33 ms on 172.18.0.14 (executor 1) (2/4)
26/02/13 12:04:14 INFO TaskSetManager: Finished task 2.0 in stage 53.0 (TID 97) in 29 ms on 172.18.0.14 (executor 1) (3/4)
26/02/13 12:04:14 INFO TaskSetManager: Finished task 3.0 in stage 53.0 (TID 98) in 31 ms on 172.18.0.14 (executor 1) (4/4)
26/02/13 12:04:14 INFO TaskSchedulerImpl: Removed TaskSet 53.0, whose tasks have all completed, from pool 
26/02/13 12:04:14 INFO DAGScheduler: ResultStage 53 (start at NativeMethodAccessorImpl.java:0) finished in 0.071 s
26/02/13 12:04:14 INFO DAGScheduler: Job 33 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 12:04:14 INFO TaskSchedulerImpl: Killing all running tasks in stage 53: Stage finished
26/02/13 12:04:14 INFO DAGScheduler: Job 33 finished: start at NativeMethodAccessorImpl.java:0, took 0.073351 s
26/02/13 12:04:14 INFO MemoryStore: Block broadcast_50_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.6 MiB)
26/02/13 12:04:14 INFO BlockManagerInfo: Added broadcast_50_piece0 in memory on spark-master:42073 (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 12:04:14 INFO SparkContext: Created broadcast 50 from start at NativeMethodAccessorImpl.java:0
26/02/13 12:04:14 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 135, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@7d0763cf]. The input RDD has 3 partitions.
26/02/13 12:04:14 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 12:04:14 INFO DAGScheduler: Got job 34 (start at NativeMethodAccessorImpl.java:0) with 3 output partitions
26/02/13 12:04:14 INFO DAGScheduler: Final stage: ResultStage 54 (start at NativeMethodAccessorImpl.java:0)
26/02/13 12:04:14 INFO DAGScheduler: Parents of final stage: List()
26/02/13 12:04:14 INFO DAGScheduler: Missing parents: List()
26/02/13 12:04:14 INFO DAGScheduler: Submitting ResultStage 54 (MapPartitionsRDD[181] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:04:14 INFO MemoryStore: Block broadcast_51 stored as values in memory (estimated size 49.9 KiB, free 433.6 MiB)
26/02/13 12:04:14 INFO MemoryStore: Block broadcast_51_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.6 MiB)
26/02/13 12:04:14 INFO BlockManagerInfo: Added broadcast_51_piece0 in memory on spark-master:42073 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 12:04:14 INFO SparkContext: Created broadcast 51 from broadcast at DAGScheduler.scala:1585
26/02/13 12:04:14 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 54 (MapPartitionsRDD[181] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
26/02/13 12:04:14 INFO TaskSchedulerImpl: Adding task set 54.0 with 3 tasks resource profile 0
26/02/13 12:04:14 INFO TaskSetManager: Starting task 1.0 in stage 54.0 (TID 99) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:04:14 INFO TaskSetManager: Starting task 0.0 in stage 54.0 (TID 100) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:04:14 INFO TaskSetManager: Starting task 2.0 in stage 54.0 (TID 101) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:04:14 INFO BlockManagerInfo: Removed broadcast_46_piece0 on spark-master:42073 in memory (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 12:04:14 INFO BlockManagerInfo: Removed broadcast_46_piece0 on 172.18.0.14:37571 in memory (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 12:04:14 INFO BlockManagerInfo: Added broadcast_51_piece0 in memory on 172.18.0.14:37571 (size: 19.5 KiB, free: 434.0 MiB)
26/02/13 12:04:14 INFO BlockManagerInfo: Removed broadcast_47_piece0 on spark-master:42073 in memory (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 12:04:14 INFO BlockManagerInfo: Removed broadcast_47_piece0 on 172.18.0.14:37571 in memory (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 12:04:14 INFO BlockManagerInfo: Removed broadcast_47_piece0 on 172.18.0.15:37717 in memory (size: 107.2 KiB, free: 434.4 MiB)
26/02/13 12:04:14 INFO BlockManagerInfo: Added broadcast_51_piece0 in memory on 172.18.0.15:37717 (size: 19.5 KiB, free: 434.4 MiB)
26/02/13 12:04:14 INFO BlockManagerInfo: Removed broadcast_49_piece0 on spark-master:42073 in memory (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 12:04:14 INFO BlockManagerInfo: Removed broadcast_49_piece0 on 172.18.0.14:37571 in memory (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 12:04:14 INFO BlockManagerInfo: Added broadcast_50_piece0 in memory on 172.18.0.14:37571 (size: 107.2 KiB, free: 434.0 MiB)
26/02/13 12:04:14 INFO BlockManagerInfo: Removed broadcast_48_piece0 on spark-master:42073 in memory (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 12:04:14 INFO BlockManagerInfo: Removed broadcast_48_piece0 on 172.18.0.15:37717 in memory (size: 19.5 KiB, free: 434.4 MiB)
26/02/13 12:04:14 INFO BlockManagerInfo: Removed broadcast_48_piece0 on 172.18.0.14:37571 in memory (size: 19.5 KiB, free: 434.0 MiB)
26/02/13 12:04:14 INFO BlockManagerInfo: Added broadcast_50_piece0 in memory on 172.18.0.15:37717 (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 12:04:15 INFO TaskSetManager: Finished task 2.0 in stage 54.0 (TID 101) in 118 ms on 172.18.0.14 (executor 1) (1/3)
26/02/13 12:04:15 INFO TaskSetManager: Finished task 1.0 in stage 54.0 (TID 99) in 164 ms on 172.18.0.15 (executor 0) (2/3)
26/02/13 12:04:15 INFO TaskSetManager: Finished task 0.0 in stage 54.0 (TID 100) in 626 ms on 172.18.0.14 (executor 1) (3/3)
26/02/13 12:04:15 INFO TaskSchedulerImpl: Removed TaskSet 54.0, whose tasks have all completed, from pool 
26/02/13 12:04:15 INFO DAGScheduler: ResultStage 54 (start at NativeMethodAccessorImpl.java:0) finished in 0.638 s
26/02/13 12:04:15 INFO DAGScheduler: Job 34 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 12:04:15 INFO TaskSchedulerImpl: Killing all running tasks in stage 54: Stage finished
26/02/13 12:04:15 INFO DAGScheduler: Job 34 finished: start at NativeMethodAccessorImpl.java:0, took 0.640636 s
26/02/13 12:04:15 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 135, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@7d0763cf] is committing.
26/02/13 12:04:15 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 135, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@7d0763cf] committed.
26/02/13 12:04:15 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/135 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.135.7fcfe160-3934-4111-9757-3dd6542a6fa5.tmp
26/02/13 12:04:15 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.135.7fcfe160-3934-4111-9757-3dd6542a6fa5.tmp to file:/tmp/spark-checkpoint-enrichment/commits/135
26/02/13 12:04:15 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "4a161d66-225b-4b29-812d-ddec3c05364e",
  "name" : null,
  "timestamp" : "2026-02-13T12:04:14.600Z",
  "batchId" : 135,
  "numInputRows" : 234,
  "inputRowsPerSecond" : 222.85714285714286,
  "processedRowsPerSecond" : 236.12512613521696,
  "durationMs" : {
    "addBatch" : 846,
    "commitOffsets" : 56,
    "getBatch" : 0,
    "latestOffset" : 2,
    "queryPlanning" : 32,
    "triggerExecution" : 991,
    "walCommit" : 54
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 955,
        "1" : 1169,
        "0" : 871
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 1031,
        "1" : 1256,
        "0" : 942
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 1031,
        "1" : 1256,
        "0" : 942
      }
    },
    "numInputRows" : 234,
    "inputRowsPerSecond" : 222.85714285714286,
    "processedRowsPerSecond" : 236.12512613521696,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@1a6a42c2",
    "numOutputRows" : 52
  }
}
26/02/13 12:04:25 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:04:33 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/136 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.136.aa15f27b-6852-4d03-9d9e-9e7ad394d286.tmp
26/02/13 12:04:33 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.136.aa15f27b-6852-4d03-9d9e-9e7ad394d286.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/136
26/02/13 12:04:33 INFO MicroBatchExecution: Committed offsets for batch 136. Metadata OffsetSeqMetadata(0,1770984273715,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 12:04:33 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:04:33 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:04:33 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:04:33 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:04:33 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:04:33 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:04:33 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#13053 - origin_code.nullCount#13052) > 0)
26/02/13 12:04:33 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#13058 - destination_code.nullCount#13057) > 0)
26/02/13 12:04:33 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#13088 - callsign.nullCount#13087) > 0)
26/02/13 12:04:33 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 12:04:33 INFO DAGScheduler: Got job 35 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 12:04:33 INFO DAGScheduler: Final stage: ResultStage 56 (start at NativeMethodAccessorImpl.java:0)
26/02/13 12:04:33 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 55)
26/02/13 12:04:33 INFO DAGScheduler: Missing parents: List()
26/02/13 12:04:33 INFO DAGScheduler: Submitting ResultStage 56 (MapPartitionsRDD[186] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:04:33 INFO MemoryStore: Block broadcast_52 stored as values in memory (estimated size 99.3 KiB, free 433.9 MiB)
26/02/13 12:04:33 INFO MemoryStore: Block broadcast_52_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 433.9 MiB)
26/02/13 12:04:33 INFO BlockManagerInfo: Added broadcast_52_piece0 in memory on spark-master:42073 (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 12:04:33 INFO SparkContext: Created broadcast 52 from broadcast at DAGScheduler.scala:1585
26/02/13 12:04:33 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 56 (MapPartitionsRDD[186] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 12:04:33 INFO TaskSchedulerImpl: Adding task set 56.0 with 4 tasks resource profile 0
26/02/13 12:04:33 INFO TaskSetManager: Starting task 0.0 in stage 56.0 (TID 102) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:04:33 INFO TaskSetManager: Starting task 1.0 in stage 56.0 (TID 103) (172.18.0.14, executor 1, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:04:34 INFO BlockManagerInfo: Added broadcast_52_piece0 in memory on 172.18.0.14:37571 (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 12:04:34 INFO TaskSetManager: Starting task 2.0 in stage 56.0 (TID 104) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:04:34 INFO TaskSetManager: Finished task 0.0 in stage 56.0 (TID 102) in 23 ms on 172.18.0.14 (executor 1) (1/4)
26/02/13 12:04:34 INFO TaskSetManager: Starting task 3.0 in stage 56.0 (TID 105) (172.18.0.14, executor 1, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:04:34 INFO TaskSetManager: Finished task 1.0 in stage 56.0 (TID 103) in 25 ms on 172.18.0.14 (executor 1) (2/4)
26/02/13 12:04:34 INFO TaskSetManager: Finished task 3.0 in stage 56.0 (TID 105) in 19 ms on 172.18.0.14 (executor 1) (3/4)
26/02/13 12:04:34 INFO TaskSetManager: Finished task 2.0 in stage 56.0 (TID 104) in 23 ms on 172.18.0.14 (executor 1) (4/4)
26/02/13 12:04:34 INFO TaskSchedulerImpl: Removed TaskSet 56.0, whose tasks have all completed, from pool 
26/02/13 12:04:34 INFO DAGScheduler: ResultStage 56 (start at NativeMethodAccessorImpl.java:0) finished in 0.054 s
26/02/13 12:04:34 INFO DAGScheduler: Job 35 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 12:04:34 INFO TaskSchedulerImpl: Killing all running tasks in stage 56: Stage finished
26/02/13 12:04:34 INFO DAGScheduler: Job 35 finished: start at NativeMethodAccessorImpl.java:0, took 0.057589 s
26/02/13 12:04:34 INFO BlockManagerInfo: Removed broadcast_50_piece0 on spark-master:42073 in memory (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 12:04:34 INFO BlockManagerInfo: Removed broadcast_50_piece0 on 172.18.0.15:37717 in memory (size: 107.2 KiB, free: 434.4 MiB)
26/02/13 12:04:34 INFO BlockManagerInfo: Removed broadcast_50_piece0 on 172.18.0.14:37571 in memory (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 12:04:34 INFO MemoryStore: Block broadcast_53_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.9 MiB)
26/02/13 12:04:34 INFO BlockManagerInfo: Added broadcast_53_piece0 in memory on spark-master:42073 (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 12:04:34 INFO SparkContext: Created broadcast 53 from start at NativeMethodAccessorImpl.java:0
26/02/13 12:04:34 INFO BlockManagerInfo: Removed broadcast_51_piece0 on 172.18.0.15:37717 in memory (size: 19.5 KiB, free: 434.4 MiB)
26/02/13 12:04:34 INFO BlockManagerInfo: Removed broadcast_51_piece0 on spark-master:42073 in memory (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 12:04:34 INFO BlockManagerInfo: Removed broadcast_51_piece0 on 172.18.0.14:37571 in memory (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 12:04:34 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 136, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@f9fbf07]. The input RDD has 3 partitions.
26/02/13 12:04:34 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 12:04:34 INFO DAGScheduler: Got job 36 (start at NativeMethodAccessorImpl.java:0) with 3 output partitions
26/02/13 12:04:34 INFO DAGScheduler: Final stage: ResultStage 57 (start at NativeMethodAccessorImpl.java:0)
26/02/13 12:04:34 INFO DAGScheduler: Parents of final stage: List()
26/02/13 12:04:34 INFO DAGScheduler: Missing parents: List()
26/02/13 12:04:34 INFO DAGScheduler: Submitting ResultStage 57 (MapPartitionsRDD[192] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:04:34 INFO MemoryStore: Block broadcast_54 stored as values in memory (estimated size 49.9 KiB, free 433.9 MiB)
26/02/13 12:04:34 INFO MemoryStore: Block broadcast_54_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.9 MiB)
26/02/13 12:04:34 INFO BlockManagerInfo: Added broadcast_54_piece0 in memory on spark-master:42073 (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 12:04:34 INFO SparkContext: Created broadcast 54 from broadcast at DAGScheduler.scala:1585
26/02/13 12:04:34 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 57 (MapPartitionsRDD[192] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
26/02/13 12:04:34 INFO TaskSchedulerImpl: Adding task set 57.0 with 3 tasks resource profile 0
26/02/13 12:04:34 INFO TaskSetManager: Starting task 1.0 in stage 57.0 (TID 106) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:04:34 INFO TaskSetManager: Starting task 0.0 in stage 57.0 (TID 107) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:04:34 INFO TaskSetManager: Starting task 2.0 in stage 57.0 (TID 108) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:04:34 INFO BlockManagerInfo: Added broadcast_54_piece0 in memory on 172.18.0.14:37571 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 12:04:34 INFO BlockManagerInfo: Added broadcast_54_piece0 in memory on 172.18.0.15:37717 (size: 19.5 KiB, free: 434.4 MiB)
26/02/13 12:04:34 INFO BlockManagerInfo: Added broadcast_53_piece0 in memory on 172.18.0.14:37571 (size: 107.2 KiB, free: 434.0 MiB)
26/02/13 12:04:34 INFO BlockManagerInfo: Added broadcast_53_piece0 in memory on 172.18.0.15:37717 (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 12:04:34 INFO TaskSetManager: Finished task 2.0 in stage 57.0 (TID 108) in 594 ms on 172.18.0.14 (executor 1) (1/3)
26/02/13 12:04:34 INFO TaskSetManager: Finished task 0.0 in stage 57.0 (TID 107) in 595 ms on 172.18.0.14 (executor 1) (2/3)
26/02/13 12:04:34 INFO TaskSetManager: Finished task 1.0 in stage 57.0 (TID 106) in 629 ms on 172.18.0.15 (executor 0) (3/3)
26/02/13 12:04:34 INFO TaskSchedulerImpl: Removed TaskSet 57.0, whose tasks have all completed, from pool 
26/02/13 12:04:34 INFO DAGScheduler: ResultStage 57 (start at NativeMethodAccessorImpl.java:0) finished in 0.635 s
26/02/13 12:04:34 INFO DAGScheduler: Job 36 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 12:04:34 INFO TaskSchedulerImpl: Killing all running tasks in stage 57: Stage finished
26/02/13 12:04:34 INFO DAGScheduler: Job 36 finished: start at NativeMethodAccessorImpl.java:0, took 0.639067 s
26/02/13 12:04:34 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 136, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@f9fbf07] is committing.
26/02/13 12:04:34 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 136, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@f9fbf07] committed.
26/02/13 12:04:34 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/136 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.136.24b9d228-0a82-4a1b-a48c-867b10022de0.tmp
26/02/13 12:04:34 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.136.24b9d228-0a82-4a1b-a48c-867b10022de0.tmp to file:/tmp/spark-checkpoint-enrichment/commits/136
26/02/13 12:04:34 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "4a161d66-225b-4b29-812d-ddec3c05364e",
  "name" : null,
  "timestamp" : "2026-02-13T12:04:33.714Z",
  "batchId" : 136,
  "numInputRows" : 87,
  "inputRowsPerSecond" : 7250.0,
  "processedRowsPerSecond" : 81.15671641791045,
  "durationMs" : {
    "addBatch" : 826,
    "commitOffsets" : 68,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 57,
    "triggerExecution" : 1072,
    "walCommit" : 119
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 1031,
        "1" : 1256,
        "0" : 942
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 1058,
        "1" : 1291,
        "0" : 967
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 1058,
        "1" : 1291,
        "0" : 967
      }
    },
    "numInputRows" : 87,
    "inputRowsPerSecond" : 7250.0,
    "processedRowsPerSecond" : 81.15671641791045,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@1a6a42c2",
    "numOutputRows" : 23
  }
}
26/02/13 12:04:34 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/137 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.137.0c598489-e2a5-4c6e-94fd-47ab092dea18.tmp
26/02/13 12:04:34 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.137.0c598489-e2a5-4c6e-94fd-47ab092dea18.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/137
26/02/13 12:04:34 INFO MicroBatchExecution: Committed offsets for batch 137. Metadata OffsetSeqMetadata(0,1770984274788,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 12:04:34 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:04:34 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:04:34 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:04:34 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:04:34 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:04:34 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:04:34 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#13907 - origin_code.nullCount#13906) > 0)
26/02/13 12:04:34 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#13912 - destination_code.nullCount#13911) > 0)
26/02/13 12:04:34 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#13942 - callsign.nullCount#13941) > 0)
26/02/13 12:04:34 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 12:04:34 INFO DAGScheduler: Got job 37 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 12:04:34 INFO DAGScheduler: Final stage: ResultStage 59 (start at NativeMethodAccessorImpl.java:0)
26/02/13 12:04:34 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 58)
26/02/13 12:04:34 INFO DAGScheduler: Missing parents: List()
26/02/13 12:04:34 INFO DAGScheduler: Submitting ResultStage 59 (MapPartitionsRDD[197] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:04:34 INFO MemoryStore: Block broadcast_55 stored as values in memory (estimated size 99.3 KiB, free 433.8 MiB)
26/02/13 12:04:34 INFO MemoryStore: Block broadcast_55_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 433.7 MiB)
26/02/13 12:04:34 INFO BlockManagerInfo: Added broadcast_55_piece0 in memory on spark-master:42073 (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 12:04:34 INFO SparkContext: Created broadcast 55 from broadcast at DAGScheduler.scala:1585
26/02/13 12:04:34 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 59 (MapPartitionsRDD[197] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 12:04:34 INFO TaskSchedulerImpl: Adding task set 59.0 with 4 tasks resource profile 0
26/02/13 12:04:34 INFO TaskSetManager: Starting task 0.0 in stage 59.0 (TID 109) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:04:34 INFO TaskSetManager: Starting task 1.0 in stage 59.0 (TID 110) (172.18.0.14, executor 1, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:04:34 INFO BlockManagerInfo: Added broadcast_55_piece0 in memory on 172.18.0.14:37571 (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 12:04:34 INFO TaskSetManager: Starting task 2.0 in stage 59.0 (TID 111) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:04:34 INFO TaskSetManager: Finished task 0.0 in stage 59.0 (TID 109) in 19 ms on 172.18.0.14 (executor 1) (1/4)
26/02/13 12:04:34 INFO TaskSetManager: Starting task 3.0 in stage 59.0 (TID 112) (172.18.0.14, executor 1, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:04:34 INFO TaskSetManager: Finished task 1.0 in stage 59.0 (TID 110) in 29 ms on 172.18.0.14 (executor 1) (2/4)
26/02/13 12:04:34 INFO TaskSetManager: Finished task 2.0 in stage 59.0 (TID 111) in 16 ms on 172.18.0.14 (executor 1) (3/4)
26/02/13 12:04:35 INFO TaskSetManager: Finished task 3.0 in stage 59.0 (TID 112) in 17 ms on 172.18.0.14 (executor 1) (4/4)
26/02/13 12:04:35 INFO TaskSchedulerImpl: Removed TaskSet 59.0, whose tasks have all completed, from pool 
26/02/13 12:04:35 INFO DAGScheduler: ResultStage 59 (start at NativeMethodAccessorImpl.java:0) finished in 0.053 s
26/02/13 12:04:35 INFO DAGScheduler: Job 37 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 12:04:35 INFO TaskSchedulerImpl: Killing all running tasks in stage 59: Stage finished
26/02/13 12:04:35 INFO DAGScheduler: Job 37 finished: start at NativeMethodAccessorImpl.java:0, took 0.056754 s
26/02/13 12:04:35 INFO MemoryStore: Block broadcast_56_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.6 MiB)
26/02/13 12:04:35 INFO BlockManagerInfo: Added broadcast_56_piece0 in memory on spark-master:42073 (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 12:04:35 INFO SparkContext: Created broadcast 56 from start at NativeMethodAccessorImpl.java:0
26/02/13 12:04:35 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 137, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@77bf0d25]. The input RDD has 3 partitions.
26/02/13 12:04:35 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 12:04:35 INFO DAGScheduler: Got job 38 (start at NativeMethodAccessorImpl.java:0) with 3 output partitions
26/02/13 12:04:35 INFO DAGScheduler: Final stage: ResultStage 60 (start at NativeMethodAccessorImpl.java:0)
26/02/13 12:04:35 INFO DAGScheduler: Parents of final stage: List()
26/02/13 12:04:35 INFO DAGScheduler: Missing parents: List()
26/02/13 12:04:35 INFO DAGScheduler: Submitting ResultStage 60 (MapPartitionsRDD[203] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:04:35 INFO MemoryStore: Block broadcast_57 stored as values in memory (estimated size 49.9 KiB, free 433.6 MiB)
26/02/13 12:04:35 INFO MemoryStore: Block broadcast_57_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.6 MiB)
26/02/13 12:04:35 INFO BlockManagerInfo: Removed broadcast_55_piece0 on spark-master:42073 in memory (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 12:04:35 INFO BlockManagerInfo: Added broadcast_57_piece0 in memory on spark-master:42073 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 12:04:35 INFO SparkContext: Created broadcast 57 from broadcast at DAGScheduler.scala:1585
26/02/13 12:04:35 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 60 (MapPartitionsRDD[203] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
26/02/13 12:04:35 INFO TaskSchedulerImpl: Adding task set 60.0 with 3 tasks resource profile 0
26/02/13 12:04:35 INFO BlockManagerInfo: Removed broadcast_55_piece0 on 172.18.0.14:37571 in memory (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 12:04:35 INFO TaskSetManager: Starting task 0.0 in stage 60.0 (TID 113) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:04:35 INFO TaskSetManager: Starting task 1.0 in stage 60.0 (TID 114) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:04:35 INFO TaskSetManager: Starting task 2.0 in stage 60.0 (TID 115) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:04:35 INFO BlockManagerInfo: Removed broadcast_53_piece0 on spark-master:42073 in memory (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 12:04:35 INFO BlockManagerInfo: Removed broadcast_53_piece0 on 172.18.0.15:37717 in memory (size: 107.2 KiB, free: 434.4 MiB)
26/02/13 12:04:35 INFO BlockManagerInfo: Removed broadcast_53_piece0 on 172.18.0.14:37571 in memory (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 12:04:35 INFO BlockManagerInfo: Added broadcast_57_piece0 in memory on 172.18.0.14:37571 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 12:04:35 INFO BlockManagerInfo: Removed broadcast_52_piece0 on spark-master:42073 in memory (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 12:04:35 INFO BlockManagerInfo: Removed broadcast_52_piece0 on 172.18.0.14:37571 in memory (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 12:04:35 INFO BlockManagerInfo: Added broadcast_57_piece0 in memory on 172.18.0.15:37717 (size: 19.5 KiB, free: 434.4 MiB)
26/02/13 12:04:35 INFO BlockManagerInfo: Removed broadcast_54_piece0 on spark-master:42073 in memory (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 12:04:35 INFO BlockManagerInfo: Removed broadcast_54_piece0 on 172.18.0.14:37571 in memory (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 12:04:35 INFO BlockManagerInfo: Removed broadcast_54_piece0 on 172.18.0.15:37717 in memory (size: 19.5 KiB, free: 434.4 MiB)
26/02/13 12:04:35 INFO BlockManagerInfo: Added broadcast_56_piece0 in memory on 172.18.0.14:37571 (size: 107.2 KiB, free: 434.0 MiB)
26/02/13 12:04:35 INFO BlockManagerInfo: Added broadcast_56_piece0 in memory on 172.18.0.15:37717 (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 12:04:35 INFO TaskSetManager: Finished task 0.0 in stage 60.0 (TID 113) in 95 ms on 172.18.0.14 (executor 1) (1/3)
26/02/13 12:04:35 INFO TaskSetManager: Finished task 2.0 in stage 60.0 (TID 115) in 97 ms on 172.18.0.14 (executor 1) (2/3)
26/02/13 12:04:35 INFO TaskSetManager: Finished task 1.0 in stage 60.0 (TID 114) in 127 ms on 172.18.0.15 (executor 0) (3/3)
26/02/13 12:04:35 INFO TaskSchedulerImpl: Removed TaskSet 60.0, whose tasks have all completed, from pool 
26/02/13 12:04:35 INFO DAGScheduler: ResultStage 60 (start at NativeMethodAccessorImpl.java:0) finished in 0.141 s
26/02/13 12:04:35 INFO DAGScheduler: Job 38 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 12:04:35 INFO TaskSchedulerImpl: Killing all running tasks in stage 60: Stage finished
26/02/13 12:04:35 INFO DAGScheduler: Job 38 finished: start at NativeMethodAccessorImpl.java:0, took 0.143332 s
26/02/13 12:04:35 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 137, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@77bf0d25] is committing.
26/02/13 12:04:35 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 137, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@77bf0d25] committed.
26/02/13 12:04:35 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/137 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.137.2bf0c7bc-5fa6-4eb3-b8c4-80868815413a.tmp
26/02/13 12:04:35 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.137.2bf0c7bc-5fa6-4eb3-b8c4-80868815413a.tmp to file:/tmp/spark-checkpoint-enrichment/commits/137
26/02/13 12:04:35 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "4a161d66-225b-4b29-812d-ddec3c05364e",
  "name" : null,
  "timestamp" : "2026-02-13T12:04:34.787Z",
  "batchId" : 137,
  "numInputRows" : 154,
  "inputRowsPerSecond" : 143.52283317800558,
  "processedRowsPerSecond" : 350.7972665148064,
  "durationMs" : {
    "addBatch" : 300,
    "commitOffsets" : 54,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 30,
    "triggerExecution" : 439,
    "walCommit" : 54
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 1058,
        "1" : 1291,
        "0" : 967
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 1107,
        "1" : 1348,
        "0" : 1015
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 1107,
        "1" : 1348,
        "0" : 1015
      }
    },
    "numInputRows" : 154,
    "inputRowsPerSecond" : 143.52283317800558,
    "processedRowsPerSecond" : 350.7972665148064,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@1a6a42c2",
    "numOutputRows" : 30
  }
}
26/02/13 12:04:35 INFO BlockManagerInfo: Removed broadcast_57_piece0 on spark-master:42073 in memory (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 12:04:35 INFO BlockManagerInfo: Removed broadcast_57_piece0 on 172.18.0.15:37717 in memory (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 12:04:35 INFO BlockManagerInfo: Removed broadcast_57_piece0 on 172.18.0.14:37571 in memory (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 12:04:45 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:04:50 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/138 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.138.cf13517b-ed54-4df9-b073-96aff101e341.tmp
26/02/13 12:04:50 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.138.cf13517b-ed54-4df9-b073-96aff101e341.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/138
26/02/13 12:04:50 INFO MicroBatchExecution: Committed offsets for batch 138. Metadata OffsetSeqMetadata(0,1770984290705,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 12:04:50 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:04:50 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:04:50 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:04:50 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:04:50 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:04:50 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:04:50 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#14761 - origin_code.nullCount#14760) > 0)
26/02/13 12:04:50 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#14766 - destination_code.nullCount#14765) > 0)
26/02/13 12:04:50 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#14796 - callsign.nullCount#14795) > 0)
26/02/13 12:04:50 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 12:04:50 INFO DAGScheduler: Got job 39 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 12:04:50 INFO DAGScheduler: Final stage: ResultStage 62 (start at NativeMethodAccessorImpl.java:0)
26/02/13 12:04:50 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 61)
26/02/13 12:04:50 INFO DAGScheduler: Missing parents: List()
26/02/13 12:04:50 INFO DAGScheduler: Submitting ResultStage 62 (MapPartitionsRDD[208] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:04:50 INFO MemoryStore: Block broadcast_58 stored as values in memory (estimated size 99.3 KiB, free 434.0 MiB)
26/02/13 12:04:50 INFO MemoryStore: Block broadcast_58_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 433.9 MiB)
26/02/13 12:04:50 INFO BlockManagerInfo: Added broadcast_58_piece0 in memory on spark-master:42073 (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 12:04:50 INFO SparkContext: Created broadcast 58 from broadcast at DAGScheduler.scala:1585
26/02/13 12:04:50 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 62 (MapPartitionsRDD[208] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 12:04:50 INFO TaskSchedulerImpl: Adding task set 62.0 with 4 tasks resource profile 0
26/02/13 12:04:50 INFO TaskSetManager: Starting task 0.0 in stage 62.0 (TID 116) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:04:50 INFO TaskSetManager: Starting task 1.0 in stage 62.0 (TID 117) (172.18.0.14, executor 1, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:04:50 INFO BlockManagerInfo: Added broadcast_58_piece0 in memory on 172.18.0.14:37571 (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 12:04:50 INFO TaskSetManager: Starting task 2.0 in stage 62.0 (TID 118) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:04:50 INFO TaskSetManager: Starting task 3.0 in stage 62.0 (TID 119) (172.18.0.14, executor 1, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:04:50 INFO TaskSetManager: Finished task 0.0 in stage 62.0 (TID 116) in 27 ms on 172.18.0.14 (executor 1) (1/4)
26/02/13 12:04:50 INFO TaskSetManager: Finished task 1.0 in stage 62.0 (TID 117) in 28 ms on 172.18.0.14 (executor 1) (2/4)
26/02/13 12:04:50 INFO BlockManagerInfo: Removed broadcast_56_piece0 on spark-master:42073 in memory (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 12:04:50 INFO TaskSetManager: Finished task 2.0 in stage 62.0 (TID 118) in 33 ms on 172.18.0.14 (executor 1) (3/4)
26/02/13 12:04:50 INFO BlockManagerInfo: Removed broadcast_56_piece0 on 172.18.0.14:37571 in memory (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 12:04:50 INFO BlockManagerInfo: Removed broadcast_56_piece0 on 172.18.0.15:37717 in memory (size: 107.2 KiB, free: 434.4 MiB)
26/02/13 12:04:50 INFO TaskSetManager: Finished task 3.0 in stage 62.0 (TID 119) in 33 ms on 172.18.0.14 (executor 1) (4/4)
26/02/13 12:04:50 INFO TaskSchedulerImpl: Removed TaskSet 62.0, whose tasks have all completed, from pool 
26/02/13 12:04:50 INFO DAGScheduler: ResultStage 62 (start at NativeMethodAccessorImpl.java:0) finished in 0.071 s
26/02/13 12:04:50 INFO DAGScheduler: Job 39 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 12:04:50 INFO TaskSchedulerImpl: Killing all running tasks in stage 62: Stage finished
26/02/13 12:04:50 INFO DAGScheduler: Job 39 finished: start at NativeMethodAccessorImpl.java:0, took 0.074958 s
26/02/13 12:04:50 INFO MemoryStore: Block broadcast_59_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.9 MiB)
26/02/13 12:04:50 INFO BlockManagerInfo: Added broadcast_59_piece0 in memory on spark-master:42073 (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 12:04:50 INFO SparkContext: Created broadcast 59 from start at NativeMethodAccessorImpl.java:0
26/02/13 12:04:51 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 138, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@30937bcb]. The input RDD has 1 partitions.
26/02/13 12:04:51 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 12:04:51 INFO DAGScheduler: Got job 40 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/02/13 12:04:51 INFO DAGScheduler: Final stage: ResultStage 63 (start at NativeMethodAccessorImpl.java:0)
26/02/13 12:04:51 INFO DAGScheduler: Parents of final stage: List()
26/02/13 12:04:51 INFO DAGScheduler: Missing parents: List()
26/02/13 12:04:51 INFO DAGScheduler: Submitting ResultStage 63 (MapPartitionsRDD[214] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:04:51 INFO MemoryStore: Block broadcast_60 stored as values in memory (estimated size 49.9 KiB, free 433.9 MiB)
26/02/13 12:04:51 INFO MemoryStore: Block broadcast_60_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.9 MiB)
26/02/13 12:04:51 INFO BlockManagerInfo: Added broadcast_60_piece0 in memory on spark-master:42073 (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 12:04:51 INFO SparkContext: Created broadcast 60 from broadcast at DAGScheduler.scala:1585
26/02/13 12:04:51 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 63 (MapPartitionsRDD[214] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/02/13 12:04:51 INFO TaskSchedulerImpl: Adding task set 63.0 with 1 tasks resource profile 0
26/02/13 12:04:51 INFO TaskSetManager: Starting task 0.0 in stage 63.0 (TID 120) (172.18.0.15, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:04:51 INFO BlockManagerInfo: Added broadcast_60_piece0 in memory on 172.18.0.15:37717 (size: 19.5 KiB, free: 434.4 MiB)
26/02/13 12:04:51 INFO BlockManagerInfo: Added broadcast_59_piece0 in memory on 172.18.0.15:37717 (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 12:04:51 INFO TaskSetManager: Finished task 0.0 in stage 63.0 (TID 120) in 574 ms on 172.18.0.15 (executor 0) (1/1)
26/02/13 12:04:51 INFO TaskSchedulerImpl: Removed TaskSet 63.0, whose tasks have all completed, from pool 
26/02/13 12:04:51 INFO DAGScheduler: ResultStage 63 (start at NativeMethodAccessorImpl.java:0) finished in 0.580 s
26/02/13 12:04:51 INFO DAGScheduler: Job 40 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 12:04:51 INFO TaskSchedulerImpl: Killing all running tasks in stage 63: Stage finished
26/02/13 12:04:51 INFO DAGScheduler: Job 40 finished: start at NativeMethodAccessorImpl.java:0, took 0.581505 s
26/02/13 12:04:51 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 138, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@30937bcb] is committing.
26/02/13 12:04:51 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 138, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@30937bcb] committed.
26/02/13 12:04:51 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/138 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.138.4129a523-2351-408a-8e25-a0a9ff402863.tmp
26/02/13 12:04:51 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.138.4129a523-2351-408a-8e25-a0a9ff402863.tmp to file:/tmp/spark-checkpoint-enrichment/commits/138
26/02/13 12:04:51 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "4a161d66-225b-4b29-812d-ddec3c05364e",
  "name" : null,
  "timestamp" : "2026-02-13T12:04:50.703Z",
  "batchId" : 138,
  "numInputRows" : 33,
  "inputRowsPerSecond" : 2750.0,
  "processedRowsPerSecond" : 34.51882845188285,
  "durationMs" : {
    "addBatch" : 769,
    "commitOffsets" : 76,
    "getBatch" : 0,
    "latestOffset" : 2,
    "queryPlanning" : 40,
    "triggerExecution" : 956,
    "walCommit" : 69
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 1107,
        "1" : 1348,
        "0" : 1015
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 1107,
        "1" : 1381,
        "0" : 1015
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 1107,
        "1" : 1381,
        "0" : 1015
      }
    },
    "numInputRows" : 33,
    "inputRowsPerSecond" : 2750.0,
    "processedRowsPerSecond" : 34.51882845188285,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@1a6a42c2",
    "numOutputRows" : 10
  }
}
26/02/13 12:04:51 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/139 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.139.2dd5c359-e337-4e88-ad97-675bb394f7f0.tmp
26/02/13 12:04:51 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.139.2dd5c359-e337-4e88-ad97-675bb394f7f0.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/139
26/02/13 12:04:51 INFO MicroBatchExecution: Committed offsets for batch 139. Metadata OffsetSeqMetadata(0,1770984291662,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 12:04:51 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:04:51 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:04:51 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:04:51 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:04:51 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:04:51 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:04:51 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#15615 - origin_code.nullCount#15614) > 0)
26/02/13 12:04:51 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#15620 - destination_code.nullCount#15619) > 0)
26/02/13 12:04:51 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#15650 - callsign.nullCount#15649) > 0)
26/02/13 12:04:51 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 12:04:51 INFO DAGScheduler: Got job 41 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 12:04:51 INFO DAGScheduler: Final stage: ResultStage 65 (start at NativeMethodAccessorImpl.java:0)
26/02/13 12:04:51 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 64)
26/02/13 12:04:51 INFO DAGScheduler: Missing parents: List()
26/02/13 12:04:51 INFO DAGScheduler: Submitting ResultStage 65 (MapPartitionsRDD[219] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:04:51 INFO MemoryStore: Block broadcast_61 stored as values in memory (estimated size 99.3 KiB, free 433.8 MiB)
26/02/13 12:04:51 INFO MemoryStore: Block broadcast_61_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 433.7 MiB)
26/02/13 12:04:51 INFO BlockManagerInfo: Added broadcast_61_piece0 in memory on spark-master:42073 (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 12:04:51 INFO SparkContext: Created broadcast 61 from broadcast at DAGScheduler.scala:1585
26/02/13 12:04:51 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 65 (MapPartitionsRDD[219] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 12:04:51 INFO TaskSchedulerImpl: Adding task set 65.0 with 4 tasks resource profile 0
26/02/13 12:04:51 INFO TaskSetManager: Starting task 0.0 in stage 65.0 (TID 121) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:04:51 INFO TaskSetManager: Starting task 1.0 in stage 65.0 (TID 122) (172.18.0.14, executor 1, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:04:51 INFO BlockManagerInfo: Added broadcast_61_piece0 in memory on 172.18.0.14:37571 (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 12:04:51 INFO TaskSetManager: Starting task 2.0 in stage 65.0 (TID 123) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:04:51 INFO TaskSetManager: Starting task 3.0 in stage 65.0 (TID 124) (172.18.0.14, executor 1, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:04:51 INFO TaskSetManager: Finished task 1.0 in stage 65.0 (TID 122) in 25 ms on 172.18.0.14 (executor 1) (1/4)
26/02/13 12:04:51 INFO TaskSetManager: Finished task 0.0 in stage 65.0 (TID 121) in 26 ms on 172.18.0.14 (executor 1) (2/4)
26/02/13 12:04:51 INFO TaskSetManager: Finished task 2.0 in stage 65.0 (TID 123) in 22 ms on 172.18.0.14 (executor 1) (3/4)
26/02/13 12:04:51 INFO TaskSetManager: Finished task 3.0 in stage 65.0 (TID 124) in 25 ms on 172.18.0.14 (executor 1) (4/4)
26/02/13 12:04:51 INFO TaskSchedulerImpl: Removed TaskSet 65.0, whose tasks have all completed, from pool 
26/02/13 12:04:51 INFO DAGScheduler: ResultStage 65 (start at NativeMethodAccessorImpl.java:0) finished in 0.061 s
26/02/13 12:04:51 INFO DAGScheduler: Job 41 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 12:04:51 INFO TaskSchedulerImpl: Killing all running tasks in stage 65: Stage finished
26/02/13 12:04:51 INFO DAGScheduler: Job 41 finished: start at NativeMethodAccessorImpl.java:0, took 0.065696 s
26/02/13 12:04:51 INFO MemoryStore: Block broadcast_62_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.6 MiB)
26/02/13 12:04:51 INFO BlockManagerInfo: Added broadcast_62_piece0 in memory on spark-master:42073 (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 12:04:51 INFO SparkContext: Created broadcast 62 from start at NativeMethodAccessorImpl.java:0
26/02/13 12:04:51 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 139, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@7ec6fbc7]. The input RDD has 3 partitions.
26/02/13 12:04:51 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 12:04:51 INFO DAGScheduler: Got job 42 (start at NativeMethodAccessorImpl.java:0) with 3 output partitions
26/02/13 12:04:51 INFO DAGScheduler: Final stage: ResultStage 66 (start at NativeMethodAccessorImpl.java:0)
26/02/13 12:04:51 INFO DAGScheduler: Parents of final stage: List()
26/02/13 12:04:51 INFO DAGScheduler: Missing parents: List()
26/02/13 12:04:51 INFO DAGScheduler: Submitting ResultStage 66 (MapPartitionsRDD[225] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:04:51 INFO MemoryStore: Block broadcast_63 stored as values in memory (estimated size 49.9 KiB, free 433.6 MiB)
26/02/13 12:04:51 INFO MemoryStore: Block broadcast_63_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.6 MiB)
26/02/13 12:04:51 INFO BlockManagerInfo: Added broadcast_63_piece0 in memory on spark-master:42073 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 12:04:51 INFO SparkContext: Created broadcast 63 from broadcast at DAGScheduler.scala:1585
26/02/13 12:04:51 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 66 (MapPartitionsRDD[225] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
26/02/13 12:04:51 INFO TaskSchedulerImpl: Adding task set 66.0 with 3 tasks resource profile 0
26/02/13 12:04:51 INFO BlockManagerInfo: Removed broadcast_61_piece0 on spark-master:42073 in memory (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 12:04:51 INFO TaskSetManager: Starting task 0.0 in stage 66.0 (TID 125) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:04:51 INFO BlockManagerInfo: Removed broadcast_61_piece0 on 172.18.0.14:37571 in memory (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 12:04:51 INFO TaskSetManager: Starting task 1.0 in stage 66.0 (TID 126) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:04:51 INFO TaskSetManager: Starting task 2.0 in stage 66.0 (TID 127) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:04:51 INFO BlockManagerInfo: Removed broadcast_59_piece0 on spark-master:42073 in memory (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 12:04:51 INFO BlockManagerInfo: Added broadcast_63_piece0 in memory on 172.18.0.14:37571 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 12:04:51 INFO BlockManagerInfo: Removed broadcast_59_piece0 on 172.18.0.15:37717 in memory (size: 107.2 KiB, free: 434.4 MiB)
26/02/13 12:04:51 INFO BlockManagerInfo: Added broadcast_63_piece0 in memory on 172.18.0.15:37717 (size: 19.5 KiB, free: 434.4 MiB)
26/02/13 12:04:51 INFO BlockManagerInfo: Removed broadcast_58_piece0 on spark-master:42073 in memory (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 12:04:52 INFO BlockManagerInfo: Removed broadcast_58_piece0 on 172.18.0.14:37571 in memory (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 12:04:52 INFO BlockManagerInfo: Added broadcast_62_piece0 in memory on 172.18.0.14:37571 (size: 107.2 KiB, free: 434.0 MiB)
26/02/13 12:04:52 INFO BlockManagerInfo: Removed broadcast_60_piece0 on spark-master:42073 in memory (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 12:04:52 INFO BlockManagerInfo: Removed broadcast_60_piece0 on 172.18.0.15:37717 in memory (size: 19.5 KiB, free: 434.4 MiB)
26/02/13 12:04:52 INFO BlockManagerInfo: Added broadcast_62_piece0 in memory on 172.18.0.15:37717 (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 12:04:52 INFO TaskSetManager: Finished task 1.0 in stage 66.0 (TID 126) in 146 ms on 172.18.0.15 (executor 0) (1/3)
26/02/13 12:04:52 INFO TaskSetManager: Finished task 2.0 in stage 66.0 (TID 127) in 626 ms on 172.18.0.14 (executor 1) (2/3)
26/02/13 12:04:52 INFO TaskSetManager: Finished task 0.0 in stage 66.0 (TID 125) in 631 ms on 172.18.0.14 (executor 1) (3/3)
26/02/13 12:04:52 INFO TaskSchedulerImpl: Removed TaskSet 66.0, whose tasks have all completed, from pool 
26/02/13 12:04:52 INFO DAGScheduler: ResultStage 66 (start at NativeMethodAccessorImpl.java:0) finished in 0.646 s
26/02/13 12:04:52 INFO DAGScheduler: Job 42 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 12:04:52 INFO TaskSchedulerImpl: Killing all running tasks in stage 66: Stage finished
26/02/13 12:04:52 INFO DAGScheduler: Job 42 finished: start at NativeMethodAccessorImpl.java:0, took 0.650932 s
26/02/13 12:04:52 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 139, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@7ec6fbc7] is committing.
26/02/13 12:04:52 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 139, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@7ec6fbc7] committed.
26/02/13 12:04:52 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/139 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.139.7491cd3a-e2d7-4ef8-ac14-7879b73e7d4c.tmp
26/02/13 12:04:52 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.139.7491cd3a-e2d7-4ef8-ac14-7879b73e7d4c.tmp to file:/tmp/spark-checkpoint-enrichment/commits/139
26/02/13 12:04:52 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "4a161d66-225b-4b29-812d-ddec3c05364e",
  "name" : null,
  "timestamp" : "2026-02-13T12:04:51.661Z",
  "batchId" : 139,
  "numInputRows" : 208,
  "inputRowsPerSecond" : 217.11899791231733,
  "processedRowsPerSecond" : 202.53164556962028,
  "durationMs" : {
    "addBatch" : 825,
    "commitOffsets" : 89,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 52,
    "triggerExecution" : 1027,
    "walCommit" : 60
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 1107,
        "1" : 1381,
        "0" : 1015
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 1184,
        "1" : 1441,
        "0" : 1086
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 1184,
        "1" : 1441,
        "0" : 1086
      }
    },
    "numInputRows" : 208,
    "inputRowsPerSecond" : 217.11899791231733,
    "processedRowsPerSecond" : 202.53164556962028,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@1a6a42c2",
    "numOutputRows" : 42
  }
}
26/02/13 12:04:53 INFO BlockManagerInfo: Removed broadcast_63_piece0 on spark-master:42073 in memory (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 12:04:53 INFO BlockManagerInfo: Removed broadcast_63_piece0 on 172.18.0.14:37571 in memory (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 12:04:53 INFO BlockManagerInfo: Removed broadcast_63_piece0 on 172.18.0.15:37717 in memory (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 12:05:02 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:05:09 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/140 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.140.7c977ad0-158e-440b-9a25-a223ae49fc68.tmp
26/02/13 12:05:09 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.140.7c977ad0-158e-440b-9a25-a223ae49fc68.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/140
26/02/13 12:05:09 INFO MicroBatchExecution: Committed offsets for batch 140. Metadata OffsetSeqMetadata(0,1770984309125,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 12:05:09 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:05:09 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:05:09 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:05:09 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:05:09 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:05:09 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:05:09 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#16469 - origin_code.nullCount#16468) > 0)
26/02/13 12:05:09 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#16474 - destination_code.nullCount#16473) > 0)
26/02/13 12:05:09 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#16504 - callsign.nullCount#16503) > 0)
26/02/13 12:05:09 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 12:05:09 INFO DAGScheduler: Got job 43 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 12:05:09 INFO DAGScheduler: Final stage: ResultStage 68 (start at NativeMethodAccessorImpl.java:0)
26/02/13 12:05:09 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 67)
26/02/13 12:05:09 INFO DAGScheduler: Missing parents: List()
26/02/13 12:05:09 INFO DAGScheduler: Submitting ResultStage 68 (MapPartitionsRDD[230] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:05:09 INFO MemoryStore: Block broadcast_64 stored as values in memory (estimated size 99.3 KiB, free 434.0 MiB)
26/02/13 12:05:09 INFO MemoryStore: Block broadcast_64_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 433.9 MiB)
26/02/13 12:05:09 INFO BlockManagerInfo: Added broadcast_64_piece0 in memory on spark-master:42073 (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 12:05:09 INFO SparkContext: Created broadcast 64 from broadcast at DAGScheduler.scala:1585
26/02/13 12:05:09 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 68 (MapPartitionsRDD[230] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 12:05:09 INFO TaskSchedulerImpl: Adding task set 68.0 with 4 tasks resource profile 0
26/02/13 12:05:09 INFO TaskSetManager: Starting task 0.0 in stage 68.0 (TID 128) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:05:09 INFO TaskSetManager: Starting task 1.0 in stage 68.0 (TID 129) (172.18.0.14, executor 1, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:05:09 INFO BlockManagerInfo: Added broadcast_64_piece0 in memory on 172.18.0.14:37571 (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 12:05:09 INFO TaskSetManager: Starting task 2.0 in stage 68.0 (TID 130) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:05:09 INFO TaskSetManager: Finished task 1.0 in stage 68.0 (TID 129) in 24 ms on 172.18.0.14 (executor 1) (1/4)
26/02/13 12:05:09 INFO TaskSetManager: Starting task 3.0 in stage 68.0 (TID 131) (172.18.0.14, executor 1, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:05:09 INFO TaskSetManager: Finished task 0.0 in stage 68.0 (TID 128) in 27 ms on 172.18.0.14 (executor 1) (2/4)
26/02/13 12:05:09 INFO TaskSetManager: Finished task 3.0 in stage 68.0 (TID 131) in 20 ms on 172.18.0.14 (executor 1) (3/4)
26/02/13 12:05:09 INFO TaskSetManager: Finished task 2.0 in stage 68.0 (TID 130) in 22 ms on 172.18.0.14 (executor 1) (4/4)
26/02/13 12:05:09 INFO TaskSchedulerImpl: Removed TaskSet 68.0, whose tasks have all completed, from pool 
26/02/13 12:05:09 INFO DAGScheduler: ResultStage 68 (start at NativeMethodAccessorImpl.java:0) finished in 0.058 s
26/02/13 12:05:09 INFO DAGScheduler: Job 43 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 12:05:09 INFO TaskSchedulerImpl: Killing all running tasks in stage 68: Stage finished
26/02/13 12:05:09 INFO DAGScheduler: Job 43 finished: start at NativeMethodAccessorImpl.java:0, took 0.061096 s
26/02/13 12:05:09 INFO BlockManagerInfo: Removed broadcast_62_piece0 on spark-master:42073 in memory (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 12:05:09 INFO BlockManagerInfo: Removed broadcast_62_piece0 on 172.18.0.14:37571 in memory (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 12:05:09 INFO BlockManagerInfo: Removed broadcast_62_piece0 on 172.18.0.15:37717 in memory (size: 107.2 KiB, free: 434.4 MiB)
26/02/13 12:05:09 INFO MemoryStore: Block broadcast_65_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.9 MiB)
26/02/13 12:05:09 INFO BlockManagerInfo: Added broadcast_65_piece0 in memory on spark-master:42073 (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 12:05:09 INFO SparkContext: Created broadcast 65 from start at NativeMethodAccessorImpl.java:0
26/02/13 12:05:09 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 140, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@29969ba9]. The input RDD has 3 partitions.
26/02/13 12:05:09 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 12:05:09 INFO DAGScheduler: Got job 44 (start at NativeMethodAccessorImpl.java:0) with 3 output partitions
26/02/13 12:05:09 INFO DAGScheduler: Final stage: ResultStage 69 (start at NativeMethodAccessorImpl.java:0)
26/02/13 12:05:09 INFO DAGScheduler: Parents of final stage: List()
26/02/13 12:05:09 INFO DAGScheduler: Missing parents: List()
26/02/13 12:05:09 INFO DAGScheduler: Submitting ResultStage 69 (MapPartitionsRDD[236] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:05:09 INFO MemoryStore: Block broadcast_66 stored as values in memory (estimated size 49.9 KiB, free 433.9 MiB)
26/02/13 12:05:09 INFO MemoryStore: Block broadcast_66_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.9 MiB)
26/02/13 12:05:09 INFO BlockManagerInfo: Added broadcast_66_piece0 in memory on spark-master:42073 (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 12:05:09 INFO SparkContext: Created broadcast 66 from broadcast at DAGScheduler.scala:1585
26/02/13 12:05:09 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 69 (MapPartitionsRDD[236] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
26/02/13 12:05:09 INFO TaskSchedulerImpl: Adding task set 69.0 with 3 tasks resource profile 0
26/02/13 12:05:09 INFO TaskSetManager: Starting task 1.0 in stage 69.0 (TID 132) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:05:09 INFO TaskSetManager: Starting task 0.0 in stage 69.0 (TID 133) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:05:09 INFO TaskSetManager: Starting task 2.0 in stage 69.0 (TID 134) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:05:09 INFO BlockManagerInfo: Added broadcast_66_piece0 in memory on 172.18.0.14:37571 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 12:05:09 INFO BlockManagerInfo: Added broadcast_66_piece0 in memory on 172.18.0.15:37717 (size: 19.5 KiB, free: 434.4 MiB)
26/02/13 12:05:09 INFO BlockManagerInfo: Added broadcast_65_piece0 in memory on 172.18.0.14:37571 (size: 107.2 KiB, free: 434.0 MiB)
26/02/13 12:05:09 INFO BlockManagerInfo: Added broadcast_65_piece0 in memory on 172.18.0.15:37717 (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 12:05:10 INFO TaskSetManager: Finished task 0.0 in stage 69.0 (TID 133) in 579 ms on 172.18.0.14 (executor 1) (1/3)
26/02/13 12:05:10 INFO TaskSetManager: Finished task 2.0 in stage 69.0 (TID 134) in 584 ms on 172.18.0.14 (executor 1) (2/3)
26/02/13 12:05:10 INFO TaskSetManager: Finished task 1.0 in stage 69.0 (TID 132) in 613 ms on 172.18.0.15 (executor 0) (3/3)
26/02/13 12:05:10 INFO TaskSchedulerImpl: Removed TaskSet 69.0, whose tasks have all completed, from pool 
26/02/13 12:05:10 INFO DAGScheduler: ResultStage 69 (start at NativeMethodAccessorImpl.java:0) finished in 0.618 s
26/02/13 12:05:10 INFO DAGScheduler: Job 44 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 12:05:10 INFO TaskSchedulerImpl: Killing all running tasks in stage 69: Stage finished
26/02/13 12:05:10 INFO DAGScheduler: Job 44 finished: start at NativeMethodAccessorImpl.java:0, took 0.620515 s
26/02/13 12:05:10 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 140, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@29969ba9] is committing.
26/02/13 12:05:10 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 140, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@29969ba9] committed.
26/02/13 12:05:10 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/140 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.140.c94dac20-be0d-45d3-b0a5-9eeb3cde3ffa.tmp
26/02/13 12:05:10 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.140.c94dac20-be0d-45d3-b0a5-9eeb3cde3ffa.tmp to file:/tmp/spark-checkpoint-enrichment/commits/140
26/02/13 12:05:10 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "4a161d66-225b-4b29-812d-ddec3c05364e",
  "name" : null,
  "timestamp" : "2026-02-13T12:05:09.123Z",
  "batchId" : 140,
  "numInputRows" : 51,
  "inputRowsPerSecond" : 4250.0,
  "processedRowsPerSecond" : 51.93482688391039,
  "durationMs" : {
    "addBatch" : 821,
    "commitOffsets" : 54,
    "getBatch" : 0,
    "latestOffset" : 2,
    "queryPlanning" : 39,
    "triggerExecution" : 982,
    "walCommit" : 65
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 1184,
        "1" : 1441,
        "0" : 1086
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 1199,
        "1" : 1463,
        "0" : 1100
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 1199,
        "1" : 1463,
        "0" : 1100
      }
    },
    "numInputRows" : 51,
    "inputRowsPerSecond" : 4250.0,
    "processedRowsPerSecond" : 51.93482688391039,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@1a6a42c2",
    "numOutputRows" : 13
  }
}
26/02/13 12:05:10 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/141 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.141.9723bd61-4b46-48b0-b1d8-aa604406d475.tmp
26/02/13 12:05:10 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.141.9723bd61-4b46-48b0-b1d8-aa604406d475.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/141
26/02/13 12:05:10 INFO MicroBatchExecution: Committed offsets for batch 141. Metadata OffsetSeqMetadata(0,1770984310106,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 12:05:10 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:05:10 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:05:10 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:05:10 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:05:10 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:05:10 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:05:10 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#17323 - origin_code.nullCount#17322) > 0)
26/02/13 12:05:10 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#17328 - destination_code.nullCount#17327) > 0)
26/02/13 12:05:10 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#17358 - callsign.nullCount#17357) > 0)
26/02/13 12:05:10 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 12:05:10 INFO DAGScheduler: Got job 45 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 12:05:10 INFO DAGScheduler: Final stage: ResultStage 71 (start at NativeMethodAccessorImpl.java:0)
26/02/13 12:05:10 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 70)
26/02/13 12:05:10 INFO DAGScheduler: Missing parents: List()
26/02/13 12:05:10 INFO DAGScheduler: Submitting ResultStage 71 (MapPartitionsRDD[241] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:05:10 INFO MemoryStore: Block broadcast_67 stored as values in memory (estimated size 99.3 KiB, free 433.8 MiB)
26/02/13 12:05:10 INFO MemoryStore: Block broadcast_67_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 433.7 MiB)
26/02/13 12:05:10 INFO BlockManagerInfo: Added broadcast_67_piece0 in memory on spark-master:42073 (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 12:05:10 INFO SparkContext: Created broadcast 67 from broadcast at DAGScheduler.scala:1585
26/02/13 12:05:10 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 71 (MapPartitionsRDD[241] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 12:05:10 INFO TaskSchedulerImpl: Adding task set 71.0 with 4 tasks resource profile 0
26/02/13 12:05:10 INFO TaskSetManager: Starting task 0.0 in stage 71.0 (TID 135) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:05:10 INFO TaskSetManager: Starting task 1.0 in stage 71.0 (TID 136) (172.18.0.14, executor 1, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:05:10 INFO BlockManagerInfo: Added broadcast_67_piece0 in memory on 172.18.0.14:37571 (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 12:05:10 INFO TaskSetManager: Starting task 2.0 in stage 71.0 (TID 137) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:05:10 INFO TaskSetManager: Starting task 3.0 in stage 71.0 (TID 138) (172.18.0.14, executor 1, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:05:10 INFO TaskSetManager: Finished task 0.0 in stage 71.0 (TID 135) in 28 ms on 172.18.0.14 (executor 1) (1/4)
26/02/13 12:05:10 INFO TaskSetManager: Finished task 1.0 in stage 71.0 (TID 136) in 27 ms on 172.18.0.14 (executor 1) (2/4)
26/02/13 12:05:10 INFO TaskSetManager: Finished task 2.0 in stage 71.0 (TID 137) in 15 ms on 172.18.0.14 (executor 1) (3/4)
26/02/13 12:05:10 INFO TaskSetManager: Finished task 3.0 in stage 71.0 (TID 138) in 16 ms on 172.18.0.14 (executor 1) (4/4)
26/02/13 12:05:10 INFO TaskSchedulerImpl: Removed TaskSet 71.0, whose tasks have all completed, from pool 
26/02/13 12:05:10 INFO DAGScheduler: ResultStage 71 (start at NativeMethodAccessorImpl.java:0) finished in 0.049 s
26/02/13 12:05:10 INFO DAGScheduler: Job 45 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 12:05:10 INFO TaskSchedulerImpl: Killing all running tasks in stage 71: Stage finished
26/02/13 12:05:10 INFO DAGScheduler: Job 45 finished: start at NativeMethodAccessorImpl.java:0, took 0.052082 s
26/02/13 12:05:10 INFO MemoryStore: Block broadcast_68_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.6 MiB)
26/02/13 12:05:10 INFO BlockManagerInfo: Added broadcast_68_piece0 in memory on spark-master:42073 (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 12:05:10 INFO SparkContext: Created broadcast 68 from start at NativeMethodAccessorImpl.java:0
26/02/13 12:05:10 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 141, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@37ee1700]. The input RDD has 3 partitions.
26/02/13 12:05:10 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 12:05:10 INFO DAGScheduler: Got job 46 (start at NativeMethodAccessorImpl.java:0) with 3 output partitions
26/02/13 12:05:10 INFO DAGScheduler: Final stage: ResultStage 72 (start at NativeMethodAccessorImpl.java:0)
26/02/13 12:05:10 INFO DAGScheduler: Parents of final stage: List()
26/02/13 12:05:10 INFO DAGScheduler: Missing parents: List()
26/02/13 12:05:10 INFO DAGScheduler: Submitting ResultStage 72 (MapPartitionsRDD[247] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:05:10 INFO MemoryStore: Block broadcast_69 stored as values in memory (estimated size 49.9 KiB, free 433.6 MiB)
26/02/13 12:05:10 INFO MemoryStore: Block broadcast_69_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.6 MiB)
26/02/13 12:05:10 INFO BlockManagerInfo: Removed broadcast_65_piece0 on spark-master:42073 in memory (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 12:05:10 INFO BlockManagerInfo: Added broadcast_69_piece0 in memory on spark-master:42073 (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 12:05:10 INFO SparkContext: Created broadcast 69 from broadcast at DAGScheduler.scala:1585
26/02/13 12:05:10 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 72 (MapPartitionsRDD[247] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
26/02/13 12:05:10 INFO TaskSchedulerImpl: Adding task set 72.0 with 3 tasks resource profile 0
26/02/13 12:05:10 INFO BlockManagerInfo: Removed broadcast_65_piece0 on 172.18.0.14:37571 in memory (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 12:05:10 INFO BlockManagerInfo: Removed broadcast_65_piece0 on 172.18.0.15:37717 in memory (size: 107.2 KiB, free: 434.4 MiB)
26/02/13 12:05:10 INFO TaskSetManager: Starting task 0.0 in stage 72.0 (TID 139) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:05:10 INFO TaskSetManager: Starting task 1.0 in stage 72.0 (TID 140) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:05:10 INFO TaskSetManager: Starting task 2.0 in stage 72.0 (TID 141) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:05:10 INFO BlockManagerInfo: Removed broadcast_64_piece0 on spark-master:42073 in memory (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 12:05:10 INFO BlockManagerInfo: Removed broadcast_64_piece0 on 172.18.0.14:37571 in memory (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 12:05:10 INFO BlockManagerInfo: Added broadcast_69_piece0 in memory on 172.18.0.14:37571 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 12:05:10 INFO BlockManagerInfo: Removed broadcast_67_piece0 on spark-master:42073 in memory (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 12:05:10 INFO BlockManagerInfo: Added broadcast_69_piece0 in memory on 172.18.0.15:37717 (size: 19.5 KiB, free: 434.4 MiB)
26/02/13 12:05:10 INFO BlockManagerInfo: Removed broadcast_67_piece0 on 172.18.0.14:37571 in memory (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 12:05:10 INFO BlockManagerInfo: Removed broadcast_66_piece0 on spark-master:42073 in memory (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 12:05:10 INFO BlockManagerInfo: Added broadcast_68_piece0 in memory on 172.18.0.14:37571 (size: 107.2 KiB, free: 434.0 MiB)
26/02/13 12:05:10 INFO BlockManagerInfo: Removed broadcast_66_piece0 on 172.18.0.15:37717 in memory (size: 19.5 KiB, free: 434.4 MiB)
26/02/13 12:05:10 INFO BlockManagerInfo: Removed broadcast_66_piece0 on 172.18.0.14:37571 in memory (size: 19.5 KiB, free: 434.0 MiB)
26/02/13 12:05:10 INFO BlockManagerInfo: Added broadcast_68_piece0 in memory on 172.18.0.15:37717 (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 12:05:10 INFO TaskSetManager: Finished task 0.0 in stage 72.0 (TID 139) in 116 ms on 172.18.0.14 (executor 1) (1/3)
26/02/13 12:05:10 INFO TaskSetManager: Finished task 2.0 in stage 72.0 (TID 141) in 115 ms on 172.18.0.14 (executor 1) (2/3)
26/02/13 12:05:10 INFO TaskSetManager: Finished task 1.0 in stage 72.0 (TID 140) in 156 ms on 172.18.0.15 (executor 0) (3/3)
26/02/13 12:05:10 INFO TaskSchedulerImpl: Removed TaskSet 72.0, whose tasks have all completed, from pool 
26/02/13 12:05:10 INFO DAGScheduler: ResultStage 72 (start at NativeMethodAccessorImpl.java:0) finished in 0.171 s
26/02/13 12:05:10 INFO DAGScheduler: Job 46 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 12:05:10 INFO TaskSchedulerImpl: Killing all running tasks in stage 72: Stage finished
26/02/13 12:05:10 INFO DAGScheduler: Job 46 finished: start at NativeMethodAccessorImpl.java:0, took 0.173016 s
26/02/13 12:05:10 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 141, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@37ee1700] is committing.
26/02/13 12:05:10 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 141, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@37ee1700] committed.
26/02/13 12:05:10 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/141 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.141.d102b610-1abd-48d4-b635-eff5f869bcab.tmp
26/02/13 12:05:10 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.141.d102b610-1abd-48d4-b635-eff5f869bcab.tmp to file:/tmp/spark-checkpoint-enrichment/commits/141
26/02/13 12:05:10 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "4a161d66-225b-4b29-812d-ddec3c05364e",
  "name" : null,
  "timestamp" : "2026-02-13T12:05:10.105Z",
  "batchId" : 141,
  "numInputRows" : 191,
  "inputRowsPerSecond" : 194.50101832993892,
  "processedRowsPerSecond" : 405.52016985138005,
  "durationMs" : {
    "addBatch" : 319,
    "commitOffsets" : 72,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 28,
    "triggerExecution" : 471,
    "walCommit" : 51
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 1199,
        "1" : 1463,
        "0" : 1100
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 1263,
        "1" : 1533,
        "0" : 1157
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 1263,
        "1" : 1533,
        "0" : 1157
      }
    },
    "numInputRows" : 191,
    "inputRowsPerSecond" : 194.50101832993892,
    "processedRowsPerSecond" : 405.52016985138005,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@1a6a42c2",
    "numOutputRows" : 41
  }
}
26/02/13 12:05:11 INFO BlockManagerInfo: Removed broadcast_69_piece0 on spark-master:42073 in memory (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 12:05:11 INFO BlockManagerInfo: Removed broadcast_69_piece0 on 172.18.0.14:37571 in memory (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 12:05:11 INFO BlockManagerInfo: Removed broadcast_69_piece0 on 172.18.0.15:37717 in memory (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 12:05:20 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:05:25 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/142 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.142.d6a6748b-6029-4450-842c-4c89dc970583.tmp
26/02/13 12:05:25 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.142.d6a6748b-6029-4450-842c-4c89dc970583.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/142
26/02/13 12:05:25 INFO MicroBatchExecution: Committed offsets for batch 142. Metadata OffsetSeqMetadata(0,1770984325620,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 12:05:25 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:05:25 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:05:25 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:05:25 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:05:25 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:05:25 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:05:25 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#18177 - origin_code.nullCount#18176) > 0)
26/02/13 12:05:25 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#18182 - destination_code.nullCount#18181) > 0)
26/02/13 12:05:25 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#18212 - callsign.nullCount#18211) > 0)
26/02/13 12:05:25 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 12:05:25 INFO DAGScheduler: Got job 47 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 12:05:25 INFO DAGScheduler: Final stage: ResultStage 74 (start at NativeMethodAccessorImpl.java:0)
26/02/13 12:05:25 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 73)
26/02/13 12:05:25 INFO DAGScheduler: Missing parents: List()
26/02/13 12:05:25 INFO DAGScheduler: Submitting ResultStage 74 (MapPartitionsRDD[252] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:05:25 INFO MemoryStore: Block broadcast_70 stored as values in memory (estimated size 99.3 KiB, free 434.0 MiB)
26/02/13 12:05:25 INFO MemoryStore: Block broadcast_70_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 433.9 MiB)
26/02/13 12:05:25 INFO BlockManagerInfo: Added broadcast_70_piece0 in memory on spark-master:42073 (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 12:05:25 INFO SparkContext: Created broadcast 70 from broadcast at DAGScheduler.scala:1585
26/02/13 12:05:25 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 74 (MapPartitionsRDD[252] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 12:05:25 INFO TaskSchedulerImpl: Adding task set 74.0 with 4 tasks resource profile 0
26/02/13 12:05:25 INFO TaskSetManager: Starting task 0.0 in stage 74.0 (TID 142) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:05:25 INFO TaskSetManager: Starting task 1.0 in stage 74.0 (TID 143) (172.18.0.14, executor 1, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:05:25 INFO BlockManagerInfo: Added broadcast_70_piece0 in memory on 172.18.0.14:37571 (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 12:05:25 INFO TaskSetManager: Starting task 2.0 in stage 74.0 (TID 144) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:05:25 INFO TaskSetManager: Finished task 0.0 in stage 74.0 (TID 142) in 18 ms on 172.18.0.14 (executor 1) (1/4)
26/02/13 12:05:25 INFO TaskSetManager: Starting task 3.0 in stage 74.0 (TID 145) (172.18.0.14, executor 1, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:05:25 INFO TaskSetManager: Finished task 1.0 in stage 74.0 (TID 143) in 22 ms on 172.18.0.14 (executor 1) (2/4)
26/02/13 12:05:25 INFO TaskSetManager: Finished task 2.0 in stage 74.0 (TID 144) in 18 ms on 172.18.0.14 (executor 1) (3/4)
26/02/13 12:05:25 INFO TaskSetManager: Finished task 3.0 in stage 74.0 (TID 145) in 19 ms on 172.18.0.14 (executor 1) (4/4)
26/02/13 12:05:25 INFO TaskSchedulerImpl: Removed TaskSet 74.0, whose tasks have all completed, from pool 
26/02/13 12:05:25 INFO DAGScheduler: ResultStage 74 (start at NativeMethodAccessorImpl.java:0) finished in 0.050 s
26/02/13 12:05:25 INFO DAGScheduler: Job 47 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 12:05:25 INFO TaskSchedulerImpl: Killing all running tasks in stage 74: Stage finished
26/02/13 12:05:25 INFO DAGScheduler: Job 47 finished: start at NativeMethodAccessorImpl.java:0, took 0.052281 s
26/02/13 12:05:25 INFO BlockManagerInfo: Removed broadcast_70_piece0 on spark-master:42073 in memory (size: 33.3 KiB, free: 434.3 MiB)
26/02/13 12:05:25 INFO BlockManagerInfo: Removed broadcast_70_piece0 on 172.18.0.14:37571 in memory (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 12:05:25 INFO MemoryStore: Block broadcast_71_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 434.0 MiB)
26/02/13 12:05:25 INFO BlockManagerInfo: Added broadcast_71_piece0 in memory on spark-master:42073 (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 12:05:25 INFO SparkContext: Created broadcast 71 from start at NativeMethodAccessorImpl.java:0
26/02/13 12:05:25 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 142, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@6364a0cb]. The input RDD has 3 partitions.
26/02/13 12:05:25 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 12:05:25 INFO DAGScheduler: Got job 48 (start at NativeMethodAccessorImpl.java:0) with 3 output partitions
26/02/13 12:05:25 INFO DAGScheduler: Final stage: ResultStage 75 (start at NativeMethodAccessorImpl.java:0)
26/02/13 12:05:25 INFO DAGScheduler: Parents of final stage: List()
26/02/13 12:05:25 INFO DAGScheduler: Missing parents: List()
26/02/13 12:05:25 INFO DAGScheduler: Submitting ResultStage 75 (MapPartitionsRDD[258] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:05:25 INFO MemoryStore: Block broadcast_72 stored as values in memory (estimated size 49.9 KiB, free 433.9 MiB)
26/02/13 12:05:25 INFO MemoryStore: Block broadcast_72_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.9 MiB)
26/02/13 12:05:25 INFO BlockManagerInfo: Added broadcast_72_piece0 in memory on spark-master:42073 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 12:05:25 INFO SparkContext: Created broadcast 72 from broadcast at DAGScheduler.scala:1585
26/02/13 12:05:25 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 75 (MapPartitionsRDD[258] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
26/02/13 12:05:25 INFO TaskSchedulerImpl: Adding task set 75.0 with 3 tasks resource profile 0
26/02/13 12:05:25 INFO TaskSetManager: Starting task 1.0 in stage 75.0 (TID 146) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:05:25 INFO TaskSetManager: Starting task 0.0 in stage 75.0 (TID 147) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:05:25 INFO TaskSetManager: Starting task 2.0 in stage 75.0 (TID 148) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:05:25 INFO BlockManagerInfo: Added broadcast_72_piece0 in memory on 172.18.0.14:37571 (size: 19.5 KiB, free: 434.0 MiB)
26/02/13 12:05:25 INFO BlockManagerInfo: Added broadcast_72_piece0 in memory on 172.18.0.15:37717 (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 12:05:25 INFO BlockManagerInfo: Added broadcast_71_piece0 in memory on 172.18.0.14:37571 (size: 107.2 KiB, free: 433.9 MiB)
26/02/13 12:05:25 INFO BlockManagerInfo: Added broadcast_71_piece0 in memory on 172.18.0.15:37717 (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 12:05:25 INFO BlockManagerInfo: Removed broadcast_68_piece0 on spark-master:42073 in memory (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 12:05:25 INFO BlockManagerInfo: Removed broadcast_68_piece0 on 172.18.0.15:37717 in memory (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 12:05:25 INFO BlockManagerInfo: Removed broadcast_68_piece0 on 172.18.0.14:37571 in memory (size: 107.2 KiB, free: 434.0 MiB)
26/02/13 12:05:26 INFO TaskSetManager: Finished task 2.0 in stage 75.0 (TID 148) in 580 ms on 172.18.0.14 (executor 1) (1/3)
26/02/13 12:05:26 INFO TaskSetManager: Finished task 0.0 in stage 75.0 (TID 147) in 581 ms on 172.18.0.14 (executor 1) (2/3)
26/02/13 12:05:26 INFO TaskSetManager: Finished task 1.0 in stage 75.0 (TID 146) in 618 ms on 172.18.0.15 (executor 0) (3/3)
26/02/13 12:05:26 INFO TaskSchedulerImpl: Removed TaskSet 75.0, whose tasks have all completed, from pool 
26/02/13 12:05:26 INFO DAGScheduler: ResultStage 75 (start at NativeMethodAccessorImpl.java:0) finished in 0.626 s
26/02/13 12:05:26 INFO DAGScheduler: Job 48 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 12:05:26 INFO TaskSchedulerImpl: Killing all running tasks in stage 75: Stage finished
26/02/13 12:05:26 INFO DAGScheduler: Job 48 finished: start at NativeMethodAccessorImpl.java:0, took 0.628363 s
26/02/13 12:05:26 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 142, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@6364a0cb] is committing.
26/02/13 12:05:26 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 142, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@6364a0cb] committed.
26/02/13 12:05:26 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/142 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.142.8fa6d313-8249-455c-b186-d88c4a13be73.tmp
26/02/13 12:05:26 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.142.8fa6d313-8249-455c-b186-d88c4a13be73.tmp to file:/tmp/spark-checkpoint-enrichment/commits/142
26/02/13 12:05:26 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "4a161d66-225b-4b29-812d-ddec3c05364e",
  "name" : null,
  "timestamp" : "2026-02-13T12:05:25.618Z",
  "batchId" : 142,
  "numInputRows" : 111,
  "inputRowsPerSecond" : 10090.909090909092,
  "processedRowsPerSecond" : 114.55108359133128,
  "durationMs" : {
    "addBatch" : 793,
    "commitOffsets" : 53,
    "getBatch" : 0,
    "latestOffset" : 2,
    "queryPlanning" : 40,
    "triggerExecution" : 969,
    "walCommit" : 81
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 1263,
        "1" : 1533,
        "0" : 1157
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 1300,
        "1" : 1570,
        "0" : 1194
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 1300,
        "1" : 1570,
        "0" : 1194
      }
    },
    "numInputRows" : 111,
    "inputRowsPerSecond" : 10090.909090909092,
    "processedRowsPerSecond" : 114.55108359133128,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@1a6a42c2",
    "numOutputRows" : 28
  }
}
26/02/13 12:05:26 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/143 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.143.addb4a2b-2d56-4076-8fe4-3646a3ecdacd.tmp
26/02/13 12:05:26 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.143.addb4a2b-2d56-4076-8fe4-3646a3ecdacd.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/143
26/02/13 12:05:26 INFO MicroBatchExecution: Committed offsets for batch 143. Metadata OffsetSeqMetadata(0,1770984326589,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 12:05:26 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:05:26 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:05:26 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:05:26 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:05:26 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:05:26 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:05:26 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#19031 - origin_code.nullCount#19030) > 0)
26/02/13 12:05:26 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#19036 - destination_code.nullCount#19035) > 0)
26/02/13 12:05:26 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#19066 - callsign.nullCount#19065) > 0)
26/02/13 12:05:26 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 12:05:26 INFO DAGScheduler: Got job 49 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 12:05:26 INFO DAGScheduler: Final stage: ResultStage 77 (start at NativeMethodAccessorImpl.java:0)
26/02/13 12:05:26 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 76)
26/02/13 12:05:26 INFO DAGScheduler: Missing parents: List()
26/02/13 12:05:26 INFO DAGScheduler: Submitting ResultStage 77 (MapPartitionsRDD[263] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:05:26 INFO MemoryStore: Block broadcast_73 stored as values in memory (estimated size 99.3 KiB, free 433.9 MiB)
26/02/13 12:05:26 INFO MemoryStore: Block broadcast_73_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 433.9 MiB)
26/02/13 12:05:26 INFO BlockManagerInfo: Added broadcast_73_piece0 in memory on spark-master:42073 (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 12:05:26 INFO SparkContext: Created broadcast 73 from broadcast at DAGScheduler.scala:1585
26/02/13 12:05:26 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 77 (MapPartitionsRDD[263] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 12:05:26 INFO TaskSchedulerImpl: Adding task set 77.0 with 4 tasks resource profile 0
26/02/13 12:05:26 INFO TaskSetManager: Starting task 0.0 in stage 77.0 (TID 149) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:05:26 INFO TaskSetManager: Starting task 1.0 in stage 77.0 (TID 150) (172.18.0.14, executor 1, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:05:26 INFO BlockManagerInfo: Added broadcast_73_piece0 in memory on 172.18.0.14:37571 (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 12:05:26 INFO TaskSetManager: Starting task 2.0 in stage 77.0 (TID 151) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:05:26 INFO TaskSetManager: Finished task 0.0 in stage 77.0 (TID 149) in 23 ms on 172.18.0.14 (executor 1) (1/4)
26/02/13 12:05:26 INFO TaskSetManager: Starting task 3.0 in stage 77.0 (TID 152) (172.18.0.14, executor 1, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:05:26 INFO TaskSetManager: Finished task 1.0 in stage 77.0 (TID 150) in 25 ms on 172.18.0.14 (executor 1) (2/4)
26/02/13 12:05:26 INFO TaskSetManager: Finished task 2.0 in stage 77.0 (TID 151) in 17 ms on 172.18.0.14 (executor 1) (3/4)
26/02/13 12:05:26 INFO TaskSetManager: Finished task 3.0 in stage 77.0 (TID 152) in 15 ms on 172.18.0.14 (executor 1) (4/4)
26/02/13 12:05:26 INFO TaskSchedulerImpl: Removed TaskSet 77.0, whose tasks have all completed, from pool 
26/02/13 12:05:26 INFO DAGScheduler: ResultStage 77 (start at NativeMethodAccessorImpl.java:0) finished in 0.047 s
26/02/13 12:05:26 INFO DAGScheduler: Job 49 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 12:05:26 INFO TaskSchedulerImpl: Killing all running tasks in stage 77: Stage finished
26/02/13 12:05:26 INFO DAGScheduler: Job 49 finished: start at NativeMethodAccessorImpl.java:0, took 0.049416 s
26/02/13 12:05:26 INFO MemoryStore: Block broadcast_74_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.8 MiB)
26/02/13 12:05:26 INFO BlockManagerInfo: Added broadcast_74_piece0 in memory on spark-master:42073 (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 12:05:26 INFO SparkContext: Created broadcast 74 from start at NativeMethodAccessorImpl.java:0
26/02/13 12:05:26 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 143, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@6979880e]. The input RDD has 3 partitions.
26/02/13 12:05:26 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 12:05:26 INFO DAGScheduler: Got job 50 (start at NativeMethodAccessorImpl.java:0) with 3 output partitions
26/02/13 12:05:26 INFO DAGScheduler: Final stage: ResultStage 78 (start at NativeMethodAccessorImpl.java:0)
26/02/13 12:05:26 INFO DAGScheduler: Parents of final stage: List()
26/02/13 12:05:26 INFO DAGScheduler: Missing parents: List()
26/02/13 12:05:26 INFO DAGScheduler: Submitting ResultStage 78 (MapPartitionsRDD[269] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:05:26 INFO MemoryStore: Block broadcast_75 stored as values in memory (estimated size 49.9 KiB, free 433.7 MiB)
26/02/13 12:05:26 INFO MemoryStore: Block broadcast_75_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.7 MiB)
26/02/13 12:05:26 INFO BlockManagerInfo: Added broadcast_75_piece0 in memory on spark-master:42073 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 12:05:26 INFO SparkContext: Created broadcast 75 from broadcast at DAGScheduler.scala:1585
26/02/13 12:05:26 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 78 (MapPartitionsRDD[269] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
26/02/13 12:05:26 INFO TaskSchedulerImpl: Adding task set 78.0 with 3 tasks resource profile 0
26/02/13 12:05:26 INFO TaskSetManager: Starting task 0.0 in stage 78.0 (TID 153) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:05:26 INFO TaskSetManager: Starting task 1.0 in stage 78.0 (TID 154) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:05:26 INFO TaskSetManager: Starting task 2.0 in stage 78.0 (TID 155) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:05:26 INFO BlockManagerInfo: Added broadcast_75_piece0 in memory on 172.18.0.14:37571 (size: 19.5 KiB, free: 434.0 MiB)
26/02/13 12:05:26 INFO BlockManagerInfo: Added broadcast_75_piece0 in memory on 172.18.0.15:37717 (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 12:05:26 INFO BlockManagerInfo: Added broadcast_74_piece0 in memory on 172.18.0.14:37571 (size: 107.2 KiB, free: 433.9 MiB)
26/02/13 12:05:26 INFO BlockManagerInfo: Added broadcast_74_piece0 in memory on 172.18.0.15:37717 (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 12:05:26 INFO TaskSetManager: Finished task 2.0 in stage 78.0 (TID 155) in 104 ms on 172.18.0.14 (executor 1) (1/3)
26/02/13 12:05:26 INFO TaskSetManager: Finished task 0.0 in stage 78.0 (TID 153) in 105 ms on 172.18.0.14 (executor 1) (2/3)
26/02/13 12:05:26 INFO TaskSetManager: Finished task 1.0 in stage 78.0 (TID 154) in 157 ms on 172.18.0.15 (executor 0) (3/3)
26/02/13 12:05:26 INFO TaskSchedulerImpl: Removed TaskSet 78.0, whose tasks have all completed, from pool 
26/02/13 12:05:26 INFO DAGScheduler: ResultStage 78 (start at NativeMethodAccessorImpl.java:0) finished in 0.163 s
26/02/13 12:05:26 INFO DAGScheduler: Job 50 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 12:05:26 INFO TaskSchedulerImpl: Killing all running tasks in stage 78: Stage finished
26/02/13 12:05:26 INFO DAGScheduler: Job 50 finished: start at NativeMethodAccessorImpl.java:0, took 0.165443 s
26/02/13 12:05:26 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 143, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@6979880e] is committing.
26/02/13 12:05:26 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 143, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@6979880e] committed.
26/02/13 12:05:26 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/143 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.143.64de0f7c-7bb6-42f5-ac85-76ea6669cabe.tmp
26/02/13 12:05:27 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.143.64de0f7c-7bb6-42f5-ac85-76ea6669cabe.tmp to file:/tmp/spark-checkpoint-enrichment/commits/143
26/02/13 12:05:27 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "4a161d66-225b-4b29-812d-ddec3c05364e",
  "name" : null,
  "timestamp" : "2026-02-13T12:05:26.588Z",
  "batchId" : 143,
  "numInputRows" : 131,
  "inputRowsPerSecond" : 135.0515463917526,
  "processedRowsPerSecond" : 282.9373650107991,
  "durationMs" : {
    "addBatch" : 308,
    "commitOffsets" : 69,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 29,
    "triggerExecution" : 463,
    "walCommit" : 56
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 1300,
        "1" : 1570,
        "0" : 1194
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 1342,
        "1" : 1625,
        "0" : 1228
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 1342,
        "1" : 1625,
        "0" : 1228
      }
    },
    "numInputRows" : 131,
    "inputRowsPerSecond" : 135.0515463917526,
    "processedRowsPerSecond" : 282.9373650107991,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@1a6a42c2",
    "numOutputRows" : 26
  }
}
26/02/13 12:05:28 INFO BlockManagerInfo: Removed broadcast_71_piece0 on spark-master:42073 in memory (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 12:05:28 INFO BlockManagerInfo: Removed broadcast_71_piece0 on 172.18.0.14:37571 in memory (size: 107.2 KiB, free: 434.0 MiB)
26/02/13 12:05:28 INFO BlockManagerInfo: Removed broadcast_71_piece0 on 172.18.0.15:37717 in memory (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 12:05:28 INFO BlockManagerInfo: Removed broadcast_75_piece0 on spark-master:42073 in memory (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 12:05:28 INFO BlockManagerInfo: Removed broadcast_75_piece0 on 172.18.0.15:37717 in memory (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 12:05:28 INFO BlockManagerInfo: Removed broadcast_75_piece0 on 172.18.0.14:37571 in memory (size: 19.5 KiB, free: 434.0 MiB)
26/02/13 12:05:28 INFO BlockManagerInfo: Removed broadcast_73_piece0 on spark-master:42073 in memory (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 12:05:28 INFO BlockManagerInfo: Removed broadcast_73_piece0 on 172.18.0.14:37571 in memory (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 12:05:28 INFO BlockManagerInfo: Removed broadcast_72_piece0 on spark-master:42073 in memory (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 12:05:28 INFO BlockManagerInfo: Removed broadcast_72_piece0 on 172.18.0.14:37571 in memory (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 12:05:28 INFO BlockManagerInfo: Removed broadcast_72_piece0 on 172.18.0.15:37717 in memory (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 12:05:37 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:05:42 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/144 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.144.4a6ff533-3ecf-402a-9c64-7221ca8658c2.tmp
26/02/13 12:05:42 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.144.4a6ff533-3ecf-402a-9c64-7221ca8658c2.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/144
26/02/13 12:05:42 INFO MicroBatchExecution: Committed offsets for batch 144. Metadata OffsetSeqMetadata(0,1770984342319,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 12:05:42 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:05:42 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:05:42 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:05:42 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:05:42 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:05:42 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:05:42 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#19885 - origin_code.nullCount#19884) > 0)
26/02/13 12:05:42 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#19890 - destination_code.nullCount#19889) > 0)
26/02/13 12:05:42 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#19920 - callsign.nullCount#19919) > 0)
26/02/13 12:05:42 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 12:05:42 INFO DAGScheduler: Got job 51 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 12:05:42 INFO DAGScheduler: Final stage: ResultStage 80 (start at NativeMethodAccessorImpl.java:0)
26/02/13 12:05:42 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 79)
26/02/13 12:05:42 INFO DAGScheduler: Missing parents: List()
26/02/13 12:05:42 INFO DAGScheduler: Submitting ResultStage 80 (MapPartitionsRDD[274] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:05:42 INFO MemoryStore: Block broadcast_76 stored as values in memory (estimated size 99.3 KiB, free 434.0 MiB)
26/02/13 12:05:42 INFO MemoryStore: Block broadcast_76_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 433.9 MiB)
26/02/13 12:05:42 INFO BlockManagerInfo: Added broadcast_76_piece0 in memory on spark-master:42073 (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 12:05:42 INFO SparkContext: Created broadcast 76 from broadcast at DAGScheduler.scala:1585
26/02/13 12:05:42 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 80 (MapPartitionsRDD[274] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 12:05:42 INFO TaskSchedulerImpl: Adding task set 80.0 with 4 tasks resource profile 0
26/02/13 12:05:42 INFO TaskSetManager: Starting task 0.0 in stage 80.0 (TID 156) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:05:42 INFO TaskSetManager: Starting task 1.0 in stage 80.0 (TID 157) (172.18.0.14, executor 1, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:05:42 INFO BlockManagerInfo: Added broadcast_76_piece0 in memory on 172.18.0.14:37571 (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 12:05:42 INFO TaskSetManager: Starting task 2.0 in stage 80.0 (TID 158) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:05:42 INFO TaskSetManager: Finished task 0.0 in stage 80.0 (TID 156) in 20 ms on 172.18.0.14 (executor 1) (1/4)
26/02/13 12:05:42 INFO TaskSetManager: Starting task 3.0 in stage 80.0 (TID 159) (172.18.0.14, executor 1, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:05:42 INFO TaskSetManager: Finished task 1.0 in stage 80.0 (TID 157) in 24 ms on 172.18.0.14 (executor 1) (2/4)
26/02/13 12:05:42 INFO TaskSetManager: Finished task 2.0 in stage 80.0 (TID 158) in 15 ms on 172.18.0.14 (executor 1) (3/4)
26/02/13 12:05:42 INFO TaskSetManager: Finished task 3.0 in stage 80.0 (TID 159) in 18 ms on 172.18.0.14 (executor 1) (4/4)
26/02/13 12:05:42 INFO TaskSchedulerImpl: Removed TaskSet 80.0, whose tasks have all completed, from pool 
26/02/13 12:05:42 INFO DAGScheduler: ResultStage 80 (start at NativeMethodAccessorImpl.java:0) finished in 0.048 s
26/02/13 12:05:42 INFO DAGScheduler: Job 51 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 12:05:42 INFO TaskSchedulerImpl: Killing all running tasks in stage 80: Stage finished
26/02/13 12:05:42 INFO DAGScheduler: Job 51 finished: start at NativeMethodAccessorImpl.java:0, took 0.050834 s
26/02/13 12:05:42 INFO MemoryStore: Block broadcast_77_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.8 MiB)
26/02/13 12:05:42 INFO BlockManagerInfo: Added broadcast_77_piece0 in memory on spark-master:42073 (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 12:05:42 INFO SparkContext: Created broadcast 77 from start at NativeMethodAccessorImpl.java:0
26/02/13 12:05:42 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 144, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@78c71024]. The input RDD has 3 partitions.
26/02/13 12:05:42 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 12:05:42 INFO DAGScheduler: Got job 52 (start at NativeMethodAccessorImpl.java:0) with 3 output partitions
26/02/13 12:05:42 INFO DAGScheduler: Final stage: ResultStage 81 (start at NativeMethodAccessorImpl.java:0)
26/02/13 12:05:42 INFO DAGScheduler: Parents of final stage: List()
26/02/13 12:05:42 INFO DAGScheduler: Missing parents: List()
26/02/13 12:05:42 INFO DAGScheduler: Submitting ResultStage 81 (MapPartitionsRDD[280] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:05:42 INFO MemoryStore: Block broadcast_78 stored as values in memory (estimated size 49.9 KiB, free 433.8 MiB)
26/02/13 12:05:42 INFO MemoryStore: Block broadcast_78_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.8 MiB)
26/02/13 12:05:42 INFO BlockManagerInfo: Added broadcast_78_piece0 in memory on spark-master:42073 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 12:05:42 INFO SparkContext: Created broadcast 78 from broadcast at DAGScheduler.scala:1585
26/02/13 12:05:42 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 81 (MapPartitionsRDD[280] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
26/02/13 12:05:42 INFO TaskSchedulerImpl: Adding task set 81.0 with 3 tasks resource profile 0
26/02/13 12:05:42 INFO TaskSetManager: Starting task 0.0 in stage 81.0 (TID 160) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:05:42 INFO TaskSetManager: Starting task 1.0 in stage 81.0 (TID 161) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:05:42 INFO TaskSetManager: Starting task 2.0 in stage 81.0 (TID 162) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:05:42 INFO BlockManagerInfo: Added broadcast_78_piece0 in memory on 172.18.0.14:37571 (size: 19.5 KiB, free: 434.0 MiB)
26/02/13 12:05:42 INFO BlockManagerInfo: Added broadcast_78_piece0 in memory on 172.18.0.15:37717 (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 12:05:42 INFO BlockManagerInfo: Added broadcast_77_piece0 in memory on 172.18.0.14:37571 (size: 107.2 KiB, free: 433.9 MiB)
26/02/13 12:05:42 INFO BlockManagerInfo: Added broadcast_77_piece0 in memory on 172.18.0.15:37717 (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 12:05:43 INFO TaskSetManager: Finished task 0.0 in stage 81.0 (TID 160) in 566 ms on 172.18.0.14 (executor 1) (1/3)
26/02/13 12:05:43 INFO TaskSetManager: Finished task 2.0 in stage 81.0 (TID 162) in 566 ms on 172.18.0.14 (executor 1) (2/3)
26/02/13 12:05:43 INFO TaskSetManager: Finished task 1.0 in stage 81.0 (TID 161) in 595 ms on 172.18.0.15 (executor 0) (3/3)
26/02/13 12:05:43 INFO TaskSchedulerImpl: Removed TaskSet 81.0, whose tasks have all completed, from pool 
26/02/13 12:05:43 INFO DAGScheduler: ResultStage 81 (start at NativeMethodAccessorImpl.java:0) finished in 0.601 s
26/02/13 12:05:43 INFO DAGScheduler: Job 52 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 12:05:43 INFO TaskSchedulerImpl: Killing all running tasks in stage 81: Stage finished
26/02/13 12:05:43 INFO DAGScheduler: Job 52 finished: start at NativeMethodAccessorImpl.java:0, took 0.603387 s
26/02/13 12:05:43 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 144, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@78c71024] is committing.
26/02/13 12:05:43 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 144, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@78c71024] committed.
26/02/13 12:05:43 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/144 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.144.a2afafed-ff77-4ce3-a58a-ae4e86bc4326.tmp
26/02/13 12:05:43 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.144.a2afafed-ff77-4ce3-a58a-ae4e86bc4326.tmp to file:/tmp/spark-checkpoint-enrichment/commits/144
26/02/13 12:05:43 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "4a161d66-225b-4b29-812d-ddec3c05364e",
  "name" : null,
  "timestamp" : "2026-02-13T12:05:42.318Z",
  "batchId" : 144,
  "numInputRows" : 89,
  "inputRowsPerSecond" : 7416.666666666666,
  "processedRowsPerSecond" : 91.75257731958763,
  "durationMs" : {
    "addBatch" : 742,
    "commitOffsets" : 58,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 49,
    "triggerExecution" : 970,
    "walCommit" : 118
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 1342,
        "1" : 1625,
        "0" : 1228
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 1373,
        "1" : 1658,
        "0" : 1253
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 1373,
        "1" : 1658,
        "0" : 1253
      }
    },
    "numInputRows" : 89,
    "inputRowsPerSecond" : 7416.666666666666,
    "processedRowsPerSecond" : 91.75257731958763,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@1a6a42c2",
    "numOutputRows" : 24
  }
}
26/02/13 12:05:43 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/145 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.145.b95780c0-d475-410a-80a6-e08631c12b0e.tmp
26/02/13 12:05:43 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.145.b95780c0-d475-410a-80a6-e08631c12b0e.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/145
26/02/13 12:05:43 INFO MicroBatchExecution: Committed offsets for batch 145. Metadata OffsetSeqMetadata(0,1770984343289,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 12:05:43 INFO BlockManagerInfo: Removed broadcast_74_piece0 on spark-master:42073 in memory (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 12:05:43 INFO BlockManagerInfo: Removed broadcast_74_piece0 on 172.18.0.14:37571 in memory (size: 107.2 KiB, free: 434.0 MiB)
26/02/13 12:05:43 INFO BlockManagerInfo: Removed broadcast_74_piece0 on 172.18.0.15:37717 in memory (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 12:05:43 INFO BlockManagerInfo: Removed broadcast_76_piece0 on spark-master:42073 in memory (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 12:05:43 INFO BlockManagerInfo: Removed broadcast_76_piece0 on 172.18.0.14:37571 in memory (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 12:05:43 INFO BlockManagerInfo: Removed broadcast_78_piece0 on spark-master:42073 in memory (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 12:05:43 INFO BlockManagerInfo: Removed broadcast_78_piece0 on 172.18.0.15:37717 in memory (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 12:05:43 INFO BlockManagerInfo: Removed broadcast_78_piece0 on 172.18.0.14:37571 in memory (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 12:05:43 INFO BlockManagerInfo: Removed broadcast_77_piece0 on spark-master:42073 in memory (size: 107.2 KiB, free: 434.4 MiB)
26/02/13 12:05:43 INFO BlockManagerInfo: Removed broadcast_77_piece0 on 172.18.0.15:37717 in memory (size: 107.2 KiB, free: 434.4 MiB)
26/02/13 12:05:43 INFO BlockManagerInfo: Removed broadcast_77_piece0 on 172.18.0.14:37571 in memory (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 12:05:43 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:05:43 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:05:43 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:05:43 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:05:43 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:05:43 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:05:43 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#20739 - origin_code.nullCount#20738) > 0)
26/02/13 12:05:43 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#20744 - destination_code.nullCount#20743) > 0)
26/02/13 12:05:43 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#20774 - callsign.nullCount#20773) > 0)
26/02/13 12:05:43 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 12:05:43 INFO DAGScheduler: Got job 53 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 12:05:43 INFO DAGScheduler: Final stage: ResultStage 83 (start at NativeMethodAccessorImpl.java:0)
26/02/13 12:05:43 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 82)
26/02/13 12:05:43 INFO DAGScheduler: Missing parents: List()
26/02/13 12:05:43 INFO DAGScheduler: Submitting ResultStage 83 (MapPartitionsRDD[285] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:05:43 INFO MemoryStore: Block broadcast_79 stored as values in memory (estimated size 99.3 KiB, free 434.1 MiB)
26/02/13 12:05:43 INFO MemoryStore: Block broadcast_79_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 434.0 MiB)
26/02/13 12:05:43 INFO BlockManagerInfo: Added broadcast_79_piece0 in memory on spark-master:42073 (size: 33.3 KiB, free: 434.3 MiB)
26/02/13 12:05:43 INFO SparkContext: Created broadcast 79 from broadcast at DAGScheduler.scala:1585
26/02/13 12:05:43 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 83 (MapPartitionsRDD[285] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 12:05:43 INFO TaskSchedulerImpl: Adding task set 83.0 with 4 tasks resource profile 0
26/02/13 12:05:43 INFO TaskSetManager: Starting task 0.0 in stage 83.0 (TID 163) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:05:43 INFO TaskSetManager: Starting task 1.0 in stage 83.0 (TID 164) (172.18.0.14, executor 1, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:05:43 INFO BlockManagerInfo: Added broadcast_79_piece0 in memory on 172.18.0.14:37571 (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 12:05:43 INFO TaskSetManager: Starting task 2.0 in stage 83.0 (TID 165) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:05:43 INFO TaskSetManager: Starting task 3.0 in stage 83.0 (TID 166) (172.18.0.14, executor 1, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:05:43 INFO TaskSetManager: Finished task 0.0 in stage 83.0 (TID 163) in 30 ms on 172.18.0.14 (executor 1) (1/4)
26/02/13 12:05:43 INFO TaskSetManager: Finished task 1.0 in stage 83.0 (TID 164) in 31 ms on 172.18.0.14 (executor 1) (2/4)
26/02/13 12:05:43 INFO TaskSetManager: Finished task 3.0 in stage 83.0 (TID 166) in 24 ms on 172.18.0.14 (executor 1) (3/4)
26/02/13 12:05:43 INFO TaskSetManager: Finished task 2.0 in stage 83.0 (TID 165) in 26 ms on 172.18.0.14 (executor 1) (4/4)
26/02/13 12:05:43 INFO TaskSchedulerImpl: Removed TaskSet 83.0, whose tasks have all completed, from pool 
26/02/13 12:05:43 INFO DAGScheduler: ResultStage 83 (start at NativeMethodAccessorImpl.java:0) finished in 0.063 s
26/02/13 12:05:43 INFO DAGScheduler: Job 53 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 12:05:43 INFO TaskSchedulerImpl: Killing all running tasks in stage 83: Stage finished
26/02/13 12:05:43 INFO DAGScheduler: Job 53 finished: start at NativeMethodAccessorImpl.java:0, took 0.065896 s
26/02/13 12:05:43 INFO MemoryStore: Block broadcast_80_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.9 MiB)
26/02/13 12:05:43 INFO BlockManagerInfo: Added broadcast_80_piece0 in memory on spark-master:42073 (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 12:05:43 INFO SparkContext: Created broadcast 80 from start at NativeMethodAccessorImpl.java:0
26/02/13 12:05:43 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 145, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@180d0354]. The input RDD has 3 partitions.
26/02/13 12:05:43 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 12:05:43 INFO DAGScheduler: Got job 54 (start at NativeMethodAccessorImpl.java:0) with 3 output partitions
26/02/13 12:05:43 INFO DAGScheduler: Final stage: ResultStage 84 (start at NativeMethodAccessorImpl.java:0)
26/02/13 12:05:43 INFO DAGScheduler: Parents of final stage: List()
26/02/13 12:05:43 INFO DAGScheduler: Missing parents: List()
26/02/13 12:05:43 INFO DAGScheduler: Submitting ResultStage 84 (MapPartitionsRDD[291] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:05:43 INFO MemoryStore: Block broadcast_81 stored as values in memory (estimated size 49.9 KiB, free 433.9 MiB)
26/02/13 12:05:43 INFO MemoryStore: Block broadcast_81_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.9 MiB)
26/02/13 12:05:43 INFO BlockManagerInfo: Added broadcast_81_piece0 in memory on spark-master:42073 (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 12:05:43 INFO SparkContext: Created broadcast 81 from broadcast at DAGScheduler.scala:1585
26/02/13 12:05:43 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 84 (MapPartitionsRDD[291] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
26/02/13 12:05:43 INFO TaskSchedulerImpl: Adding task set 84.0 with 3 tasks resource profile 0
26/02/13 12:05:43 INFO TaskSetManager: Starting task 1.0 in stage 84.0 (TID 167) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:05:43 INFO TaskSetManager: Starting task 0.0 in stage 84.0 (TID 168) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:05:43 INFO TaskSetManager: Starting task 2.0 in stage 84.0 (TID 169) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:05:43 INFO BlockManagerInfo: Added broadcast_81_piece0 in memory on 172.18.0.14:37571 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 12:05:43 INFO BlockManagerInfo: Added broadcast_81_piece0 in memory on 172.18.0.15:37717 (size: 19.5 KiB, free: 434.4 MiB)
26/02/13 12:05:43 INFO BlockManagerInfo: Added broadcast_80_piece0 in memory on 172.18.0.14:37571 (size: 107.2 KiB, free: 434.0 MiB)
26/02/13 12:05:43 INFO BlockManagerInfo: Added broadcast_80_piece0 in memory on 172.18.0.15:37717 (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 12:05:43 INFO TaskSetManager: Finished task 0.0 in stage 84.0 (TID 168) in 68 ms on 172.18.0.14 (executor 1) (1/3)
26/02/13 12:05:43 INFO TaskSetManager: Finished task 2.0 in stage 84.0 (TID 169) in 76 ms on 172.18.0.14 (executor 1) (2/3)
26/02/13 12:05:43 INFO TaskSetManager: Finished task 1.0 in stage 84.0 (TID 167) in 89 ms on 172.18.0.15 (executor 0) (3/3)
26/02/13 12:05:43 INFO TaskSchedulerImpl: Removed TaskSet 84.0, whose tasks have all completed, from pool 
26/02/13 12:05:43 INFO DAGScheduler: ResultStage 84 (start at NativeMethodAccessorImpl.java:0) finished in 0.095 s
26/02/13 12:05:43 INFO DAGScheduler: Job 54 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 12:05:43 INFO TaskSchedulerImpl: Killing all running tasks in stage 84: Stage finished
26/02/13 12:05:43 INFO DAGScheduler: Job 54 finished: start at NativeMethodAccessorImpl.java:0, took 0.097014 s
26/02/13 12:05:43 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 145, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@180d0354] is committing.
26/02/13 12:05:43 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 145, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@180d0354] committed.
26/02/13 12:05:43 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/145 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.145.73cbadf2-8902-4d70-8fe1-81e82ec3517f.tmp
26/02/13 12:05:43 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.145.73cbadf2-8902-4d70-8fe1-81e82ec3517f.tmp to file:/tmp/spark-checkpoint-enrichment/commits/145
26/02/13 12:05:43 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "4a161d66-225b-4b29-812d-ddec3c05364e",
  "name" : null,
  "timestamp" : "2026-02-13T12:05:43.288Z",
  "batchId" : 145,
  "numInputRows" : 156,
  "inputRowsPerSecond" : 160.82474226804123,
  "processedRowsPerSecond" : 371.42857142857144,
  "durationMs" : {
    "addBatch" : 260,
    "commitOffsets" : 57,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 45,
    "triggerExecution" : 420,
    "walCommit" : 57
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 1373,
        "1" : 1658,
        "0" : 1253
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 1425,
        "1" : 1717,
        "0" : 1298
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 1425,
        "1" : 1717,
        "0" : 1298
      }
    },
    "numInputRows" : 156,
    "inputRowsPerSecond" : 160.82474226804123,
    "processedRowsPerSecond" : 371.42857142857144,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@1a6a42c2",
    "numOutputRows" : 30
  }
}
26/02/13 12:05:52 INFO BlockManagerInfo: Removed broadcast_79_piece0 on spark-master:42073 in memory (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 12:05:52 INFO BlockManagerInfo: Removed broadcast_79_piece0 on 172.18.0.14:37571 in memory (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 12:05:52 INFO BlockManagerInfo: Removed broadcast_81_piece0 on spark-master:42073 in memory (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 12:05:52 INFO BlockManagerInfo: Removed broadcast_81_piece0 on 172.18.0.15:37717 in memory (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 12:05:52 INFO BlockManagerInfo: Removed broadcast_81_piece0 on 172.18.0.14:37571 in memory (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 12:05:53 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:05:59 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/146 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.146.ccbf1333-761d-4d69-81e7-1893a60b4c68.tmp
26/02/13 12:05:59 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.146.ccbf1333-761d-4d69-81e7-1893a60b4c68.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/146
26/02/13 12:05:59 INFO MicroBatchExecution: Committed offsets for batch 146. Metadata OffsetSeqMetadata(0,1770984359110,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 12:05:59 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:05:59 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:05:59 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:05:59 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:05:59 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:05:59 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:05:59 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#21593 - origin_code.nullCount#21592) > 0)
26/02/13 12:05:59 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#21598 - destination_code.nullCount#21597) > 0)
26/02/13 12:05:59 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#21628 - callsign.nullCount#21627) > 0)
26/02/13 12:05:59 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 12:05:59 INFO DAGScheduler: Got job 55 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 12:05:59 INFO DAGScheduler: Final stage: ResultStage 86 (start at NativeMethodAccessorImpl.java:0)
26/02/13 12:05:59 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 85)
26/02/13 12:05:59 INFO DAGScheduler: Missing parents: List()
26/02/13 12:05:59 INFO DAGScheduler: Submitting ResultStage 86 (MapPartitionsRDD[296] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:05:59 INFO MemoryStore: Block broadcast_82 stored as values in memory (estimated size 99.3 KiB, free 434.0 MiB)
26/02/13 12:05:59 INFO MemoryStore: Block broadcast_82_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 433.9 MiB)
26/02/13 12:05:59 INFO BlockManagerInfo: Added broadcast_82_piece0 in memory on spark-master:42073 (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 12:05:59 INFO SparkContext: Created broadcast 82 from broadcast at DAGScheduler.scala:1585
26/02/13 12:05:59 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 86 (MapPartitionsRDD[296] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 12:05:59 INFO TaskSchedulerImpl: Adding task set 86.0 with 4 tasks resource profile 0
26/02/13 12:05:59 INFO TaskSetManager: Starting task 0.0 in stage 86.0 (TID 170) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:05:59 INFO TaskSetManager: Starting task 1.0 in stage 86.0 (TID 171) (172.18.0.14, executor 1, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:05:59 INFO BlockManagerInfo: Added broadcast_82_piece0 in memory on 172.18.0.14:37571 (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 12:05:59 INFO TaskSetManager: Starting task 2.0 in stage 86.0 (TID 172) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:05:59 INFO TaskSetManager: Finished task 0.0 in stage 86.0 (TID 170) in 28 ms on 172.18.0.14 (executor 1) (1/4)
26/02/13 12:05:59 INFO TaskSetManager: Starting task 3.0 in stage 86.0 (TID 173) (172.18.0.14, executor 1, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:05:59 INFO TaskSetManager: Finished task 1.0 in stage 86.0 (TID 171) in 37 ms on 172.18.0.14 (executor 1) (2/4)
26/02/13 12:05:59 INFO TaskSetManager: Finished task 2.0 in stage 86.0 (TID 172) in 17 ms on 172.18.0.14 (executor 1) (3/4)
26/02/13 12:05:59 INFO TaskSetManager: Finished task 3.0 in stage 86.0 (TID 173) in 25 ms on 172.18.0.14 (executor 1) (4/4)
26/02/13 12:05:59 INFO TaskSchedulerImpl: Removed TaskSet 86.0, whose tasks have all completed, from pool 
26/02/13 12:05:59 INFO DAGScheduler: ResultStage 86 (start at NativeMethodAccessorImpl.java:0) finished in 0.073 s
26/02/13 12:05:59 INFO DAGScheduler: Job 55 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 12:05:59 INFO TaskSchedulerImpl: Killing all running tasks in stage 86: Stage finished
26/02/13 12:05:59 INFO DAGScheduler: Job 55 finished: start at NativeMethodAccessorImpl.java:0, took 0.077198 s
26/02/13 12:05:59 INFO MemoryStore: Block broadcast_83_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.8 MiB)
26/02/13 12:05:59 INFO BlockManagerInfo: Added broadcast_83_piece0 in memory on spark-master:42073 (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 12:05:59 INFO SparkContext: Created broadcast 83 from start at NativeMethodAccessorImpl.java:0
26/02/13 12:05:59 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 146, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@40aa734c]. The input RDD has 3 partitions.
26/02/13 12:05:59 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 12:05:59 INFO DAGScheduler: Got job 56 (start at NativeMethodAccessorImpl.java:0) with 3 output partitions
26/02/13 12:05:59 INFO DAGScheduler: Final stage: ResultStage 87 (start at NativeMethodAccessorImpl.java:0)
26/02/13 12:05:59 INFO DAGScheduler: Parents of final stage: List()
26/02/13 12:05:59 INFO DAGScheduler: Missing parents: List()
26/02/13 12:05:59 INFO DAGScheduler: Submitting ResultStage 87 (MapPartitionsRDD[302] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:05:59 INFO MemoryStore: Block broadcast_84 stored as values in memory (estimated size 49.9 KiB, free 433.8 MiB)
26/02/13 12:05:59 INFO MemoryStore: Block broadcast_84_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.8 MiB)
26/02/13 12:05:59 INFO BlockManagerInfo: Added broadcast_84_piece0 in memory on spark-master:42073 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 12:05:59 INFO SparkContext: Created broadcast 84 from broadcast at DAGScheduler.scala:1585
26/02/13 12:05:59 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 87 (MapPartitionsRDD[302] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
26/02/13 12:05:59 INFO TaskSchedulerImpl: Adding task set 87.0 with 3 tasks resource profile 0
26/02/13 12:05:59 INFO TaskSetManager: Starting task 0.0 in stage 87.0 (TID 174) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:05:59 INFO TaskSetManager: Starting task 1.0 in stage 87.0 (TID 175) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:05:59 INFO TaskSetManager: Starting task 2.0 in stage 87.0 (TID 176) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:05:59 INFO BlockManagerInfo: Added broadcast_84_piece0 in memory on 172.18.0.14:37571 (size: 19.5 KiB, free: 434.0 MiB)
26/02/13 12:05:59 INFO BlockManagerInfo: Added broadcast_84_piece0 in memory on 172.18.0.15:37717 (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 12:05:59 INFO BlockManagerInfo: Added broadcast_83_piece0 in memory on 172.18.0.14:37571 (size: 107.2 KiB, free: 433.9 MiB)
26/02/13 12:05:59 INFO BlockManagerInfo: Added broadcast_83_piece0 in memory on 172.18.0.15:37717 (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 12:06:00 INFO TaskSetManager: Finished task 0.0 in stage 87.0 (TID 174) in 600 ms on 172.18.0.14 (executor 1) (1/3)
26/02/13 12:06:00 INFO TaskSetManager: Finished task 2.0 in stage 87.0 (TID 176) in 603 ms on 172.18.0.14 (executor 1) (2/3)
26/02/13 12:06:00 INFO TaskSetManager: Finished task 1.0 in stage 87.0 (TID 175) in 631 ms on 172.18.0.15 (executor 0) (3/3)
26/02/13 12:06:00 INFO TaskSchedulerImpl: Removed TaskSet 87.0, whose tasks have all completed, from pool 
26/02/13 12:06:00 INFO DAGScheduler: ResultStage 87 (start at NativeMethodAccessorImpl.java:0) finished in 0.636 s
26/02/13 12:06:00 INFO DAGScheduler: Job 56 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 12:06:00 INFO TaskSchedulerImpl: Killing all running tasks in stage 87: Stage finished
26/02/13 12:06:00 INFO DAGScheduler: Job 56 finished: start at NativeMethodAccessorImpl.java:0, took 0.638942 s
26/02/13 12:06:00 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 146, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@40aa734c] is committing.
26/02/13 12:06:00 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 146, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@40aa734c] committed.
26/02/13 12:06:00 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/146 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.146.9cbdc11a-943a-4597-9a24-3ccfcf1cf80f.tmp
26/02/13 12:06:00 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.146.9cbdc11a-943a-4597-9a24-3ccfcf1cf80f.tmp to file:/tmp/spark-checkpoint-enrichment/commits/146
26/02/13 12:06:00 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "4a161d66-225b-4b29-812d-ddec3c05364e",
  "name" : null,
  "timestamp" : "2026-02-13T12:05:59.109Z",
  "batchId" : 146,
  "numInputRows" : 95,
  "inputRowsPerSecond" : 7916.666666666666,
  "processedRowsPerSecond" : 88.70214752567693,
  "durationMs" : {
    "addBatch" : 809,
    "commitOffsets" : 88,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 51,
    "triggerExecution" : 1071,
    "walCommit" : 122
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 1425,
        "1" : 1717,
        "0" : 1298
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 1455,
        "1" : 1754,
        "0" : 1326
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 1455,
        "1" : 1754,
        "0" : 1326
      }
    },
    "numInputRows" : 95,
    "inputRowsPerSecond" : 7916.666666666666,
    "processedRowsPerSecond" : 88.70214752567693,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@1a6a42c2",
    "numOutputRows" : 24
  }
}
26/02/13 12:06:00 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/147 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.147.f520c970-345e-4f8d-8960-2ee34e05675d.tmp
26/02/13 12:06:00 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.147.f520c970-345e-4f8d-8960-2ee34e05675d.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/147
26/02/13 12:06:00 INFO MicroBatchExecution: Committed offsets for batch 147. Metadata OffsetSeqMetadata(0,1770984360182,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 12:06:00 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:06:00 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:06:00 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:06:00 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:06:00 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:06:00 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:06:00 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#22447 - origin_code.nullCount#22446) > 0)
26/02/13 12:06:00 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#22452 - destination_code.nullCount#22451) > 0)
26/02/13 12:06:00 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#22482 - callsign.nullCount#22481) > 0)
26/02/13 12:06:00 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 12:06:00 INFO DAGScheduler: Got job 57 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 12:06:00 INFO DAGScheduler: Final stage: ResultStage 89 (start at NativeMethodAccessorImpl.java:0)
26/02/13 12:06:00 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 88)
26/02/13 12:06:00 INFO DAGScheduler: Missing parents: List()
26/02/13 12:06:00 INFO DAGScheduler: Submitting ResultStage 89 (MapPartitionsRDD[307] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:06:00 INFO MemoryStore: Block broadcast_85 stored as values in memory (estimated size 99.3 KiB, free 433.7 MiB)
26/02/13 12:06:00 INFO MemoryStore: Block broadcast_85_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 433.6 MiB)
26/02/13 12:06:00 INFO BlockManagerInfo: Added broadcast_85_piece0 in memory on spark-master:42073 (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 12:06:00 INFO SparkContext: Created broadcast 85 from broadcast at DAGScheduler.scala:1585
26/02/13 12:06:00 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 89 (MapPartitionsRDD[307] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 12:06:00 INFO TaskSchedulerImpl: Adding task set 89.0 with 4 tasks resource profile 0
26/02/13 12:06:00 INFO TaskSetManager: Starting task 0.0 in stage 89.0 (TID 177) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:06:00 INFO TaskSetManager: Starting task 1.0 in stage 89.0 (TID 178) (172.18.0.14, executor 1, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:06:00 INFO BlockManagerInfo: Added broadcast_85_piece0 in memory on 172.18.0.14:37571 (size: 33.3 KiB, free: 433.9 MiB)
26/02/13 12:06:00 INFO TaskSetManager: Starting task 2.0 in stage 89.0 (TID 179) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:06:00 INFO TaskSetManager: Finished task 0.0 in stage 89.0 (TID 177) in 33 ms on 172.18.0.14 (executor 1) (1/4)
26/02/13 12:06:00 INFO TaskSetManager: Starting task 3.0 in stage 89.0 (TID 180) (172.18.0.14, executor 1, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:06:00 INFO TaskSetManager: Finished task 1.0 in stage 89.0 (TID 178) in 34 ms on 172.18.0.14 (executor 1) (2/4)
26/02/13 12:06:00 INFO TaskSetManager: Finished task 3.0 in stage 89.0 (TID 180) in 18 ms on 172.18.0.14 (executor 1) (3/4)
26/02/13 12:06:00 INFO TaskSetManager: Finished task 2.0 in stage 89.0 (TID 179) in 20 ms on 172.18.0.14 (executor 1) (4/4)
26/02/13 12:06:00 INFO TaskSchedulerImpl: Removed TaskSet 89.0, whose tasks have all completed, from pool 
26/02/13 12:06:00 INFO DAGScheduler: ResultStage 89 (start at NativeMethodAccessorImpl.java:0) finished in 0.060 s
26/02/13 12:06:00 INFO DAGScheduler: Job 57 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 12:06:00 INFO TaskSchedulerImpl: Killing all running tasks in stage 89: Stage finished
26/02/13 12:06:00 INFO DAGScheduler: Job 57 finished: start at NativeMethodAccessorImpl.java:0, took 0.062755 s
26/02/13 12:06:00 INFO MemoryStore: Block broadcast_86_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.5 MiB)
26/02/13 12:06:00 INFO BlockManagerInfo: Added broadcast_86_piece0 in memory on spark-master:42073 (size: 107.2 KiB, free: 434.0 MiB)
26/02/13 12:06:00 INFO SparkContext: Created broadcast 86 from start at NativeMethodAccessorImpl.java:0
26/02/13 12:06:00 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 147, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@5c5c8d6c]. The input RDD has 3 partitions.
26/02/13 12:06:00 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 12:06:00 INFO DAGScheduler: Got job 58 (start at NativeMethodAccessorImpl.java:0) with 3 output partitions
26/02/13 12:06:00 INFO DAGScheduler: Final stage: ResultStage 90 (start at NativeMethodAccessorImpl.java:0)
26/02/13 12:06:00 INFO DAGScheduler: Parents of final stage: List()
26/02/13 12:06:00 INFO DAGScheduler: Missing parents: List()
26/02/13 12:06:00 INFO DAGScheduler: Submitting ResultStage 90 (MapPartitionsRDD[313] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:06:00 INFO BlockManagerInfo: Removed broadcast_82_piece0 on spark-master:42073 in memory (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 12:06:00 INFO MemoryStore: Block broadcast_87 stored as values in memory (estimated size 49.9 KiB, free 433.6 MiB)
26/02/13 12:06:00 INFO BlockManagerInfo: Removed broadcast_82_piece0 on 172.18.0.14:37571 in memory (size: 33.3 KiB, free: 433.9 MiB)
26/02/13 12:06:00 INFO MemoryStore: Block broadcast_87_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.6 MiB)
26/02/13 12:06:00 INFO BlockManagerInfo: Added broadcast_87_piece0 in memory on spark-master:42073 (size: 19.5 KiB, free: 434.0 MiB)
26/02/13 12:06:00 INFO SparkContext: Created broadcast 87 from broadcast at DAGScheduler.scala:1585
26/02/13 12:06:00 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 90 (MapPartitionsRDD[313] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
26/02/13 12:06:00 INFO TaskSchedulerImpl: Adding task set 90.0 with 3 tasks resource profile 0
26/02/13 12:06:00 INFO TaskSetManager: Starting task 0.0 in stage 90.0 (TID 181) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:06:00 INFO TaskSetManager: Starting task 1.0 in stage 90.0 (TID 182) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:06:00 INFO TaskSetManager: Starting task 2.0 in stage 90.0 (TID 183) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:06:00 INFO BlockManagerInfo: Removed broadcast_85_piece0 on spark-master:42073 in memory (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 12:06:00 INFO BlockManagerInfo: Removed broadcast_85_piece0 on 172.18.0.14:37571 in memory (size: 33.3 KiB, free: 433.9 MiB)
26/02/13 12:06:00 INFO BlockManagerInfo: Removed broadcast_83_piece0 on spark-master:42073 in memory (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 12:06:00 INFO BlockManagerInfo: Added broadcast_87_piece0 in memory on 172.18.0.14:37571 (size: 19.5 KiB, free: 433.9 MiB)
26/02/13 12:06:00 INFO BlockManagerInfo: Removed broadcast_83_piece0 on 172.18.0.14:37571 in memory (size: 107.2 KiB, free: 434.0 MiB)
26/02/13 12:06:00 INFO BlockManagerInfo: Added broadcast_87_piece0 in memory on 172.18.0.15:37717 (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 12:06:00 INFO BlockManagerInfo: Removed broadcast_83_piece0 on 172.18.0.15:37717 in memory (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 12:06:00 INFO BlockManagerInfo: Removed broadcast_84_piece0 on spark-master:42073 in memory (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 12:06:00 INFO BlockManagerInfo: Removed broadcast_84_piece0 on 172.18.0.15:37717 in memory (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 12:06:00 INFO BlockManagerInfo: Removed broadcast_84_piece0 on 172.18.0.14:37571 in memory (size: 19.5 KiB, free: 434.0 MiB)
26/02/13 12:06:00 INFO BlockManagerInfo: Added broadcast_86_piece0 in memory on 172.18.0.14:37571 (size: 107.2 KiB, free: 433.9 MiB)
26/02/13 12:06:00 INFO BlockManagerInfo: Added broadcast_86_piece0 in memory on 172.18.0.15:37717 (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 12:06:00 INFO TaskSetManager: Finished task 0.0 in stage 90.0 (TID 181) in 88 ms on 172.18.0.14 (executor 1) (1/3)
26/02/13 12:06:00 INFO TaskSetManager: Finished task 2.0 in stage 90.0 (TID 183) in 94 ms on 172.18.0.14 (executor 1) (2/3)
26/02/13 12:06:00 INFO TaskSetManager: Finished task 1.0 in stage 90.0 (TID 182) in 143 ms on 172.18.0.15 (executor 0) (3/3)
26/02/13 12:06:00 INFO TaskSchedulerImpl: Removed TaskSet 90.0, whose tasks have all completed, from pool 
26/02/13 12:06:00 INFO DAGScheduler: ResultStage 90 (start at NativeMethodAccessorImpl.java:0) finished in 0.152 s
26/02/13 12:06:00 INFO DAGScheduler: Job 58 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 12:06:00 INFO TaskSchedulerImpl: Killing all running tasks in stage 90: Stage finished
26/02/13 12:06:00 INFO DAGScheduler: Job 58 finished: start at NativeMethodAccessorImpl.java:0, took 0.158103 s
26/02/13 12:06:00 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 147, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@5c5c8d6c] is committing.
26/02/13 12:06:00 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 147, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@5c5c8d6c] committed.
26/02/13 12:06:00 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/147 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.147.48449c3c-127b-4dd9-982b-d385a7b27157.tmp
26/02/13 12:06:00 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.147.48449c3c-127b-4dd9-982b-d385a7b27157.tmp to file:/tmp/spark-checkpoint-enrichment/commits/147
26/02/13 12:06:00 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "4a161d66-225b-4b29-812d-ddec3c05364e",
  "name" : null,
  "timestamp" : "2026-02-13T12:06:00.181Z",
  "batchId" : 147,
  "numInputRows" : 147,
  "inputRowsPerSecond" : 137.12686567164178,
  "processedRowsPerSecond" : 280.53435114503816,
  "durationMs" : {
    "addBatch" : 316,
    "commitOffsets" : 121,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 25,
    "triggerExecution" : 524,
    "walCommit" : 60
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 1455,
        "1" : 1754,
        "0" : 1326
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 1506,
        "1" : 1810,
        "0" : 1366
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 1506,
        "1" : 1810,
        "0" : 1366
      }
    },
    "numInputRows" : 147,
    "inputRowsPerSecond" : 137.12686567164178,
    "processedRowsPerSecond" : 280.53435114503816,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@1a6a42c2",
    "numOutputRows" : 29
  }
}
26/02/13 12:06:01 INFO BlockManagerInfo: Removed broadcast_87_piece0 on spark-master:42073 in memory (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 12:06:01 INFO BlockManagerInfo: Removed broadcast_87_piece0 on 172.18.0.14:37571 in memory (size: 19.5 KiB, free: 434.0 MiB)
26/02/13 12:06:01 INFO BlockManagerInfo: Removed broadcast_87_piece0 on 172.18.0.15:37717 in memory (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 12:06:10 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:06:12 INFO BlockManagerInfo: Removed broadcast_80_piece0 on spark-master:42073 in memory (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 12:06:12 INFO BlockManagerInfo: Removed broadcast_80_piece0 on 172.18.0.14:37571 in memory (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 12:06:12 INFO BlockManagerInfo: Removed broadcast_80_piece0 on 172.18.0.15:37717 in memory (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 12:06:19 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/148 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.148.939462ac-b373-4c93-adbe-9243564f4895.tmp
26/02/13 12:06:19 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.148.939462ac-b373-4c93-adbe-9243564f4895.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/148
26/02/13 12:06:19 INFO MicroBatchExecution: Committed offsets for batch 148. Metadata OffsetSeqMetadata(0,1770984379183,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 12:06:19 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:06:19 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:06:19 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:06:19 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:06:19 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:06:19 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:06:19 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#23301 - origin_code.nullCount#23300) > 0)
26/02/13 12:06:19 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#23306 - destination_code.nullCount#23305) > 0)
26/02/13 12:06:19 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#23336 - callsign.nullCount#23335) > 0)
26/02/13 12:06:19 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 12:06:19 INFO DAGScheduler: Got job 59 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 12:06:19 INFO DAGScheduler: Final stage: ResultStage 92 (start at NativeMethodAccessorImpl.java:0)
26/02/13 12:06:19 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 91)
26/02/13 12:06:19 INFO DAGScheduler: Missing parents: List()
26/02/13 12:06:19 INFO DAGScheduler: Submitting ResultStage 92 (MapPartitionsRDD[318] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:06:19 INFO MemoryStore: Block broadcast_88 stored as values in memory (estimated size 99.3 KiB, free 434.0 MiB)
26/02/13 12:06:19 INFO MemoryStore: Block broadcast_88_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 433.9 MiB)
26/02/13 12:06:19 INFO BlockManagerInfo: Added broadcast_88_piece0 in memory on spark-master:42073 (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 12:06:19 INFO SparkContext: Created broadcast 88 from broadcast at DAGScheduler.scala:1585
26/02/13 12:06:19 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 92 (MapPartitionsRDD[318] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 12:06:19 INFO TaskSchedulerImpl: Adding task set 92.0 with 4 tasks resource profile 0
26/02/13 12:06:19 INFO TaskSetManager: Starting task 0.0 in stage 92.0 (TID 184) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:06:19 INFO TaskSetManager: Starting task 1.0 in stage 92.0 (TID 185) (172.18.0.14, executor 1, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:06:19 INFO BlockManagerInfo: Added broadcast_88_piece0 in memory on 172.18.0.14:37571 (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 12:06:19 INFO TaskSetManager: Starting task 2.0 in stage 92.0 (TID 186) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:06:19 INFO TaskSetManager: Finished task 0.0 in stage 92.0 (TID 184) in 32 ms on 172.18.0.14 (executor 1) (1/4)
26/02/13 12:06:19 INFO TaskSetManager: Finished task 1.0 in stage 92.0 (TID 185) in 33 ms on 172.18.0.14 (executor 1) (2/4)
26/02/13 12:06:19 INFO TaskSetManager: Starting task 3.0 in stage 92.0 (TID 187) (172.18.0.14, executor 1, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:06:19 INFO TaskSetManager: Finished task 2.0 in stage 92.0 (TID 186) in 20 ms on 172.18.0.14 (executor 1) (3/4)
26/02/13 12:06:19 INFO TaskSetManager: Finished task 3.0 in stage 92.0 (TID 187) in 20 ms on 172.18.0.14 (executor 1) (4/4)
26/02/13 12:06:19 INFO TaskSchedulerImpl: Removed TaskSet 92.0, whose tasks have all completed, from pool 
26/02/13 12:06:19 INFO BlockManagerInfo: Removed broadcast_86_piece0 on spark-master:42073 in memory (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 12:06:19 INFO DAGScheduler: ResultStage 92 (start at NativeMethodAccessorImpl.java:0) finished in 0.065 s
26/02/13 12:06:19 INFO DAGScheduler: Job 59 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 12:06:19 INFO TaskSchedulerImpl: Killing all running tasks in stage 92: Stage finished
26/02/13 12:06:19 INFO DAGScheduler: Job 59 finished: start at NativeMethodAccessorImpl.java:0, took 0.067720 s
26/02/13 12:06:19 INFO BlockManagerInfo: Removed broadcast_86_piece0 on 172.18.0.15:37717 in memory (size: 107.2 KiB, free: 434.4 MiB)
26/02/13 12:06:19 INFO BlockManagerInfo: Removed broadcast_86_piece0 on 172.18.0.14:37571 in memory (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 12:06:19 INFO MemoryStore: Block broadcast_89_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.9 MiB)
26/02/13 12:06:19 INFO BlockManagerInfo: Added broadcast_89_piece0 in memory on spark-master:42073 (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 12:06:19 INFO SparkContext: Created broadcast 89 from start at NativeMethodAccessorImpl.java:0
26/02/13 12:06:19 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 148, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@2c0ac038]. The input RDD has 3 partitions.
26/02/13 12:06:19 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 12:06:19 INFO DAGScheduler: Got job 60 (start at NativeMethodAccessorImpl.java:0) with 3 output partitions
26/02/13 12:06:19 INFO DAGScheduler: Final stage: ResultStage 93 (start at NativeMethodAccessorImpl.java:0)
26/02/13 12:06:19 INFO DAGScheduler: Parents of final stage: List()
26/02/13 12:06:19 INFO DAGScheduler: Missing parents: List()
26/02/13 12:06:19 INFO DAGScheduler: Submitting ResultStage 93 (MapPartitionsRDD[324] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:06:19 INFO MemoryStore: Block broadcast_90 stored as values in memory (estimated size 49.9 KiB, free 433.9 MiB)
26/02/13 12:06:19 INFO MemoryStore: Block broadcast_90_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.9 MiB)
26/02/13 12:06:19 INFO BlockManagerInfo: Added broadcast_90_piece0 in memory on spark-master:42073 (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 12:06:19 INFO SparkContext: Created broadcast 90 from broadcast at DAGScheduler.scala:1585
26/02/13 12:06:19 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 93 (MapPartitionsRDD[324] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
26/02/13 12:06:19 INFO TaskSchedulerImpl: Adding task set 93.0 with 3 tasks resource profile 0
26/02/13 12:06:19 INFO TaskSetManager: Starting task 1.0 in stage 93.0 (TID 188) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:06:19 INFO TaskSetManager: Starting task 0.0 in stage 93.0 (TID 189) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:06:19 INFO TaskSetManager: Starting task 2.0 in stage 93.0 (TID 190) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:06:19 INFO BlockManagerInfo: Added broadcast_90_piece0 in memory on 172.18.0.14:37571 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 12:06:19 INFO BlockManagerInfo: Added broadcast_90_piece0 in memory on 172.18.0.15:37717 (size: 19.5 KiB, free: 434.4 MiB)
26/02/13 12:06:19 INFO BlockManagerInfo: Added broadcast_89_piece0 in memory on 172.18.0.14:37571 (size: 107.2 KiB, free: 434.0 MiB)
26/02/13 12:06:19 INFO BlockManagerInfo: Added broadcast_89_piece0 in memory on 172.18.0.15:37717 (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 12:06:20 INFO TaskSetManager: Finished task 0.0 in stage 93.0 (TID 189) in 569 ms on 172.18.0.14 (executor 1) (1/3)
26/02/13 12:06:20 INFO TaskSetManager: Finished task 2.0 in stage 93.0 (TID 190) in 570 ms on 172.18.0.14 (executor 1) (2/3)
26/02/13 12:06:20 INFO TaskSetManager: Finished task 1.0 in stage 93.0 (TID 188) in 593 ms on 172.18.0.15 (executor 0) (3/3)
26/02/13 12:06:20 INFO TaskSchedulerImpl: Removed TaskSet 93.0, whose tasks have all completed, from pool 
26/02/13 12:06:20 INFO DAGScheduler: ResultStage 93 (start at NativeMethodAccessorImpl.java:0) finished in 0.599 s
26/02/13 12:06:20 INFO DAGScheduler: Job 60 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 12:06:20 INFO TaskSchedulerImpl: Killing all running tasks in stage 93: Stage finished
26/02/13 12:06:20 INFO DAGScheduler: Job 60 finished: start at NativeMethodAccessorImpl.java:0, took 0.600482 s
26/02/13 12:06:20 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 148, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@2c0ac038] is committing.
26/02/13 12:06:20 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 148, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@2c0ac038] committed.
26/02/13 12:06:20 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/148 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.148.50857a76-6973-4bcf-853d-59492c12d16e.tmp
26/02/13 12:06:20 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.148.50857a76-6973-4bcf-853d-59492c12d16e.tmp to file:/tmp/spark-checkpoint-enrichment/commits/148
26/02/13 12:06:20 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "4a161d66-225b-4b29-812d-ddec3c05364e",
  "name" : null,
  "timestamp" : "2026-02-13T12:06:19.181Z",
  "batchId" : 148,
  "numInputRows" : 95,
  "inputRowsPerSecond" : 8636.363636363636,
  "processedRowsPerSecond" : 97.53593429158111,
  "durationMs" : {
    "addBatch" : 761,
    "commitOffsets" : 58,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 43,
    "triggerExecution" : 974,
    "walCommit" : 109
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 1506,
        "1" : 1810,
        "0" : 1366
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 1536,
        "1" : 1847,
        "0" : 1394
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 1536,
        "1" : 1847,
        "0" : 1394
      }
    },
    "numInputRows" : 95,
    "inputRowsPerSecond" : 8636.363636363636,
    "processedRowsPerSecond" : 97.53593429158111,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@1a6a42c2",
    "numOutputRows" : 24
  }
}
26/02/13 12:06:20 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/149 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.149.34d75236-bda9-4992-933c-88608a258d15.tmp
26/02/13 12:06:20 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.149.34d75236-bda9-4992-933c-88608a258d15.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/149
26/02/13 12:06:20 INFO MicroBatchExecution: Committed offsets for batch 149. Metadata OffsetSeqMetadata(0,1770984380157,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 12:06:20 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:06:20 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:06:20 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:06:20 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:06:20 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:06:20 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:06:20 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#24155 - origin_code.nullCount#24154) > 0)
26/02/13 12:06:20 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#24160 - destination_code.nullCount#24159) > 0)
26/02/13 12:06:20 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#24190 - callsign.nullCount#24189) > 0)
26/02/13 12:06:20 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 12:06:20 INFO DAGScheduler: Got job 61 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 12:06:20 INFO DAGScheduler: Final stage: ResultStage 95 (start at NativeMethodAccessorImpl.java:0)
26/02/13 12:06:20 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 94)
26/02/13 12:06:20 INFO DAGScheduler: Missing parents: List()
26/02/13 12:06:20 INFO DAGScheduler: Submitting ResultStage 95 (MapPartitionsRDD[329] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:06:20 INFO MemoryStore: Block broadcast_91 stored as values in memory (estimated size 99.3 KiB, free 433.8 MiB)
26/02/13 12:06:20 INFO MemoryStore: Block broadcast_91_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 433.7 MiB)
26/02/13 12:06:20 INFO BlockManagerInfo: Added broadcast_91_piece0 in memory on spark-master:42073 (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 12:06:20 INFO SparkContext: Created broadcast 91 from broadcast at DAGScheduler.scala:1585
26/02/13 12:06:20 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 95 (MapPartitionsRDD[329] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 12:06:20 INFO TaskSchedulerImpl: Adding task set 95.0 with 4 tasks resource profile 0
26/02/13 12:06:20 INFO TaskSetManager: Starting task 0.0 in stage 95.0 (TID 191) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:06:20 INFO TaskSetManager: Starting task 1.0 in stage 95.0 (TID 192) (172.18.0.14, executor 1, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:06:20 INFO BlockManagerInfo: Added broadcast_91_piece0 in memory on 172.18.0.14:37571 (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 12:06:20 INFO TaskSetManager: Starting task 2.0 in stage 95.0 (TID 193) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:06:20 INFO TaskSetManager: Finished task 1.0 in stage 95.0 (TID 192) in 17 ms on 172.18.0.14 (executor 1) (1/4)
26/02/13 12:06:20 INFO TaskSetManager: Starting task 3.0 in stage 95.0 (TID 194) (172.18.0.14, executor 1, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:06:20 INFO TaskSetManager: Finished task 0.0 in stage 95.0 (TID 191) in 25 ms on 172.18.0.14 (executor 1) (2/4)
26/02/13 12:06:20 INFO TaskSetManager: Finished task 2.0 in stage 95.0 (TID 193) in 21 ms on 172.18.0.14 (executor 1) (3/4)
26/02/13 12:06:20 INFO TaskSetManager: Finished task 3.0 in stage 95.0 (TID 194) in 24 ms on 172.18.0.14 (executor 1) (4/4)
26/02/13 12:06:20 INFO TaskSchedulerImpl: Removed TaskSet 95.0, whose tasks have all completed, from pool 
26/02/13 12:06:20 INFO DAGScheduler: ResultStage 95 (start at NativeMethodAccessorImpl.java:0) finished in 0.055 s
26/02/13 12:06:20 INFO DAGScheduler: Job 61 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 12:06:20 INFO TaskSchedulerImpl: Killing all running tasks in stage 95: Stage finished
26/02/13 12:06:20 INFO DAGScheduler: Job 61 finished: start at NativeMethodAccessorImpl.java:0, took 0.056378 s
26/02/13 12:06:20 INFO MemoryStore: Block broadcast_92_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.6 MiB)
26/02/13 12:06:20 INFO BlockManagerInfo: Added broadcast_92_piece0 in memory on spark-master:42073 (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 12:06:20 INFO SparkContext: Created broadcast 92 from start at NativeMethodAccessorImpl.java:0
26/02/13 12:06:20 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 149, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@c05e1d4]. The input RDD has 3 partitions.
26/02/13 12:06:20 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 12:06:20 INFO DAGScheduler: Got job 62 (start at NativeMethodAccessorImpl.java:0) with 3 output partitions
26/02/13 12:06:20 INFO DAGScheduler: Final stage: ResultStage 96 (start at NativeMethodAccessorImpl.java:0)
26/02/13 12:06:20 INFO DAGScheduler: Parents of final stage: List()
26/02/13 12:06:20 INFO DAGScheduler: Missing parents: List()
26/02/13 12:06:20 INFO DAGScheduler: Submitting ResultStage 96 (MapPartitionsRDD[335] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:06:20 INFO MemoryStore: Block broadcast_93 stored as values in memory (estimated size 49.9 KiB, free 433.6 MiB)
26/02/13 12:06:20 INFO MemoryStore: Block broadcast_93_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.6 MiB)
26/02/13 12:06:20 INFO BlockManagerInfo: Added broadcast_93_piece0 in memory on spark-master:42073 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 12:06:20 INFO SparkContext: Created broadcast 93 from broadcast at DAGScheduler.scala:1585
26/02/13 12:06:20 INFO BlockManagerInfo: Removed broadcast_90_piece0 on spark-master:42073 in memory (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 12:06:20 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 96 (MapPartitionsRDD[335] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
26/02/13 12:06:20 INFO TaskSchedulerImpl: Adding task set 96.0 with 3 tasks resource profile 0
26/02/13 12:06:20 INFO BlockManagerInfo: Removed broadcast_90_piece0 on 172.18.0.14:37571 in memory (size: 19.5 KiB, free: 434.0 MiB)
26/02/13 12:06:20 INFO TaskSetManager: Starting task 1.0 in stage 96.0 (TID 195) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:06:20 INFO BlockManagerInfo: Removed broadcast_90_piece0 on 172.18.0.15:37717 in memory (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 12:06:20 INFO TaskSetManager: Starting task 0.0 in stage 96.0 (TID 196) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:06:20 INFO TaskSetManager: Starting task 2.0 in stage 96.0 (TID 197) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:06:20 INFO BlockManagerInfo: Removed broadcast_89_piece0 on spark-master:42073 in memory (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 12:06:20 INFO BlockManagerInfo: Removed broadcast_89_piece0 on 172.18.0.15:37717 in memory (size: 107.2 KiB, free: 434.4 MiB)
26/02/13 12:06:20 INFO BlockManagerInfo: Removed broadcast_89_piece0 on 172.18.0.14:37571 in memory (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 12:06:20 INFO BlockManagerInfo: Removed broadcast_91_piece0 on spark-master:42073 in memory (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 12:06:20 INFO BlockManagerInfo: Added broadcast_93_piece0 in memory on 172.18.0.14:37571 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 12:06:20 INFO BlockManagerInfo: Removed broadcast_91_piece0 on 172.18.0.14:37571 in memory (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 12:06:20 INFO BlockManagerInfo: Added broadcast_93_piece0 in memory on 172.18.0.15:37717 (size: 19.5 KiB, free: 434.4 MiB)
26/02/13 12:06:20 INFO BlockManagerInfo: Removed broadcast_88_piece0 on spark-master:42073 in memory (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 12:06:20 INFO BlockManagerInfo: Removed broadcast_88_piece0 on 172.18.0.14:37571 in memory (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 12:06:20 INFO BlockManagerInfo: Added broadcast_92_piece0 in memory on 172.18.0.14:37571 (size: 107.2 KiB, free: 434.0 MiB)
26/02/13 12:06:20 INFO BlockManagerInfo: Added broadcast_92_piece0 in memory on 172.18.0.15:37717 (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 12:06:20 INFO TaskSetManager: Finished task 0.0 in stage 96.0 (TID 196) in 88 ms on 172.18.0.14 (executor 1) (1/3)
26/02/13 12:06:20 INFO TaskSetManager: Finished task 2.0 in stage 96.0 (TID 197) in 91 ms on 172.18.0.14 (executor 1) (2/3)
26/02/13 12:06:20 INFO TaskSetManager: Finished task 1.0 in stage 96.0 (TID 195) in 132 ms on 172.18.0.15 (executor 0) (3/3)
26/02/13 12:06:20 INFO TaskSchedulerImpl: Removed TaskSet 96.0, whose tasks have all completed, from pool 
26/02/13 12:06:20 INFO DAGScheduler: ResultStage 96 (start at NativeMethodAccessorImpl.java:0) finished in 0.147 s
26/02/13 12:06:20 INFO DAGScheduler: Job 62 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 12:06:20 INFO TaskSchedulerImpl: Killing all running tasks in stage 96: Stage finished
26/02/13 12:06:20 INFO DAGScheduler: Job 62 finished: start at NativeMethodAccessorImpl.java:0, took 0.149628 s
26/02/13 12:06:20 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 149, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@c05e1d4] is committing.
26/02/13 12:06:20 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 149, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@c05e1d4] committed.
26/02/13 12:06:20 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/149 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.149.c81861f4-c393-4b12-85b5-058d6f8aca19.tmp
26/02/13 12:06:20 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.149.c81861f4-c393-4b12-85b5-058d6f8aca19.tmp to file:/tmp/spark-checkpoint-enrichment/commits/149
26/02/13 12:06:20 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "4a161d66-225b-4b29-812d-ddec3c05364e",
  "name" : null,
  "timestamp" : "2026-02-13T12:06:20.156Z",
  "batchId" : 149,
  "numInputRows" : 147,
  "inputRowsPerSecond" : 150.76923076923077,
  "processedRowsPerSecond" : 327.3942093541203,
  "durationMs" : {
    "addBatch" : 294,
    "commitOffsets" : 69,
    "getBatch" : 1,
    "latestOffset" : 1,
    "queryPlanning" : 33,
    "triggerExecution" : 449,
    "walCommit" : 51
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 1536,
        "1" : 1847,
        "0" : 1394
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 1587,
        "1" : 1902,
        "0" : 1435
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 1587,
        "1" : 1902,
        "0" : 1435
      }
    },
    "numInputRows" : 147,
    "inputRowsPerSecond" : 150.76923076923077,
    "processedRowsPerSecond" : 327.3942093541203,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@1a6a42c2",
    "numOutputRows" : 29
  }
}
26/02/13 12:06:21 INFO BlockManagerInfo: Removed broadcast_93_piece0 on spark-master:42073 in memory (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 12:06:21 INFO BlockManagerInfo: Removed broadcast_93_piece0 on 172.18.0.14:37571 in memory (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 12:06:21 INFO BlockManagerInfo: Removed broadcast_93_piece0 on 172.18.0.15:37717 in memory (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 12:06:30 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:06:37 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/150 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.150.a73c1a71-6864-4e79-b44f-73760a9a8d77.tmp
26/02/13 12:06:37 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.150.a73c1a71-6864-4e79-b44f-73760a9a8d77.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/150
26/02/13 12:06:37 INFO MicroBatchExecution: Committed offsets for batch 150. Metadata OffsetSeqMetadata(0,1770984397498,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 12:06:37 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:06:37 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:06:37 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:06:37 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:06:37 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:06:37 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:06:37 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#25009 - origin_code.nullCount#25008) > 0)
26/02/13 12:06:37 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#25014 - destination_code.nullCount#25013) > 0)
26/02/13 12:06:37 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#25044 - callsign.nullCount#25043) > 0)
26/02/13 12:06:37 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 12:06:37 INFO DAGScheduler: Got job 63 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 12:06:37 INFO DAGScheduler: Final stage: ResultStage 98 (start at NativeMethodAccessorImpl.java:0)
26/02/13 12:06:37 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 97)
26/02/13 12:06:37 INFO DAGScheduler: Missing parents: List()
26/02/13 12:06:37 INFO DAGScheduler: Submitting ResultStage 98 (MapPartitionsRDD[340] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:06:37 INFO MemoryStore: Block broadcast_94 stored as values in memory (estimated size 99.3 KiB, free 434.0 MiB)
26/02/13 12:06:37 INFO MemoryStore: Block broadcast_94_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 433.9 MiB)
26/02/13 12:06:37 INFO BlockManagerInfo: Added broadcast_94_piece0 in memory on spark-master:42073 (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 12:06:37 INFO SparkContext: Created broadcast 94 from broadcast at DAGScheduler.scala:1585
26/02/13 12:06:37 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 98 (MapPartitionsRDD[340] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 12:06:37 INFO TaskSchedulerImpl: Adding task set 98.0 with 4 tasks resource profile 0
26/02/13 12:06:37 INFO TaskSetManager: Starting task 0.0 in stage 98.0 (TID 198) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:06:37 INFO TaskSetManager: Starting task 1.0 in stage 98.0 (TID 199) (172.18.0.14, executor 1, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:06:37 INFO BlockManagerInfo: Added broadcast_94_piece0 in memory on 172.18.0.14:37571 (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 12:06:37 INFO TaskSetManager: Starting task 2.0 in stage 98.0 (TID 200) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:06:37 INFO TaskSetManager: Finished task 1.0 in stage 98.0 (TID 199) in 19 ms on 172.18.0.14 (executor 1) (1/4)
26/02/13 12:06:37 INFO TaskSetManager: Starting task 3.0 in stage 98.0 (TID 201) (172.18.0.14, executor 1, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:06:37 INFO TaskSetManager: Finished task 0.0 in stage 98.0 (TID 198) in 23 ms on 172.18.0.14 (executor 1) (2/4)
26/02/13 12:06:37 INFO TaskSetManager: Finished task 2.0 in stage 98.0 (TID 200) in 15 ms on 172.18.0.14 (executor 1) (3/4)
26/02/13 12:06:37 INFO TaskSetManager: Finished task 3.0 in stage 98.0 (TID 201) in 13 ms on 172.18.0.14 (executor 1) (4/4)
26/02/13 12:06:37 INFO TaskSchedulerImpl: Removed TaskSet 98.0, whose tasks have all completed, from pool 
26/02/13 12:06:37 INFO DAGScheduler: ResultStage 98 (start at NativeMethodAccessorImpl.java:0) finished in 0.041 s
26/02/13 12:06:37 INFO DAGScheduler: Job 63 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 12:06:37 INFO TaskSchedulerImpl: Killing all running tasks in stage 98: Stage finished
26/02/13 12:06:37 INFO DAGScheduler: Job 63 finished: start at NativeMethodAccessorImpl.java:0, took 0.043400 s
26/02/13 12:06:37 INFO MemoryStore: Block broadcast_95_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.8 MiB)
26/02/13 12:06:37 INFO BlockManagerInfo: Added broadcast_95_piece0 in memory on spark-master:42073 (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 12:06:37 INFO SparkContext: Created broadcast 95 from start at NativeMethodAccessorImpl.java:0
26/02/13 12:06:37 INFO BlockManagerInfo: Removed broadcast_94_piece0 on spark-master:42073 in memory (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 12:06:37 INFO BlockManagerInfo: Removed broadcast_94_piece0 on 172.18.0.14:37571 in memory (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 12:06:37 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 150, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@3d8a4ba6]. The input RDD has 3 partitions.
26/02/13 12:06:37 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 12:06:37 INFO DAGScheduler: Got job 64 (start at NativeMethodAccessorImpl.java:0) with 3 output partitions
26/02/13 12:06:37 INFO DAGScheduler: Final stage: ResultStage 99 (start at NativeMethodAccessorImpl.java:0)
26/02/13 12:06:37 INFO DAGScheduler: Parents of final stage: List()
26/02/13 12:06:37 INFO DAGScheduler: Missing parents: List()
26/02/13 12:06:37 INFO DAGScheduler: Submitting ResultStage 99 (MapPartitionsRDD[346] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:06:37 INFO MemoryStore: Block broadcast_96 stored as values in memory (estimated size 49.9 KiB, free 433.9 MiB)
26/02/13 12:06:37 INFO MemoryStore: Block broadcast_96_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.9 MiB)
26/02/13 12:06:37 INFO BlockManagerInfo: Added broadcast_96_piece0 in memory on spark-master:42073 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 12:06:37 INFO SparkContext: Created broadcast 96 from broadcast at DAGScheduler.scala:1585
26/02/13 12:06:37 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 99 (MapPartitionsRDD[346] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
26/02/13 12:06:37 INFO TaskSchedulerImpl: Adding task set 99.0 with 3 tasks resource profile 0
26/02/13 12:06:37 INFO TaskSetManager: Starting task 1.0 in stage 99.0 (TID 202) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:06:37 INFO TaskSetManager: Starting task 0.0 in stage 99.0 (TID 203) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:06:37 INFO TaskSetManager: Starting task 2.0 in stage 99.0 (TID 204) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:06:37 INFO BlockManagerInfo: Added broadcast_96_piece0 in memory on 172.18.0.14:37571 (size: 19.5 KiB, free: 434.0 MiB)
26/02/13 12:06:37 INFO BlockManagerInfo: Added broadcast_96_piece0 in memory on 172.18.0.15:37717 (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 12:06:37 INFO BlockManagerInfo: Added broadcast_95_piece0 in memory on 172.18.0.14:37571 (size: 107.2 KiB, free: 433.9 MiB)
26/02/13 12:06:37 INFO BlockManagerInfo: Added broadcast_95_piece0 in memory on 172.18.0.15:37717 (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 12:06:38 INFO TaskSetManager: Finished task 0.0 in stage 99.0 (TID 203) in 575 ms on 172.18.0.14 (executor 1) (1/3)
26/02/13 12:06:38 INFO TaskSetManager: Finished task 2.0 in stage 99.0 (TID 204) in 577 ms on 172.18.0.14 (executor 1) (2/3)
26/02/13 12:06:38 INFO TaskSetManager: Finished task 1.0 in stage 99.0 (TID 202) in 587 ms on 172.18.0.15 (executor 0) (3/3)
26/02/13 12:06:38 INFO TaskSchedulerImpl: Removed TaskSet 99.0, whose tasks have all completed, from pool 
26/02/13 12:06:38 INFO DAGScheduler: ResultStage 99 (start at NativeMethodAccessorImpl.java:0) finished in 0.591 s
26/02/13 12:06:38 INFO DAGScheduler: Job 64 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 12:06:38 INFO TaskSchedulerImpl: Killing all running tasks in stage 99: Stage finished
26/02/13 12:06:38 INFO DAGScheduler: Job 64 finished: start at NativeMethodAccessorImpl.java:0, took 0.593151 s
26/02/13 12:06:38 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 150, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@3d8a4ba6] is committing.
26/02/13 12:06:38 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 150, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@3d8a4ba6] committed.
26/02/13 12:06:38 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/150 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.150.63ce7734-1cab-472a-b705-7545041f1ceb.tmp
26/02/13 12:06:38 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.150.63ce7734-1cab-472a-b705-7545041f1ceb.tmp to file:/tmp/spark-checkpoint-enrichment/commits/150
26/02/13 12:06:38 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "4a161d66-225b-4b29-812d-ddec3c05364e",
  "name" : null,
  "timestamp" : "2026-02-13T12:06:37.497Z",
  "batchId" : 150,
  "numInputRows" : 111,
  "inputRowsPerSecond" : 9250.0,
  "processedRowsPerSecond" : 117.21224920802536,
  "durationMs" : {
    "addBatch" : 727,
    "commitOffsets" : 61,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 50,
    "triggerExecution" : 947,
    "walCommit" : 106
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 1587,
        "1" : 1902,
        "0" : 1435
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 1624,
        "1" : 1939,
        "0" : 1472
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 1624,
        "1" : 1939,
        "0" : 1472
      }
    },
    "numInputRows" : 111,
    "inputRowsPerSecond" : 9250.0,
    "processedRowsPerSecond" : 117.21224920802536,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@1a6a42c2",
    "numOutputRows" : 27
  }
}
26/02/13 12:06:38 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/151 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.151.9f9feb32-c049-48b5-8f99-4271eb39e5cf.tmp
26/02/13 12:06:38 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.151.9f9feb32-c049-48b5-8f99-4271eb39e5cf.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/151
26/02/13 12:06:38 INFO MicroBatchExecution: Committed offsets for batch 151. Metadata OffsetSeqMetadata(0,1770984398446,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 12:06:38 INFO BlockManagerInfo: Removed broadcast_96_piece0 on spark-master:42073 in memory (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 12:06:38 INFO BlockManagerInfo: Removed broadcast_96_piece0 on 172.18.0.15:37717 in memory (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 12:06:38 INFO BlockManagerInfo: Removed broadcast_96_piece0 on 172.18.0.14:37571 in memory (size: 19.5 KiB, free: 434.0 MiB)
26/02/13 12:06:38 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:06:38 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:06:38 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:06:38 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:06:38 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:06:38 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:06:38 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#25863 - origin_code.nullCount#25862) > 0)
26/02/13 12:06:38 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#25868 - destination_code.nullCount#25867) > 0)
26/02/13 12:06:38 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#25898 - callsign.nullCount#25897) > 0)
26/02/13 12:06:38 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 12:06:38 INFO DAGScheduler: Got job 65 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 12:06:38 INFO DAGScheduler: Final stage: ResultStage 101 (start at NativeMethodAccessorImpl.java:0)
26/02/13 12:06:38 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 100)
26/02/13 12:06:38 INFO DAGScheduler: Missing parents: List()
26/02/13 12:06:38 INFO DAGScheduler: Submitting ResultStage 101 (MapPartitionsRDD[351] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:06:38 INFO MemoryStore: Block broadcast_97 stored as values in memory (estimated size 99.3 KiB, free 433.9 MiB)
26/02/13 12:06:38 INFO MemoryStore: Block broadcast_97_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 433.8 MiB)
26/02/13 12:06:38 INFO BlockManagerInfo: Added broadcast_97_piece0 in memory on spark-master:42073 (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 12:06:38 INFO SparkContext: Created broadcast 97 from broadcast at DAGScheduler.scala:1585
26/02/13 12:06:38 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 101 (MapPartitionsRDD[351] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 12:06:38 INFO TaskSchedulerImpl: Adding task set 101.0 with 4 tasks resource profile 0
26/02/13 12:06:38 INFO TaskSetManager: Starting task 0.0 in stage 101.0 (TID 205) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:06:38 INFO TaskSetManager: Starting task 1.0 in stage 101.0 (TID 206) (172.18.0.14, executor 1, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:06:38 INFO BlockManagerInfo: Added broadcast_97_piece0 in memory on 172.18.0.14:37571 (size: 33.3 KiB, free: 433.9 MiB)
26/02/13 12:06:38 INFO TaskSetManager: Starting task 2.0 in stage 101.0 (TID 207) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:06:38 INFO TaskSetManager: Finished task 0.0 in stage 101.0 (TID 205) in 33 ms on 172.18.0.14 (executor 1) (1/4)
26/02/13 12:06:38 INFO TaskSetManager: Starting task 3.0 in stage 101.0 (TID 208) (172.18.0.14, executor 1, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:06:38 INFO TaskSetManager: Finished task 1.0 in stage 101.0 (TID 206) in 42 ms on 172.18.0.14 (executor 1) (2/4)
26/02/13 12:06:38 INFO TaskSetManager: Finished task 3.0 in stage 101.0 (TID 208) in 26 ms on 172.18.0.14 (executor 1) (3/4)
26/02/13 12:06:38 INFO TaskSetManager: Finished task 2.0 in stage 101.0 (TID 207) in 35 ms on 172.18.0.14 (executor 1) (4/4)
26/02/13 12:06:38 INFO TaskSchedulerImpl: Removed TaskSet 101.0, whose tasks have all completed, from pool 
26/02/13 12:06:38 INFO DAGScheduler: ResultStage 101 (start at NativeMethodAccessorImpl.java:0) finished in 0.081 s
26/02/13 12:06:38 INFO DAGScheduler: Job 65 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 12:06:38 INFO TaskSchedulerImpl: Killing all running tasks in stage 101: Stage finished
26/02/13 12:06:38 INFO DAGScheduler: Job 65 finished: start at NativeMethodAccessorImpl.java:0, took 0.085041 s
26/02/13 12:06:38 INFO BlockManagerInfo: Removed broadcast_92_piece0 on spark-master:42073 in memory (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 12:06:38 INFO BlockManagerInfo: Removed broadcast_92_piece0 on 172.18.0.14:37571 in memory (size: 107.2 KiB, free: 434.0 MiB)
26/02/13 12:06:38 INFO BlockManagerInfo: Removed broadcast_92_piece0 on 172.18.0.15:37717 in memory (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 12:06:38 INFO MemoryStore: Block broadcast_98_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.8 MiB)
26/02/13 12:06:38 INFO BlockManagerInfo: Added broadcast_98_piece0 in memory on spark-master:42073 (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 12:06:38 INFO SparkContext: Created broadcast 98 from start at NativeMethodAccessorImpl.java:0
26/02/13 12:06:38 INFO BlockManagerInfo: Removed broadcast_95_piece0 on spark-master:42073 in memory (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 12:06:38 INFO BlockManagerInfo: Removed broadcast_95_piece0 on 172.18.0.14:37571 in memory (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 12:06:38 INFO BlockManagerInfo: Removed broadcast_95_piece0 on 172.18.0.15:37717 in memory (size: 107.2 KiB, free: 434.4 MiB)
26/02/13 12:06:38 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 151, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@72e342cc]. The input RDD has 3 partitions.
26/02/13 12:06:38 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 12:06:38 INFO DAGScheduler: Got job 66 (start at NativeMethodAccessorImpl.java:0) with 3 output partitions
26/02/13 12:06:38 INFO DAGScheduler: Final stage: ResultStage 102 (start at NativeMethodAccessorImpl.java:0)
26/02/13 12:06:38 INFO DAGScheduler: Parents of final stage: List()
26/02/13 12:06:38 INFO DAGScheduler: Missing parents: List()
26/02/13 12:06:38 INFO DAGScheduler: Submitting ResultStage 102 (MapPartitionsRDD[357] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:06:38 INFO MemoryStore: Block broadcast_99 stored as values in memory (estimated size 49.9 KiB, free 433.9 MiB)
26/02/13 12:06:38 INFO MemoryStore: Block broadcast_99_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.9 MiB)
26/02/13 12:06:38 INFO BlockManagerInfo: Added broadcast_99_piece0 in memory on spark-master:42073 (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 12:06:38 INFO SparkContext: Created broadcast 99 from broadcast at DAGScheduler.scala:1585
26/02/13 12:06:38 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 102 (MapPartitionsRDD[357] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
26/02/13 12:06:38 INFO TaskSchedulerImpl: Adding task set 102.0 with 3 tasks resource profile 0
26/02/13 12:06:38 INFO TaskSetManager: Starting task 1.0 in stage 102.0 (TID 209) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:06:38 INFO TaskSetManager: Starting task 0.0 in stage 102.0 (TID 210) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:06:38 INFO TaskSetManager: Starting task 2.0 in stage 102.0 (TID 211) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:06:38 INFO BlockManagerInfo: Added broadcast_99_piece0 in memory on 172.18.0.14:37571 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 12:06:38 INFO BlockManagerInfo: Added broadcast_99_piece0 in memory on 172.18.0.15:37717 (size: 19.5 KiB, free: 434.4 MiB)
26/02/13 12:06:38 INFO BlockManagerInfo: Added broadcast_98_piece0 in memory on 172.18.0.15:37717 (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 12:06:38 INFO BlockManagerInfo: Added broadcast_98_piece0 in memory on 172.18.0.14:37571 (size: 107.2 KiB, free: 434.0 MiB)
26/02/13 12:06:38 INFO TaskSetManager: Finished task 0.0 in stage 102.0 (TID 210) in 101 ms on 172.18.0.14 (executor 1) (1/3)
26/02/13 12:06:38 INFO TaskSetManager: Finished task 2.0 in stage 102.0 (TID 211) in 102 ms on 172.18.0.14 (executor 1) (2/3)
26/02/13 12:06:38 INFO TaskSetManager: Finished task 1.0 in stage 102.0 (TID 209) in 140 ms on 172.18.0.15 (executor 0) (3/3)
26/02/13 12:06:38 INFO TaskSchedulerImpl: Removed TaskSet 102.0, whose tasks have all completed, from pool 
26/02/13 12:06:38 INFO DAGScheduler: ResultStage 102 (start at NativeMethodAccessorImpl.java:0) finished in 0.147 s
26/02/13 12:06:38 INFO DAGScheduler: Job 66 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 12:06:38 INFO TaskSchedulerImpl: Killing all running tasks in stage 102: Stage finished
26/02/13 12:06:38 INFO DAGScheduler: Job 66 finished: start at NativeMethodAccessorImpl.java:0, took 0.149746 s
26/02/13 12:06:38 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 151, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@72e342cc] is committing.
26/02/13 12:06:38 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 151, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@72e342cc] committed.
26/02/13 12:06:38 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/151 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.151.8f709476-c817-415c-b7be-cbbf4441439c.tmp
26/02/13 12:06:39 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.151.8f709476-c817-415c-b7be-cbbf4441439c.tmp to file:/tmp/spark-checkpoint-enrichment/commits/151
26/02/13 12:06:39 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "4a161d66-225b-4b29-812d-ddec3c05364e",
  "name" : null,
  "timestamp" : "2026-02-13T12:06:38.445Z",
  "batchId" : 151,
  "numInputRows" : 131,
  "inputRowsPerSecond" : 138.18565400843883,
  "processedRowsPerSecond" : 227.43055555555557,
  "durationMs" : {
    "addBatch" : 396,
    "commitOffsets" : 76,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 47,
    "triggerExecution" : 576,
    "walCommit" : 56
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 1624,
        "1" : 1939,
        "0" : 1472
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 1670,
        "1" : 1994,
        "0" : 1502
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 1670,
        "1" : 1994,
        "0" : 1502
      }
    },
    "numInputRows" : 131,
    "inputRowsPerSecond" : 138.18565400843883,
    "processedRowsPerSecond" : 227.43055555555557,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@1a6a42c2",
    "numOutputRows" : 26
  }
}
26/02/13 12:06:48 INFO BlockManagerInfo: Removed broadcast_99_piece0 on spark-master:42073 in memory (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 12:06:48 INFO BlockManagerInfo: Removed broadcast_99_piece0 on 172.18.0.14:37571 in memory (size: 19.5 KiB, free: 434.0 MiB)
26/02/13 12:06:48 INFO BlockManagerInfo: Removed broadcast_99_piece0 on 172.18.0.15:37717 in memory (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 12:06:48 INFO BlockManagerInfo: Removed broadcast_97_piece0 on spark-master:42073 in memory (size: 33.3 KiB, free: 434.3 MiB)
26/02/13 12:06:48 INFO BlockManagerInfo: Removed broadcast_97_piece0 on 172.18.0.14:37571 in memory (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 12:06:49 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:06:53 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/152 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.152.ec496ced-15b7-4da2-8ff6-7185e3b085df.tmp
26/02/13 12:06:53 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.152.ec496ced-15b7-4da2-8ff6-7185e3b085df.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/152
26/02/13 12:06:53 INFO MicroBatchExecution: Committed offsets for batch 152. Metadata OffsetSeqMetadata(0,1770984413697,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 12:06:53 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:06:53 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:06:53 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:06:53 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:06:53 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:06:53 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:06:53 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#26717 - origin_code.nullCount#26716) > 0)
26/02/13 12:06:53 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#26722 - destination_code.nullCount#26721) > 0)
26/02/13 12:06:53 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#26752 - callsign.nullCount#26751) > 0)
26/02/13 12:06:53 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 12:06:53 INFO DAGScheduler: Got job 67 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 12:06:53 INFO DAGScheduler: Final stage: ResultStage 104 (start at NativeMethodAccessorImpl.java:0)
26/02/13 12:06:53 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 103)
26/02/13 12:06:53 INFO DAGScheduler: Missing parents: List()
26/02/13 12:06:53 INFO DAGScheduler: Submitting ResultStage 104 (MapPartitionsRDD[362] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:06:53 INFO MemoryStore: Block broadcast_100 stored as values in memory (estimated size 99.3 KiB, free 434.0 MiB)
26/02/13 12:06:53 INFO MemoryStore: Block broadcast_100_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 433.9 MiB)
26/02/13 12:06:53 INFO BlockManagerInfo: Added broadcast_100_piece0 in memory on spark-master:42073 (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 12:06:53 INFO BlockManagerInfo: Removed broadcast_98_piece0 on spark-master:42073 in memory (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 12:06:53 INFO SparkContext: Created broadcast 100 from broadcast at DAGScheduler.scala:1585
26/02/13 12:06:53 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 104 (MapPartitionsRDD[362] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 12:06:53 INFO TaskSchedulerImpl: Adding task set 104.0 with 4 tasks resource profile 0
26/02/13 12:06:53 INFO TaskSetManager: Starting task 0.0 in stage 104.0 (TID 212) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:06:53 INFO TaskSetManager: Starting task 1.0 in stage 104.0 (TID 213) (172.18.0.14, executor 1, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:06:53 INFO BlockManagerInfo: Removed broadcast_98_piece0 on 172.18.0.14:37571 in memory (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 12:06:53 INFO BlockManagerInfo: Removed broadcast_98_piece0 on 172.18.0.15:37717 in memory (size: 107.2 KiB, free: 434.4 MiB)
26/02/13 12:06:53 INFO BlockManagerInfo: Added broadcast_100_piece0 in memory on 172.18.0.14:37571 (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 12:06:53 INFO TaskSetManager: Starting task 2.0 in stage 104.0 (TID 214) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:06:53 INFO TaskSetManager: Finished task 1.0 in stage 104.0 (TID 213) in 24 ms on 172.18.0.14 (executor 1) (1/4)
26/02/13 12:06:53 INFO TaskSetManager: Starting task 3.0 in stage 104.0 (TID 215) (172.18.0.14, executor 1, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:06:53 INFO TaskSetManager: Finished task 0.0 in stage 104.0 (TID 212) in 28 ms on 172.18.0.14 (executor 1) (2/4)
26/02/13 12:06:53 INFO TaskSetManager: Finished task 2.0 in stage 104.0 (TID 214) in 19 ms on 172.18.0.14 (executor 1) (3/4)
26/02/13 12:06:53 INFO TaskSetManager: Finished task 3.0 in stage 104.0 (TID 215) in 20 ms on 172.18.0.14 (executor 1) (4/4)
26/02/13 12:06:53 INFO TaskSchedulerImpl: Removed TaskSet 104.0, whose tasks have all completed, from pool 
26/02/13 12:06:53 INFO DAGScheduler: ResultStage 104 (start at NativeMethodAccessorImpl.java:0) finished in 0.057 s
26/02/13 12:06:53 INFO DAGScheduler: Job 67 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 12:06:53 INFO TaskSchedulerImpl: Killing all running tasks in stage 104: Stage finished
26/02/13 12:06:53 INFO DAGScheduler: Job 67 finished: start at NativeMethodAccessorImpl.java:0, took 0.060238 s
26/02/13 12:06:53 INFO MemoryStore: Block broadcast_101_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.9 MiB)
26/02/13 12:06:53 INFO BlockManagerInfo: Added broadcast_101_piece0 in memory on spark-master:42073 (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 12:06:53 INFO SparkContext: Created broadcast 101 from start at NativeMethodAccessorImpl.java:0
26/02/13 12:06:53 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 152, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@7fdcc4d4]. The input RDD has 3 partitions.
26/02/13 12:06:53 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 12:06:53 INFO DAGScheduler: Got job 68 (start at NativeMethodAccessorImpl.java:0) with 3 output partitions
26/02/13 12:06:53 INFO DAGScheduler: Final stage: ResultStage 105 (start at NativeMethodAccessorImpl.java:0)
26/02/13 12:06:53 INFO DAGScheduler: Parents of final stage: List()
26/02/13 12:06:53 INFO DAGScheduler: Missing parents: List()
26/02/13 12:06:53 INFO DAGScheduler: Submitting ResultStage 105 (MapPartitionsRDD[368] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:06:53 INFO MemoryStore: Block broadcast_102 stored as values in memory (estimated size 49.9 KiB, free 433.9 MiB)
26/02/13 12:06:53 INFO MemoryStore: Block broadcast_102_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.9 MiB)
26/02/13 12:06:53 INFO BlockManagerInfo: Added broadcast_102_piece0 in memory on spark-master:42073 (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 12:06:53 INFO SparkContext: Created broadcast 102 from broadcast at DAGScheduler.scala:1585
26/02/13 12:06:53 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 105 (MapPartitionsRDD[368] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
26/02/13 12:06:53 INFO TaskSchedulerImpl: Adding task set 105.0 with 3 tasks resource profile 0
26/02/13 12:06:53 INFO TaskSetManager: Starting task 1.0 in stage 105.0 (TID 216) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:06:53 INFO TaskSetManager: Starting task 0.0 in stage 105.0 (TID 217) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:06:53 INFO TaskSetManager: Starting task 2.0 in stage 105.0 (TID 218) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:06:54 INFO BlockManagerInfo: Added broadcast_102_piece0 in memory on 172.18.0.14:37571 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 12:06:54 INFO BlockManagerInfo: Added broadcast_102_piece0 in memory on 172.18.0.15:37717 (size: 19.5 KiB, free: 434.4 MiB)
26/02/13 12:06:54 INFO BlockManagerInfo: Added broadcast_101_piece0 in memory on 172.18.0.14:37571 (size: 107.2 KiB, free: 434.0 MiB)
26/02/13 12:06:54 INFO BlockManagerInfo: Added broadcast_101_piece0 in memory on 172.18.0.15:37717 (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 12:06:54 INFO TaskSetManager: Finished task 0.0 in stage 105.0 (TID 217) in 569 ms on 172.18.0.14 (executor 1) (1/3)
26/02/13 12:06:54 INFO TaskSetManager: Finished task 2.0 in stage 105.0 (TID 218) in 572 ms on 172.18.0.14 (executor 1) (2/3)
26/02/13 12:06:54 INFO TaskSetManager: Finished task 1.0 in stage 105.0 (TID 216) in 616 ms on 172.18.0.15 (executor 0) (3/3)
26/02/13 12:06:54 INFO TaskSchedulerImpl: Removed TaskSet 105.0, whose tasks have all completed, from pool 
26/02/13 12:06:54 INFO DAGScheduler: ResultStage 105 (start at NativeMethodAccessorImpl.java:0) finished in 0.621 s
26/02/13 12:06:54 INFO DAGScheduler: Job 68 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 12:06:54 INFO TaskSchedulerImpl: Killing all running tasks in stage 105: Stage finished
26/02/13 12:06:54 INFO DAGScheduler: Job 68 finished: start at NativeMethodAccessorImpl.java:0, took 0.623046 s
26/02/13 12:06:54 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 152, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@7fdcc4d4] is committing.
26/02/13 12:06:54 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 152, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@7fdcc4d4] committed.
26/02/13 12:06:54 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/152 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.152.ddf3d09b-b518-456c-9e1c-403974627c33.tmp
26/02/13 12:06:54 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.152.ddf3d09b-b518-456c-9e1c-403974627c33.tmp to file:/tmp/spark-checkpoint-enrichment/commits/152
26/02/13 12:06:54 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "4a161d66-225b-4b29-812d-ddec3c05364e",
  "name" : null,
  "timestamp" : "2026-02-13T12:06:53.696Z",
  "batchId" : 152,
  "numInputRows" : 111,
  "inputRowsPerSecond" : 9250.0,
  "processedRowsPerSecond" : 113.14984709480123,
  "durationMs" : {
    "addBatch" : 780,
    "commitOffsets" : 68,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 32,
    "triggerExecution" : 981,
    "walCommit" : 100
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 1670,
        "1" : 1994,
        "0" : 1502
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 1707,
        "1" : 2031,
        "0" : 1539
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 1707,
        "1" : 2031,
        "0" : 1539
      }
    },
    "numInputRows" : 111,
    "inputRowsPerSecond" : 9250.0,
    "processedRowsPerSecond" : 113.14984709480123,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@1a6a42c2",
    "numOutputRows" : 27
  }
}
26/02/13 12:06:54 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/153 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.153.d291f1c4-f3cd-41e1-948f-4ce2ad11ec82.tmp
26/02/13 12:06:54 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.153.d291f1c4-f3cd-41e1-948f-4ce2ad11ec82.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/153
26/02/13 12:06:54 INFO MicroBatchExecution: Committed offsets for batch 153. Metadata OffsetSeqMetadata(0,1770984414679,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 12:06:54 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:06:54 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:06:54 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:06:54 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:06:54 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:06:54 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:06:54 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#27571 - origin_code.nullCount#27570) > 0)
26/02/13 12:06:54 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#27576 - destination_code.nullCount#27575) > 0)
26/02/13 12:06:54 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#27606 - callsign.nullCount#27605) > 0)
26/02/13 12:06:54 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 12:06:54 INFO DAGScheduler: Got job 69 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 12:06:54 INFO DAGScheduler: Final stage: ResultStage 107 (start at NativeMethodAccessorImpl.java:0)
26/02/13 12:06:54 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 106)
26/02/13 12:06:54 INFO DAGScheduler: Missing parents: List()
26/02/13 12:06:54 INFO DAGScheduler: Submitting ResultStage 107 (MapPartitionsRDD[373] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:06:54 INFO MemoryStore: Block broadcast_103 stored as values in memory (estimated size 99.3 KiB, free 433.8 MiB)
26/02/13 12:06:54 INFO MemoryStore: Block broadcast_103_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 433.7 MiB)
26/02/13 12:06:54 INFO BlockManagerInfo: Added broadcast_103_piece0 in memory on spark-master:42073 (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 12:06:54 INFO SparkContext: Created broadcast 103 from broadcast at DAGScheduler.scala:1585
26/02/13 12:06:54 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 107 (MapPartitionsRDD[373] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 12:06:54 INFO TaskSchedulerImpl: Adding task set 107.0 with 4 tasks resource profile 0
26/02/13 12:06:54 INFO TaskSetManager: Starting task 0.0 in stage 107.0 (TID 219) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:06:54 INFO TaskSetManager: Starting task 1.0 in stage 107.0 (TID 220) (172.18.0.14, executor 1, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:06:54 INFO BlockManagerInfo: Added broadcast_103_piece0 in memory on 172.18.0.14:37571 (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 12:06:54 INFO TaskSetManager: Starting task 2.0 in stage 107.0 (TID 221) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:06:54 INFO TaskSetManager: Finished task 1.0 in stage 107.0 (TID 220) in 21 ms on 172.18.0.14 (executor 1) (1/4)
26/02/13 12:06:54 INFO TaskSetManager: Starting task 3.0 in stage 107.0 (TID 222) (172.18.0.14, executor 1, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:06:54 INFO TaskSetManager: Finished task 0.0 in stage 107.0 (TID 219) in 23 ms on 172.18.0.14 (executor 1) (2/4)
26/02/13 12:06:54 INFO TaskSetManager: Finished task 3.0 in stage 107.0 (TID 222) in 15 ms on 172.18.0.14 (executor 1) (3/4)
26/02/13 12:06:54 INFO TaskSetManager: Finished task 2.0 in stage 107.0 (TID 221) in 18 ms on 172.18.0.14 (executor 1) (4/4)
26/02/13 12:06:54 INFO TaskSchedulerImpl: Removed TaskSet 107.0, whose tasks have all completed, from pool 
26/02/13 12:06:54 INFO DAGScheduler: ResultStage 107 (start at NativeMethodAccessorImpl.java:0) finished in 0.045 s
26/02/13 12:06:54 INFO DAGScheduler: Job 69 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 12:06:54 INFO TaskSchedulerImpl: Killing all running tasks in stage 107: Stage finished
26/02/13 12:06:54 INFO DAGScheduler: Job 69 finished: start at NativeMethodAccessorImpl.java:0, took 0.048192 s
26/02/13 12:06:54 INFO MemoryStore: Block broadcast_104_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.6 MiB)
26/02/13 12:06:54 INFO BlockManagerInfo: Added broadcast_104_piece0 in memory on spark-master:42073 (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 12:06:54 INFO SparkContext: Created broadcast 104 from start at NativeMethodAccessorImpl.java:0
26/02/13 12:06:54 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 153, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@785d7119]. The input RDD has 3 partitions.
26/02/13 12:06:54 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 12:06:54 INFO DAGScheduler: Got job 70 (start at NativeMethodAccessorImpl.java:0) with 3 output partitions
26/02/13 12:06:54 INFO DAGScheduler: Final stage: ResultStage 108 (start at NativeMethodAccessorImpl.java:0)
26/02/13 12:06:54 INFO DAGScheduler: Parents of final stage: List()
26/02/13 12:06:54 INFO DAGScheduler: Missing parents: List()
26/02/13 12:06:54 INFO DAGScheduler: Submitting ResultStage 108 (MapPartitionsRDD[379] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:06:54 INFO MemoryStore: Block broadcast_105 stored as values in memory (estimated size 49.9 KiB, free 433.6 MiB)
26/02/13 12:06:54 INFO MemoryStore: Block broadcast_105_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.6 MiB)
26/02/13 12:06:54 INFO BlockManagerInfo: Added broadcast_105_piece0 in memory on spark-master:42073 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 12:06:54 INFO BlockManagerInfo: Removed broadcast_101_piece0 on spark-master:42073 in memory (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 12:06:54 INFO SparkContext: Created broadcast 105 from broadcast at DAGScheduler.scala:1585
26/02/13 12:06:54 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 108 (MapPartitionsRDD[379] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
26/02/13 12:06:54 INFO TaskSchedulerImpl: Adding task set 108.0 with 3 tasks resource profile 0
26/02/13 12:06:54 INFO BlockManagerInfo: Removed broadcast_101_piece0 on 172.18.0.14:37571 in memory (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 12:06:54 INFO BlockManagerInfo: Removed broadcast_101_piece0 on 172.18.0.15:37717 in memory (size: 107.2 KiB, free: 434.4 MiB)
26/02/13 12:06:54 INFO TaskSetManager: Starting task 0.0 in stage 108.0 (TID 223) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:06:54 INFO TaskSetManager: Starting task 1.0 in stage 108.0 (TID 224) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:06:54 INFO TaskSetManager: Starting task 2.0 in stage 108.0 (TID 225) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:06:54 INFO BlockManagerInfo: Removed broadcast_103_piece0 on spark-master:42073 in memory (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 12:06:54 INFO BlockManagerInfo: Added broadcast_105_piece0 in memory on 172.18.0.14:37571 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 12:06:54 INFO BlockManagerInfo: Removed broadcast_103_piece0 on 172.18.0.14:37571 in memory (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 12:06:54 INFO BlockManagerInfo: Added broadcast_105_piece0 in memory on 172.18.0.15:37717 (size: 19.5 KiB, free: 434.4 MiB)
26/02/13 12:06:54 INFO BlockManagerInfo: Removed broadcast_100_piece0 on spark-master:42073 in memory (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 12:06:54 INFO BlockManagerInfo: Removed broadcast_100_piece0 on 172.18.0.14:37571 in memory (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 12:06:54 INFO BlockManagerInfo: Removed broadcast_102_piece0 on spark-master:42073 in memory (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 12:06:54 INFO BlockManagerInfo: Added broadcast_104_piece0 in memory on 172.18.0.14:37571 (size: 107.2 KiB, free: 434.0 MiB)
26/02/13 12:06:54 INFO BlockManagerInfo: Removed broadcast_102_piece0 on 172.18.0.14:37571 in memory (size: 19.5 KiB, free: 434.0 MiB)
26/02/13 12:06:54 INFO BlockManagerInfo: Removed broadcast_102_piece0 on 172.18.0.15:37717 in memory (size: 19.5 KiB, free: 434.4 MiB)
26/02/13 12:06:55 INFO BlockManagerInfo: Added broadcast_104_piece0 in memory on 172.18.0.15:37717 (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 12:06:55 INFO TaskSetManager: Finished task 0.0 in stage 108.0 (TID 223) in 99 ms on 172.18.0.14 (executor 1) (1/3)
26/02/13 12:06:55 INFO TaskSetManager: Finished task 2.0 in stage 108.0 (TID 225) in 98 ms on 172.18.0.14 (executor 1) (2/3)
26/02/13 12:06:55 INFO TaskSetManager: Finished task 1.0 in stage 108.0 (TID 224) in 137 ms on 172.18.0.15 (executor 0) (3/3)
26/02/13 12:06:55 INFO TaskSchedulerImpl: Removed TaskSet 108.0, whose tasks have all completed, from pool 
26/02/13 12:06:55 INFO DAGScheduler: ResultStage 108 (start at NativeMethodAccessorImpl.java:0) finished in 0.150 s
26/02/13 12:06:55 INFO DAGScheduler: Job 70 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 12:06:55 INFO TaskSchedulerImpl: Killing all running tasks in stage 108: Stage finished
26/02/13 12:06:55 INFO DAGScheduler: Job 70 finished: start at NativeMethodAccessorImpl.java:0, took 0.152297 s
26/02/13 12:06:55 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 153, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@785d7119] is committing.
26/02/13 12:06:55 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 153, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@785d7119] committed.
26/02/13 12:06:55 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/153 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.153.c5d0c2ef-a82d-4425-a679-49aa0e696425.tmp
26/02/13 12:06:55 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.153.c5d0c2ef-a82d-4425-a679-49aa0e696425.tmp to file:/tmp/spark-checkpoint-enrichment/commits/153
26/02/13 12:06:55 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "4a161d66-225b-4b29-812d-ddec3c05364e",
  "name" : null,
  "timestamp" : "2026-02-13T12:06:54.678Z",
  "batchId" : 153,
  "numInputRows" : 134,
  "inputRowsPerSecond" : 136.4562118126273,
  "processedRowsPerSecond" : 286.9379014989293,
  "durationMs" : {
    "addBatch" : 305,
    "commitOffsets" : 70,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 27,
    "triggerExecution" : 467,
    "walCommit" : 64
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 1707,
        "1" : 2031,
        "0" : 1539
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 1755,
        "1" : 2085,
        "0" : 1571
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 1755,
        "1" : 2085,
        "0" : 1571
      }
    },
    "numInputRows" : 134,
    "inputRowsPerSecond" : 136.4562118126273,
    "processedRowsPerSecond" : 286.9379014989293,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@1a6a42c2",
    "numOutputRows" : 26
  }
}
26/02/13 12:06:55 INFO BlockManagerInfo: Removed broadcast_105_piece0 on spark-master:42073 in memory (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 12:06:55 INFO BlockManagerInfo: Removed broadcast_105_piece0 on 172.18.0.14:37571 in memory (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 12:06:55 INFO BlockManagerInfo: Removed broadcast_105_piece0 on 172.18.0.15:37717 in memory (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 12:07:05 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:07:10 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/154 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.154.fe85c22a-9b50-492b-b345-763eb51424b6.tmp
26/02/13 12:07:10 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.154.fe85c22a-9b50-492b-b345-763eb51424b6.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/154
26/02/13 12:07:10 INFO MicroBatchExecution: Committed offsets for batch 154. Metadata OffsetSeqMetadata(0,1770984430569,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 12:07:10 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:07:10 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:07:10 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:07:10 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:07:10 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:07:10 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:07:10 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#28425 - origin_code.nullCount#28424) > 0)
26/02/13 12:07:10 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#28430 - destination_code.nullCount#28429) > 0)
26/02/13 12:07:10 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#28460 - callsign.nullCount#28459) > 0)
26/02/13 12:07:10 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 12:07:10 INFO DAGScheduler: Got job 71 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 12:07:10 INFO DAGScheduler: Final stage: ResultStage 110 (start at NativeMethodAccessorImpl.java:0)
26/02/13 12:07:10 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 109)
26/02/13 12:07:10 INFO DAGScheduler: Missing parents: List()
26/02/13 12:07:10 INFO DAGScheduler: Submitting ResultStage 110 (MapPartitionsRDD[384] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:07:10 INFO MemoryStore: Block broadcast_106 stored as values in memory (estimated size 99.3 KiB, free 434.0 MiB)
26/02/13 12:07:10 INFO MemoryStore: Block broadcast_106_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 433.9 MiB)
26/02/13 12:07:10 INFO BlockManagerInfo: Added broadcast_106_piece0 in memory on spark-master:42073 (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 12:07:10 INFO SparkContext: Created broadcast 106 from broadcast at DAGScheduler.scala:1585
26/02/13 12:07:10 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 110 (MapPartitionsRDD[384] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 12:07:10 INFO TaskSchedulerImpl: Adding task set 110.0 with 4 tasks resource profile 0
26/02/13 12:07:10 INFO TaskSetManager: Starting task 0.0 in stage 110.0 (TID 226) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:07:10 INFO TaskSetManager: Starting task 1.0 in stage 110.0 (TID 227) (172.18.0.14, executor 1, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:07:10 INFO BlockManagerInfo: Added broadcast_106_piece0 in memory on 172.18.0.14:37571 (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 12:07:10 INFO TaskSetManager: Starting task 2.0 in stage 110.0 (TID 228) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:07:10 INFO TaskSetManager: Starting task 3.0 in stage 110.0 (TID 229) (172.18.0.14, executor 1, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:07:10 INFO TaskSetManager: Finished task 1.0 in stage 110.0 (TID 227) in 21 ms on 172.18.0.14 (executor 1) (1/4)
26/02/13 12:07:10 INFO TaskSetManager: Finished task 0.0 in stage 110.0 (TID 226) in 22 ms on 172.18.0.14 (executor 1) (2/4)
26/02/13 12:07:10 INFO TaskSetManager: Finished task 2.0 in stage 110.0 (TID 228) in 15 ms on 172.18.0.14 (executor 1) (3/4)
26/02/13 12:07:10 INFO TaskSetManager: Finished task 3.0 in stage 110.0 (TID 229) in 17 ms on 172.18.0.14 (executor 1) (4/4)
26/02/13 12:07:10 INFO TaskSchedulerImpl: Removed TaskSet 110.0, whose tasks have all completed, from pool 
26/02/13 12:07:10 INFO DAGScheduler: ResultStage 110 (start at NativeMethodAccessorImpl.java:0) finished in 0.044 s
26/02/13 12:07:10 INFO DAGScheduler: Job 71 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 12:07:10 INFO TaskSchedulerImpl: Killing all running tasks in stage 110: Stage finished
26/02/13 12:07:10 INFO DAGScheduler: Job 71 finished: start at NativeMethodAccessorImpl.java:0, took 0.046588 s
26/02/13 12:07:10 INFO MemoryStore: Block broadcast_107_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.8 MiB)
26/02/13 12:07:10 INFO BlockManagerInfo: Added broadcast_107_piece0 in memory on spark-master:42073 (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 12:07:10 INFO SparkContext: Created broadcast 107 from start at NativeMethodAccessorImpl.java:0
26/02/13 12:07:10 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 154, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@7c9eb24f]. The input RDD has 3 partitions.
26/02/13 12:07:10 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 12:07:10 INFO DAGScheduler: Got job 72 (start at NativeMethodAccessorImpl.java:0) with 3 output partitions
26/02/13 12:07:10 INFO DAGScheduler: Final stage: ResultStage 111 (start at NativeMethodAccessorImpl.java:0)
26/02/13 12:07:10 INFO DAGScheduler: Parents of final stage: List()
26/02/13 12:07:10 INFO DAGScheduler: Missing parents: List()
26/02/13 12:07:10 INFO DAGScheduler: Submitting ResultStage 111 (MapPartitionsRDD[390] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:07:10 INFO MemoryStore: Block broadcast_108 stored as values in memory (estimated size 49.9 KiB, free 433.8 MiB)
26/02/13 12:07:10 INFO MemoryStore: Block broadcast_108_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.8 MiB)
26/02/13 12:07:10 INFO BlockManagerInfo: Added broadcast_108_piece0 in memory on spark-master:42073 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 12:07:10 INFO SparkContext: Created broadcast 108 from broadcast at DAGScheduler.scala:1585
26/02/13 12:07:10 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 111 (MapPartitionsRDD[390] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
26/02/13 12:07:10 INFO TaskSchedulerImpl: Adding task set 111.0 with 3 tasks resource profile 0
26/02/13 12:07:10 INFO TaskSetManager: Starting task 0.0 in stage 111.0 (TID 230) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:07:10 INFO TaskSetManager: Starting task 1.0 in stage 111.0 (TID 231) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:07:10 INFO TaskSetManager: Starting task 2.0 in stage 111.0 (TID 232) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:07:10 INFO BlockManagerInfo: Added broadcast_108_piece0 in memory on 172.18.0.14:37571 (size: 19.5 KiB, free: 434.0 MiB)
26/02/13 12:07:10 INFO BlockManagerInfo: Added broadcast_108_piece0 in memory on 172.18.0.15:37717 (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 12:07:10 INFO BlockManagerInfo: Added broadcast_107_piece0 in memory on 172.18.0.14:37571 (size: 107.2 KiB, free: 433.9 MiB)
26/02/13 12:07:10 INFO BlockManagerInfo: Added broadcast_107_piece0 in memory on 172.18.0.15:37717 (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 12:07:11 INFO TaskSetManager: Finished task 0.0 in stage 111.0 (TID 230) in 570 ms on 172.18.0.14 (executor 1) (1/3)
26/02/13 12:07:11 INFO TaskSetManager: Finished task 2.0 in stage 111.0 (TID 232) in 570 ms on 172.18.0.14 (executor 1) (2/3)
26/02/13 12:07:11 INFO TaskSetManager: Finished task 1.0 in stage 111.0 (TID 231) in 592 ms on 172.18.0.15 (executor 0) (3/3)
26/02/13 12:07:11 INFO TaskSchedulerImpl: Removed TaskSet 111.0, whose tasks have all completed, from pool 
26/02/13 12:07:11 INFO DAGScheduler: ResultStage 111 (start at NativeMethodAccessorImpl.java:0) finished in 0.598 s
26/02/13 12:07:11 INFO DAGScheduler: Job 72 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 12:07:11 INFO TaskSchedulerImpl: Killing all running tasks in stage 111: Stage finished
26/02/13 12:07:11 INFO DAGScheduler: Job 72 finished: start at NativeMethodAccessorImpl.java:0, took 0.599902 s
26/02/13 12:07:11 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 154, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@7c9eb24f] is committing.
26/02/13 12:07:11 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 154, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@7c9eb24f] committed.
26/02/13 12:07:11 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/154 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.154.594659da-a620-4540-b57d-531e4d8a769d.tmp
26/02/13 12:07:11 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.154.594659da-a620-4540-b57d-531e4d8a769d.tmp to file:/tmp/spark-checkpoint-enrichment/commits/154
26/02/13 12:07:11 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "4a161d66-225b-4b29-812d-ddec3c05364e",
  "name" : null,
  "timestamp" : "2026-02-13T12:07:10.568Z",
  "batchId" : 154,
  "numInputRows" : 111,
  "inputRowsPerSecond" : 10090.909090909092,
  "processedRowsPerSecond" : 114.31513903192585,
  "durationMs" : {
    "addBatch" : 755,
    "commitOffsets" : 57,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 47,
    "triggerExecution" : 971,
    "walCommit" : 111
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 1755,
        "1" : 2085,
        "0" : 1571
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 1792,
        "1" : 2122,
        "0" : 1608
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 1792,
        "1" : 2122,
        "0" : 1608
      }
    },
    "numInputRows" : 111,
    "inputRowsPerSecond" : 10090.909090909092,
    "processedRowsPerSecond" : 114.31513903192585,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@1a6a42c2",
    "numOutputRows" : 28
  }
}
26/02/13 12:07:11 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/155 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.155.5beed3c8-4d15-43d4-815f-60c41e580668.tmp
26/02/13 12:07:11 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.155.5beed3c8-4d15-43d4-815f-60c41e580668.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/155
26/02/13 12:07:11 INFO MicroBatchExecution: Committed offsets for batch 155. Metadata OffsetSeqMetadata(0,1770984431540,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 12:07:11 INFO BlockManagerInfo: Removed broadcast_108_piece0 on spark-master:42073 in memory (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 12:07:11 INFO BlockManagerInfo: Removed broadcast_108_piece0 on 172.18.0.15:37717 in memory (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 12:07:11 INFO BlockManagerInfo: Removed broadcast_108_piece0 on 172.18.0.14:37571 in memory (size: 19.5 KiB, free: 433.9 MiB)
26/02/13 12:07:11 INFO BlockManagerInfo: Removed broadcast_107_piece0 on spark-master:42073 in memory (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 12:07:11 INFO BlockManagerInfo: Removed broadcast_107_piece0 on 172.18.0.14:37571 in memory (size: 107.2 KiB, free: 434.0 MiB)
26/02/13 12:07:11 INFO BlockManagerInfo: Removed broadcast_107_piece0 on 172.18.0.15:37717 in memory (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 12:07:11 INFO BlockManagerInfo: Removed broadcast_106_piece0 on spark-master:42073 in memory (size: 33.3 KiB, free: 434.3 MiB)
26/02/13 12:07:11 INFO BlockManagerInfo: Removed broadcast_106_piece0 on 172.18.0.14:37571 in memory (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 12:07:11 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:07:11 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:07:11 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:07:11 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:07:11 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:07:11 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:07:11 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#29279 - origin_code.nullCount#29278) > 0)
26/02/13 12:07:11 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#29284 - destination_code.nullCount#29283) > 0)
26/02/13 12:07:11 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#29314 - callsign.nullCount#29313) > 0)
26/02/13 12:07:11 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 12:07:11 INFO DAGScheduler: Got job 73 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 12:07:11 INFO DAGScheduler: Final stage: ResultStage 113 (start at NativeMethodAccessorImpl.java:0)
26/02/13 12:07:11 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 112)
26/02/13 12:07:11 INFO DAGScheduler: Missing parents: List()
26/02/13 12:07:11 INFO DAGScheduler: Submitting ResultStage 113 (MapPartitionsRDD[395] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:07:11 INFO MemoryStore: Block broadcast_109 stored as values in memory (estimated size 99.3 KiB, free 434.0 MiB)
26/02/13 12:07:11 INFO MemoryStore: Block broadcast_109_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 433.9 MiB)
26/02/13 12:07:11 INFO BlockManagerInfo: Added broadcast_109_piece0 in memory on spark-master:42073 (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 12:07:11 INFO SparkContext: Created broadcast 109 from broadcast at DAGScheduler.scala:1585
26/02/13 12:07:11 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 113 (MapPartitionsRDD[395] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 12:07:11 INFO TaskSchedulerImpl: Adding task set 113.0 with 4 tasks resource profile 0
26/02/13 12:07:11 INFO TaskSetManager: Starting task 0.0 in stage 113.0 (TID 233) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:07:11 INFO TaskSetManager: Starting task 1.0 in stage 113.0 (TID 234) (172.18.0.14, executor 1, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:07:11 INFO BlockManagerInfo: Added broadcast_109_piece0 in memory on 172.18.0.14:37571 (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 12:07:11 INFO TaskSetManager: Starting task 2.0 in stage 113.0 (TID 235) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:07:11 INFO TaskSetManager: Finished task 0.0 in stage 113.0 (TID 233) in 17 ms on 172.18.0.14 (executor 1) (1/4)
26/02/13 12:07:11 INFO TaskSetManager: Starting task 3.0 in stage 113.0 (TID 236) (172.18.0.14, executor 1, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:07:11 INFO TaskSetManager: Finished task 1.0 in stage 113.0 (TID 234) in 21 ms on 172.18.0.14 (executor 1) (2/4)
26/02/13 12:07:11 INFO TaskSetManager: Finished task 2.0 in stage 113.0 (TID 235) in 12 ms on 172.18.0.14 (executor 1) (3/4)
26/02/13 12:07:11 INFO TaskSetManager: Finished task 3.0 in stage 113.0 (TID 236) in 14 ms on 172.18.0.14 (executor 1) (4/4)
26/02/13 12:07:11 INFO TaskSchedulerImpl: Removed TaskSet 113.0, whose tasks have all completed, from pool 
26/02/13 12:07:11 INFO DAGScheduler: ResultStage 113 (start at NativeMethodAccessorImpl.java:0) finished in 0.044 s
26/02/13 12:07:11 INFO DAGScheduler: Job 73 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 12:07:11 INFO TaskSchedulerImpl: Killing all running tasks in stage 113: Stage finished
26/02/13 12:07:11 INFO DAGScheduler: Job 73 finished: start at NativeMethodAccessorImpl.java:0, took 0.045869 s
26/02/13 12:07:11 INFO MemoryStore: Block broadcast_110_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.8 MiB)
26/02/13 12:07:11 INFO BlockManagerInfo: Added broadcast_110_piece0 in memory on spark-master:42073 (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 12:07:11 INFO SparkContext: Created broadcast 110 from start at NativeMethodAccessorImpl.java:0
26/02/13 12:07:11 INFO BlockManagerInfo: Removed broadcast_104_piece0 on spark-master:42073 in memory (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 12:07:11 INFO BlockManagerInfo: Removed broadcast_104_piece0 on 172.18.0.15:37717 in memory (size: 107.2 KiB, free: 434.4 MiB)
26/02/13 12:07:11 INFO BlockManagerInfo: Removed broadcast_104_piece0 on 172.18.0.14:37571 in memory (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 12:07:11 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 155, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@41c83377]. The input RDD has 3 partitions.
26/02/13 12:07:11 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 12:07:11 INFO DAGScheduler: Got job 74 (start at NativeMethodAccessorImpl.java:0) with 3 output partitions
26/02/13 12:07:11 INFO DAGScheduler: Final stage: ResultStage 114 (start at NativeMethodAccessorImpl.java:0)
26/02/13 12:07:11 INFO DAGScheduler: Parents of final stage: List()
26/02/13 12:07:11 INFO DAGScheduler: Missing parents: List()
26/02/13 12:07:11 INFO DAGScheduler: Submitting ResultStage 114 (MapPartitionsRDD[401] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:07:11 INFO MemoryStore: Block broadcast_111 stored as values in memory (estimated size 49.9 KiB, free 433.9 MiB)
26/02/13 12:07:11 INFO MemoryStore: Block broadcast_111_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.9 MiB)
26/02/13 12:07:11 INFO BlockManagerInfo: Added broadcast_111_piece0 in memory on spark-master:42073 (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 12:07:11 INFO SparkContext: Created broadcast 111 from broadcast at DAGScheduler.scala:1585
26/02/13 12:07:11 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 114 (MapPartitionsRDD[401] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
26/02/13 12:07:11 INFO TaskSchedulerImpl: Adding task set 114.0 with 3 tasks resource profile 0
26/02/13 12:07:11 INFO TaskSetManager: Starting task 1.0 in stage 114.0 (TID 237) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:07:11 INFO TaskSetManager: Starting task 0.0 in stage 114.0 (TID 238) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:07:11 INFO TaskSetManager: Starting task 2.0 in stage 114.0 (TID 239) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:07:11 INFO BlockManagerInfo: Added broadcast_111_piece0 in memory on 172.18.0.14:37571 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 12:07:11 INFO BlockManagerInfo: Added broadcast_111_piece0 in memory on 172.18.0.15:37717 (size: 19.5 KiB, free: 434.4 MiB)
26/02/13 12:07:11 INFO BlockManagerInfo: Added broadcast_110_piece0 in memory on 172.18.0.14:37571 (size: 107.2 KiB, free: 434.0 MiB)
26/02/13 12:07:11 INFO BlockManagerInfo: Added broadcast_110_piece0 in memory on 172.18.0.15:37717 (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 12:07:11 INFO TaskSetManager: Finished task 2.0 in stage 114.0 (TID 239) in 55 ms on 172.18.0.14 (executor 1) (1/3)
26/02/13 12:07:11 INFO TaskSetManager: Finished task 0.0 in stage 114.0 (TID 238) in 57 ms on 172.18.0.14 (executor 1) (2/3)
26/02/13 12:07:11 INFO TaskSetManager: Finished task 1.0 in stage 114.0 (TID 237) in 81 ms on 172.18.0.15 (executor 0) (3/3)
26/02/13 12:07:11 INFO TaskSchedulerImpl: Removed TaskSet 114.0, whose tasks have all completed, from pool 
26/02/13 12:07:11 INFO DAGScheduler: ResultStage 114 (start at NativeMethodAccessorImpl.java:0) finished in 0.086 s
26/02/13 12:07:11 INFO DAGScheduler: Job 74 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 12:07:11 INFO TaskSchedulerImpl: Killing all running tasks in stage 114: Stage finished
26/02/13 12:07:11 INFO DAGScheduler: Job 74 finished: start at NativeMethodAccessorImpl.java:0, took 0.088867 s
26/02/13 12:07:11 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 155, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@41c83377] is committing.
26/02/13 12:07:11 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 155, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@41c83377] committed.
26/02/13 12:07:11 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/155 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.155.17f6d1c2-c486-47cf-b2c2-4edb48e5a885.tmp
26/02/13 12:07:11 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.155.17f6d1c2-c486-47cf-b2c2-4edb48e5a885.tmp to file:/tmp/spark-checkpoint-enrichment/commits/155
26/02/13 12:07:11 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "4a161d66-225b-4b29-812d-ddec3c05364e",
  "name" : null,
  "timestamp" : "2026-02-13T12:07:11.540Z",
  "batchId" : 155,
  "numInputRows" : 135,
  "inputRowsPerSecond" : 138.88888888888889,
  "processedRowsPerSecond" : 323.7410071942446,
  "durationMs" : {
    "addBatch" : 257,
    "commitOffsets" : 59,
    "getBatch" : 0,
    "latestOffset" : 0,
    "queryPlanning" : 47,
    "triggerExecution" : 417,
    "walCommit" : 53
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 1792,
        "1" : 2122,
        "0" : 1608
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 1840,
        "1" : 2175,
        "0" : 1642
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 1840,
        "1" : 2175,
        "0" : 1642
      }
    },
    "numInputRows" : 135,
    "inputRowsPerSecond" : 138.88888888888889,
    "processedRowsPerSecond" : 323.7410071942446,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@1a6a42c2",
    "numOutputRows" : 26
  }
}
26/02/13 12:07:21 INFO BlockManagerInfo: Removed broadcast_111_piece0 on spark-master:42073 in memory (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 12:07:21 INFO BlockManagerInfo: Removed broadcast_111_piece0 on 172.18.0.14:37571 in memory (size: 19.5 KiB, free: 434.0 MiB)
26/02/13 12:07:21 INFO BlockManagerInfo: Removed broadcast_111_piece0 on 172.18.0.15:37717 in memory (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 12:07:21 INFO BlockManagerInfo: Removed broadcast_109_piece0 on spark-master:42073 in memory (size: 33.3 KiB, free: 434.3 MiB)
26/02/13 12:07:21 INFO BlockManagerInfo: Removed broadcast_109_piece0 on 172.18.0.14:37571 in memory (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 12:07:21 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:07:29 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/156 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.156.ff74c354-dfc1-4555-9b24-961b7498921e.tmp
26/02/13 12:07:29 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.156.ff74c354-dfc1-4555-9b24-961b7498921e.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/156
26/02/13 12:07:29 INFO MicroBatchExecution: Committed offsets for batch 156. Metadata OffsetSeqMetadata(0,1770984449508,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 12:07:29 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:07:29 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:07:29 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:07:29 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:07:29 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:07:29 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:07:29 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#30133 - origin_code.nullCount#30132) > 0)
26/02/13 12:07:29 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#30138 - destination_code.nullCount#30137) > 0)
26/02/13 12:07:29 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#30168 - callsign.nullCount#30167) > 0)
26/02/13 12:07:29 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 12:07:29 INFO DAGScheduler: Got job 75 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 12:07:29 INFO DAGScheduler: Final stage: ResultStage 116 (start at NativeMethodAccessorImpl.java:0)
26/02/13 12:07:29 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 115)
26/02/13 12:07:29 INFO DAGScheduler: Missing parents: List()
26/02/13 12:07:29 INFO DAGScheduler: Submitting ResultStage 116 (MapPartitionsRDD[406] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:07:29 INFO MemoryStore: Block broadcast_112 stored as values in memory (estimated size 99.3 KiB, free 434.0 MiB)
26/02/13 12:07:29 INFO MemoryStore: Block broadcast_112_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 433.9 MiB)
26/02/13 12:07:29 INFO BlockManagerInfo: Added broadcast_112_piece0 in memory on spark-master:42073 (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 12:07:29 INFO SparkContext: Created broadcast 112 from broadcast at DAGScheduler.scala:1585
26/02/13 12:07:29 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 116 (MapPartitionsRDD[406] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 12:07:29 INFO TaskSchedulerImpl: Adding task set 116.0 with 4 tasks resource profile 0
26/02/13 12:07:29 INFO TaskSetManager: Starting task 0.0 in stage 116.0 (TID 240) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:07:29 INFO TaskSetManager: Starting task 1.0 in stage 116.0 (TID 241) (172.18.0.14, executor 1, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:07:29 INFO BlockManagerInfo: Added broadcast_112_piece0 in memory on 172.18.0.14:37571 (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 12:07:29 INFO TaskSetManager: Starting task 2.0 in stage 116.0 (TID 242) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:07:29 INFO TaskSetManager: Finished task 1.0 in stage 116.0 (TID 241) in 37 ms on 172.18.0.14 (executor 1) (1/4)
26/02/13 12:07:29 INFO TaskSetManager: Starting task 3.0 in stage 116.0 (TID 243) (172.18.0.14, executor 1, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:07:29 INFO TaskSetManager: Finished task 0.0 in stage 116.0 (TID 240) in 39 ms on 172.18.0.14 (executor 1) (2/4)
26/02/13 12:07:29 INFO TaskSetManager: Finished task 2.0 in stage 116.0 (TID 242) in 20 ms on 172.18.0.14 (executor 1) (3/4)
26/02/13 12:07:29 INFO TaskSetManager: Finished task 3.0 in stage 116.0 (TID 243) in 19 ms on 172.18.0.14 (executor 1) (4/4)
26/02/13 12:07:29 INFO TaskSchedulerImpl: Removed TaskSet 116.0, whose tasks have all completed, from pool 
26/02/13 12:07:29 INFO DAGScheduler: ResultStage 116 (start at NativeMethodAccessorImpl.java:0) finished in 0.066 s
26/02/13 12:07:29 INFO DAGScheduler: Job 75 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 12:07:29 INFO TaskSchedulerImpl: Killing all running tasks in stage 116: Stage finished
26/02/13 12:07:29 INFO DAGScheduler: Job 75 finished: start at NativeMethodAccessorImpl.java:0, took 0.070407 s
26/02/13 12:07:29 INFO MemoryStore: Block broadcast_113_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.8 MiB)
26/02/13 12:07:29 INFO BlockManagerInfo: Added broadcast_113_piece0 in memory on spark-master:42073 (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 12:07:29 INFO SparkContext: Created broadcast 113 from start at NativeMethodAccessorImpl.java:0
26/02/13 12:07:29 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 156, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@fc4be51]. The input RDD has 1 partitions.
26/02/13 12:07:29 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 12:07:29 INFO DAGScheduler: Got job 76 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/02/13 12:07:29 INFO DAGScheduler: Final stage: ResultStage 117 (start at NativeMethodAccessorImpl.java:0)
26/02/13 12:07:29 INFO DAGScheduler: Parents of final stage: List()
26/02/13 12:07:29 INFO DAGScheduler: Missing parents: List()
26/02/13 12:07:29 INFO DAGScheduler: Submitting ResultStage 117 (MapPartitionsRDD[412] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:07:29 INFO MemoryStore: Block broadcast_114 stored as values in memory (estimated size 49.9 KiB, free 433.8 MiB)
26/02/13 12:07:29 INFO MemoryStore: Block broadcast_114_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.8 MiB)
26/02/13 12:07:29 INFO BlockManagerInfo: Added broadcast_114_piece0 in memory on spark-master:42073 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 12:07:29 INFO SparkContext: Created broadcast 114 from broadcast at DAGScheduler.scala:1585
26/02/13 12:07:29 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 117 (MapPartitionsRDD[412] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/02/13 12:07:29 INFO TaskSchedulerImpl: Adding task set 117.0 with 1 tasks resource profile 0
26/02/13 12:07:29 INFO TaskSetManager: Starting task 0.0 in stage 117.0 (TID 244) (172.18.0.15, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:07:29 INFO BlockManagerInfo: Added broadcast_114_piece0 in memory on 172.18.0.15:37717 (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 12:07:29 INFO BlockManagerInfo: Added broadcast_113_piece0 in memory on 172.18.0.15:37717 (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 12:07:30 INFO TaskSetManager: Finished task 0.0 in stage 117.0 (TID 244) in 567 ms on 172.18.0.15 (executor 0) (1/1)
26/02/13 12:07:30 INFO TaskSchedulerImpl: Removed TaskSet 117.0, whose tasks have all completed, from pool 
26/02/13 12:07:30 INFO DAGScheduler: ResultStage 117 (start at NativeMethodAccessorImpl.java:0) finished in 0.574 s
26/02/13 12:07:30 INFO DAGScheduler: Job 76 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 12:07:30 INFO TaskSchedulerImpl: Killing all running tasks in stage 117: Stage finished
26/02/13 12:07:30 INFO DAGScheduler: Job 76 finished: start at NativeMethodAccessorImpl.java:0, took 0.576055 s
26/02/13 12:07:30 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 156, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@fc4be51] is committing.
26/02/13 12:07:30 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 156, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@fc4be51] committed.
26/02/13 12:07:30 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/156 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.156.62114ce1-769b-488a-923c-8904582406a2.tmp
26/02/13 12:07:30 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.156.62114ce1-769b-488a-923c-8904582406a2.tmp to file:/tmp/spark-checkpoint-enrichment/commits/156
26/02/13 12:07:30 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "4a161d66-225b-4b29-812d-ddec3c05364e",
  "name" : null,
  "timestamp" : "2026-02-13T12:07:29.506Z",
  "batchId" : 156,
  "numInputRows" : 37,
  "inputRowsPerSecond" : 3083.3333333333335,
  "processedRowsPerSecond" : 40.43715846994535,
  "durationMs" : {
    "addBatch" : 744,
    "commitOffsets" : 65,
    "getBatch" : 0,
    "latestOffset" : 2,
    "queryPlanning" : 30,
    "triggerExecution" : 915,
    "walCommit" : 73
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 1840,
        "1" : 2175,
        "0" : 1642
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 1840,
        "1" : 2212,
        "0" : 1642
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 1840,
        "1" : 2212,
        "0" : 1642
      }
    },
    "numInputRows" : 37,
    "inputRowsPerSecond" : 3083.3333333333335,
    "processedRowsPerSecond" : 40.43715846994535,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@1a6a42c2",
    "numOutputRows" : 10
  }
}
26/02/13 12:07:30 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/157 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.157.9f4449c1-76c0-4394-b586-25c8515bf99c.tmp
26/02/13 12:07:30 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.157.9f4449c1-76c0-4394-b586-25c8515bf99c.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/157
26/02/13 12:07:30 INFO MicroBatchExecution: Committed offsets for batch 157. Metadata OffsetSeqMetadata(0,1770984450422,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 12:07:30 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:07:30 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:07:30 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:07:30 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:07:30 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:07:30 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:07:30 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#30987 - origin_code.nullCount#30986) > 0)
26/02/13 12:07:30 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#30992 - destination_code.nullCount#30991) > 0)
26/02/13 12:07:30 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#31022 - callsign.nullCount#31021) > 0)
26/02/13 12:07:30 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 12:07:30 INFO DAGScheduler: Got job 77 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 12:07:30 INFO DAGScheduler: Final stage: ResultStage 119 (start at NativeMethodAccessorImpl.java:0)
26/02/13 12:07:30 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 118)
26/02/13 12:07:30 INFO DAGScheduler: Missing parents: List()
26/02/13 12:07:30 INFO DAGScheduler: Submitting ResultStage 119 (MapPartitionsRDD[417] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:07:30 INFO MemoryStore: Block broadcast_115 stored as values in memory (estimated size 99.3 KiB, free 433.7 MiB)
26/02/13 12:07:30 INFO MemoryStore: Block broadcast_115_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 433.6 MiB)
26/02/13 12:07:30 INFO BlockManagerInfo: Added broadcast_115_piece0 in memory on spark-master:42073 (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 12:07:30 INFO SparkContext: Created broadcast 115 from broadcast at DAGScheduler.scala:1585
26/02/13 12:07:30 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 119 (MapPartitionsRDD[417] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 12:07:30 INFO TaskSchedulerImpl: Adding task set 119.0 with 4 tasks resource profile 0
26/02/13 12:07:30 INFO TaskSetManager: Starting task 0.0 in stage 119.0 (TID 245) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:07:30 INFO TaskSetManager: Starting task 1.0 in stage 119.0 (TID 246) (172.18.0.14, executor 1, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:07:30 INFO BlockManagerInfo: Added broadcast_115_piece0 in memory on 172.18.0.14:37571 (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 12:07:30 INFO TaskSetManager: Starting task 2.0 in stage 119.0 (TID 247) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:07:30 INFO TaskSetManager: Starting task 3.0 in stage 119.0 (TID 248) (172.18.0.14, executor 1, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:07:30 INFO TaskSetManager: Finished task 1.0 in stage 119.0 (TID 246) in 36 ms on 172.18.0.14 (executor 1) (1/4)
26/02/13 12:07:30 INFO TaskSetManager: Finished task 0.0 in stage 119.0 (TID 245) in 37 ms on 172.18.0.14 (executor 1) (2/4)
26/02/13 12:07:30 INFO TaskSetManager: Finished task 3.0 in stage 119.0 (TID 248) in 23 ms on 172.18.0.14 (executor 1) (3/4)
26/02/13 12:07:30 INFO TaskSetManager: Finished task 2.0 in stage 119.0 (TID 247) in 26 ms on 172.18.0.14 (executor 1) (4/4)
26/02/13 12:07:30 INFO TaskSchedulerImpl: Removed TaskSet 119.0, whose tasks have all completed, from pool 
26/02/13 12:07:30 INFO DAGScheduler: ResultStage 119 (start at NativeMethodAccessorImpl.java:0) finished in 0.067 s
26/02/13 12:07:30 INFO DAGScheduler: Job 77 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 12:07:30 INFO TaskSchedulerImpl: Killing all running tasks in stage 119: Stage finished
26/02/13 12:07:30 INFO DAGScheduler: Job 77 finished: start at NativeMethodAccessorImpl.java:0, took 0.069712 s
26/02/13 12:07:30 INFO MemoryStore: Block broadcast_116_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.5 MiB)
26/02/13 12:07:30 INFO BlockManagerInfo: Added broadcast_116_piece0 in memory on spark-master:42073 (size: 107.2 KiB, free: 434.0 MiB)
26/02/13 12:07:30 INFO SparkContext: Created broadcast 116 from start at NativeMethodAccessorImpl.java:0
26/02/13 12:07:30 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 157, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@309350ec]. The input RDD has 3 partitions.
26/02/13 12:07:30 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 12:07:30 INFO BlockManagerInfo: Removed broadcast_113_piece0 on spark-master:42073 in memory (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 12:07:30 INFO DAGScheduler: Got job 78 (start at NativeMethodAccessorImpl.java:0) with 3 output partitions
26/02/13 12:07:30 INFO DAGScheduler: Final stage: ResultStage 120 (start at NativeMethodAccessorImpl.java:0)
26/02/13 12:07:30 INFO DAGScheduler: Parents of final stage: List()
26/02/13 12:07:30 INFO DAGScheduler: Missing parents: List()
26/02/13 12:07:30 INFO DAGScheduler: Submitting ResultStage 120 (MapPartitionsRDD[423] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:07:30 INFO MemoryStore: Block broadcast_117 stored as values in memory (estimated size 49.9 KiB, free 433.6 MiB)
26/02/13 12:07:30 INFO MemoryStore: Block broadcast_117_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.6 MiB)
26/02/13 12:07:30 INFO BlockManagerInfo: Added broadcast_117_piece0 in memory on spark-master:42073 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 12:07:30 INFO BlockManagerInfo: Removed broadcast_113_piece0 on 172.18.0.15:37717 in memory (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 12:07:30 INFO SparkContext: Created broadcast 117 from broadcast at DAGScheduler.scala:1585
26/02/13 12:07:30 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 120 (MapPartitionsRDD[423] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
26/02/13 12:07:30 INFO TaskSchedulerImpl: Adding task set 120.0 with 3 tasks resource profile 0
26/02/13 12:07:30 INFO TaskSetManager: Starting task 1.0 in stage 120.0 (TID 249) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:07:30 INFO TaskSetManager: Starting task 0.0 in stage 120.0 (TID 250) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:07:30 INFO TaskSetManager: Starting task 2.0 in stage 120.0 (TID 251) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:07:30 INFO BlockManagerInfo: Removed broadcast_115_piece0 on spark-master:42073 in memory (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 12:07:30 INFO BlockManagerInfo: Removed broadcast_115_piece0 on 172.18.0.14:37571 in memory (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 12:07:30 INFO BlockManagerInfo: Added broadcast_117_piece0 in memory on 172.18.0.14:37571 (size: 19.5 KiB, free: 434.0 MiB)
26/02/13 12:07:30 INFO BlockManagerInfo: Added broadcast_117_piece0 in memory on 172.18.0.15:37717 (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 12:07:30 INFO BlockManagerInfo: Removed broadcast_114_piece0 on spark-master:42073 in memory (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 12:07:30 INFO BlockManagerInfo: Removed broadcast_114_piece0 on 172.18.0.15:37717 in memory (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 12:07:30 INFO BlockManagerInfo: Removed broadcast_112_piece0 on spark-master:42073 in memory (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 12:07:30 INFO BlockManagerInfo: Removed broadcast_112_piece0 on 172.18.0.14:37571 in memory (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 12:07:30 INFO BlockManagerInfo: Added broadcast_116_piece0 in memory on 172.18.0.14:37571 (size: 107.2 KiB, free: 433.9 MiB)
26/02/13 12:07:30 INFO BlockManagerInfo: Added broadcast_116_piece0 in memory on 172.18.0.15:37717 (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 12:07:30 INFO TaskSetManager: Finished task 1.0 in stage 120.0 (TID 249) in 104 ms on 172.18.0.15 (executor 0) (1/3)
26/02/13 12:07:31 INFO TaskSetManager: Finished task 2.0 in stage 120.0 (TID 251) in 593 ms on 172.18.0.14 (executor 1) (2/3)
26/02/13 12:07:31 INFO TaskSetManager: Finished task 0.0 in stage 120.0 (TID 250) in 602 ms on 172.18.0.14 (executor 1) (3/3)
26/02/13 12:07:31 INFO TaskSchedulerImpl: Removed TaskSet 120.0, whose tasks have all completed, from pool 
26/02/13 12:07:31 INFO DAGScheduler: ResultStage 120 (start at NativeMethodAccessorImpl.java:0) finished in 0.615 s
26/02/13 12:07:31 INFO DAGScheduler: Job 78 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 12:07:31 INFO TaskSchedulerImpl: Killing all running tasks in stage 120: Stage finished
26/02/13 12:07:31 INFO DAGScheduler: Job 78 finished: start at NativeMethodAccessorImpl.java:0, took 0.619402 s
26/02/13 12:07:31 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 157, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@309350ec] is committing.
26/02/13 12:07:31 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 157, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@309350ec] committed.
26/02/13 12:07:31 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/157 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.157.a4ec6410-db79-43fe-851a-0c9f9d74ebda.tmp
26/02/13 12:07:31 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.157.a4ec6410-db79-43fe-851a-0c9f9d74ebda.tmp to file:/tmp/spark-checkpoint-enrichment/commits/157
26/02/13 12:07:31 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "4a161d66-225b-4b29-812d-ddec3c05364e",
  "name" : null,
  "timestamp" : "2026-02-13T12:07:30.422Z",
  "batchId" : 157,
  "numInputRows" : 210,
  "inputRowsPerSecond" : 229.25764192139738,
  "processedRowsPerSecond" : 225.08038585209002,
  "durationMs" : {
    "addBatch" : 785,
    "commitOffsets" : 69,
    "getBatch" : 0,
    "latestOffset" : 0,
    "queryPlanning" : 23,
    "triggerExecution" : 933,
    "walCommit" : 55
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 1840,
        "1" : 2212,
        "0" : 1642
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 1924,
        "1" : 2265,
        "0" : 1715
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 1924,
        "1" : 2265,
        "0" : 1715
      }
    },
    "numInputRows" : 210,
    "inputRowsPerSecond" : 229.25764192139738,
    "processedRowsPerSecond" : 225.08038585209002,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@1a6a42c2",
    "numOutputRows" : 45
  }
}
26/02/13 12:07:31 INFO BlockManagerInfo: Removed broadcast_117_piece0 on spark-master:42073 in memory (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 12:07:31 INFO BlockManagerInfo: Removed broadcast_117_piece0 on 172.18.0.14:37571 in memory (size: 19.5 KiB, free: 434.0 MiB)
26/02/13 12:07:31 INFO BlockManagerInfo: Removed broadcast_117_piece0 on 172.18.0.15:37717 in memory (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 12:07:35 INFO NetworkClient: [AdminClient clientId=adminclient-1] Node -1 disconnected.
26/02/13 12:07:41 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:07:43 INFO BlockManagerInfo: Removed broadcast_110_piece0 on spark-master:42073 in memory (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 12:07:43 INFO BlockManagerInfo: Removed broadcast_110_piece0 on 172.18.0.15:37717 in memory (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 12:07:43 INFO BlockManagerInfo: Removed broadcast_110_piece0 on 172.18.0.14:37571 in memory (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 12:07:48 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/158 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.158.6616bfe3-0faa-40e0-8153-49201365cc6a.tmp
26/02/13 12:07:48 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.158.6616bfe3-0faa-40e0-8153-49201365cc6a.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/158
26/02/13 12:07:48 INFO MicroBatchExecution: Committed offsets for batch 158. Metadata OffsetSeqMetadata(0,1770984468340,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 12:07:48 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:07:48 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:07:48 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:07:48 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:07:48 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:07:48 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:07:48 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#31841 - origin_code.nullCount#31840) > 0)
26/02/13 12:07:48 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#31846 - destination_code.nullCount#31845) > 0)
26/02/13 12:07:48 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#31876 - callsign.nullCount#31875) > 0)
26/02/13 12:07:48 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 12:07:48 INFO DAGScheduler: Got job 79 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 12:07:48 INFO DAGScheduler: Final stage: ResultStage 122 (start at NativeMethodAccessorImpl.java:0)
26/02/13 12:07:48 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 121)
26/02/13 12:07:48 INFO DAGScheduler: Missing parents: List()
26/02/13 12:07:48 INFO DAGScheduler: Submitting ResultStage 122 (MapPartitionsRDD[428] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:07:48 INFO MemoryStore: Block broadcast_118 stored as values in memory (estimated size 99.3 KiB, free 434.0 MiB)
26/02/13 12:07:48 INFO MemoryStore: Block broadcast_118_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 433.9 MiB)
26/02/13 12:07:48 INFO BlockManagerInfo: Added broadcast_118_piece0 in memory on spark-master:42073 (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 12:07:48 INFO SparkContext: Created broadcast 118 from broadcast at DAGScheduler.scala:1585
26/02/13 12:07:48 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 122 (MapPartitionsRDD[428] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 12:07:48 INFO TaskSchedulerImpl: Adding task set 122.0 with 4 tasks resource profile 0
26/02/13 12:07:48 INFO TaskSetManager: Starting task 0.0 in stage 122.0 (TID 252) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:07:48 INFO TaskSetManager: Starting task 1.0 in stage 122.0 (TID 253) (172.18.0.14, executor 1, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:07:48 INFO BlockManagerInfo: Added broadcast_118_piece0 in memory on 172.18.0.14:37571 (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 12:07:48 INFO TaskSetManager: Starting task 2.0 in stage 122.0 (TID 254) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:07:48 INFO TaskSetManager: Finished task 1.0 in stage 122.0 (TID 253) in 20 ms on 172.18.0.14 (executor 1) (1/4)
26/02/13 12:07:48 INFO TaskSetManager: Starting task 3.0 in stage 122.0 (TID 255) (172.18.0.14, executor 1, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:07:48 INFO TaskSetManager: Finished task 0.0 in stage 122.0 (TID 252) in 26 ms on 172.18.0.14 (executor 1) (2/4)
26/02/13 12:07:48 INFO TaskSetManager: Finished task 2.0 in stage 122.0 (TID 254) in 21 ms on 172.18.0.14 (executor 1) (3/4)
26/02/13 12:07:48 INFO TaskSetManager: Finished task 3.0 in stage 122.0 (TID 255) in 20 ms on 172.18.0.14 (executor 1) (4/4)
26/02/13 12:07:48 INFO TaskSchedulerImpl: Removed TaskSet 122.0, whose tasks have all completed, from pool 
26/02/13 12:07:48 INFO DAGScheduler: ResultStage 122 (start at NativeMethodAccessorImpl.java:0) finished in 0.053 s
26/02/13 12:07:48 INFO DAGScheduler: Job 79 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 12:07:48 INFO TaskSchedulerImpl: Killing all running tasks in stage 122: Stage finished
26/02/13 12:07:48 INFO DAGScheduler: Job 79 finished: start at NativeMethodAccessorImpl.java:0, took 0.055623 s
26/02/13 12:07:48 INFO MemoryStore: Block broadcast_119_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.8 MiB)
26/02/13 12:07:48 INFO BlockManagerInfo: Added broadcast_119_piece0 in memory on spark-master:42073 (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 12:07:48 INFO SparkContext: Created broadcast 119 from start at NativeMethodAccessorImpl.java:0
26/02/13 12:07:48 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 158, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@262b3e17]. The input RDD has 3 partitions.
26/02/13 12:07:48 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 12:07:48 INFO DAGScheduler: Got job 80 (start at NativeMethodAccessorImpl.java:0) with 3 output partitions
26/02/13 12:07:48 INFO DAGScheduler: Final stage: ResultStage 123 (start at NativeMethodAccessorImpl.java:0)
26/02/13 12:07:48 INFO DAGScheduler: Parents of final stage: List()
26/02/13 12:07:48 INFO DAGScheduler: Missing parents: List()
26/02/13 12:07:48 INFO DAGScheduler: Submitting ResultStage 123 (MapPartitionsRDD[434] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:07:48 INFO MemoryStore: Block broadcast_120 stored as values in memory (estimated size 49.9 KiB, free 433.8 MiB)
26/02/13 12:07:48 INFO MemoryStore: Block broadcast_120_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.8 MiB)
26/02/13 12:07:48 INFO BlockManagerInfo: Added broadcast_120_piece0 in memory on spark-master:42073 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 12:07:48 INFO SparkContext: Created broadcast 120 from broadcast at DAGScheduler.scala:1585
26/02/13 12:07:48 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 123 (MapPartitionsRDD[434] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
26/02/13 12:07:48 INFO TaskSchedulerImpl: Adding task set 123.0 with 3 tasks resource profile 0
26/02/13 12:07:48 INFO TaskSetManager: Starting task 0.0 in stage 123.0 (TID 256) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:07:48 INFO TaskSetManager: Starting task 1.0 in stage 123.0 (TID 257) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:07:48 INFO TaskSetManager: Starting task 2.0 in stage 123.0 (TID 258) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:07:48 INFO BlockManagerInfo: Added broadcast_120_piece0 in memory on 172.18.0.14:37571 (size: 19.5 KiB, free: 434.0 MiB)
26/02/13 12:07:48 INFO BlockManagerInfo: Added broadcast_120_piece0 in memory on 172.18.0.15:37717 (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 12:07:48 INFO BlockManagerInfo: Added broadcast_119_piece0 in memory on 172.18.0.14:37571 (size: 107.2 KiB, free: 433.9 MiB)
26/02/13 12:07:48 INFO BlockManagerInfo: Added broadcast_119_piece0 in memory on 172.18.0.15:37717 (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 12:07:48 INFO BlockManagerInfo: Removed broadcast_118_piece0 on spark-master:42073 in memory (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 12:07:48 INFO BlockManagerInfo: Removed broadcast_118_piece0 on 172.18.0.14:37571 in memory (size: 33.3 KiB, free: 433.9 MiB)
26/02/13 12:07:49 INFO TaskSetManager: Finished task 0.0 in stage 123.0 (TID 256) in 560 ms on 172.18.0.14 (executor 1) (1/3)
26/02/13 12:07:49 INFO TaskSetManager: Finished task 2.0 in stage 123.0 (TID 258) in 563 ms on 172.18.0.14 (executor 1) (2/3)
26/02/13 12:07:49 INFO TaskSetManager: Finished task 1.0 in stage 123.0 (TID 257) in 575 ms on 172.18.0.15 (executor 0) (3/3)
26/02/13 12:07:49 INFO TaskSchedulerImpl: Removed TaskSet 123.0, whose tasks have all completed, from pool 
26/02/13 12:07:49 INFO DAGScheduler: ResultStage 123 (start at NativeMethodAccessorImpl.java:0) finished in 0.581 s
26/02/13 12:07:49 INFO DAGScheduler: Job 80 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 12:07:49 INFO TaskSchedulerImpl: Killing all running tasks in stage 123: Stage finished
26/02/13 12:07:49 INFO DAGScheduler: Job 80 finished: start at NativeMethodAccessorImpl.java:0, took 0.583256 s
26/02/13 12:07:49 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 158, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@262b3e17] is committing.
26/02/13 12:07:49 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 158, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@262b3e17] committed.
26/02/13 12:07:49 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/158 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.158.d26b4bb3-9b0f-4227-8b7d-380d51e8554e.tmp
26/02/13 12:07:49 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.158.d26b4bb3-9b0f-4227-8b7d-380d51e8554e.tmp to file:/tmp/spark-checkpoint-enrichment/commits/158
26/02/13 12:07:49 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "4a161d66-225b-4b29-812d-ddec3c05364e",
  "name" : null,
  "timestamp" : "2026-02-13T12:07:48.339Z",
  "batchId" : 158,
  "numInputRows" : 16,
  "inputRowsPerSecond" : 1333.3333333333333,
  "processedRowsPerSecond" : 17.094017094017094,
  "durationMs" : {
    "addBatch" : 734,
    "commitOffsets" : 73,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 28,
    "triggerExecution" : 936,
    "walCommit" : 99
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 1924,
        "1" : 2265,
        "0" : 1715
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 1927,
        "1" : 2272,
        "0" : 1721
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 1927,
        "1" : 2272,
        "0" : 1721
      }
    },
    "numInputRows" : 16,
    "inputRowsPerSecond" : 1333.3333333333333,
    "processedRowsPerSecond" : 17.094017094017094,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@1a6a42c2",
    "numOutputRows" : 5
  }
}
26/02/13 12:07:49 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/159 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.159.75197ffa-d3d8-4a42-b75c-faa3152af150.tmp
26/02/13 12:07:49 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.159.75197ffa-d3d8-4a42-b75c-faa3152af150.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/159
26/02/13 12:07:49 INFO MicroBatchExecution: Committed offsets for batch 159. Metadata OffsetSeqMetadata(0,1770984469277,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 12:07:49 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:07:49 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:07:49 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:07:49 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:07:49 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:07:49 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:07:49 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#32695 - origin_code.nullCount#32694) > 0)
26/02/13 12:07:49 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#32700 - destination_code.nullCount#32699) > 0)
26/02/13 12:07:49 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#32730 - callsign.nullCount#32729) > 0)
26/02/13 12:07:49 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 12:07:49 INFO DAGScheduler: Got job 81 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 12:07:49 INFO DAGScheduler: Final stage: ResultStage 125 (start at NativeMethodAccessorImpl.java:0)
26/02/13 12:07:49 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 124)
26/02/13 12:07:49 INFO DAGScheduler: Missing parents: List()
26/02/13 12:07:49 INFO DAGScheduler: Submitting ResultStage 125 (MapPartitionsRDD[439] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:07:49 INFO MemoryStore: Block broadcast_121 stored as values in memory (estimated size 99.3 KiB, free 433.8 MiB)
26/02/13 12:07:49 INFO MemoryStore: Block broadcast_121_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 433.8 MiB)
26/02/13 12:07:49 INFO BlockManagerInfo: Added broadcast_121_piece0 in memory on spark-master:42073 (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 12:07:49 INFO BlockManagerInfo: Removed broadcast_120_piece0 on spark-master:42073 in memory (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 12:07:49 INFO BlockManagerInfo: Removed broadcast_120_piece0 on 172.18.0.15:37717 in memory (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 12:07:49 INFO SparkContext: Created broadcast 121 from broadcast at DAGScheduler.scala:1585
26/02/13 12:07:49 INFO BlockManagerInfo: Removed broadcast_120_piece0 on 172.18.0.14:37571 in memory (size: 19.5 KiB, free: 434.0 MiB)
26/02/13 12:07:49 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 125 (MapPartitionsRDD[439] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 12:07:49 INFO TaskSchedulerImpl: Adding task set 125.0 with 4 tasks resource profile 0
26/02/13 12:07:49 INFO TaskSetManager: Starting task 0.0 in stage 125.0 (TID 259) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:07:49 INFO TaskSetManager: Starting task 1.0 in stage 125.0 (TID 260) (172.18.0.14, executor 1, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:07:49 INFO BlockManagerInfo: Removed broadcast_119_piece0 on spark-master:42073 in memory (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 12:07:49 INFO BlockManagerInfo: Removed broadcast_119_piece0 on 172.18.0.15:37717 in memory (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 12:07:49 INFO BlockManagerInfo: Removed broadcast_119_piece0 on 172.18.0.14:37571 in memory (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 12:07:49 INFO BlockManagerInfo: Added broadcast_121_piece0 in memory on 172.18.0.14:37571 (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 12:07:49 INFO TaskSetManager: Starting task 2.0 in stage 125.0 (TID 261) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:07:49 INFO TaskSetManager: Starting task 3.0 in stage 125.0 (TID 262) (172.18.0.14, executor 1, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:07:49 INFO TaskSetManager: Finished task 1.0 in stage 125.0 (TID 260) in 48 ms on 172.18.0.14 (executor 1) (1/4)
26/02/13 12:07:49 INFO TaskSetManager: Finished task 0.0 in stage 125.0 (TID 259) in 53 ms on 172.18.0.14 (executor 1) (2/4)
26/02/13 12:07:49 INFO TaskSetManager: Finished task 3.0 in stage 125.0 (TID 262) in 28 ms on 172.18.0.14 (executor 1) (3/4)
26/02/13 12:07:49 INFO BlockManagerInfo: Removed broadcast_116_piece0 on spark-master:42073 in memory (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 12:07:49 INFO BlockManagerInfo: Removed broadcast_116_piece0 on 172.18.0.14:37571 in memory (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 12:07:49 INFO BlockManagerInfo: Removed broadcast_116_piece0 on 172.18.0.15:37717 in memory (size: 107.2 KiB, free: 434.4 MiB)
26/02/13 12:07:49 INFO TaskSetManager: Finished task 2.0 in stage 125.0 (TID 261) in 58 ms on 172.18.0.14 (executor 1) (4/4)
26/02/13 12:07:49 INFO TaskSchedulerImpl: Removed TaskSet 125.0, whose tasks have all completed, from pool 
26/02/13 12:07:49 INFO DAGScheduler: ResultStage 125 (start at NativeMethodAccessorImpl.java:0) finished in 0.131 s
26/02/13 12:07:49 INFO DAGScheduler: Job 81 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 12:07:49 INFO TaskSchedulerImpl: Killing all running tasks in stage 125: Stage finished
26/02/13 12:07:49 INFO DAGScheduler: Job 81 finished: start at NativeMethodAccessorImpl.java:0, took 0.135015 s
26/02/13 12:07:49 INFO MemoryStore: Block broadcast_122_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.9 MiB)
26/02/13 12:07:49 INFO BlockManagerInfo: Added broadcast_122_piece0 in memory on spark-master:42073 (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 12:07:49 INFO SparkContext: Created broadcast 122 from start at NativeMethodAccessorImpl.java:0
26/02/13 12:07:49 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 159, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@7fa647c7]. The input RDD has 3 partitions.
26/02/13 12:07:49 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 12:07:49 INFO DAGScheduler: Got job 82 (start at NativeMethodAccessorImpl.java:0) with 3 output partitions
26/02/13 12:07:49 INFO DAGScheduler: Final stage: ResultStage 126 (start at NativeMethodAccessorImpl.java:0)
26/02/13 12:07:49 INFO DAGScheduler: Parents of final stage: List()
26/02/13 12:07:49 INFO DAGScheduler: Missing parents: List()
26/02/13 12:07:49 INFO DAGScheduler: Submitting ResultStage 126 (MapPartitionsRDD[445] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:07:49 INFO MemoryStore: Block broadcast_123 stored as values in memory (estimated size 49.9 KiB, free 433.9 MiB)
26/02/13 12:07:49 INFO MemoryStore: Block broadcast_123_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.9 MiB)
26/02/13 12:07:49 INFO BlockManagerInfo: Added broadcast_123_piece0 in memory on spark-master:42073 (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 12:07:49 INFO SparkContext: Created broadcast 123 from broadcast at DAGScheduler.scala:1585
26/02/13 12:07:49 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 126 (MapPartitionsRDD[445] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
26/02/13 12:07:49 INFO TaskSchedulerImpl: Adding task set 126.0 with 3 tasks resource profile 0
26/02/13 12:07:49 INFO TaskSetManager: Starting task 0.0 in stage 126.0 (TID 263) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:07:49 INFO TaskSetManager: Starting task 1.0 in stage 126.0 (TID 264) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:07:49 INFO TaskSetManager: Starting task 2.0 in stage 126.0 (TID 265) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:07:49 INFO BlockManagerInfo: Added broadcast_123_piece0 in memory on 172.18.0.14:37571 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 12:07:49 INFO BlockManagerInfo: Added broadcast_123_piece0 in memory on 172.18.0.15:37717 (size: 19.5 KiB, free: 434.4 MiB)
26/02/13 12:07:49 INFO BlockManagerInfo: Added broadcast_122_piece0 in memory on 172.18.0.14:37571 (size: 107.2 KiB, free: 434.0 MiB)
26/02/13 12:07:49 INFO BlockManagerInfo: Added broadcast_122_piece0 in memory on 172.18.0.15:37717 (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 12:07:49 INFO TaskSetManager: Finished task 2.0 in stage 126.0 (TID 265) in 95 ms on 172.18.0.14 (executor 1) (1/3)
26/02/13 12:07:49 INFO TaskSetManager: Finished task 0.0 in stage 126.0 (TID 263) in 101 ms on 172.18.0.14 (executor 1) (2/3)
26/02/13 12:07:49 INFO TaskSetManager: Finished task 1.0 in stage 126.0 (TID 264) in 147 ms on 172.18.0.15 (executor 0) (3/3)
26/02/13 12:07:49 INFO TaskSchedulerImpl: Removed TaskSet 126.0, whose tasks have all completed, from pool 
26/02/13 12:07:49 INFO DAGScheduler: ResultStage 126 (start at NativeMethodAccessorImpl.java:0) finished in 0.156 s
26/02/13 12:07:49 INFO DAGScheduler: Job 82 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 12:07:49 INFO TaskSchedulerImpl: Killing all running tasks in stage 126: Stage finished
26/02/13 12:07:49 INFO DAGScheduler: Job 82 finished: start at NativeMethodAccessorImpl.java:0, took 0.157424 s
26/02/13 12:07:49 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 159, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@7fa647c7] is committing.
26/02/13 12:07:49 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 159, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@7fa647c7] committed.
26/02/13 12:07:49 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/159 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.159.ef85cd34-3d1e-4ea0-9934-7986de468afc.tmp
26/02/13 12:07:49 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.159.ef85cd34-3d1e-4ea0-9934-7986de468afc.tmp to file:/tmp/spark-checkpoint-enrichment/commits/159
26/02/13 12:07:49 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "4a161d66-225b-4b29-812d-ddec3c05364e",
  "name" : null,
  "timestamp" : "2026-02-13T12:07:49.276Z",
  "batchId" : 159,
  "numInputRows" : 230,
  "inputRowsPerSecond" : 245.4642475987193,
  "processedRowsPerSecond" : 384.61538461538464,
  "durationMs" : {
    "addBatch" : 430,
    "commitOffsets" : 65,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 40,
    "triggerExecution" : 598,
    "walCommit" : 62
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 1927,
        "1" : 2272,
        "0" : 1721
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 2006,
        "1" : 2356,
        "0" : 1788
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 2006,
        "1" : 2356,
        "0" : 1788
      }
    },
    "numInputRows" : 230,
    "inputRowsPerSecond" : 245.4642475987193,
    "processedRowsPerSecond" : 384.61538461538464,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@1a6a42c2",
    "numOutputRows" : 49
  }
}
26/02/13 12:07:59 INFO BlockManagerInfo: Removed broadcast_123_piece0 on spark-master:42073 in memory (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 12:07:59 INFO BlockManagerInfo: Removed broadcast_123_piece0 on 172.18.0.14:37571 in memory (size: 19.5 KiB, free: 434.0 MiB)
26/02/13 12:07:59 INFO BlockManagerInfo: Removed broadcast_123_piece0 on 172.18.0.15:37717 in memory (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 12:07:59 INFO BlockManagerInfo: Removed broadcast_121_piece0 on spark-master:42073 in memory (size: 33.3 KiB, free: 434.3 MiB)
26/02/13 12:07:59 INFO BlockManagerInfo: Removed broadcast_121_piece0 on 172.18.0.14:37571 in memory (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 12:07:59 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:08:09 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/160 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.160.c4f89346-d639-493b-9a5f-26a2a937f047.tmp
26/02/13 12:08:09 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.160.c4f89346-d639-493b-9a5f-26a2a937f047.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/160
26/02/13 12:08:09 INFO MicroBatchExecution: Committed offsets for batch 160. Metadata OffsetSeqMetadata(0,1770984489676,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 12:08:09 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:08:09 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:08:09 INFO BlockManagerInfo: Removed broadcast_122_piece0 on spark-master:42073 in memory (size: 107.2 KiB, free: 434.4 MiB)
26/02/13 12:08:09 INFO BlockManagerInfo: Removed broadcast_122_piece0 on 172.18.0.14:37571 in memory (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 12:08:09 INFO BlockManagerInfo: Removed broadcast_122_piece0 on 172.18.0.15:37717 in memory (size: 107.2 KiB, free: 434.4 MiB)
26/02/13 12:08:09 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:08:09 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:08:09 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:08:09 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:08:09 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#33549 - origin_code.nullCount#33548) > 0)
26/02/13 12:08:09 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#33554 - destination_code.nullCount#33553) > 0)
26/02/13 12:08:09 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#33584 - callsign.nullCount#33583) > 0)
26/02/13 12:08:09 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 12:08:09 INFO DAGScheduler: Got job 83 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 12:08:09 INFO DAGScheduler: Final stage: ResultStage 128 (start at NativeMethodAccessorImpl.java:0)
26/02/13 12:08:09 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 127)
26/02/13 12:08:09 INFO DAGScheduler: Missing parents: List()
26/02/13 12:08:09 INFO DAGScheduler: Submitting ResultStage 128 (MapPartitionsRDD[450] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:08:09 INFO MemoryStore: Block broadcast_124 stored as values in memory (estimated size 99.3 KiB, free 434.1 MiB)
26/02/13 12:08:09 INFO MemoryStore: Block broadcast_124_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 434.0 MiB)
26/02/13 12:08:09 INFO BlockManagerInfo: Added broadcast_124_piece0 in memory on spark-master:42073 (size: 33.3 KiB, free: 434.3 MiB)
26/02/13 12:08:09 INFO SparkContext: Created broadcast 124 from broadcast at DAGScheduler.scala:1585
26/02/13 12:08:09 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 128 (MapPartitionsRDD[450] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 12:08:09 INFO TaskSchedulerImpl: Adding task set 128.0 with 4 tasks resource profile 0
26/02/13 12:08:09 INFO TaskSetManager: Starting task 0.0 in stage 128.0 (TID 266) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:08:09 INFO TaskSetManager: Starting task 1.0 in stage 128.0 (TID 267) (172.18.0.14, executor 1, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:08:09 INFO BlockManagerInfo: Added broadcast_124_piece0 in memory on 172.18.0.14:37571 (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 12:08:09 INFO TaskSetManager: Starting task 2.0 in stage 128.0 (TID 268) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:08:09 INFO TaskSetManager: Finished task 0.0 in stage 128.0 (TID 266) in 38 ms on 172.18.0.14 (executor 1) (1/4)
26/02/13 12:08:09 INFO TaskSetManager: Starting task 3.0 in stage 128.0 (TID 269) (172.18.0.14, executor 1, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:08:10 INFO TaskSetManager: Finished task 1.0 in stage 128.0 (TID 267) in 41 ms on 172.18.0.14 (executor 1) (2/4)
26/02/13 12:08:10 INFO TaskSetManager: Finished task 3.0 in stage 128.0 (TID 269) in 22 ms on 172.18.0.14 (executor 1) (3/4)
26/02/13 12:08:10 INFO TaskSetManager: Finished task 2.0 in stage 128.0 (TID 268) in 25 ms on 172.18.0.14 (executor 1) (4/4)
26/02/13 12:08:10 INFO DAGScheduler: ResultStage 128 (start at NativeMethodAccessorImpl.java:0) finished in 0.071 s
26/02/13 12:08:10 INFO DAGScheduler: Job 83 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 12:08:10 INFO TaskSchedulerImpl: Removed TaskSet 128.0, whose tasks have all completed, from pool 
26/02/13 12:08:10 INFO TaskSchedulerImpl: Killing all running tasks in stage 128: Stage finished
26/02/13 12:08:10 INFO DAGScheduler: Job 83 finished: start at NativeMethodAccessorImpl.java:0, took 0.074979 s
26/02/13 12:08:10 INFO MemoryStore: Block broadcast_125_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.9 MiB)
26/02/13 12:08:10 INFO BlockManagerInfo: Added broadcast_125_piece0 in memory on spark-master:42073 (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 12:08:10 INFO SparkContext: Created broadcast 125 from start at NativeMethodAccessorImpl.java:0
26/02/13 12:08:10 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 160, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@271bcd59]. The input RDD has 3 partitions.
26/02/13 12:08:10 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 12:08:10 INFO DAGScheduler: Got job 84 (start at NativeMethodAccessorImpl.java:0) with 3 output partitions
26/02/13 12:08:10 INFO DAGScheduler: Final stage: ResultStage 129 (start at NativeMethodAccessorImpl.java:0)
26/02/13 12:08:10 INFO DAGScheduler: Parents of final stage: List()
26/02/13 12:08:10 INFO DAGScheduler: Missing parents: List()
26/02/13 12:08:10 INFO DAGScheduler: Submitting ResultStage 129 (MapPartitionsRDD[456] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:08:10 INFO MemoryStore: Block broadcast_126 stored as values in memory (estimated size 49.9 KiB, free 433.9 MiB)
26/02/13 12:08:10 INFO MemoryStore: Block broadcast_126_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.9 MiB)
26/02/13 12:08:10 INFO BlockManagerInfo: Added broadcast_126_piece0 in memory on spark-master:42073 (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 12:08:10 INFO SparkContext: Created broadcast 126 from broadcast at DAGScheduler.scala:1585
26/02/13 12:08:10 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 129 (MapPartitionsRDD[456] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
26/02/13 12:08:10 INFO TaskSchedulerImpl: Adding task set 129.0 with 3 tasks resource profile 0
26/02/13 12:08:10 INFO TaskSetManager: Starting task 1.0 in stage 129.0 (TID 270) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:08:10 INFO TaskSetManager: Starting task 0.0 in stage 129.0 (TID 271) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:08:10 INFO TaskSetManager: Starting task 2.0 in stage 129.0 (TID 272) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:08:10 INFO BlockManagerInfo: Added broadcast_126_piece0 in memory on 172.18.0.14:37571 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 12:08:10 INFO BlockManagerInfo: Added broadcast_126_piece0 in memory on 172.18.0.15:37717 (size: 19.5 KiB, free: 434.4 MiB)
26/02/13 12:08:10 INFO BlockManagerInfo: Added broadcast_125_piece0 in memory on 172.18.0.14:37571 (size: 107.2 KiB, free: 434.0 MiB)
26/02/13 12:08:10 INFO BlockManagerInfo: Added broadcast_125_piece0 in memory on 172.18.0.15:37717 (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 12:08:10 INFO TaskSetManager: Finished task 2.0 in stage 129.0 (TID 272) in 623 ms on 172.18.0.14 (executor 1) (1/3)
26/02/13 12:08:10 INFO TaskSetManager: Finished task 0.0 in stage 129.0 (TID 271) in 639 ms on 172.18.0.14 (executor 1) (2/3)
26/02/13 12:08:10 INFO TaskSetManager: Finished task 1.0 in stage 129.0 (TID 270) in 677 ms on 172.18.0.15 (executor 0) (3/3)
26/02/13 12:08:10 INFO TaskSchedulerImpl: Removed TaskSet 129.0, whose tasks have all completed, from pool 
26/02/13 12:08:10 INFO DAGScheduler: ResultStage 129 (start at NativeMethodAccessorImpl.java:0) finished in 0.686 s
26/02/13 12:08:10 INFO DAGScheduler: Job 84 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 12:08:10 INFO TaskSchedulerImpl: Killing all running tasks in stage 129: Stage finished
26/02/13 12:08:10 INFO DAGScheduler: Job 84 finished: start at NativeMethodAccessorImpl.java:0, took 0.689328 s
26/02/13 12:08:10 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 160, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@271bcd59] is committing.
26/02/13 12:08:10 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 160, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@271bcd59] committed.
26/02/13 12:08:10 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/160 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.160.f0430d7c-c0f9-4041-83e6-c0d357bdb801.tmp
26/02/13 12:08:10 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.160.f0430d7c-c0f9-4041-83e6-c0d357bdb801.tmp to file:/tmp/spark-checkpoint-enrichment/commits/160
26/02/13 12:08:10 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "4a161d66-225b-4b29-812d-ddec3c05364e",
  "name" : null,
  "timestamp" : "2026-02-13T12:08:09.674Z",
  "batchId" : 160,
  "numInputRows" : 85,
  "inputRowsPerSecond" : 7083.333333333333,
  "processedRowsPerSecond" : 75.48845470692719,
  "durationMs" : {
    "addBatch" : 921,
    "commitOffsets" : 61,
    "getBatch" : 0,
    "latestOffset" : 2,
    "queryPlanning" : 60,
    "triggerExecution" : 1126,
    "walCommit" : 81
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 2006,
        "1" : 2356,
        "0" : 1788
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 2035,
        "1" : 2386,
        "0" : 1814
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 2035,
        "1" : 2386,
        "0" : 1814
      }
    },
    "numInputRows" : 85,
    "inputRowsPerSecond" : 7083.333333333333,
    "processedRowsPerSecond" : 75.48845470692719,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@1a6a42c2",
    "numOutputRows" : 26
  }
}
26/02/13 12:08:10 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/161 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.161.112aec68-82cf-4c26-ad66-0ce9314cf9a8.tmp
26/02/13 12:08:10 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.161.112aec68-82cf-4c26-ad66-0ce9314cf9a8.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/161
26/02/13 12:08:10 INFO MicroBatchExecution: Committed offsets for batch 161. Metadata OffsetSeqMetadata(0,1770984490802,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 12:08:10 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:08:10 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:08:10 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:08:10 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:08:10 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:08:10 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:08:10 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#34403 - origin_code.nullCount#34402) > 0)
26/02/13 12:08:10 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#34408 - destination_code.nullCount#34407) > 0)
26/02/13 12:08:10 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#34438 - callsign.nullCount#34437) > 0)
26/02/13 12:08:10 INFO BlockManagerInfo: Removed broadcast_126_piece0 on spark-master:42073 in memory (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 12:08:10 INFO BlockManagerInfo: Removed broadcast_126_piece0 on 172.18.0.14:37571 in memory (size: 19.5 KiB, free: 434.0 MiB)
26/02/13 12:08:10 INFO BlockManagerInfo: Removed broadcast_126_piece0 on 172.18.0.15:37717 in memory (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 12:08:10 INFO BlockManagerInfo: Removed broadcast_124_piece0 on spark-master:42073 in memory (size: 33.3 KiB, free: 434.3 MiB)
26/02/13 12:08:10 INFO BlockManagerInfo: Removed broadcast_124_piece0 on 172.18.0.14:37571 in memory (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 12:08:10 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 12:08:10 INFO DAGScheduler: Got job 85 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 12:08:10 INFO DAGScheduler: Final stage: ResultStage 131 (start at NativeMethodAccessorImpl.java:0)
26/02/13 12:08:10 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 130)
26/02/13 12:08:10 INFO DAGScheduler: Missing parents: List()
26/02/13 12:08:10 INFO BlockManagerInfo: Removed broadcast_125_piece0 on spark-master:42073 in memory (size: 107.2 KiB, free: 434.4 MiB)
26/02/13 12:08:10 INFO DAGScheduler: Submitting ResultStage 131 (MapPartitionsRDD[461] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:08:10 INFO BlockManagerInfo: Removed broadcast_125_piece0 on 172.18.0.14:37571 in memory (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 12:08:10 INFO BlockManagerInfo: Removed broadcast_125_piece0 on 172.18.0.15:37717 in memory (size: 107.2 KiB, free: 434.4 MiB)
26/02/13 12:08:10 INFO MemoryStore: Block broadcast_127 stored as values in memory (estimated size 99.3 KiB, free 434.1 MiB)
26/02/13 12:08:10 INFO MemoryStore: Block broadcast_127_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 434.0 MiB)
26/02/13 12:08:10 INFO BlockManagerInfo: Added broadcast_127_piece0 in memory on spark-master:42073 (size: 33.3 KiB, free: 434.3 MiB)
26/02/13 12:08:10 INFO SparkContext: Created broadcast 127 from broadcast at DAGScheduler.scala:1585
26/02/13 12:08:10 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 131 (MapPartitionsRDD[461] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 12:08:10 INFO TaskSchedulerImpl: Adding task set 131.0 with 4 tasks resource profile 0
26/02/13 12:08:10 INFO TaskSetManager: Starting task 0.0 in stage 131.0 (TID 273) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:08:10 INFO TaskSetManager: Starting task 1.0 in stage 131.0 (TID 274) (172.18.0.14, executor 1, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:08:10 INFO BlockManagerInfo: Added broadcast_127_piece0 in memory on 172.18.0.14:37571 (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 12:08:10 INFO TaskSetManager: Starting task 2.0 in stage 131.0 (TID 275) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:08:10 INFO TaskSetManager: Finished task 0.0 in stage 131.0 (TID 273) in 19 ms on 172.18.0.14 (executor 1) (1/4)
26/02/13 12:08:11 INFO TaskSetManager: Starting task 3.0 in stage 131.0 (TID 276) (172.18.0.14, executor 1, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:08:11 INFO TaskSetManager: Finished task 1.0 in stage 131.0 (TID 274) in 25 ms on 172.18.0.14 (executor 1) (2/4)
26/02/13 12:08:11 INFO TaskSetManager: Finished task 2.0 in stage 131.0 (TID 275) in 14 ms on 172.18.0.14 (executor 1) (3/4)
26/02/13 12:08:11 INFO TaskSetManager: Finished task 3.0 in stage 131.0 (TID 276) in 15 ms on 172.18.0.14 (executor 1) (4/4)
26/02/13 12:08:11 INFO TaskSchedulerImpl: Removed TaskSet 131.0, whose tasks have all completed, from pool 
26/02/13 12:08:11 INFO DAGScheduler: ResultStage 131 (start at NativeMethodAccessorImpl.java:0) finished in 0.044 s
26/02/13 12:08:11 INFO DAGScheduler: Job 85 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 12:08:11 INFO TaskSchedulerImpl: Killing all running tasks in stage 131: Stage finished
26/02/13 12:08:11 INFO DAGScheduler: Job 85 finished: start at NativeMethodAccessorImpl.java:0, took 0.050108 s
26/02/13 12:08:11 INFO BlockManagerInfo: Removed broadcast_127_piece0 on spark-master:42073 in memory (size: 33.3 KiB, free: 434.4 MiB)
26/02/13 12:08:11 INFO BlockManagerInfo: Removed broadcast_127_piece0 on 172.18.0.14:37571 in memory (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 12:08:11 INFO MemoryStore: Block broadcast_128_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 434.1 MiB)
26/02/13 12:08:11 INFO BlockManagerInfo: Added broadcast_128_piece0 in memory on spark-master:42073 (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 12:08:11 INFO SparkContext: Created broadcast 128 from start at NativeMethodAccessorImpl.java:0
26/02/13 12:08:11 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 161, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@f5f31fe]. The input RDD has 3 partitions.
26/02/13 12:08:11 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 12:08:11 INFO DAGScheduler: Got job 86 (start at NativeMethodAccessorImpl.java:0) with 3 output partitions
26/02/13 12:08:11 INFO DAGScheduler: Final stage: ResultStage 132 (start at NativeMethodAccessorImpl.java:0)
26/02/13 12:08:11 INFO DAGScheduler: Parents of final stage: List()
26/02/13 12:08:11 INFO DAGScheduler: Missing parents: List()
26/02/13 12:08:11 INFO DAGScheduler: Submitting ResultStage 132 (MapPartitionsRDD[467] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:08:11 INFO MemoryStore: Block broadcast_129 stored as values in memory (estimated size 49.9 KiB, free 434.0 MiB)
26/02/13 12:08:11 INFO MemoryStore: Block broadcast_129_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 434.0 MiB)
26/02/13 12:08:11 INFO BlockManagerInfo: Added broadcast_129_piece0 in memory on spark-master:42073 (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 12:08:11 INFO SparkContext: Created broadcast 129 from broadcast at DAGScheduler.scala:1585
26/02/13 12:08:11 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 132 (MapPartitionsRDD[467] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
26/02/13 12:08:11 INFO TaskSchedulerImpl: Adding task set 132.0 with 3 tasks resource profile 0
26/02/13 12:08:11 INFO TaskSetManager: Starting task 1.0 in stage 132.0 (TID 277) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:08:11 INFO TaskSetManager: Starting task 0.0 in stage 132.0 (TID 278) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:08:11 INFO TaskSetManager: Starting task 2.0 in stage 132.0 (TID 279) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:08:11 INFO BlockManagerInfo: Added broadcast_129_piece0 in memory on 172.18.0.15:37717 (size: 19.5 KiB, free: 434.4 MiB)
26/02/13 12:08:11 INFO BlockManagerInfo: Added broadcast_129_piece0 in memory on 172.18.0.14:37571 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 12:08:11 INFO BlockManagerInfo: Added broadcast_128_piece0 in memory on 172.18.0.14:37571 (size: 107.2 KiB, free: 434.0 MiB)
26/02/13 12:08:11 INFO BlockManagerInfo: Added broadcast_128_piece0 in memory on 172.18.0.15:37717 (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 12:08:11 INFO TaskSetManager: Finished task 1.0 in stage 132.0 (TID 277) in 122 ms on 172.18.0.15 (executor 0) (1/3)
26/02/13 12:08:11 INFO TaskSetManager: Finished task 0.0 in stage 132.0 (TID 278) in 142 ms on 172.18.0.14 (executor 1) (2/3)
26/02/13 12:08:11 INFO TaskSetManager: Finished task 2.0 in stage 132.0 (TID 279) in 146 ms on 172.18.0.14 (executor 1) (3/3)
26/02/13 12:08:11 INFO TaskSchedulerImpl: Removed TaskSet 132.0, whose tasks have all completed, from pool 
26/02/13 12:08:11 INFO DAGScheduler: ResultStage 132 (start at NativeMethodAccessorImpl.java:0) finished in 0.153 s
26/02/13 12:08:11 INFO DAGScheduler: Job 86 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 12:08:11 INFO TaskSchedulerImpl: Killing all running tasks in stage 132: Stage finished
26/02/13 12:08:11 INFO DAGScheduler: Job 86 finished: start at NativeMethodAccessorImpl.java:0, took 0.155919 s
26/02/13 12:08:11 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 161, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@f5f31fe] is committing.
26/02/13 12:08:11 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 161, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@f5f31fe] committed.
26/02/13 12:08:11 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/161 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.161.34198df0-ad2d-4d27-923c-d27d8e37068e.tmp
26/02/13 12:08:11 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.161.34198df0-ad2d-4d27-923c-d27d8e37068e.tmp to file:/tmp/spark-checkpoint-enrichment/commits/161
26/02/13 12:08:11 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "4a161d66-225b-4b29-812d-ddec3c05364e",
  "name" : null,
  "timestamp" : "2026-02-13T12:08:10.801Z",
  "batchId" : 161,
  "numInputRows" : 160,
  "inputRowsPerSecond" : 141.9698314108252,
  "processedRowsPerSecond" : 333.33333333333337,
  "durationMs" : {
    "addBatch" : 313,
    "commitOffsets" : 80,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 22,
    "triggerExecution" : 480,
    "walCommit" : 63
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 2035,
        "1" : 2386,
        "0" : 1814
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 2088,
        "1" : 2447,
        "0" : 1860
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 2088,
        "1" : 2447,
        "0" : 1860
      }
    },
    "numInputRows" : 160,
    "inputRowsPerSecond" : 141.9698314108252,
    "processedRowsPerSecond" : 333.33333333333337,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@1a6a42c2",
    "numOutputRows" : 29
  }
}
26/02/13 12:08:20 INFO BlockManagerInfo: Removed broadcast_129_piece0 on spark-master:42073 in memory (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 12:08:20 INFO BlockManagerInfo: Removed broadcast_129_piece0 on 172.18.0.14:37571 in memory (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 12:08:20 INFO BlockManagerInfo: Removed broadcast_129_piece0 on 172.18.0.15:37717 in memory (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 12:08:21 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:08:28 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/162 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.162.9a130bd3-e4a4-4944-8f5a-e7f7bc5ab7b4.tmp
26/02/13 12:08:28 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.162.9a130bd3-e4a4-4944-8f5a-e7f7bc5ab7b4.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/162
26/02/13 12:08:28 INFO MicroBatchExecution: Committed offsets for batch 162. Metadata OffsetSeqMetadata(0,1770984508301,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 12:08:28 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:08:28 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:08:28 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:08:28 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:08:28 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:08:28 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:08:28 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#35257 - origin_code.nullCount#35256) > 0)
26/02/13 12:08:28 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#35262 - destination_code.nullCount#35261) > 0)
26/02/13 12:08:28 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#35292 - callsign.nullCount#35291) > 0)
26/02/13 12:08:28 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 12:08:28 INFO DAGScheduler: Got job 87 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 12:08:28 INFO DAGScheduler: Final stage: ResultStage 134 (start at NativeMethodAccessorImpl.java:0)
26/02/13 12:08:28 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 133)
26/02/13 12:08:28 INFO DAGScheduler: Missing parents: List()
26/02/13 12:08:28 INFO DAGScheduler: Submitting ResultStage 134 (MapPartitionsRDD[472] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:08:28 INFO MemoryStore: Block broadcast_130 stored as values in memory (estimated size 99.3 KiB, free 434.0 MiB)
26/02/13 12:08:28 INFO MemoryStore: Block broadcast_130_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 433.9 MiB)
26/02/13 12:08:28 INFO BlockManagerInfo: Added broadcast_130_piece0 in memory on spark-master:42073 (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 12:08:28 INFO SparkContext: Created broadcast 130 from broadcast at DAGScheduler.scala:1585
26/02/13 12:08:28 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 134 (MapPartitionsRDD[472] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 12:08:28 INFO TaskSchedulerImpl: Adding task set 134.0 with 4 tasks resource profile 0
26/02/13 12:08:28 INFO TaskSetManager: Starting task 0.0 in stage 134.0 (TID 280) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:08:28 INFO TaskSetManager: Starting task 1.0 in stage 134.0 (TID 281) (172.18.0.14, executor 1, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:08:28 INFO BlockManagerInfo: Added broadcast_130_piece0 in memory on 172.18.0.14:37571 (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 12:08:28 INFO TaskSetManager: Starting task 2.0 in stage 134.0 (TID 282) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:08:28 INFO TaskSetManager: Finished task 1.0 in stage 134.0 (TID 281) in 19 ms on 172.18.0.14 (executor 1) (1/4)
26/02/13 12:08:28 INFO TaskSetManager: Starting task 3.0 in stage 134.0 (TID 283) (172.18.0.14, executor 1, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:08:28 INFO TaskSetManager: Finished task 0.0 in stage 134.0 (TID 280) in 22 ms on 172.18.0.14 (executor 1) (2/4)
26/02/13 12:08:28 INFO TaskSetManager: Finished task 2.0 in stage 134.0 (TID 282) in 28 ms on 172.18.0.14 (executor 1) (3/4)
26/02/13 12:08:28 INFO TaskSetManager: Finished task 3.0 in stage 134.0 (TID 283) in 25 ms on 172.18.0.14 (executor 1) (4/4)
26/02/13 12:08:28 INFO TaskSchedulerImpl: Removed TaskSet 134.0, whose tasks have all completed, from pool 
26/02/13 12:08:28 INFO DAGScheduler: ResultStage 134 (start at NativeMethodAccessorImpl.java:0) finished in 0.052 s
26/02/13 12:08:28 INFO DAGScheduler: Job 87 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 12:08:28 INFO TaskSchedulerImpl: Killing all running tasks in stage 134: Stage finished
26/02/13 12:08:28 INFO DAGScheduler: Job 87 finished: start at NativeMethodAccessorImpl.java:0, took 0.054973 s
26/02/13 12:08:28 INFO MemoryStore: Block broadcast_131_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.8 MiB)
26/02/13 12:08:28 INFO BlockManagerInfo: Added broadcast_131_piece0 in memory on spark-master:42073 (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 12:08:28 INFO SparkContext: Created broadcast 131 from start at NativeMethodAccessorImpl.java:0
26/02/13 12:08:28 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 162, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@24d28d91]. The input RDD has 3 partitions.
26/02/13 12:08:28 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 12:08:28 INFO DAGScheduler: Got job 88 (start at NativeMethodAccessorImpl.java:0) with 3 output partitions
26/02/13 12:08:28 INFO DAGScheduler: Final stage: ResultStage 135 (start at NativeMethodAccessorImpl.java:0)
26/02/13 12:08:28 INFO DAGScheduler: Parents of final stage: List()
26/02/13 12:08:28 INFO DAGScheduler: Missing parents: List()
26/02/13 12:08:28 INFO DAGScheduler: Submitting ResultStage 135 (MapPartitionsRDD[478] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:08:28 INFO MemoryStore: Block broadcast_132 stored as values in memory (estimated size 49.9 KiB, free 433.8 MiB)
26/02/13 12:08:28 INFO MemoryStore: Block broadcast_132_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.8 MiB)
26/02/13 12:08:28 INFO BlockManagerInfo: Added broadcast_132_piece0 in memory on spark-master:42073 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 12:08:28 INFO SparkContext: Created broadcast 132 from broadcast at DAGScheduler.scala:1585
26/02/13 12:08:28 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 135 (MapPartitionsRDD[478] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
26/02/13 12:08:28 INFO TaskSchedulerImpl: Adding task set 135.0 with 3 tasks resource profile 0
26/02/13 12:08:28 INFO TaskSetManager: Starting task 0.0 in stage 135.0 (TID 284) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:08:28 INFO TaskSetManager: Starting task 1.0 in stage 135.0 (TID 285) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:08:28 INFO TaskSetManager: Starting task 2.0 in stage 135.0 (TID 286) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:08:28 INFO BlockManagerInfo: Added broadcast_132_piece0 in memory on 172.18.0.14:37571 (size: 19.5 KiB, free: 434.0 MiB)
26/02/13 12:08:28 INFO BlockManagerInfo: Added broadcast_132_piece0 in memory on 172.18.0.15:37717 (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 12:08:28 INFO BlockManagerInfo: Added broadcast_131_piece0 in memory on 172.18.0.15:37717 (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 12:08:28 INFO BlockManagerInfo: Added broadcast_131_piece0 in memory on 172.18.0.14:37571 (size: 107.2 KiB, free: 433.9 MiB)
26/02/13 12:08:29 INFO TaskSetManager: Finished task 0.0 in stage 135.0 (TID 284) in 584 ms on 172.18.0.14 (executor 1) (1/3)
26/02/13 12:08:29 INFO TaskSetManager: Finished task 2.0 in stage 135.0 (TID 286) in 588 ms on 172.18.0.14 (executor 1) (2/3)
26/02/13 12:08:29 INFO TaskSetManager: Finished task 1.0 in stage 135.0 (TID 285) in 606 ms on 172.18.0.15 (executor 0) (3/3)
26/02/13 12:08:29 INFO TaskSchedulerImpl: Removed TaskSet 135.0, whose tasks have all completed, from pool 
26/02/13 12:08:29 INFO DAGScheduler: ResultStage 135 (start at NativeMethodAccessorImpl.java:0) finished in 0.614 s
26/02/13 12:08:29 INFO DAGScheduler: Job 88 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 12:08:29 INFO TaskSchedulerImpl: Killing all running tasks in stage 135: Stage finished
26/02/13 12:08:29 INFO DAGScheduler: Job 88 finished: start at NativeMethodAccessorImpl.java:0, took 0.616676 s
26/02/13 12:08:29 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 162, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@24d28d91] is committing.
26/02/13 12:08:29 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 162, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@24d28d91] committed.
26/02/13 12:08:29 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/162 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.162.d0df7bd7-ed13-4fd8-af34-e2ada4fd44de.tmp
26/02/13 12:08:29 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.162.d0df7bd7-ed13-4fd8-af34-e2ada4fd44de.tmp to file:/tmp/spark-checkpoint-enrichment/commits/162
26/02/13 12:08:29 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "4a161d66-225b-4b29-812d-ddec3c05364e",
  "name" : null,
  "timestamp" : "2026-02-13T12:08:28.299Z",
  "batchId" : 162,
  "numInputRows" : 111,
  "inputRowsPerSecond" : 8538.461538461539,
  "processedRowsPerSecond" : 112.91963377416073,
  "durationMs" : {
    "addBatch" : 766,
    "commitOffsets" : 64,
    "getBatch" : 0,
    "latestOffset" : 2,
    "queryPlanning" : 46,
    "triggerExecution" : 983,
    "walCommit" : 104
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 2088,
        "1" : 2447,
        "0" : 1860
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 2125,
        "1" : 2484,
        "0" : 1897
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 2125,
        "1" : 2484,
        "0" : 1897
      }
    },
    "numInputRows" : 111,
    "inputRowsPerSecond" : 8538.461538461539,
    "processedRowsPerSecond" : 112.91963377416073,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@1a6a42c2",
    "numOutputRows" : 30
  }
}
26/02/13 12:08:29 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/163 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.163.c77d8a43-693f-4ddd-a9e8-667288153b2b.tmp
26/02/13 12:08:29 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.163.c77d8a43-693f-4ddd-a9e8-667288153b2b.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/163
26/02/13 12:08:29 INFO MicroBatchExecution: Committed offsets for batch 163. Metadata OffsetSeqMetadata(0,1770984509284,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 12:08:29 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:08:29 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:08:29 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:08:29 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:08:29 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:08:29 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:08:29 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#36111 - origin_code.nullCount#36110) > 0)
26/02/13 12:08:29 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#36116 - destination_code.nullCount#36115) > 0)
26/02/13 12:08:29 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#36146 - callsign.nullCount#36145) > 0)
26/02/13 12:08:29 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 12:08:29 INFO DAGScheduler: Got job 89 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 12:08:29 INFO DAGScheduler: Final stage: ResultStage 137 (start at NativeMethodAccessorImpl.java:0)
26/02/13 12:08:29 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 136)
26/02/13 12:08:29 INFO DAGScheduler: Missing parents: List()
26/02/13 12:08:29 INFO DAGScheduler: Submitting ResultStage 137 (MapPartitionsRDD[483] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:08:29 INFO MemoryStore: Block broadcast_133 stored as values in memory (estimated size 99.3 KiB, free 433.7 MiB)
26/02/13 12:08:29 INFO MemoryStore: Block broadcast_133_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 433.6 MiB)
26/02/13 12:08:29 INFO BlockManagerInfo: Added broadcast_133_piece0 in memory on spark-master:42073 (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 12:08:29 INFO SparkContext: Created broadcast 133 from broadcast at DAGScheduler.scala:1585
26/02/13 12:08:29 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 137 (MapPartitionsRDD[483] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 12:08:29 INFO TaskSchedulerImpl: Adding task set 137.0 with 4 tasks resource profile 0
26/02/13 12:08:29 INFO TaskSetManager: Starting task 0.0 in stage 137.0 (TID 287) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:08:29 INFO TaskSetManager: Starting task 1.0 in stage 137.0 (TID 288) (172.18.0.14, executor 1, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:08:29 INFO BlockManagerInfo: Added broadcast_133_piece0 in memory on 172.18.0.14:37571 (size: 33.3 KiB, free: 433.9 MiB)
26/02/13 12:08:29 INFO TaskSetManager: Starting task 2.0 in stage 137.0 (TID 289) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:08:29 INFO TaskSetManager: Finished task 0.0 in stage 137.0 (TID 287) in 22 ms on 172.18.0.14 (executor 1) (1/4)
26/02/13 12:08:29 INFO TaskSetManager: Starting task 3.0 in stage 137.0 (TID 290) (172.18.0.14, executor 1, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:08:29 INFO TaskSetManager: Finished task 1.0 in stage 137.0 (TID 288) in 23 ms on 172.18.0.14 (executor 1) (2/4)
26/02/13 12:08:29 INFO TaskSetManager: Finished task 3.0 in stage 137.0 (TID 290) in 15 ms on 172.18.0.14 (executor 1) (3/4)
26/02/13 12:08:29 INFO TaskSetManager: Finished task 2.0 in stage 137.0 (TID 289) in 17 ms on 172.18.0.14 (executor 1) (4/4)
26/02/13 12:08:29 INFO TaskSchedulerImpl: Removed TaskSet 137.0, whose tasks have all completed, from pool 
26/02/13 12:08:29 INFO DAGScheduler: ResultStage 137 (start at NativeMethodAccessorImpl.java:0) finished in 0.046 s
26/02/13 12:08:29 INFO DAGScheduler: Job 89 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 12:08:29 INFO TaskSchedulerImpl: Killing all running tasks in stage 137: Stage finished
26/02/13 12:08:29 INFO DAGScheduler: Job 89 finished: start at NativeMethodAccessorImpl.java:0, took 0.048366 s
26/02/13 12:08:29 INFO MemoryStore: Block broadcast_134_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.5 MiB)
26/02/13 12:08:29 INFO BlockManagerInfo: Added broadcast_134_piece0 in memory on spark-master:42073 (size: 107.2 KiB, free: 434.0 MiB)
26/02/13 12:08:29 INFO SparkContext: Created broadcast 134 from start at NativeMethodAccessorImpl.java:0
26/02/13 12:08:29 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 163, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@583d3ec7]. The input RDD has 3 partitions.
26/02/13 12:08:29 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 12:08:29 INFO DAGScheduler: Got job 90 (start at NativeMethodAccessorImpl.java:0) with 3 output partitions
26/02/13 12:08:29 INFO DAGScheduler: Final stage: ResultStage 138 (start at NativeMethodAccessorImpl.java:0)
26/02/13 12:08:29 INFO DAGScheduler: Parents of final stage: List()
26/02/13 12:08:29 INFO DAGScheduler: Missing parents: List()
26/02/13 12:08:29 INFO DAGScheduler: Submitting ResultStage 138 (MapPartitionsRDD[489] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:08:29 INFO MemoryStore: Block broadcast_135 stored as values in memory (estimated size 49.9 KiB, free 433.5 MiB)
26/02/13 12:08:29 INFO MemoryStore: Block broadcast_135_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.5 MiB)
26/02/13 12:08:29 INFO BlockManagerInfo: Added broadcast_135_piece0 in memory on spark-master:42073 (size: 19.5 KiB, free: 433.9 MiB)
26/02/13 12:08:29 INFO SparkContext: Created broadcast 135 from broadcast at DAGScheduler.scala:1585
26/02/13 12:08:29 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 138 (MapPartitionsRDD[489] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
26/02/13 12:08:29 INFO TaskSchedulerImpl: Adding task set 138.0 with 3 tasks resource profile 0
26/02/13 12:08:29 INFO TaskSetManager: Starting task 1.0 in stage 138.0 (TID 291) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:08:29 INFO TaskSetManager: Starting task 0.0 in stage 138.0 (TID 292) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:08:29 INFO TaskSetManager: Starting task 2.0 in stage 138.0 (TID 293) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:08:29 INFO BlockManagerInfo: Added broadcast_135_piece0 in memory on 172.18.0.14:37571 (size: 19.5 KiB, free: 433.9 MiB)
26/02/13 12:08:29 INFO BlockManagerInfo: Added broadcast_135_piece0 in memory on 172.18.0.15:37717 (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 12:08:29 INFO BlockManagerInfo: Added broadcast_134_piece0 in memory on 172.18.0.14:37571 (size: 107.2 KiB, free: 433.7 MiB)
26/02/13 12:08:29 INFO BlockManagerInfo: Added broadcast_134_piece0 in memory on 172.18.0.15:37717 (size: 107.2 KiB, free: 434.0 MiB)
26/02/13 12:08:29 INFO TaskSetManager: Finished task 2.0 in stage 138.0 (TID 293) in 52 ms on 172.18.0.14 (executor 1) (1/3)
26/02/13 12:08:29 INFO TaskSetManager: Finished task 0.0 in stage 138.0 (TID 292) in 58 ms on 172.18.0.14 (executor 1) (2/3)
26/02/13 12:08:29 INFO TaskSetManager: Finished task 1.0 in stage 138.0 (TID 291) in 76 ms on 172.18.0.15 (executor 0) (3/3)
26/02/13 12:08:29 INFO TaskSchedulerImpl: Removed TaskSet 138.0, whose tasks have all completed, from pool 
26/02/13 12:08:29 INFO DAGScheduler: ResultStage 138 (start at NativeMethodAccessorImpl.java:0) finished in 0.083 s
26/02/13 12:08:29 INFO DAGScheduler: Job 90 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 12:08:29 INFO TaskSchedulerImpl: Killing all running tasks in stage 138: Stage finished
26/02/13 12:08:29 INFO DAGScheduler: Job 90 finished: start at NativeMethodAccessorImpl.java:0, took 0.084508 s
26/02/13 12:08:29 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 163, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@583d3ec7] is committing.
26/02/13 12:08:29 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 163, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@583d3ec7] committed.
26/02/13 12:08:29 INFO BlockManagerInfo: Removed broadcast_135_piece0 on spark-master:42073 in memory (size: 19.5 KiB, free: 434.0 MiB)
26/02/13 12:08:29 INFO BlockManagerInfo: Removed broadcast_135_piece0 on 172.18.0.14:37571 in memory (size: 19.5 KiB, free: 433.8 MiB)
26/02/13 12:08:29 INFO BlockManagerInfo: Removed broadcast_135_piece0 on 172.18.0.15:37717 in memory (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 12:08:29 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/163 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.163.0915af6e-d94f-404f-b2c5-ffc6620af710.tmp
26/02/13 12:08:29 INFO BlockManagerInfo: Removed broadcast_133_piece0 on spark-master:42073 in memory (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 12:08:29 INFO BlockManagerInfo: Removed broadcast_133_piece0 on 172.18.0.14:37571 in memory (size: 33.3 KiB, free: 433.8 MiB)
26/02/13 12:08:29 INFO BlockManagerInfo: Removed broadcast_131_piece0 on spark-master:42073 in memory (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 12:08:29 INFO BlockManagerInfo: Removed broadcast_131_piece0 on 172.18.0.15:37717 in memory (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 12:08:29 INFO BlockManagerInfo: Removed broadcast_131_piece0 on 172.18.0.14:37571 in memory (size: 107.2 KiB, free: 433.9 MiB)
26/02/13 12:08:29 INFO BlockManagerInfo: Removed broadcast_128_piece0 on spark-master:42073 in memory (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 12:08:29 INFO BlockManagerInfo: Removed broadcast_128_piece0 on 172.18.0.14:37571 in memory (size: 107.2 KiB, free: 434.0 MiB)
26/02/13 12:08:29 INFO BlockManagerInfo: Removed broadcast_128_piece0 on 172.18.0.15:37717 in memory (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 12:08:29 INFO BlockManagerInfo: Removed broadcast_132_piece0 on spark-master:42073 in memory (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 12:08:29 INFO BlockManagerInfo: Removed broadcast_132_piece0 on 172.18.0.14:37571 in memory (size: 19.5 KiB, free: 434.0 MiB)
26/02/13 12:08:29 INFO BlockManagerInfo: Removed broadcast_132_piece0 on 172.18.0.15:37717 in memory (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 12:08:29 INFO BlockManagerInfo: Removed broadcast_130_piece0 on spark-master:42073 in memory (size: 33.3 KiB, free: 434.3 MiB)
26/02/13 12:08:29 INFO BlockManagerInfo: Removed broadcast_130_piece0 on 172.18.0.14:37571 in memory (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 12:08:29 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.163.0915af6e-d94f-404f-b2c5-ffc6620af710.tmp to file:/tmp/spark-checkpoint-enrichment/commits/163
26/02/13 12:08:29 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "4a161d66-225b-4b29-812d-ddec3c05364e",
  "name" : null,
  "timestamp" : "2026-02-13T12:08:29.283Z",
  "batchId" : 163,
  "numInputRows" : 137,
  "inputRowsPerSecond" : 139.22764227642276,
  "processedRowsPerSecond" : 367.29222520107237,
  "durationMs" : {
    "addBatch" : 211,
    "commitOffsets" : 80,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 26,
    "triggerExecution" : 373,
    "walCommit" : 54
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 2125,
        "1" : 2484,
        "0" : 1897
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 2171,
        "1" : 2538,
        "0" : 1934
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 2171,
        "1" : 2538,
        "0" : 1934
      }
    },
    "numInputRows" : 137,
    "inputRowsPerSecond" : 139.22764227642276,
    "processedRowsPerSecond" : 367.29222520107237,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@1a6a42c2",
    "numOutputRows" : 26
  }
}
26/02/13 12:08:39 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:08:44 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/164 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.164.71876693-6c8f-4f85-bd05-26f648637de1.tmp
26/02/13 12:08:44 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.164.71876693-6c8f-4f85-bd05-26f648637de1.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/164
26/02/13 12:08:44 INFO MicroBatchExecution: Committed offsets for batch 164. Metadata OffsetSeqMetadata(0,1770984524790,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 12:08:44 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:08:44 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:08:44 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:08:44 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:08:44 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:08:44 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:08:44 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#36965 - origin_code.nullCount#36964) > 0)
26/02/13 12:08:44 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#36970 - destination_code.nullCount#36969) > 0)
26/02/13 12:08:44 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#37000 - callsign.nullCount#36999) > 0)
26/02/13 12:08:45 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 12:08:45 INFO DAGScheduler: Got job 91 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 12:08:45 INFO DAGScheduler: Final stage: ResultStage 140 (start at NativeMethodAccessorImpl.java:0)
26/02/13 12:08:45 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 139)
26/02/13 12:08:45 INFO DAGScheduler: Missing parents: List()
26/02/13 12:08:45 INFO DAGScheduler: Submitting ResultStage 140 (MapPartitionsRDD[494] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:08:45 INFO MemoryStore: Block broadcast_136 stored as values in memory (estimated size 99.3 KiB, free 434.0 MiB)
26/02/13 12:08:45 INFO MemoryStore: Block broadcast_136_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 433.9 MiB)
26/02/13 12:08:45 INFO BlockManagerInfo: Added broadcast_136_piece0 in memory on spark-master:42073 (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 12:08:45 INFO SparkContext: Created broadcast 136 from broadcast at DAGScheduler.scala:1585
26/02/13 12:08:45 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 140 (MapPartitionsRDD[494] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 12:08:45 INFO TaskSchedulerImpl: Adding task set 140.0 with 4 tasks resource profile 0
26/02/13 12:08:45 INFO TaskSetManager: Starting task 0.0 in stage 140.0 (TID 294) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:08:45 INFO TaskSetManager: Starting task 1.0 in stage 140.0 (TID 295) (172.18.0.14, executor 1, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:08:45 INFO BlockManagerInfo: Added broadcast_136_piece0 in memory on 172.18.0.14:37571 (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 12:08:45 INFO TaskSetManager: Starting task 2.0 in stage 140.0 (TID 296) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:08:45 INFO TaskSetManager: Finished task 1.0 in stage 140.0 (TID 295) in 20 ms on 172.18.0.14 (executor 1) (1/4)
26/02/13 12:08:45 INFO TaskSetManager: Starting task 3.0 in stage 140.0 (TID 297) (172.18.0.14, executor 1, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:08:45 INFO TaskSetManager: Finished task 0.0 in stage 140.0 (TID 294) in 22 ms on 172.18.0.14 (executor 1) (2/4)
26/02/13 12:08:45 INFO TaskSetManager: Finished task 2.0 in stage 140.0 (TID 296) in 12 ms on 172.18.0.14 (executor 1) (3/4)
26/02/13 12:08:45 INFO TaskSetManager: Finished task 3.0 in stage 140.0 (TID 297) in 11 ms on 172.18.0.14 (executor 1) (4/4)
26/02/13 12:08:45 INFO TaskSchedulerImpl: Removed TaskSet 140.0, whose tasks have all completed, from pool 
26/02/13 12:08:45 INFO DAGScheduler: ResultStage 140 (start at NativeMethodAccessorImpl.java:0) finished in 0.039 s
26/02/13 12:08:45 INFO DAGScheduler: Job 91 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 12:08:45 INFO TaskSchedulerImpl: Killing all running tasks in stage 140: Stage finished
26/02/13 12:08:45 INFO DAGScheduler: Job 91 finished: start at NativeMethodAccessorImpl.java:0, took 0.040161 s
26/02/13 12:08:45 INFO MemoryStore: Block broadcast_137_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.8 MiB)
26/02/13 12:08:45 INFO BlockManagerInfo: Added broadcast_137_piece0 in memory on spark-master:42073 (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 12:08:45 INFO SparkContext: Created broadcast 137 from start at NativeMethodAccessorImpl.java:0
26/02/13 12:08:45 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 164, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@3109d382]. The input RDD has 3 partitions.
26/02/13 12:08:45 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 12:08:45 INFO DAGScheduler: Got job 92 (start at NativeMethodAccessorImpl.java:0) with 3 output partitions
26/02/13 12:08:45 INFO DAGScheduler: Final stage: ResultStage 141 (start at NativeMethodAccessorImpl.java:0)
26/02/13 12:08:45 INFO DAGScheduler: Parents of final stage: List()
26/02/13 12:08:45 INFO DAGScheduler: Missing parents: List()
26/02/13 12:08:45 INFO DAGScheduler: Submitting ResultStage 141 (MapPartitionsRDD[500] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:08:45 INFO MemoryStore: Block broadcast_138 stored as values in memory (estimated size 49.9 KiB, free 433.8 MiB)
26/02/13 12:08:45 INFO MemoryStore: Block broadcast_138_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.8 MiB)
26/02/13 12:08:45 INFO BlockManagerInfo: Added broadcast_138_piece0 in memory on spark-master:42073 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 12:08:45 INFO SparkContext: Created broadcast 138 from broadcast at DAGScheduler.scala:1585
26/02/13 12:08:45 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 141 (MapPartitionsRDD[500] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
26/02/13 12:08:45 INFO TaskSchedulerImpl: Adding task set 141.0 with 3 tasks resource profile 0
26/02/13 12:08:45 INFO TaskSetManager: Starting task 0.0 in stage 141.0 (TID 298) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:08:45 INFO TaskSetManager: Starting task 1.0 in stage 141.0 (TID 299) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:08:45 INFO TaskSetManager: Starting task 2.0 in stage 141.0 (TID 300) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:08:45 INFO BlockManagerInfo: Added broadcast_138_piece0 in memory on 172.18.0.14:37571 (size: 19.5 KiB, free: 434.0 MiB)
26/02/13 12:08:45 INFO BlockManagerInfo: Added broadcast_138_piece0 in memory on 172.18.0.15:37717 (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 12:08:45 INFO BlockManagerInfo: Added broadcast_137_piece0 in memory on 172.18.0.14:37571 (size: 107.2 KiB, free: 433.9 MiB)
26/02/13 12:08:45 INFO BlockManagerInfo: Added broadcast_137_piece0 in memory on 172.18.0.15:37717 (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 12:08:45 INFO TaskSetManager: Finished task 0.0 in stage 141.0 (TID 298) in 553 ms on 172.18.0.14 (executor 1) (1/3)
26/02/13 12:08:45 INFO TaskSetManager: Finished task 2.0 in stage 141.0 (TID 300) in 559 ms on 172.18.0.14 (executor 1) (2/3)
26/02/13 12:08:45 INFO TaskSetManager: Finished task 1.0 in stage 141.0 (TID 299) in 585 ms on 172.18.0.15 (executor 0) (3/3)
26/02/13 12:08:45 INFO TaskSchedulerImpl: Removed TaskSet 141.0, whose tasks have all completed, from pool 
26/02/13 12:08:45 INFO DAGScheduler: ResultStage 141 (start at NativeMethodAccessorImpl.java:0) finished in 0.589 s
26/02/13 12:08:45 INFO DAGScheduler: Job 92 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 12:08:45 INFO TaskSchedulerImpl: Killing all running tasks in stage 141: Stage finished
26/02/13 12:08:45 INFO DAGScheduler: Job 92 finished: start at NativeMethodAccessorImpl.java:0, took 0.590106 s
26/02/13 12:08:45 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 164, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@3109d382] is committing.
26/02/13 12:08:45 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 164, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@3109d382] committed.
26/02/13 12:08:45 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/164 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.164.9f174f49-a7f6-49e9-9bca-46d299bb4284.tmp
26/02/13 12:08:45 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.164.9f174f49-a7f6-49e9-9bca-46d299bb4284.tmp to file:/tmp/spark-checkpoint-enrichment/commits/164
26/02/13 12:08:45 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "4a161d66-225b-4b29-812d-ddec3c05364e",
  "name" : null,
  "timestamp" : "2026-02-13T12:08:44.788Z",
  "batchId" : 164,
  "numInputRows" : 81,
  "inputRowsPerSecond" : 6750.0,
  "processedRowsPerSecond" : 88.91328210757409,
  "durationMs" : {
    "addBatch" : 717,
    "commitOffsets" : 56,
    "getBatch" : 1,
    "latestOffset" : 2,
    "queryPlanning" : 48,
    "triggerExecution" : 911,
    "walCommit" : 87
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 2171,
        "1" : 2538,
        "0" : 1934
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 2199,
        "1" : 2566,
        "0" : 1959
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 2199,
        "1" : 2566,
        "0" : 1959
      }
    },
    "numInputRows" : 81,
    "inputRowsPerSecond" : 6750.0,
    "processedRowsPerSecond" : 88.91328210757409,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@1a6a42c2",
    "numOutputRows" : 26
  }
}
26/02/13 12:08:45 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/165 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.165.29a6bb09-6790-4f75-b6a5-f3ebe08f6288.tmp
26/02/13 12:08:45 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.165.29a6bb09-6790-4f75-b6a5-f3ebe08f6288.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/165
26/02/13 12:08:45 INFO MicroBatchExecution: Committed offsets for batch 165. Metadata OffsetSeqMetadata(0,1770984525700,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 12:08:45 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:08:45 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:08:45 INFO BlockManagerInfo: Removed broadcast_138_piece0 on spark-master:42073 in memory (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 12:08:45 INFO BlockManagerInfo: Removed broadcast_138_piece0 on 172.18.0.14:37571 in memory (size: 19.5 KiB, free: 433.9 MiB)
26/02/13 12:08:45 INFO BlockManagerInfo: Removed broadcast_138_piece0 on 172.18.0.15:37717 in memory (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 12:08:45 INFO BlockManagerInfo: Removed broadcast_136_piece0 on spark-master:42073 in memory (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 12:08:45 INFO BlockManagerInfo: Removed broadcast_136_piece0 on 172.18.0.14:37571 in memory (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 12:08:45 INFO BlockManagerInfo: Removed broadcast_137_piece0 on spark-master:42073 in memory (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 12:08:45 INFO BlockManagerInfo: Removed broadcast_137_piece0 on 172.18.0.14:37571 in memory (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 12:08:45 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:08:45 INFO BlockManagerInfo: Removed broadcast_137_piece0 on 172.18.0.15:37717 in memory (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 12:08:45 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:08:45 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:08:45 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:08:45 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#37819 - origin_code.nullCount#37818) > 0)
26/02/13 12:08:45 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#37824 - destination_code.nullCount#37823) > 0)
26/02/13 12:08:45 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#37854 - callsign.nullCount#37853) > 0)
26/02/13 12:08:45 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 12:08:45 INFO DAGScheduler: Got job 93 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 12:08:45 INFO DAGScheduler: Final stage: ResultStage 143 (start at NativeMethodAccessorImpl.java:0)
26/02/13 12:08:45 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 142)
26/02/13 12:08:45 INFO DAGScheduler: Missing parents: List()
26/02/13 12:08:45 INFO DAGScheduler: Submitting ResultStage 143 (MapPartitionsRDD[505] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:08:45 INFO MemoryStore: Block broadcast_139 stored as values in memory (estimated size 99.3 KiB, free 434.0 MiB)
26/02/13 12:08:45 INFO MemoryStore: Block broadcast_139_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 433.9 MiB)
26/02/13 12:08:45 INFO BlockManagerInfo: Added broadcast_139_piece0 in memory on spark-master:42073 (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 12:08:45 INFO SparkContext: Created broadcast 139 from broadcast at DAGScheduler.scala:1585
26/02/13 12:08:45 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 143 (MapPartitionsRDD[505] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 12:08:45 INFO TaskSchedulerImpl: Adding task set 143.0 with 4 tasks resource profile 0
26/02/13 12:08:45 INFO TaskSetManager: Starting task 0.0 in stage 143.0 (TID 301) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:08:45 INFO TaskSetManager: Starting task 1.0 in stage 143.0 (TID 302) (172.18.0.14, executor 1, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:08:45 INFO BlockManagerInfo: Added broadcast_139_piece0 in memory on 172.18.0.14:37571 (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 12:08:45 INFO TaskSetManager: Starting task 2.0 in stage 143.0 (TID 303) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:08:45 INFO TaskSetManager: Starting task 3.0 in stage 143.0 (TID 304) (172.18.0.14, executor 1, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:08:45 INFO TaskSetManager: Finished task 1.0 in stage 143.0 (TID 302) in 26 ms on 172.18.0.14 (executor 1) (1/4)
26/02/13 12:08:45 INFO TaskSetManager: Finished task 0.0 in stage 143.0 (TID 301) in 28 ms on 172.18.0.14 (executor 1) (2/4)
26/02/13 12:08:45 INFO TaskSetManager: Finished task 3.0 in stage 143.0 (TID 304) in 18 ms on 172.18.0.14 (executor 1) (3/4)
26/02/13 12:08:45 INFO TaskSetManager: Finished task 2.0 in stage 143.0 (TID 303) in 21 ms on 172.18.0.14 (executor 1) (4/4)
26/02/13 12:08:45 INFO TaskSchedulerImpl: Removed TaskSet 143.0, whose tasks have all completed, from pool 
26/02/13 12:08:45 INFO DAGScheduler: ResultStage 143 (start at NativeMethodAccessorImpl.java:0) finished in 0.055 s
26/02/13 12:08:45 INFO DAGScheduler: Job 93 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 12:08:45 INFO TaskSchedulerImpl: Killing all running tasks in stage 143: Stage finished
26/02/13 12:08:45 INFO DAGScheduler: Job 93 finished: start at NativeMethodAccessorImpl.java:0, took 0.058150 s
26/02/13 12:08:45 INFO BlockManagerInfo: Removed broadcast_134_piece0 on spark-master:42073 in memory (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 12:08:45 INFO MemoryStore: Block broadcast_140_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.9 MiB)
26/02/13 12:08:45 INFO BlockManagerInfo: Added broadcast_140_piece0 in memory on spark-master:42073 (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 12:08:45 INFO SparkContext: Created broadcast 140 from start at NativeMethodAccessorImpl.java:0
26/02/13 12:08:45 INFO BlockManagerInfo: Removed broadcast_134_piece0 on 172.18.0.15:37717 in memory (size: 107.2 KiB, free: 434.4 MiB)
26/02/13 12:08:45 INFO BlockManagerInfo: Removed broadcast_134_piece0 on 172.18.0.14:37571 in memory (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 12:08:45 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 165, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@5a0df5dd]. The input RDD has 3 partitions.
26/02/13 12:08:45 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 12:08:45 INFO DAGScheduler: Got job 94 (start at NativeMethodAccessorImpl.java:0) with 3 output partitions
26/02/13 12:08:45 INFO DAGScheduler: Final stage: ResultStage 144 (start at NativeMethodAccessorImpl.java:0)
26/02/13 12:08:45 INFO DAGScheduler: Parents of final stage: List()
26/02/13 12:08:45 INFO DAGScheduler: Missing parents: List()
26/02/13 12:08:45 INFO DAGScheduler: Submitting ResultStage 144 (MapPartitionsRDD[511] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:08:45 INFO MemoryStore: Block broadcast_141 stored as values in memory (estimated size 49.9 KiB, free 433.9 MiB)
26/02/13 12:08:45 INFO MemoryStore: Block broadcast_141_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.9 MiB)
26/02/13 12:08:45 INFO BlockManagerInfo: Added broadcast_141_piece0 in memory on spark-master:42073 (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 12:08:45 INFO SparkContext: Created broadcast 141 from broadcast at DAGScheduler.scala:1585
26/02/13 12:08:45 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 144 (MapPartitionsRDD[511] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
26/02/13 12:08:45 INFO TaskSchedulerImpl: Adding task set 144.0 with 3 tasks resource profile 0
26/02/13 12:08:45 INFO TaskSetManager: Starting task 0.0 in stage 144.0 (TID 305) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:08:45 INFO TaskSetManager: Starting task 1.0 in stage 144.0 (TID 306) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:08:45 INFO TaskSetManager: Starting task 2.0 in stage 144.0 (TID 307) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:08:45 INFO BlockManagerInfo: Added broadcast_141_piece0 in memory on 172.18.0.14:37571 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 12:08:45 INFO BlockManagerInfo: Added broadcast_141_piece0 in memory on 172.18.0.15:37717 (size: 19.5 KiB, free: 434.4 MiB)
26/02/13 12:08:45 INFO BlockManagerInfo: Added broadcast_140_piece0 in memory on 172.18.0.14:37571 (size: 107.2 KiB, free: 434.0 MiB)
26/02/13 12:08:45 INFO BlockManagerInfo: Added broadcast_140_piece0 in memory on 172.18.0.15:37717 (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 12:08:46 INFO TaskSetManager: Finished task 2.0 in stage 144.0 (TID 307) in 86 ms on 172.18.0.14 (executor 1) (1/3)
26/02/13 12:08:46 INFO TaskSetManager: Finished task 0.0 in stage 144.0 (TID 305) in 94 ms on 172.18.0.14 (executor 1) (2/3)
26/02/13 12:08:46 INFO TaskSetManager: Finished task 1.0 in stage 144.0 (TID 306) in 109 ms on 172.18.0.15 (executor 0) (3/3)
26/02/13 12:08:46 INFO TaskSchedulerImpl: Removed TaskSet 144.0, whose tasks have all completed, from pool 
26/02/13 12:08:46 INFO DAGScheduler: ResultStage 144 (start at NativeMethodAccessorImpl.java:0) finished in 0.116 s
26/02/13 12:08:46 INFO DAGScheduler: Job 94 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 12:08:46 INFO TaskSchedulerImpl: Killing all running tasks in stage 144: Stage finished
26/02/13 12:08:46 INFO DAGScheduler: Job 94 finished: start at NativeMethodAccessorImpl.java:0, took 0.118884 s
26/02/13 12:08:46 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 165, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@5a0df5dd] is committing.
26/02/13 12:08:46 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 165, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@5a0df5dd] committed.
26/02/13 12:08:46 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/165 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.165.f4dafeaf-5c4e-4746-887e-d88d211541cd.tmp
26/02/13 12:08:46 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.165.f4dafeaf-5c4e-4746-887e-d88d211541cd.tmp to file:/tmp/spark-checkpoint-enrichment/commits/165
26/02/13 12:08:46 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "4a161d66-225b-4b29-812d-ddec3c05364e",
  "name" : null,
  "timestamp" : "2026-02-13T12:08:45.699Z",
  "batchId" : 165,
  "numInputRows" : 166,
  "inputRowsPerSecond" : 182.21734357848518,
  "processedRowsPerSecond" : 387.85046728971963,
  "durationMs" : {
    "addBatch" : 287,
    "commitOffsets" : 64,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 24,
    "triggerExecution" : 428,
    "walCommit" : 52
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 2199,
        "1" : 2566,
        "0" : 1959
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 2254,
        "1" : 2627,
        "0" : 2009
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 2254,
        "1" : 2627,
        "0" : 2009
      }
    },
    "numInputRows" : 166,
    "inputRowsPerSecond" : 182.21734357848518,
    "processedRowsPerSecond" : 387.85046728971963,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@1a6a42c2",
    "numOutputRows" : 30
  }
}
26/02/13 12:08:56 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:08:56 INFO BlockManagerInfo: Removed broadcast_141_piece0 on spark-master:42073 in memory (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 12:08:56 INFO BlockManagerInfo: Removed broadcast_141_piece0 on 172.18.0.14:37571 in memory (size: 19.5 KiB, free: 434.0 MiB)
26/02/13 12:08:56 INFO BlockManagerInfo: Removed broadcast_141_piece0 on 172.18.0.15:37717 in memory (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 12:08:56 INFO BlockManagerInfo: Removed broadcast_139_piece0 on spark-master:42073 in memory (size: 33.3 KiB, free: 434.3 MiB)
26/02/13 12:08:56 INFO BlockManagerInfo: Removed broadcast_139_piece0 on 172.18.0.14:37571 in memory (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 12:09:01 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/166 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.166.694b75f7-35ad-4952-8743-4b8b60ff464c.tmp
26/02/13 12:09:01 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.166.694b75f7-35ad-4952-8743-4b8b60ff464c.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/166
26/02/13 12:09:01 INFO MicroBatchExecution: Committed offsets for batch 166. Metadata OffsetSeqMetadata(0,1770984541196,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 12:09:01 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:09:01 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:09:01 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:09:01 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:09:01 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:09:01 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:09:01 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#38673 - origin_code.nullCount#38672) > 0)
26/02/13 12:09:01 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#38678 - destination_code.nullCount#38677) > 0)
26/02/13 12:09:01 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#38708 - callsign.nullCount#38707) > 0)
26/02/13 12:09:01 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 12:09:01 INFO DAGScheduler: Got job 95 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 12:09:01 INFO DAGScheduler: Final stage: ResultStage 146 (start at NativeMethodAccessorImpl.java:0)
26/02/13 12:09:01 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 145)
26/02/13 12:09:01 INFO DAGScheduler: Missing parents: List()
26/02/13 12:09:01 INFO DAGScheduler: Submitting ResultStage 146 (MapPartitionsRDD[516] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:09:01 INFO MemoryStore: Block broadcast_142 stored as values in memory (estimated size 99.3 KiB, free 434.0 MiB)
26/02/13 12:09:01 INFO MemoryStore: Block broadcast_142_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 433.9 MiB)
26/02/13 12:09:01 INFO BlockManagerInfo: Added broadcast_142_piece0 in memory on spark-master:42073 (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 12:09:01 INFO SparkContext: Created broadcast 142 from broadcast at DAGScheduler.scala:1585
26/02/13 12:09:01 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 146 (MapPartitionsRDD[516] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 12:09:01 INFO TaskSchedulerImpl: Adding task set 146.0 with 4 tasks resource profile 0
26/02/13 12:09:01 INFO TaskSetManager: Starting task 0.0 in stage 146.0 (TID 308) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:09:01 INFO TaskSetManager: Starting task 1.0 in stage 146.0 (TID 309) (172.18.0.14, executor 1, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:09:01 INFO BlockManagerInfo: Added broadcast_142_piece0 in memory on 172.18.0.14:37571 (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 12:09:01 INFO TaskSetManager: Starting task 2.0 in stage 146.0 (TID 310) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:09:01 INFO TaskSetManager: Finished task 1.0 in stage 146.0 (TID 309) in 23 ms on 172.18.0.14 (executor 1) (1/4)
26/02/13 12:09:01 INFO TaskSetManager: Starting task 3.0 in stage 146.0 (TID 311) (172.18.0.14, executor 1, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:09:01 INFO TaskSetManager: Finished task 0.0 in stage 146.0 (TID 308) in 32 ms on 172.18.0.14 (executor 1) (2/4)
26/02/13 12:09:01 INFO TaskSetManager: Finished task 2.0 in stage 146.0 (TID 310) in 21 ms on 172.18.0.14 (executor 1) (3/4)
26/02/13 12:09:01 INFO TaskSetManager: Finished task 3.0 in stage 146.0 (TID 311) in 39 ms on 172.18.0.14 (executor 1) (4/4)
26/02/13 12:09:01 INFO TaskSchedulerImpl: Removed TaskSet 146.0, whose tasks have all completed, from pool 
26/02/13 12:09:01 INFO DAGScheduler: ResultStage 146 (start at NativeMethodAccessorImpl.java:0) finished in 0.079 s
26/02/13 12:09:01 INFO DAGScheduler: Job 95 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 12:09:01 INFO TaskSchedulerImpl: Killing all running tasks in stage 146: Stage finished
26/02/13 12:09:01 INFO DAGScheduler: Job 95 finished: start at NativeMethodAccessorImpl.java:0, took 0.082590 s
26/02/13 12:09:01 INFO MemoryStore: Block broadcast_143_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.8 MiB)
26/02/13 12:09:01 INFO BlockManagerInfo: Added broadcast_143_piece0 in memory on spark-master:42073 (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 12:09:01 INFO SparkContext: Created broadcast 143 from start at NativeMethodAccessorImpl.java:0
26/02/13 12:09:01 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 166, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@267f2840]. The input RDD has 3 partitions.
26/02/13 12:09:01 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 12:09:01 INFO DAGScheduler: Got job 96 (start at NativeMethodAccessorImpl.java:0) with 3 output partitions
26/02/13 12:09:01 INFO DAGScheduler: Final stage: ResultStage 147 (start at NativeMethodAccessorImpl.java:0)
26/02/13 12:09:01 INFO DAGScheduler: Parents of final stage: List()
26/02/13 12:09:01 INFO DAGScheduler: Missing parents: List()
26/02/13 12:09:01 INFO DAGScheduler: Submitting ResultStage 147 (MapPartitionsRDD[522] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:09:01 INFO MemoryStore: Block broadcast_144 stored as values in memory (estimated size 49.9 KiB, free 433.8 MiB)
26/02/13 12:09:01 INFO MemoryStore: Block broadcast_144_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.8 MiB)
26/02/13 12:09:01 INFO BlockManagerInfo: Added broadcast_144_piece0 in memory on spark-master:42073 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 12:09:01 INFO SparkContext: Created broadcast 144 from broadcast at DAGScheduler.scala:1585
26/02/13 12:09:01 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 147 (MapPartitionsRDD[522] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
26/02/13 12:09:01 INFO TaskSchedulerImpl: Adding task set 147.0 with 3 tasks resource profile 0
26/02/13 12:09:01 INFO TaskSetManager: Starting task 0.0 in stage 147.0 (TID 312) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:09:01 INFO TaskSetManager: Starting task 1.0 in stage 147.0 (TID 313) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:09:01 INFO TaskSetManager: Starting task 2.0 in stage 147.0 (TID 314) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:09:01 INFO BlockManagerInfo: Added broadcast_144_piece0 in memory on 172.18.0.15:37717 (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 12:09:01 INFO BlockManagerInfo: Added broadcast_144_piece0 in memory on 172.18.0.14:37571 (size: 19.5 KiB, free: 434.0 MiB)
26/02/13 12:09:01 INFO BlockManagerInfo: Added broadcast_143_piece0 in memory on 172.18.0.15:37717 (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 12:09:01 INFO BlockManagerInfo: Added broadcast_143_piece0 in memory on 172.18.0.14:37571 (size: 107.2 KiB, free: 433.9 MiB)
26/02/13 12:09:02 INFO TaskSetManager: Finished task 0.0 in stage 147.0 (TID 312) in 584 ms on 172.18.0.14 (executor 1) (1/3)
26/02/13 12:09:02 INFO TaskSetManager: Finished task 2.0 in stage 147.0 (TID 314) in 584 ms on 172.18.0.14 (executor 1) (2/3)
26/02/13 12:09:02 INFO TaskSetManager: Finished task 1.0 in stage 147.0 (TID 313) in 604 ms on 172.18.0.15 (executor 0) (3/3)
26/02/13 12:09:02 INFO TaskSchedulerImpl: Removed TaskSet 147.0, whose tasks have all completed, from pool 
26/02/13 12:09:02 INFO DAGScheduler: ResultStage 147 (start at NativeMethodAccessorImpl.java:0) finished in 0.611 s
26/02/13 12:09:02 INFO DAGScheduler: Job 96 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 12:09:02 INFO TaskSchedulerImpl: Killing all running tasks in stage 147: Stage finished
26/02/13 12:09:02 INFO DAGScheduler: Job 96 finished: start at NativeMethodAccessorImpl.java:0, took 0.613094 s
26/02/13 12:09:02 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 166, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@267f2840] is committing.
26/02/13 12:09:02 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 166, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@267f2840] committed.
26/02/13 12:09:02 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/166 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.166.d5dbcd0f-98b1-4360-81b5-757c705828a4.tmp
26/02/13 12:09:02 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.166.d5dbcd0f-98b1-4360-81b5-757c705828a4.tmp to file:/tmp/spark-checkpoint-enrichment/commits/166
26/02/13 12:09:02 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "4a161d66-225b-4b29-812d-ddec3c05364e",
  "name" : null,
  "timestamp" : "2026-02-13T12:09:01.195Z",
  "batchId" : 166,
  "numInputRows" : 111,
  "inputRowsPerSecond" : 8538.461538461539,
  "processedRowsPerSecond" : 121.44420131291028,
  "durationMs" : {
    "addBatch" : 772,
    "commitOffsets" : 60,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 20,
    "triggerExecution" : 914,
    "walCommit" : 59
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 2254,
        "1" : 2627,
        "0" : 2009
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 2291,
        "1" : 2664,
        "0" : 2046
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 2291,
        "1" : 2664,
        "0" : 2046
      }
    },
    "numInputRows" : 111,
    "inputRowsPerSecond" : 8538.461538461539,
    "processedRowsPerSecond" : 121.44420131291028,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@1a6a42c2",
    "numOutputRows" : 29
  }
}
26/02/13 12:09:02 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/167 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.167.483afdcf-c2b6-45a5-abe5-20d2830a7204.tmp
26/02/13 12:09:02 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.167.483afdcf-c2b6-45a5-abe5-20d2830a7204.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/167
26/02/13 12:09:02 INFO MicroBatchExecution: Committed offsets for batch 167. Metadata OffsetSeqMetadata(0,1770984542110,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 12:09:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:09:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:09:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:09:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:09:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:09:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:09:02 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#39527 - origin_code.nullCount#39526) > 0)
26/02/13 12:09:02 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#39532 - destination_code.nullCount#39531) > 0)
26/02/13 12:09:02 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#39562 - callsign.nullCount#39561) > 0)
26/02/13 12:09:02 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 12:09:02 INFO DAGScheduler: Got job 97 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 12:09:02 INFO DAGScheduler: Final stage: ResultStage 149 (start at NativeMethodAccessorImpl.java:0)
26/02/13 12:09:02 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 148)
26/02/13 12:09:02 INFO DAGScheduler: Missing parents: List()
26/02/13 12:09:02 INFO DAGScheduler: Submitting ResultStage 149 (MapPartitionsRDD[527] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:09:02 INFO MemoryStore: Block broadcast_145 stored as values in memory (estimated size 99.3 KiB, free 433.7 MiB)
26/02/13 12:09:02 INFO MemoryStore: Block broadcast_145_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 433.6 MiB)
26/02/13 12:09:02 INFO BlockManagerInfo: Added broadcast_145_piece0 in memory on spark-master:42073 (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 12:09:02 INFO SparkContext: Created broadcast 145 from broadcast at DAGScheduler.scala:1585
26/02/13 12:09:02 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 149 (MapPartitionsRDD[527] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 12:09:02 INFO TaskSchedulerImpl: Adding task set 149.0 with 4 tasks resource profile 0
26/02/13 12:09:02 INFO TaskSetManager: Starting task 0.0 in stage 149.0 (TID 315) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:09:02 INFO TaskSetManager: Starting task 1.0 in stage 149.0 (TID 316) (172.18.0.14, executor 1, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:09:02 INFO BlockManagerInfo: Added broadcast_145_piece0 in memory on 172.18.0.14:37571 (size: 33.3 KiB, free: 433.9 MiB)
26/02/13 12:09:02 INFO TaskSetManager: Starting task 2.0 in stage 149.0 (TID 317) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:09:02 INFO TaskSetManager: Finished task 0.0 in stage 149.0 (TID 315) in 29 ms on 172.18.0.14 (executor 1) (1/4)
26/02/13 12:09:02 INFO TaskSetManager: Starting task 3.0 in stage 149.0 (TID 318) (172.18.0.14, executor 1, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:09:02 INFO TaskSetManager: Finished task 1.0 in stage 149.0 (TID 316) in 31 ms on 172.18.0.14 (executor 1) (2/4)
26/02/13 12:09:02 INFO TaskSetManager: Finished task 2.0 in stage 149.0 (TID 317) in 23 ms on 172.18.0.14 (executor 1) (3/4)
26/02/13 12:09:02 INFO TaskSetManager: Finished task 3.0 in stage 149.0 (TID 318) in 24 ms on 172.18.0.14 (executor 1) (4/4)
26/02/13 12:09:02 INFO TaskSchedulerImpl: Removed TaskSet 149.0, whose tasks have all completed, from pool 
26/02/13 12:09:02 INFO DAGScheduler: ResultStage 149 (start at NativeMethodAccessorImpl.java:0) finished in 0.065 s
26/02/13 12:09:02 INFO DAGScheduler: Job 97 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 12:09:02 INFO TaskSchedulerImpl: Killing all running tasks in stage 149: Stage finished
26/02/13 12:09:02 INFO DAGScheduler: Job 97 finished: start at NativeMethodAccessorImpl.java:0, took 0.067050 s
26/02/13 12:09:02 INFO MemoryStore: Block broadcast_146_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.5 MiB)
26/02/13 12:09:02 INFO BlockManagerInfo: Added broadcast_146_piece0 in memory on spark-master:42073 (size: 107.2 KiB, free: 434.0 MiB)
26/02/13 12:09:02 INFO SparkContext: Created broadcast 146 from start at NativeMethodAccessorImpl.java:0
26/02/13 12:09:02 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 167, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@7338e8c3]. The input RDD has 3 partitions.
26/02/13 12:09:02 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 12:09:02 INFO DAGScheduler: Got job 98 (start at NativeMethodAccessorImpl.java:0) with 3 output partitions
26/02/13 12:09:02 INFO DAGScheduler: Final stage: ResultStage 150 (start at NativeMethodAccessorImpl.java:0)
26/02/13 12:09:02 INFO DAGScheduler: Parents of final stage: List()
26/02/13 12:09:02 INFO DAGScheduler: Missing parents: List()
26/02/13 12:09:02 INFO DAGScheduler: Submitting ResultStage 150 (MapPartitionsRDD[533] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:09:02 INFO MemoryStore: Block broadcast_147 stored as values in memory (estimated size 49.9 KiB, free 433.5 MiB)
26/02/13 12:09:02 INFO MemoryStore: Block broadcast_147_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.5 MiB)
26/02/13 12:09:02 INFO BlockManagerInfo: Added broadcast_147_piece0 in memory on spark-master:42073 (size: 19.5 KiB, free: 433.9 MiB)
26/02/13 12:09:02 INFO SparkContext: Created broadcast 147 from broadcast at DAGScheduler.scala:1585
26/02/13 12:09:02 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 150 (MapPartitionsRDD[533] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
26/02/13 12:09:02 INFO TaskSchedulerImpl: Adding task set 150.0 with 3 tasks resource profile 0
26/02/13 12:09:02 INFO TaskSetManager: Starting task 1.0 in stage 150.0 (TID 319) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:09:02 INFO TaskSetManager: Starting task 0.0 in stage 150.0 (TID 320) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:09:02 INFO TaskSetManager: Starting task 2.0 in stage 150.0 (TID 321) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:09:02 INFO BlockManagerInfo: Added broadcast_147_piece0 in memory on 172.18.0.15:37717 (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 12:09:02 INFO BlockManagerInfo: Added broadcast_147_piece0 in memory on 172.18.0.14:37571 (size: 19.5 KiB, free: 433.9 MiB)
26/02/13 12:09:02 INFO BlockManagerInfo: Added broadcast_146_piece0 in memory on 172.18.0.14:37571 (size: 107.2 KiB, free: 433.7 MiB)
26/02/13 12:09:02 INFO BlockManagerInfo: Added broadcast_146_piece0 in memory on 172.18.0.15:37717 (size: 107.2 KiB, free: 434.0 MiB)
26/02/13 12:09:02 INFO TaskSetManager: Finished task 0.0 in stage 150.0 (TID 320) in 92 ms on 172.18.0.14 (executor 1) (1/3)
26/02/13 12:09:02 INFO TaskSetManager: Finished task 2.0 in stage 150.0 (TID 321) in 106 ms on 172.18.0.14 (executor 1) (2/3)
26/02/13 12:09:02 INFO TaskSetManager: Finished task 1.0 in stage 150.0 (TID 319) in 138 ms on 172.18.0.15 (executor 0) (3/3)
26/02/13 12:09:02 INFO TaskSchedulerImpl: Removed TaskSet 150.0, whose tasks have all completed, from pool 
26/02/13 12:09:02 INFO DAGScheduler: ResultStage 150 (start at NativeMethodAccessorImpl.java:0) finished in 0.145 s
26/02/13 12:09:02 INFO DAGScheduler: Job 98 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 12:09:02 INFO TaskSchedulerImpl: Killing all running tasks in stage 150: Stage finished
26/02/13 12:09:02 INFO DAGScheduler: Job 98 finished: start at NativeMethodAccessorImpl.java:0, took 0.151001 s
26/02/13 12:09:02 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 167, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@7338e8c3] is committing.
26/02/13 12:09:02 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 167, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@7338e8c3] committed.
26/02/13 12:09:02 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/167 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.167.c00ab240-ca8d-4ab7-b763-c1056cfcf506.tmp
26/02/13 12:09:02 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.167.c00ab240-ca8d-4ab7-b763-c1056cfcf506.tmp to file:/tmp/spark-checkpoint-enrichment/commits/167
26/02/13 12:09:02 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "4a161d66-225b-4b29-812d-ddec3c05364e",
  "name" : null,
  "timestamp" : "2026-02-13T12:09:02.110Z",
  "batchId" : 167,
  "numInputRows" : 139,
  "inputRowsPerSecond" : 151.9125683060109,
  "processedRowsPerSecond" : 300.21598272138226,
  "durationMs" : {
    "addBatch" : 308,
    "commitOffsets" : 72,
    "getBatch" : 0,
    "latestOffset" : 0,
    "queryPlanning" : 21,
    "triggerExecution" : 463,
    "walCommit" : 60
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 2291,
        "1" : 2664,
        "0" : 2046
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 2339,
        "1" : 2716,
        "0" : 2085
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 2339,
        "1" : 2716,
        "0" : 2085
      }
    },
    "numInputRows" : 139,
    "inputRowsPerSecond" : 151.9125683060109,
    "processedRowsPerSecond" : 300.21598272138226,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@1a6a42c2",
    "numOutputRows" : 26
  }
}
26/02/13 12:09:02 INFO BlockManagerInfo: Removed broadcast_145_piece0 on spark-master:42073 in memory (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 12:09:02 INFO BlockManagerInfo: Removed broadcast_145_piece0 on 172.18.0.14:37571 in memory (size: 33.3 KiB, free: 433.8 MiB)
26/02/13 12:09:02 INFO BlockManagerInfo: Removed broadcast_143_piece0 on spark-master:42073 in memory (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 12:09:02 INFO BlockManagerInfo: Removed broadcast_143_piece0 on 172.18.0.14:37571 in memory (size: 107.2 KiB, free: 433.9 MiB)
26/02/13 12:09:02 INFO BlockManagerInfo: Removed broadcast_143_piece0 on 172.18.0.15:37717 in memory (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 12:09:02 INFO BlockManagerInfo: Removed broadcast_142_piece0 on spark-master:42073 in memory (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 12:09:02 INFO BlockManagerInfo: Removed broadcast_142_piece0 on 172.18.0.14:37571 in memory (size: 33.3 KiB, free: 433.9 MiB)
26/02/13 12:09:02 INFO BlockManagerInfo: Removed broadcast_147_piece0 on spark-master:42073 in memory (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 12:09:02 INFO BlockManagerInfo: Removed broadcast_147_piece0 on 172.18.0.14:37571 in memory (size: 19.5 KiB, free: 433.9 MiB)
26/02/13 12:09:02 INFO BlockManagerInfo: Removed broadcast_147_piece0 on 172.18.0.15:37717 in memory (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 12:09:02 INFO BlockManagerInfo: Removed broadcast_144_piece0 on spark-master:42073 in memory (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 12:09:02 INFO BlockManagerInfo: Removed broadcast_144_piece0 on 172.18.0.15:37717 in memory (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 12:09:02 INFO BlockManagerInfo: Removed broadcast_144_piece0 on 172.18.0.14:37571 in memory (size: 19.5 KiB, free: 434.0 MiB)
26/02/13 12:09:12 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:09:15 INFO BlockManagerInfo: Removed broadcast_140_piece0 on spark-master:42073 in memory (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 12:09:15 INFO BlockManagerInfo: Removed broadcast_140_piece0 on 172.18.0.14:37571 in memory (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 12:09:15 INFO BlockManagerInfo: Removed broadcast_140_piece0 on 172.18.0.15:37717 in memory (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 12:09:17 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/168 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.168.1a2ea72a-b04e-4985-ae98-e4303e1cb31c.tmp
26/02/13 12:09:17 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.168.1a2ea72a-b04e-4985-ae98-e4303e1cb31c.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/168
26/02/13 12:09:17 INFO MicroBatchExecution: Committed offsets for batch 168. Metadata OffsetSeqMetadata(0,1770984557664,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 12:09:17 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:09:17 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:09:17 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:09:17 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:09:17 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:09:17 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:09:17 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#40381 - origin_code.nullCount#40380) > 0)
26/02/13 12:09:17 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#40386 - destination_code.nullCount#40385) > 0)
26/02/13 12:09:17 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#40416 - callsign.nullCount#40415) > 0)
26/02/13 12:09:17 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 12:09:17 INFO DAGScheduler: Got job 99 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 12:09:17 INFO DAGScheduler: Final stage: ResultStage 152 (start at NativeMethodAccessorImpl.java:0)
26/02/13 12:09:17 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 151)
26/02/13 12:09:17 INFO DAGScheduler: Missing parents: List()
26/02/13 12:09:17 INFO DAGScheduler: Submitting ResultStage 152 (MapPartitionsRDD[538] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:09:17 INFO MemoryStore: Block broadcast_148 stored as values in memory (estimated size 99.3 KiB, free 434.0 MiB)
26/02/13 12:09:17 INFO MemoryStore: Block broadcast_148_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 433.9 MiB)
26/02/13 12:09:17 INFO BlockManagerInfo: Added broadcast_148_piece0 in memory on spark-master:42073 (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 12:09:17 INFO SparkContext: Created broadcast 148 from broadcast at DAGScheduler.scala:1585
26/02/13 12:09:17 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 152 (MapPartitionsRDD[538] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 12:09:17 INFO TaskSchedulerImpl: Adding task set 152.0 with 4 tasks resource profile 0
26/02/13 12:09:17 INFO TaskSetManager: Starting task 0.0 in stage 152.0 (TID 322) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:09:17 INFO TaskSetManager: Starting task 1.0 in stage 152.0 (TID 323) (172.18.0.14, executor 1, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:09:17 INFO BlockManagerInfo: Added broadcast_148_piece0 in memory on 172.18.0.14:37571 (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 12:09:17 INFO TaskSetManager: Starting task 2.0 in stage 152.0 (TID 324) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:09:17 INFO TaskSetManager: Finished task 1.0 in stage 152.0 (TID 323) in 17 ms on 172.18.0.14 (executor 1) (1/4)
26/02/13 12:09:17 INFO TaskSetManager: Starting task 3.0 in stage 152.0 (TID 325) (172.18.0.14, executor 1, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:09:17 INFO TaskSetManager: Finished task 0.0 in stage 152.0 (TID 322) in 18 ms on 172.18.0.14 (executor 1) (2/4)
26/02/13 12:09:17 INFO TaskSetManager: Finished task 2.0 in stage 152.0 (TID 324) in 12 ms on 172.18.0.14 (executor 1) (3/4)
26/02/13 12:09:17 INFO TaskSetManager: Finished task 3.0 in stage 152.0 (TID 325) in 11 ms on 172.18.0.14 (executor 1) (4/4)
26/02/13 12:09:17 INFO TaskSchedulerImpl: Removed TaskSet 152.0, whose tasks have all completed, from pool 
26/02/13 12:09:17 INFO DAGScheduler: ResultStage 152 (start at NativeMethodAccessorImpl.java:0) finished in 0.035 s
26/02/13 12:09:17 INFO DAGScheduler: Job 99 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 12:09:17 INFO TaskSchedulerImpl: Killing all running tasks in stage 152: Stage finished
26/02/13 12:09:17 INFO DAGScheduler: Job 99 finished: start at NativeMethodAccessorImpl.java:0, took 0.036849 s
26/02/13 12:09:17 INFO MemoryStore: Block broadcast_149_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.8 MiB)
26/02/13 12:09:17 INFO BlockManagerInfo: Added broadcast_149_piece0 in memory on spark-master:42073 (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 12:09:17 INFO SparkContext: Created broadcast 149 from start at NativeMethodAccessorImpl.java:0
26/02/13 12:09:17 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 168, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@3a537aa8]. The input RDD has 3 partitions.
26/02/13 12:09:17 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 12:09:17 INFO DAGScheduler: Got job 100 (start at NativeMethodAccessorImpl.java:0) with 3 output partitions
26/02/13 12:09:17 INFO DAGScheduler: Final stage: ResultStage 153 (start at NativeMethodAccessorImpl.java:0)
26/02/13 12:09:17 INFO DAGScheduler: Parents of final stage: List()
26/02/13 12:09:17 INFO DAGScheduler: Missing parents: List()
26/02/13 12:09:17 INFO DAGScheduler: Submitting ResultStage 153 (MapPartitionsRDD[544] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:09:17 INFO MemoryStore: Block broadcast_150 stored as values in memory (estimated size 49.9 KiB, free 433.8 MiB)
26/02/13 12:09:17 INFO MemoryStore: Block broadcast_150_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.8 MiB)
26/02/13 12:09:17 INFO BlockManagerInfo: Added broadcast_150_piece0 in memory on spark-master:42073 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 12:09:17 INFO SparkContext: Created broadcast 150 from broadcast at DAGScheduler.scala:1585
26/02/13 12:09:17 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 153 (MapPartitionsRDD[544] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
26/02/13 12:09:17 INFO TaskSchedulerImpl: Adding task set 153.0 with 3 tasks resource profile 0
26/02/13 12:09:17 INFO TaskSetManager: Starting task 0.0 in stage 153.0 (TID 326) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:09:17 INFO TaskSetManager: Starting task 1.0 in stage 153.0 (TID 327) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:09:17 INFO TaskSetManager: Starting task 2.0 in stage 153.0 (TID 328) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:09:17 INFO BlockManagerInfo: Added broadcast_150_piece0 in memory on 172.18.0.14:37571 (size: 19.5 KiB, free: 434.0 MiB)
26/02/13 12:09:17 INFO BlockManagerInfo: Added broadcast_150_piece0 in memory on 172.18.0.15:37717 (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 12:09:17 INFO BlockManagerInfo: Added broadcast_149_piece0 in memory on 172.18.0.14:37571 (size: 107.2 KiB, free: 433.9 MiB)
26/02/13 12:09:17 INFO BlockManagerInfo: Added broadcast_149_piece0 in memory on 172.18.0.15:37717 (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 12:09:18 INFO TaskSetManager: Finished task 0.0 in stage 153.0 (TID 326) in 560 ms on 172.18.0.14 (executor 1) (1/3)
26/02/13 12:09:18 INFO TaskSetManager: Finished task 2.0 in stage 153.0 (TID 328) in 560 ms on 172.18.0.14 (executor 1) (2/3)
26/02/13 12:09:18 INFO TaskSetManager: Finished task 1.0 in stage 153.0 (TID 327) in 579 ms on 172.18.0.15 (executor 0) (3/3)
26/02/13 12:09:18 INFO TaskSchedulerImpl: Removed TaskSet 153.0, whose tasks have all completed, from pool 
26/02/13 12:09:18 INFO DAGScheduler: ResultStage 153 (start at NativeMethodAccessorImpl.java:0) finished in 0.583 s
26/02/13 12:09:18 INFO DAGScheduler: Job 100 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 12:09:18 INFO TaskSchedulerImpl: Killing all running tasks in stage 153: Stage finished
26/02/13 12:09:18 INFO DAGScheduler: Job 100 finished: start at NativeMethodAccessorImpl.java:0, took 0.584744 s
26/02/13 12:09:18 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 168, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@3a537aa8] is committing.
26/02/13 12:09:18 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 168, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@3a537aa8] committed.
26/02/13 12:09:18 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/168 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.168.f4ab1e81-2163-4176-84b6-0ec6068fa645.tmp
26/02/13 12:09:18 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.168.f4ab1e81-2163-4176-84b6-0ec6068fa645.tmp to file:/tmp/spark-checkpoint-enrichment/commits/168
26/02/13 12:09:18 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "4a161d66-225b-4b29-812d-ddec3c05364e",
  "name" : null,
  "timestamp" : "2026-02-13T12:09:17.663Z",
  "batchId" : 168,
  "numInputRows" : 90,
  "inputRowsPerSecond" : 7500.0,
  "processedRowsPerSecond" : 102.85714285714286,
  "durationMs" : {
    "addBatch" : 701,
    "commitOffsets" : 58,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 23,
    "triggerExecution" : 875,
    "walCommit" : 91
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 2339,
        "1" : 2716,
        "0" : 2085
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 2369,
        "1" : 2747,
        "0" : 2114
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 2369,
        "1" : 2747,
        "0" : 2114
      }
    },
    "numInputRows" : 90,
    "inputRowsPerSecond" : 7500.0,
    "processedRowsPerSecond" : 102.85714285714286,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@1a6a42c2",
    "numOutputRows" : 26
  }
}
26/02/13 12:09:18 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/169 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.169.1f21c2da-f600-4724-81fc-21cebd686a07.tmp
26/02/13 12:09:18 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.169.1f21c2da-f600-4724-81fc-21cebd686a07.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/169
26/02/13 12:09:18 INFO MicroBatchExecution: Committed offsets for batch 169. Metadata OffsetSeqMetadata(0,1770984558540,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 12:09:18 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:09:18 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:09:18 INFO BlockManagerInfo: Removed broadcast_150_piece0 on spark-master:42073 in memory (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 12:09:18 INFO BlockManagerInfo: Removed broadcast_150_piece0 on 172.18.0.14:37571 in memory (size: 19.5 KiB, free: 433.9 MiB)
26/02/13 12:09:18 INFO BlockManagerInfo: Removed broadcast_150_piece0 on 172.18.0.15:37717 in memory (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 12:09:18 INFO BlockManagerInfo: Removed broadcast_149_piece0 on spark-master:42073 in memory (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 12:09:18 INFO BlockManagerInfo: Removed broadcast_149_piece0 on 172.18.0.15:37717 in memory (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 12:09:18 INFO BlockManagerInfo: Removed broadcast_149_piece0 on 172.18.0.14:37571 in memory (size: 107.2 KiB, free: 434.0 MiB)
26/02/13 12:09:18 INFO BlockManagerInfo: Removed broadcast_148_piece0 on spark-master:42073 in memory (size: 33.3 KiB, free: 434.3 MiB)
26/02/13 12:09:18 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:09:18 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:09:18 INFO BlockManagerInfo: Removed broadcast_148_piece0 on 172.18.0.14:37571 in memory (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 12:09:18 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:09:18 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:09:18 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#41235 - origin_code.nullCount#41234) > 0)
26/02/13 12:09:18 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#41240 - destination_code.nullCount#41239) > 0)
26/02/13 12:09:18 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#41270 - callsign.nullCount#41269) > 0)
26/02/13 12:09:18 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 12:09:18 INFO DAGScheduler: Got job 101 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 12:09:18 INFO DAGScheduler: Final stage: ResultStage 155 (start at NativeMethodAccessorImpl.java:0)
26/02/13 12:09:18 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 154)
26/02/13 12:09:18 INFO DAGScheduler: Missing parents: List()
26/02/13 12:09:18 INFO DAGScheduler: Submitting ResultStage 155 (MapPartitionsRDD[549] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:09:18 INFO MemoryStore: Block broadcast_151 stored as values in memory (estimated size 99.3 KiB, free 434.0 MiB)
26/02/13 12:09:18 INFO MemoryStore: Block broadcast_151_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 433.9 MiB)
26/02/13 12:09:18 INFO BlockManagerInfo: Added broadcast_151_piece0 in memory on spark-master:42073 (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 12:09:18 INFO SparkContext: Created broadcast 151 from broadcast at DAGScheduler.scala:1585
26/02/13 12:09:18 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 155 (MapPartitionsRDD[549] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 12:09:18 INFO TaskSchedulerImpl: Adding task set 155.0 with 4 tasks resource profile 0
26/02/13 12:09:18 INFO TaskSetManager: Starting task 0.0 in stage 155.0 (TID 329) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:09:18 INFO TaskSetManager: Starting task 1.0 in stage 155.0 (TID 330) (172.18.0.14, executor 1, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:09:18 INFO BlockManagerInfo: Added broadcast_151_piece0 in memory on 172.18.0.14:37571 (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 12:09:18 INFO TaskSetManager: Starting task 2.0 in stage 155.0 (TID 331) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:09:18 INFO TaskSetManager: Starting task 3.0 in stage 155.0 (TID 332) (172.18.0.14, executor 1, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:09:18 INFO TaskSetManager: Finished task 1.0 in stage 155.0 (TID 330) in 18 ms on 172.18.0.14 (executor 1) (1/4)
26/02/13 12:09:18 INFO TaskSetManager: Finished task 0.0 in stage 155.0 (TID 329) in 18 ms on 172.18.0.14 (executor 1) (2/4)
26/02/13 12:09:18 INFO TaskSetManager: Finished task 3.0 in stage 155.0 (TID 332) in 13 ms on 172.18.0.14 (executor 1) (3/4)
26/02/13 12:09:18 INFO TaskSetManager: Finished task 2.0 in stage 155.0 (TID 331) in 14 ms on 172.18.0.14 (executor 1) (4/4)
26/02/13 12:09:18 INFO TaskSchedulerImpl: Removed TaskSet 155.0, whose tasks have all completed, from pool 
26/02/13 12:09:18 INFO DAGScheduler: ResultStage 155 (start at NativeMethodAccessorImpl.java:0) finished in 0.041 s
26/02/13 12:09:18 INFO DAGScheduler: Job 101 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 12:09:18 INFO TaskSchedulerImpl: Killing all running tasks in stage 155: Stage finished
26/02/13 12:09:18 INFO DAGScheduler: Job 101 finished: start at NativeMethodAccessorImpl.java:0, took 0.042764 s
26/02/13 12:09:18 INFO MemoryStore: Block broadcast_152_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.8 MiB)
26/02/13 12:09:18 INFO BlockManagerInfo: Added broadcast_152_piece0 in memory on spark-master:42073 (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 12:09:18 INFO SparkContext: Created broadcast 152 from start at NativeMethodAccessorImpl.java:0
26/02/13 12:09:18 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 169, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@8c2ce73]. The input RDD has 3 partitions.
26/02/13 12:09:18 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 12:09:18 INFO DAGScheduler: Got job 102 (start at NativeMethodAccessorImpl.java:0) with 3 output partitions
26/02/13 12:09:18 INFO DAGScheduler: Final stage: ResultStage 156 (start at NativeMethodAccessorImpl.java:0)
26/02/13 12:09:18 INFO DAGScheduler: Parents of final stage: List()
26/02/13 12:09:18 INFO DAGScheduler: Missing parents: List()
26/02/13 12:09:18 INFO DAGScheduler: Submitting ResultStage 156 (MapPartitionsRDD[555] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:09:18 INFO MemoryStore: Block broadcast_153 stored as values in memory (estimated size 49.9 KiB, free 433.8 MiB)
26/02/13 12:09:18 INFO MemoryStore: Block broadcast_153_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.8 MiB)
26/02/13 12:09:18 INFO BlockManagerInfo: Added broadcast_153_piece0 in memory on spark-master:42073 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 12:09:18 INFO SparkContext: Created broadcast 153 from broadcast at DAGScheduler.scala:1585
26/02/13 12:09:18 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 156 (MapPartitionsRDD[555] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
26/02/13 12:09:18 INFO TaskSchedulerImpl: Adding task set 156.0 with 3 tasks resource profile 0
26/02/13 12:09:18 INFO TaskSetManager: Starting task 0.0 in stage 156.0 (TID 333) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:09:18 INFO TaskSetManager: Starting task 1.0 in stage 156.0 (TID 334) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:09:18 INFO TaskSetManager: Starting task 2.0 in stage 156.0 (TID 335) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:09:18 INFO BlockManagerInfo: Removed broadcast_146_piece0 on spark-master:42073 in memory (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 12:09:18 INFO BlockManagerInfo: Removed broadcast_146_piece0 on 172.18.0.15:37717 in memory (size: 107.2 KiB, free: 434.4 MiB)
26/02/13 12:09:18 INFO BlockManagerInfo: Removed broadcast_146_piece0 on 172.18.0.14:37571 in memory (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 12:09:18 INFO BlockManagerInfo: Added broadcast_153_piece0 in memory on 172.18.0.14:37571 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 12:09:18 INFO BlockManagerInfo: Added broadcast_153_piece0 in memory on 172.18.0.15:37717 (size: 19.5 KiB, free: 434.4 MiB)
26/02/13 12:09:18 INFO BlockManagerInfo: Added broadcast_152_piece0 in memory on 172.18.0.14:37571 (size: 107.2 KiB, free: 434.0 MiB)
26/02/13 12:09:18 INFO BlockManagerInfo: Added broadcast_152_piece0 in memory on 172.18.0.15:37717 (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 12:09:18 INFO TaskSetManager: Finished task 2.0 in stage 156.0 (TID 335) in 64 ms on 172.18.0.14 (executor 1) (1/3)
26/02/13 12:09:18 INFO TaskSetManager: Finished task 0.0 in stage 156.0 (TID 333) in 74 ms on 172.18.0.14 (executor 1) (2/3)
26/02/13 12:09:18 INFO TaskSetManager: Finished task 1.0 in stage 156.0 (TID 334) in 91 ms on 172.18.0.15 (executor 0) (3/3)
26/02/13 12:09:18 INFO TaskSchedulerImpl: Removed TaskSet 156.0, whose tasks have all completed, from pool 
26/02/13 12:09:18 INFO DAGScheduler: ResultStage 156 (start at NativeMethodAccessorImpl.java:0) finished in 0.104 s
26/02/13 12:09:18 INFO DAGScheduler: Job 102 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 12:09:18 INFO TaskSchedulerImpl: Killing all running tasks in stage 156: Stage finished
26/02/13 12:09:18 INFO DAGScheduler: Job 102 finished: start at NativeMethodAccessorImpl.java:0, took 0.105604 s
26/02/13 12:09:18 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 169, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@8c2ce73] is committing.
26/02/13 12:09:18 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 169, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@8c2ce73] committed.
26/02/13 12:09:18 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/169 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.169.68bb9884-7900-42fe-9fef-7cdfab70a731.tmp
26/02/13 12:09:18 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.169.68bb9884-7900-42fe-9fef-7cdfab70a731.tmp to file:/tmp/spark-checkpoint-enrichment/commits/169
26/02/13 12:09:18 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "4a161d66-225b-4b29-812d-ddec3c05364e",
  "name" : null,
  "timestamp" : "2026-02-13T12:09:18.539Z",
  "batchId" : 169,
  "numInputRows" : 160,
  "inputRowsPerSecond" : 182.64840182648402,
  "processedRowsPerSecond" : 404.04040404040404,
  "durationMs" : {
    "addBatch" : 248,
    "commitOffsets" : 67,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 22,
    "triggerExecution" : 396,
    "walCommit" : 57
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 2369,
        "1" : 2747,
        "0" : 2114
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 2424,
        "1" : 2805,
        "0" : 2161
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 2424,
        "1" : 2805,
        "0" : 2161
      }
    },
    "numInputRows" : 160,
    "inputRowsPerSecond" : 182.64840182648402,
    "processedRowsPerSecond" : 404.04040404040404,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@1a6a42c2",
    "numOutputRows" : 29
  }
}
26/02/13 12:09:28 INFO BlockManagerInfo: Removed broadcast_153_piece0 on spark-master:42073 in memory (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 12:09:28 INFO BlockManagerInfo: Removed broadcast_153_piece0 on 172.18.0.14:37571 in memory (size: 19.5 KiB, free: 434.0 MiB)
26/02/13 12:09:28 INFO BlockManagerInfo: Removed broadcast_153_piece0 on 172.18.0.15:37717 in memory (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 12:09:28 INFO BlockManagerInfo: Removed broadcast_151_piece0 on spark-master:42073 in memory (size: 33.3 KiB, free: 434.3 MiB)
26/02/13 12:09:28 INFO BlockManagerInfo: Removed broadcast_151_piece0 on 172.18.0.14:37571 in memory (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 12:09:28 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:09:34 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/170 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.170.1f4669ba-fa05-4cb6-96fa-aa2d0d69626a.tmp
26/02/13 12:09:34 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.170.1f4669ba-fa05-4cb6-96fa-aa2d0d69626a.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/170
26/02/13 12:09:34 INFO MicroBatchExecution: Committed offsets for batch 170. Metadata OffsetSeqMetadata(0,1770984573997,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 12:09:34 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:09:34 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:09:34 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:09:34 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:09:34 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:09:34 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:09:34 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#42089 - origin_code.nullCount#42088) > 0)
26/02/13 12:09:34 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#42094 - destination_code.nullCount#42093) > 0)
26/02/13 12:09:34 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#42124 - callsign.nullCount#42123) > 0)
26/02/13 12:09:34 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 12:09:34 INFO DAGScheduler: Got job 103 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 12:09:34 INFO DAGScheduler: Final stage: ResultStage 158 (start at NativeMethodAccessorImpl.java:0)
26/02/13 12:09:34 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 157)
26/02/13 12:09:34 INFO DAGScheduler: Missing parents: List()
26/02/13 12:09:34 INFO DAGScheduler: Submitting ResultStage 158 (MapPartitionsRDD[560] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:09:34 INFO MemoryStore: Block broadcast_154 stored as values in memory (estimated size 99.3 KiB, free 434.0 MiB)
26/02/13 12:09:34 INFO MemoryStore: Block broadcast_154_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 433.9 MiB)
26/02/13 12:09:34 INFO BlockManagerInfo: Added broadcast_154_piece0 in memory on spark-master:42073 (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 12:09:34 INFO SparkContext: Created broadcast 154 from broadcast at DAGScheduler.scala:1585
26/02/13 12:09:34 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 158 (MapPartitionsRDD[560] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 12:09:34 INFO TaskSchedulerImpl: Adding task set 158.0 with 4 tasks resource profile 0
26/02/13 12:09:34 INFO TaskSetManager: Starting task 0.0 in stage 158.0 (TID 336) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:09:34 INFO TaskSetManager: Starting task 1.0 in stage 158.0 (TID 337) (172.18.0.14, executor 1, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:09:34 INFO BlockManagerInfo: Added broadcast_154_piece0 in memory on 172.18.0.14:37571 (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 12:09:34 INFO TaskSetManager: Starting task 2.0 in stage 158.0 (TID 338) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:09:34 INFO TaskSetManager: Finished task 0.0 in stage 158.0 (TID 336) in 25 ms on 172.18.0.14 (executor 1) (1/4)
26/02/13 12:09:34 INFO TaskSetManager: Starting task 3.0 in stage 158.0 (TID 339) (172.18.0.14, executor 1, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:09:34 INFO TaskSetManager: Finished task 1.0 in stage 158.0 (TID 337) in 26 ms on 172.18.0.14 (executor 1) (2/4)
26/02/13 12:09:34 INFO TaskSetManager: Finished task 2.0 in stage 158.0 (TID 338) in 19 ms on 172.18.0.14 (executor 1) (3/4)
26/02/13 12:09:34 INFO TaskSetManager: Finished task 3.0 in stage 158.0 (TID 339) in 18 ms on 172.18.0.14 (executor 1) (4/4)
26/02/13 12:09:34 INFO TaskSchedulerImpl: Removed TaskSet 158.0, whose tasks have all completed, from pool 
26/02/13 12:09:34 INFO DAGScheduler: ResultStage 158 (start at NativeMethodAccessorImpl.java:0) finished in 0.057 s
26/02/13 12:09:34 INFO DAGScheduler: Job 103 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 12:09:34 INFO TaskSchedulerImpl: Killing all running tasks in stage 158: Stage finished
26/02/13 12:09:34 INFO DAGScheduler: Job 103 finished: start at NativeMethodAccessorImpl.java:0, took 0.059193 s
26/02/13 12:09:34 INFO MemoryStore: Block broadcast_155_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.8 MiB)
26/02/13 12:09:34 INFO BlockManagerInfo: Added broadcast_155_piece0 in memory on spark-master:42073 (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 12:09:34 INFO SparkContext: Created broadcast 155 from start at NativeMethodAccessorImpl.java:0
26/02/13 12:09:34 INFO BlockManagerInfo: Removed broadcast_154_piece0 on spark-master:42073 in memory (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 12:09:34 INFO BlockManagerInfo: Removed broadcast_154_piece0 on 172.18.0.14:37571 in memory (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 12:09:34 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 170, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@114bb4df]. The input RDD has 3 partitions.
26/02/13 12:09:34 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 12:09:34 INFO DAGScheduler: Got job 104 (start at NativeMethodAccessorImpl.java:0) with 3 output partitions
26/02/13 12:09:34 INFO DAGScheduler: Final stage: ResultStage 159 (start at NativeMethodAccessorImpl.java:0)
26/02/13 12:09:34 INFO DAGScheduler: Parents of final stage: List()
26/02/13 12:09:34 INFO DAGScheduler: Missing parents: List()
26/02/13 12:09:34 INFO DAGScheduler: Submitting ResultStage 159 (MapPartitionsRDD[566] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:09:34 INFO MemoryStore: Block broadcast_156 stored as values in memory (estimated size 49.9 KiB, free 433.9 MiB)
26/02/13 12:09:34 INFO MemoryStore: Block broadcast_156_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.9 MiB)
26/02/13 12:09:34 INFO BlockManagerInfo: Added broadcast_156_piece0 in memory on spark-master:42073 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 12:09:34 INFO SparkContext: Created broadcast 156 from broadcast at DAGScheduler.scala:1585
26/02/13 12:09:34 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 159 (MapPartitionsRDD[566] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
26/02/13 12:09:34 INFO TaskSchedulerImpl: Adding task set 159.0 with 3 tasks resource profile 0
26/02/13 12:09:34 INFO TaskSetManager: Starting task 0.0 in stage 159.0 (TID 340) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:09:34 INFO TaskSetManager: Starting task 1.0 in stage 159.0 (TID 341) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:09:34 INFO TaskSetManager: Starting task 2.0 in stage 159.0 (TID 342) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:09:34 INFO BlockManagerInfo: Added broadcast_156_piece0 in memory on 172.18.0.14:37571 (size: 19.5 KiB, free: 434.0 MiB)
26/02/13 12:09:34 INFO BlockManagerInfo: Added broadcast_156_piece0 in memory on 172.18.0.15:37717 (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 12:09:34 INFO BlockManagerInfo: Added broadcast_155_piece0 in memory on 172.18.0.14:37571 (size: 107.2 KiB, free: 433.9 MiB)
26/02/13 12:09:34 INFO BlockManagerInfo: Added broadcast_155_piece0 in memory on 172.18.0.15:37717 (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 12:09:34 INFO TaskSetManager: Finished task 1.0 in stage 159.0 (TID 341) in 597 ms on 172.18.0.15 (executor 0) (1/3)
26/02/13 12:09:34 INFO TaskSetManager: Finished task 0.0 in stage 159.0 (TID 340) in 605 ms on 172.18.0.14 (executor 1) (2/3)
26/02/13 12:09:34 INFO TaskSetManager: Finished task 2.0 in stage 159.0 (TID 342) in 606 ms on 172.18.0.14 (executor 1) (3/3)
26/02/13 12:09:34 INFO TaskSchedulerImpl: Removed TaskSet 159.0, whose tasks have all completed, from pool 
26/02/13 12:09:34 INFO DAGScheduler: ResultStage 159 (start at NativeMethodAccessorImpl.java:0) finished in 0.614 s
26/02/13 12:09:34 INFO DAGScheduler: Job 104 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 12:09:34 INFO TaskSchedulerImpl: Killing all running tasks in stage 159: Stage finished
26/02/13 12:09:34 INFO DAGScheduler: Job 104 finished: start at NativeMethodAccessorImpl.java:0, took 0.616824 s
26/02/13 12:09:34 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 170, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@114bb4df] is committing.
26/02/13 12:09:34 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 170, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@114bb4df] committed.
26/02/13 12:09:34 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/170 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.170.5c3c939e-ca8d-4660-b786-1acb3abe77d2.tmp
26/02/13 12:09:34 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.170.5c3c939e-ca8d-4660-b786-1acb3abe77d2.tmp to file:/tmp/spark-checkpoint-enrichment/commits/170
26/02/13 12:09:34 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "4a161d66-225b-4b29-812d-ddec3c05364e",
  "name" : null,
  "timestamp" : "2026-02-13T12:09:33.996Z",
  "batchId" : 170,
  "numInputRows" : 8,
  "inputRowsPerSecond" : 615.3846153846154,
  "processedRowsPerSecond" : 8.333333333333334,
  "durationMs" : {
    "addBatch" : 791,
    "commitOffsets" : 75,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 25,
    "triggerExecution" : 960,
    "walCommit" : 66
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 2424,
        "1" : 2805,
        "0" : 2161
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 2425,
        "1" : 2808,
        "0" : 2165
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 2425,
        "1" : 2808,
        "0" : 2165
      }
    },
    "numInputRows" : 8,
    "inputRowsPerSecond" : 615.3846153846154,
    "processedRowsPerSecond" : 8.333333333333334,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@1a6a42c2",
    "numOutputRows" : 3
  }
}
26/02/13 12:09:34 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/171 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.171.32f92010-0b9a-472c-bbf7-d1ec08410038.tmp
26/02/13 12:09:35 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.171.32f92010-0b9a-472c-bbf7-d1ec08410038.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/171
26/02/13 12:09:35 INFO MicroBatchExecution: Committed offsets for batch 171. Metadata OffsetSeqMetadata(0,1770984574958,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 12:09:35 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:09:35 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:09:35 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:09:35 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:09:35 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:09:35 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:09:35 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#42943 - origin_code.nullCount#42942) > 0)
26/02/13 12:09:35 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#42948 - destination_code.nullCount#42947) > 0)
26/02/13 12:09:35 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#42978 - callsign.nullCount#42977) > 0)
26/02/13 12:09:35 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 12:09:35 INFO DAGScheduler: Got job 105 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 12:09:35 INFO DAGScheduler: Final stage: ResultStage 161 (start at NativeMethodAccessorImpl.java:0)
26/02/13 12:09:35 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 160)
26/02/13 12:09:35 INFO DAGScheduler: Missing parents: List()
26/02/13 12:09:35 INFO DAGScheduler: Submitting ResultStage 161 (MapPartitionsRDD[571] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:09:35 INFO MemoryStore: Block broadcast_157 stored as values in memory (estimated size 99.3 KiB, free 433.8 MiB)
26/02/13 12:09:35 INFO MemoryStore: Block broadcast_157_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 433.8 MiB)
26/02/13 12:09:35 INFO BlockManagerInfo: Added broadcast_157_piece0 in memory on spark-master:42073 (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 12:09:35 INFO SparkContext: Created broadcast 157 from broadcast at DAGScheduler.scala:1585
26/02/13 12:09:35 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 161 (MapPartitionsRDD[571] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 12:09:35 INFO TaskSchedulerImpl: Adding task set 161.0 with 4 tasks resource profile 0
26/02/13 12:09:35 INFO TaskSetManager: Starting task 0.0 in stage 161.0 (TID 343) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:09:35 INFO TaskSetManager: Starting task 1.0 in stage 161.0 (TID 344) (172.18.0.14, executor 1, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:09:35 INFO BlockManagerInfo: Added broadcast_157_piece0 in memory on 172.18.0.14:37571 (size: 33.3 KiB, free: 433.9 MiB)
26/02/13 12:09:35 INFO TaskSetManager: Starting task 2.0 in stage 161.0 (TID 345) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:09:35 INFO TaskSetManager: Finished task 0.0 in stage 161.0 (TID 343) in 15 ms on 172.18.0.14 (executor 1) (1/4)
26/02/13 12:09:35 INFO TaskSetManager: Starting task 3.0 in stage 161.0 (TID 346) (172.18.0.14, executor 1, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:09:35 INFO TaskSetManager: Finished task 1.0 in stage 161.0 (TID 344) in 15 ms on 172.18.0.14 (executor 1) (2/4)
26/02/13 12:09:35 INFO TaskSetManager: Finished task 2.0 in stage 161.0 (TID 345) in 16 ms on 172.18.0.14 (executor 1) (3/4)
26/02/13 12:09:35 INFO TaskSetManager: Finished task 3.0 in stage 161.0 (TID 346) in 17 ms on 172.18.0.14 (executor 1) (4/4)
26/02/13 12:09:35 INFO TaskSchedulerImpl: Removed TaskSet 161.0, whose tasks have all completed, from pool 
26/02/13 12:09:35 INFO DAGScheduler: ResultStage 161 (start at NativeMethodAccessorImpl.java:0) finished in 0.038 s
26/02/13 12:09:35 INFO DAGScheduler: Job 105 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 12:09:35 INFO TaskSchedulerImpl: Killing all running tasks in stage 161: Stage finished
26/02/13 12:09:35 INFO DAGScheduler: Job 105 finished: start at NativeMethodAccessorImpl.java:0, took 0.040848 s
26/02/13 12:09:35 INFO MemoryStore: Block broadcast_158_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.7 MiB)
26/02/13 12:09:35 INFO BlockManagerInfo: Added broadcast_158_piece0 in memory on spark-master:42073 (size: 107.2 KiB, free: 434.0 MiB)
26/02/13 12:09:35 INFO SparkContext: Created broadcast 158 from start at NativeMethodAccessorImpl.java:0
26/02/13 12:09:35 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 171, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@5445c209]. The input RDD has 3 partitions.
26/02/13 12:09:35 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 12:09:35 INFO DAGScheduler: Got job 106 (start at NativeMethodAccessorImpl.java:0) with 3 output partitions
26/02/13 12:09:35 INFO DAGScheduler: Final stage: ResultStage 162 (start at NativeMethodAccessorImpl.java:0)
26/02/13 12:09:35 INFO DAGScheduler: Parents of final stage: List()
26/02/13 12:09:35 INFO DAGScheduler: Missing parents: List()
26/02/13 12:09:35 INFO DAGScheduler: Submitting ResultStage 162 (MapPartitionsRDD[577] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:09:35 INFO MemoryStore: Block broadcast_159 stored as values in memory (estimated size 49.9 KiB, free 433.6 MiB)
26/02/13 12:09:35 INFO MemoryStore: Block broadcast_159_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.6 MiB)
26/02/13 12:09:35 INFO BlockManagerInfo: Added broadcast_159_piece0 in memory on spark-master:42073 (size: 19.5 KiB, free: 434.0 MiB)
26/02/13 12:09:35 INFO SparkContext: Created broadcast 159 from broadcast at DAGScheduler.scala:1585
26/02/13 12:09:35 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 162 (MapPartitionsRDD[577] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
26/02/13 12:09:35 INFO TaskSchedulerImpl: Adding task set 162.0 with 3 tasks resource profile 0
26/02/13 12:09:35 INFO TaskSetManager: Starting task 0.0 in stage 162.0 (TID 347) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:09:35 INFO TaskSetManager: Starting task 1.0 in stage 162.0 (TID 348) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:09:35 INFO TaskSetManager: Starting task 2.0 in stage 162.0 (TID 349) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:09:35 INFO BlockManagerInfo: Added broadcast_159_piece0 in memory on 172.18.0.15:37717 (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 12:09:35 INFO BlockManagerInfo: Added broadcast_159_piece0 in memory on 172.18.0.14:37571 (size: 19.5 KiB, free: 433.9 MiB)
26/02/13 12:09:35 INFO BlockManagerInfo: Added broadcast_158_piece0 in memory on 172.18.0.14:37571 (size: 107.2 KiB, free: 433.8 MiB)
26/02/13 12:09:35 INFO BlockManagerInfo: Added broadcast_158_piece0 in memory on 172.18.0.15:37717 (size: 107.2 KiB, free: 434.0 MiB)
26/02/13 12:09:35 INFO TaskSetManager: Finished task 0.0 in stage 162.0 (TID 347) in 84 ms on 172.18.0.14 (executor 1) (1/3)
26/02/13 12:09:35 INFO TaskSetManager: Finished task 2.0 in stage 162.0 (TID 349) in 83 ms on 172.18.0.14 (executor 1) (2/3)
26/02/13 12:09:35 INFO TaskSetManager: Finished task 1.0 in stage 162.0 (TID 348) in 102 ms on 172.18.0.15 (executor 0) (3/3)
26/02/13 12:09:35 INFO TaskSchedulerImpl: Removed TaskSet 162.0, whose tasks have all completed, from pool 
26/02/13 12:09:35 INFO DAGScheduler: ResultStage 162 (start at NativeMethodAccessorImpl.java:0) finished in 0.109 s
26/02/13 12:09:35 INFO DAGScheduler: Job 106 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 12:09:35 INFO TaskSchedulerImpl: Killing all running tasks in stage 162: Stage finished
26/02/13 12:09:35 INFO DAGScheduler: Job 106 finished: start at NativeMethodAccessorImpl.java:0, took 0.111959 s
26/02/13 12:09:35 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 171, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@5445c209] is committing.
26/02/13 12:09:35 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 171, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@5445c209] committed.
26/02/13 12:09:35 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/171 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.171.553a1444-5819-4e44-afcb-337f4e9b4071.tmp
26/02/13 12:09:35 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.171.553a1444-5819-4e44-afcb-337f4e9b4071.tmp to file:/tmp/spark-checkpoint-enrichment/commits/171
26/02/13 12:09:35 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "4a161d66-225b-4b29-812d-ddec3c05364e",
  "name" : null,
  "timestamp" : "2026-02-13T12:09:34.957Z",
  "batchId" : 171,
  "numInputRows" : 243,
  "inputRowsPerSecond" : 252.86160249739856,
  "processedRowsPerSecond" : 636.1256544502618,
  "durationMs" : {
    "addBatch" : 234,
    "commitOffsets" : 69,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 22,
    "triggerExecution" : 382,
    "walCommit" : 55
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 2425,
        "1" : 2808,
        "0" : 2165
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 2510,
        "1" : 2894,
        "0" : 2237
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 2510,
        "1" : 2894,
        "0" : 2237
      }
    },
    "numInputRows" : 243,
    "inputRowsPerSecond" : 252.86160249739856,
    "processedRowsPerSecond" : 636.1256544502618,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@1a6a42c2",
    "numOutputRows" : 50
  }
}
26/02/13 12:09:38 INFO BlockManagerInfo: Removed broadcast_157_piece0 on spark-master:42073 in memory (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 12:09:38 INFO BlockManagerInfo: Removed broadcast_157_piece0 on 172.18.0.14:37571 in memory (size: 33.3 KiB, free: 433.8 MiB)
26/02/13 12:09:38 INFO BlockManagerInfo: Removed broadcast_159_piece0 on spark-master:42073 in memory (size: 19.5 KiB, free: 434.0 MiB)
26/02/13 12:09:38 INFO BlockManagerInfo: Removed broadcast_159_piece0 on 172.18.0.14:37571 in memory (size: 19.5 KiB, free: 433.8 MiB)
26/02/13 12:09:38 INFO BlockManagerInfo: Removed broadcast_159_piece0 on 172.18.0.15:37717 in memory (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 12:09:38 INFO BlockManagerInfo: Removed broadcast_155_piece0 on spark-master:42073 in memory (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 12:09:38 INFO BlockManagerInfo: Removed broadcast_155_piece0 on 172.18.0.15:37717 in memory (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 12:09:38 INFO BlockManagerInfo: Removed broadcast_155_piece0 on 172.18.0.14:37571 in memory (size: 107.2 KiB, free: 433.9 MiB)
26/02/13 12:09:38 INFO BlockManagerInfo: Removed broadcast_156_piece0 on spark-master:42073 in memory (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 12:09:38 INFO BlockManagerInfo: Removed broadcast_156_piece0 on 172.18.0.15:37717 in memory (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 12:09:38 INFO BlockManagerInfo: Removed broadcast_156_piece0 on 172.18.0.14:37571 in memory (size: 19.5 KiB, free: 434.0 MiB)
26/02/13 12:09:45 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:09:55 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:10:05 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:10:15 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:10:18 INFO BlockManagerInfo: Removed broadcast_152_piece0 on spark-master:42073 in memory (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 12:10:18 INFO BlockManagerInfo: Removed broadcast_152_piece0 on 172.18.0.15:37717 in memory (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 12:10:18 INFO BlockManagerInfo: Removed broadcast_152_piece0 on 172.18.0.14:37571 in memory (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 12:10:25 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:10:35 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:10:45 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:10:55 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:11:05 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:11:15 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:11:25 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:11:35 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:11:45 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:11:55 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:12:05 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:12:15 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:12:25 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:12:35 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:12:45 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:12:55 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:13:05 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:13:15 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:13:25 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:13:35 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:13:45 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:13:55 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:14:05 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:14:15 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:14:25 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:14:35 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:14:45 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:14:55 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:15:05 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:15:15 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:15:25 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:15:35 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:15:45 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:15:55 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:16:05 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:16:15 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:16:25 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:16:35 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:16:45 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:16:55 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:17:05 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:17:15 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:17:25 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:17:35 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:17:45 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:17:55 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:18:05 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:18:15 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:18:25 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:18:35 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:18:45 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:18:55 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:19:05 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:19:15 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:19:25 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:19:35 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:19:45 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:19:55 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:20:05 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:20:15 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:20:25 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:20:35 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:20:45 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:20:55 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:21:05 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:21:15 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:21:25 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:21:35 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:21:45 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:21:55 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:22:05 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:22:15 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:22:25 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:22:35 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:22:45 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:22:55 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:23:05 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:23:15 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:23:25 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:23:35 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:23:45 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:23:55 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:24:05 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:24:15 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:24:25 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:24:35 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:24:45 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:24:55 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:25:05 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:25:15 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:25:25 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:25:35 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:25:45 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:25:55 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:26:05 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:26:15 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:26:25 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:26:35 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:26:45 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:26:55 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:27:05 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:27:15 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:27:25 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:27:35 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:27:45 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:27:55 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:28:05 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:28:15 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:28:25 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:28:35 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:28:45 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:28:55 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:29:05 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:29:15 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:29:26 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:29:36 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:29:46 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:29:56 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:30:06 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:30:16 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:30:26 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:30:36 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:30:46 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:30:56 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:31:06 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:31:16 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:31:26 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:31:36 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:31:46 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:31:56 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:32:06 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:32:16 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:32:26 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:32:36 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:32:46 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:32:56 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:33:06 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:33:16 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:33:26 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:33:36 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:33:46 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:33:56 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:34:06 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:34:16 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:34:26 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:34:36 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:34:46 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:34:56 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:35:06 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:35:16 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:35:26 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:35:36 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:35:46 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:35:56 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:36:06 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:36:16 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:36:26 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:36:36 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:36:46 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:36:56 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:37:06 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:37:16 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:37:26 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:37:36 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:37:46 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:37:56 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:38:06 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:38:16 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:38:26 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:38:36 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:38:46 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:38:56 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:39:06 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:39:16 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:39:26 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:39:36 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:39:46 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:39:56 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:40:06 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:40:16 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:40:26 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:40:36 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:40:46 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:40:56 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:41:06 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:41:16 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:41:26 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:41:36 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:41:46 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:41:56 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:42:06 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:42:16 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:42:26 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:42:36 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:42:46 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:42:56 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:43:06 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:43:16 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:43:26 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20260213120207-0001/0 is now LOST (worker lost: 172.18.0.15:35515 got disassociated)
26/02/13 12:43:26 INFO StandaloneSchedulerBackend: Executor app-20260213120207-0001/0 removed: worker lost: 172.18.0.15:35515 got disassociated
26/02/13 12:43:26 ERROR TaskSchedulerImpl: Lost executor 0 on 172.18.0.15: worker lost: 172.18.0.15:35515 got disassociated
26/02/13 12:43:26 INFO DAGScheduler: Executor lost: 0 (epoch 4)
26/02/13 12:43:26 INFO BlockManagerMasterEndpoint: Trying to remove executor 0 from BlockManagerMaster.
26/02/13 12:43:26 INFO BlockManagerMasterEndpoint: Removing block manager BlockManagerId(0, 172.18.0.15, 37717, None)
26/02/13 12:43:26 INFO BlockManagerMaster: Removed 0 successfully in removeExecutor
26/02/13 12:43:26 INFO DAGScheduler: Shuffle files lost for host: 172.18.0.15 (epoch 4)
26/02/13 12:43:26 INFO StandaloneAppClient$ClientEndpoint: Master removed worker worker-20260213114143-172.18.0.15-35515: 172.18.0.15:35515 got disassociated
26/02/13 12:43:26 INFO StandaloneSchedulerBackend: Worker worker-20260213114143-172.18.0.15-35515 removed: 172.18.0.15:35515 got disassociated
26/02/13 12:43:26 INFO TaskSchedulerImpl: Handle removed worker worker-20260213114143-172.18.0.15-35515: 172.18.0.15:35515 got disassociated
26/02/13 12:43:26 INFO DAGScheduler: Shuffle files lost for worker worker-20260213114143-172.18.0.15-35515 on host 172.18.0.15
26/02/13 12:43:26 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20260213120207-0001/1 is now LOST (worker lost: 172.18.0.14:33335 got disassociated)
26/02/13 12:43:26 INFO StandaloneSchedulerBackend: Executor app-20260213120207-0001/1 removed: worker lost: 172.18.0.14:33335 got disassociated
26/02/13 12:43:26 INFO StandaloneAppClient$ClientEndpoint: Master removed worker worker-20260213114143-172.18.0.14-33335: 172.18.0.14:33335 got disassociated
26/02/13 12:43:26 INFO StandaloneSchedulerBackend: Worker worker-20260213114143-172.18.0.14-33335 removed: 172.18.0.14:33335 got disassociated
26/02/13 12:43:26 ERROR TaskSchedulerImpl: Lost executor 1 on 172.18.0.14: worker lost: 172.18.0.14:33335 got disassociated
26/02/13 12:43:26 INFO DAGScheduler: Executor lost: 1 (epoch 6)
26/02/13 12:43:26 INFO TaskSchedulerImpl: Handle removed worker worker-20260213114143-172.18.0.14-33335: 172.18.0.14:33335 got disassociated
26/02/13 12:43:26 INFO BlockManagerMasterEndpoint: Trying to remove executor 1 from BlockManagerMaster.
26/02/13 12:43:26 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_29_3 !
26/02/13 12:43:26 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_29_2 !
26/02/13 12:43:26 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_29_1 !
26/02/13 12:43:26 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_29_0 !
26/02/13 12:43:26 INFO BlockManagerMasterEndpoint: Removing block manager BlockManagerId(1, 172.18.0.14, 37571, None)
26/02/13 12:43:26 INFO BlockManagerMaster: Removed 1 successfully in removeExecutor
26/02/13 12:43:26 INFO DAGScheduler: Shuffle files lost for host: 172.18.0.14 (epoch 6)
26/02/13 12:43:26 INFO DAGScheduler: Shuffle files lost for worker worker-20260213114143-172.18.0.14-33335 on host 172.18.0.14
26/02/13 12:43:26 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
