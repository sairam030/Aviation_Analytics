============================================
Spark Streaming Auto-Restart Wrapper
============================================
[Sun Jan  4 16:56:46 UTC 2026] Starting Spark Streaming attempt...
============================================
Starting Spark Streaming Enrichment Service
============================================
[Sun Jan  4 16:56:46 UTC 2026] Waiting for Kafka and topics to be ready...
  Waiting 40 seconds for Kafka initialization...
✓ Kafka should be ready now
[Sun Jan  4 16:57:26 UTC 2026] Submitting Spark Streaming job...
============================================================
Starting Spark Streaming Enrichment
============================================================
26/01/04 16:57:35 INFO SparkContext: Running Spark version 3.5.1
26/01/04 16:57:35 INFO SparkContext: OS info Linux, 6.14.0-37-generic, amd64
26/01/04 16:57:35 INFO SparkContext: Java version 17.0.17
26/01/04 16:57:35 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
26/01/04 16:57:35 INFO ResourceUtils: ==============================================================
26/01/04 16:57:35 INFO ResourceUtils: No custom resources configured for spark.driver.
26/01/04 16:57:35 INFO ResourceUtils: ==============================================================
26/01/04 16:57:35 INFO SparkContext: Submitted application: FlightStreamEnrichment
26/01/04 16:57:35 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
26/01/04 16:57:35 INFO ResourceProfile: Limiting resource is cpu
26/01/04 16:57:35 INFO ResourceProfileManager: Added ResourceProfile id: 0
26/01/04 16:57:36 INFO SecurityManager: Changing view acls to: root
26/01/04 16:57:36 INFO SecurityManager: Changing modify acls to: root
26/01/04 16:57:36 INFO SecurityManager: Changing view acls groups to: 
26/01/04 16:57:36 INFO SecurityManager: Changing modify acls groups to: 
26/01/04 16:57:36 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY
26/01/04 16:57:37 INFO Utils: Successfully started service 'sparkDriver' on port 43473.
26/01/04 16:57:37 INFO SparkEnv: Registering MapOutputTracker
26/01/04 16:57:37 INFO SparkEnv: Registering BlockManagerMaster
26/01/04 16:57:37 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
26/01/04 16:57:37 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
26/01/04 16:57:37 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
26/01/04 16:57:37 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-c520c62c-c9e4-4f27-89df-e37c138fbdbd
26/01/04 16:57:37 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
26/01/04 16:57:37 INFO SparkEnv: Registering OutputCommitCoordinator
26/01/04 16:57:37 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
26/01/04 16:57:37 INFO Utils: Successfully started service 'SparkUI' on port 4040.
26/01/04 16:57:38 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...
26/01/04 16:57:38 INFO TransportClientFactory: Successfully created connection to spark-master/172.18.0.10:7077 after 80 ms (0 ms spent in bootstraps)
26/01/04 16:57:39 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20260104165738-0000
26/01/04 16:57:39 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 33535.
26/01/04 16:57:39 INFO NettyBlockTransferService: Server created on spark-master 0.0.0.0:33535
26/01/04 16:57:39 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
26/01/04 16:57:39 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, spark-master, 33535, None)
26/01/04 16:57:39 INFO BlockManagerMasterEndpoint: Registering block manager spark-master:33535 with 434.4 MiB RAM, BlockManagerId(driver, spark-master, 33535, None)
26/01/04 16:57:39 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, spark-master, 33535, None)
26/01/04 16:57:39 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, spark-master, 33535, None)
26/01/04 16:57:39 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20260104165738-0000/0 on worker-20260104165652-172.18.0.13-40087 (172.18.0.13:40087) with 2 core(s)
26/01/04 16:57:39 INFO StandaloneSchedulerBackend: Granted executor ID app-20260104165738-0000/0 on hostPort 172.18.0.13:40087 with 2 core(s), 1024.0 MiB RAM
26/01/04 16:57:39 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20260104165738-0000/1 on worker-20260104165652-172.18.0.14-43565 (172.18.0.14:43565) with 2 core(s)
26/01/04 16:57:39 INFO StandaloneSchedulerBackend: Granted executor ID app-20260104165738-0000/1 on hostPort 172.18.0.14:43565 with 2 core(s), 1024.0 MiB RAM
26/01/04 16:57:39 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20260104165738-0000/0 is now RUNNING
26/01/04 16:57:39 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20260104165738-0000/1 is now RUNNING
26/01/04 16:57:40 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
✓ Spark session created
✓ Reading from: aviation-india-states
✓ Writing to: aviation-enriched-states
[DEBUG] Creating route mapping table...
26/01/04 16:57:43 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
26/01/04 16:57:43 INFO SharedState: Warehouse path is 'file:/opt/airflow/spark-warehouse'.
26/01/04 16:57:52 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.13:57676) with ID 0,  ResourceProfileId 0
26/01/04 16:57:52 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.14:33746) with ID 1,  ResourceProfileId 0
26/01/04 16:57:52 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.13:44401 with 434.4 MiB RAM, BlockManagerId(0, 172.18.0.13, 44401, None)
26/01/04 16:57:52 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.14:43523 with 434.4 MiB RAM, BlockManagerId(1, 172.18.0.14, 43523, None)
26/01/04 16:57:55 INFO CodeGenerator: Code generated in 718.767879 ms
26/01/04 16:57:55 INFO DAGScheduler: Got job 0 (count at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 16:57:55 INFO DAGScheduler: Final stage: ResultStage 0 (count at NativeMethodAccessorImpl.java:0)
26/01/04 16:57:55 INFO DAGScheduler: Parents of final stage: List()
26/01/04 16:57:55 INFO DAGScheduler: Missing parents: List()
26/01/04 16:57:55 INFO DAGScheduler: Submitting ResultStage 0 (*(1) Scan ExistingRDD[airline_iata#0,airline_icao#1,airline_name#2,airline_prefix#3,destination_airport#4,destination_city#5,destination_code#6,destination_lat#7,destination_lon#8,max_flight_num#9L,min_flight_num#10L,origin_airport#11,origin_city#12,origin_code#13,origin_lat#14,origin_lon#15]
 MapPartitionsRDD[7] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 16:57:55 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 20.3 KiB, free 434.4 MiB)
26/01/04 16:57:55 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 9.1 KiB, free 434.4 MiB)
26/01/04 16:57:55 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on spark-master:33535 (size: 9.1 KiB, free: 434.4 MiB)
26/01/04 16:57:55 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1585
26/01/04 16:57:55 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 0 (*(1) Scan ExistingRDD[airline_iata#0,airline_icao#1,airline_name#2,airline_prefix#3,destination_airport#4,destination_city#5,destination_code#6,destination_lat#7,destination_lon#8,max_flight_num#9L,min_flight_num#10L,origin_airport#11,origin_city#12,origin_code#13,origin_lat#14,origin_lon#15]
 MapPartitionsRDD[7] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 16:57:55 INFO TaskSchedulerImpl: Adding task set 0.0 with 2 tasks resource profile 0
26/01/04 16:57:55 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 16:57:55 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 16:57:56 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.18.0.13:44401 (size: 9.1 KiB, free: 434.4 MiB)
26/01/04 16:57:56 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.18.0.14:43523 (size: 9.1 KiB, free: 434.4 MiB)
26/01/04 16:58:02 INFO BlockManagerInfo: Added rdd_7_1 in memory on 172.18.0.13:44401 (size: 4.4 KiB, free: 434.4 MiB)
26/01/04 16:58:02 INFO BlockManagerInfo: Added rdd_7_0 in memory on 172.18.0.14:43523 (size: 4.5 KiB, free: 434.4 MiB)
26/01/04 16:58:02 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 7176 ms on 172.18.0.14 (executor 1) (1/2)
26/01/04 16:58:02 INFO PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 44815
26/01/04 16:58:03 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 7233 ms on 172.18.0.13 (executor 0) (2/2)
26/01/04 16:58:03 INFO DAGScheduler: ResultStage 0 (count at NativeMethodAccessorImpl.java:0) finished in 7.714 s
26/01/04 16:58:03 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
26/01/04 16:58:03 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 16:58:03 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
26/01/04 16:58:03 INFO CodeGenerator: Code generated in 52.784787 ms
26/01/04 16:58:03 INFO DAGScheduler: Registering RDD 12 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 0
26/01/04 16:58:03 INFO DAGScheduler: Got map stage job 1 (count at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 16:58:03 INFO DAGScheduler: Final stage: ShuffleMapStage 1 (count at NativeMethodAccessorImpl.java:0)
26/01/04 16:58:03 INFO DAGScheduler: Parents of final stage: List()
26/01/04 16:58:03 INFO DAGScheduler: Missing parents: List()
26/01/04 16:58:03 INFO DAGScheduler: Submitting ShuffleMapStage 1 (MapPartitionsRDD[12] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 16:58:03 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 27.0 KiB, free 434.3 MiB)
26/01/04 16:58:03 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 11.5 KiB, free 434.3 MiB)
26/01/04 16:58:03 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on spark-master:33535 (size: 11.5 KiB, free: 434.4 MiB)
26/01/04 16:58:03 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1585
26/01/04 16:58:03 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[12] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 16:58:03 INFO TaskSchedulerImpl: Adding task set 1.0 with 2 tasks resource profile 0
26/01/04 16:58:03 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 2) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9413 bytes) 
26/01/04 16:58:03 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 3) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9226 bytes) 
26/01/04 16:58:03 INFO BlockManagerInfo: Removed broadcast_0_piece0 on spark-master:33535 in memory (size: 9.1 KiB, free: 434.4 MiB)
26/01/04 16:58:03 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 172.18.0.14:43523 in memory (size: 9.1 KiB, free: 434.4 MiB)
26/01/04 16:58:03 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 172.18.0.13:44401 in memory (size: 9.1 KiB, free: 434.4 MiB)
26/01/04 16:58:04 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.18.0.13:44401 (size: 11.5 KiB, free: 434.4 MiB)
26/01/04 16:58:04 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.18.0.14:43523 (size: 11.5 KiB, free: 434.4 MiB)
26/01/04 16:58:05 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 3) in 1418 ms on 172.18.0.14 (executor 1) (1/2)
26/01/04 16:58:05 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 2) in 1443 ms on 172.18.0.13 (executor 0) (2/2)
26/01/04 16:58:05 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
26/01/04 16:58:05 INFO DAGScheduler: ShuffleMapStage 1 (count at NativeMethodAccessorImpl.java:0) finished in 1.614 s
26/01/04 16:58:05 INFO DAGScheduler: looking for newly runnable stages
26/01/04 16:58:05 INFO DAGScheduler: running: Set()
26/01/04 16:58:05 INFO DAGScheduler: waiting: Set()
26/01/04 16:58:05 INFO DAGScheduler: failed: Set()
26/01/04 16:58:05 INFO CodeGenerator: Code generated in 57.778974 ms
26/01/04 16:58:05 INFO SparkContext: Starting job: count at NativeMethodAccessorImpl.java:0
26/01/04 16:58:05 INFO DAGScheduler: Got job 2 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/01/04 16:58:05 INFO DAGScheduler: Final stage: ResultStage 3 (count at NativeMethodAccessorImpl.java:0)
26/01/04 16:58:05 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 2)
26/01/04 16:58:05 INFO DAGScheduler: Missing parents: List()
26/01/04 16:58:05 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[15] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 16:58:05 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 12.5 KiB, free 434.4 MiB)
26/01/04 16:58:05 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 5.9 KiB, free 434.3 MiB)
26/01/04 16:58:05 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on spark-master:33535 (size: 5.9 KiB, free: 434.4 MiB)
26/01/04 16:58:05 INFO BlockManagerInfo: Removed broadcast_1_piece0 on spark-master:33535 in memory (size: 11.5 KiB, free: 434.4 MiB)
26/01/04 16:58:05 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1585
26/01/04 16:58:05 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 172.18.0.13:44401 in memory (size: 11.5 KiB, free: 434.4 MiB)
26/01/04 16:58:05 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 172.18.0.14:43523 in memory (size: 11.5 KiB, free: 434.4 MiB)
26/01/04 16:58:05 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[15] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/01/04 16:58:05 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0
26/01/04 16:58:05 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 4) (172.18.0.14, executor 1, partition 0, NODE_LOCAL, 7619 bytes) 
26/01/04 16:58:05 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.18.0.14:43523 (size: 5.9 KiB, free: 434.4 MiB)
26/01/04 16:58:05 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 172.18.0.14:33746
26/01/04 16:58:06 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 4) in 601 ms on 172.18.0.14 (executor 1) (1/1)
26/01/04 16:58:06 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool 
26/01/04 16:58:06 INFO DAGScheduler: ResultStage 3 (count at NativeMethodAccessorImpl.java:0) finished in 0.705 s
26/01/04 16:58:06 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 16:58:06 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished
26/01/04 16:58:06 INFO DAGScheduler: Job 2 finished: count at NativeMethodAccessorImpl.java:0, took 0.752550 s
✓ Route mapping table created (41 routes)
[DEBUG] Setting up Kafka input stream...
✓ Connected to Kafka input stream
[DEBUG] Parsing JSON and extracting fields...
[DEBUG] Joining with route mapping...
[DEBUG] Setting up output stream to Kafka...
26/01/04 16:58:07 INFO StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint
26/01/04 16:58:08 INFO ResolveWriteToStream: Checkpoint root /tmp/spark-checkpoint-enrichment resolved to file:/tmp/spark-checkpoint-enrichment.
26/01/04 16:58:08 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
26/01/04 16:58:08 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/metadata using temp file file:/tmp/spark-checkpoint-enrichment/.metadata.bc5df169-8d07-492d-8ac2-31caccc25287.tmp
26/01/04 16:58:08 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/.metadata.bc5df169-8d07-492d-8ac2-31caccc25287.tmp to file:/tmp/spark-checkpoint-enrichment/metadata
26/01/04 16:58:08 INFO BlockManagerInfo: Removed broadcast_2_piece0 on spark-master:33535 in memory (size: 5.9 KiB, free: 434.4 MiB)
26/01/04 16:58:08 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 172.18.0.14:43523 in memory (size: 5.9 KiB, free: 434.4 MiB)
26/01/04 16:58:08 INFO MicroBatchExecution: Starting [id = 7ee758be-50e4-4375-8595-7bf053b44670, runId = 2b3a52fc-8b7c-44f0-bd17-98d89af32a25]. Use file:/tmp/spark-checkpoint-enrichment to store the query checkpoint.
============================================================
✓ Streaming query started successfully!
✓ Enriching flights with route information...
✓ Query ID: 7ee758be-50e4-4375-8595-7bf053b44670
============================================================
[DEBUG] Waiting for stream termination...
26/01/04 16:58:08 INFO MicroBatchExecution: Reading table [org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@c383645] from DataSourceV2 named 'kafka' [org.apache.spark.sql.kafka010.KafkaSourceProvider@4f38029c]
26/01/04 16:58:08 INFO OffsetSeqLog: BatchIds found from listing: 
26/01/04 16:58:08 INFO OffsetSeqLog: BatchIds found from listing: 
26/01/04 16:58:08 INFO MicroBatchExecution: Starting new streaming query.
26/01/04 16:58:08 INFO MicroBatchExecution: Stream started from {}
26/01/04 16:58:09 INFO AdminClientConfig: AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.servers = [kafka:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

26/01/04 16:58:09 INFO AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
26/01/04 16:58:09 INFO AppInfoParser: Kafka version: 3.5.1
26/01/04 16:58:09 INFO AppInfoParser: Kafka commitId: 2c6fb6c54472e90a
26/01/04 16:58:09 INFO AppInfoParser: Kafka startTimeMs: 1767545889841
26/01/04 16:58:11 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/sources/0/0 using temp file file:/tmp/spark-checkpoint-enrichment/sources/0/.0.edeb912a-4524-4ce9-b2fa-8d4a53e369ff.tmp
26/01/04 16:58:11 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/sources/0/.0.edeb912a-4524-4ce9-b2fa-8d4a53e369ff.tmp to file:/tmp/spark-checkpoint-enrichment/sources/0/0
26/01/04 16:58:11 INFO KafkaMicroBatchStream: Initial offsets: {"aviation-india-states":{"2":2326,"1":2603,"0":2035}}
26/01/04 16:58:11 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/0 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.0.5a135e0e-ddf1-4c45-99dc-e95b9d3d0220.tmp
26/01/04 16:58:12 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.0.5a135e0e-ddf1-4c45-99dc-e95b9d3d0220.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/0
26/01/04 16:58:12 INFO MicroBatchExecution: Committed offsets for batch 0. Metadata OffsetSeqMetadata(0,1767545891821,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 16:58:13 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 16:58:13 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 16:58:13 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 16:58:13 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 16:58:13 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
26/01/04 16:58:13 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 16:58:13 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 16:58:14 INFO CodeGenerator: Code generated in 187.015897 ms
26/01/04 16:58:14 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#1483 - airline_prefix.nullCount#1482) > 0)
26/01/04 16:58:14 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#1518 - min_flight_num.nullCount#1517) > 0)
26/01/04 16:58:14 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#1513 - max_flight_num.nullCount#1512) > 0)
26/01/04 16:58:14 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 16:58:14 INFO DAGScheduler: Got job 3 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 16:58:14 INFO DAGScheduler: Final stage: ResultStage 4 (start at NativeMethodAccessorImpl.java:0)
26/01/04 16:58:14 INFO DAGScheduler: Parents of final stage: List()
26/01/04 16:58:14 INFO DAGScheduler: Missing parents: List()
26/01/04 16:58:14 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[20] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 16:58:14 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 38.5 KiB, free 434.4 MiB)
26/01/04 16:58:14 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.3 MiB)
26/01/04 16:58:14 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 16:58:14 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1585
26/01/04 16:58:14 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 4 (MapPartitionsRDD[20] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 16:58:14 INFO TaskSchedulerImpl: Adding task set 4.0 with 2 tasks resource profile 0
26/01/04 16:58:14 INFO TaskSetManager: Starting task 1.0 in stage 4.0 (TID 5) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 16:58:14 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 6) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 16:58:14 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 16:58:14 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 16:58:15 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 6) in 1245 ms on 172.18.0.14 (executor 1) (1/2)
26/01/04 16:58:15 INFO TaskSetManager: Finished task 1.0 in stage 4.0 (TID 5) in 1320 ms on 172.18.0.13 (executor 0) (2/2)
26/01/04 16:58:15 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool 
26/01/04 16:58:15 INFO DAGScheduler: ResultStage 4 (start at NativeMethodAccessorImpl.java:0) finished in 1.415 s
26/01/04 16:58:15 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 16:58:15 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished
26/01/04 16:58:15 INFO DAGScheduler: Job 3 finished: start at NativeMethodAccessorImpl.java:0, took 1.548022 s
26/01/04 16:58:15 INFO CodeGenerator: Code generated in 60.937246 ms
26/01/04 16:58:15 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.3 MiB)
26/01/04 16:58:15 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 16:58:15 INFO SparkContext: Created broadcast 4 from start at NativeMethodAccessorImpl.java:0
26/01/04 16:58:16 INFO CodeGenerator: Code generated in 483.30833 ms
26/01/04 16:58:16 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 0, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@2d764a7d]. The input RDD has 1 partitions.
26/01/04 16:58:16 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 16:58:16 INFO DAGScheduler: Got job 4 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/01/04 16:58:16 INFO DAGScheduler: Final stage: ResultStage 5 (start at NativeMethodAccessorImpl.java:0)
26/01/04 16:58:16 INFO DAGScheduler: Parents of final stage: List()
26/01/04 16:58:16 INFO DAGScheduler: Missing parents: List()
26/01/04 16:58:16 INFO DAGScheduler: Submitting ResultStage 5 (ParallelCollectionRDD[26] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 16:58:16 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 4.4 KiB, free 434.3 MiB)
26/01/04 16:58:16 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 2.6 KiB, free 434.3 MiB)
26/01/04 16:58:16 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on spark-master:33535 (size: 2.6 KiB, free: 434.4 MiB)
26/01/04 16:58:16 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1585
26/01/04 16:58:16 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (ParallelCollectionRDD[26] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/01/04 16:58:16 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0
26/01/04 16:58:16 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 7) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 7640 bytes) 
26/01/04 16:58:16 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 172.18.0.14:43523 (size: 2.6 KiB, free: 434.4 MiB)
26/01/04 16:58:17 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 7) in 763 ms on 172.18.0.14 (executor 1) (1/1)
26/01/04 16:58:17 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool 
26/01/04 16:58:17 INFO DAGScheduler: ResultStage 5 (start at NativeMethodAccessorImpl.java:0) finished in 0.883 s
26/01/04 16:58:17 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 16:58:17 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished
26/01/04 16:58:17 INFO DAGScheduler: Job 4 finished: start at NativeMethodAccessorImpl.java:0, took 0.901841 s
26/01/04 16:58:17 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 0, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@2d764a7d] is committing.
26/01/04 16:58:17 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 0, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@2d764a7d] committed.
26/01/04 16:58:17 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/0 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.0.ab044acd-af73-449a-a8e2-31b66f88fc9c.tmp
26/01/04 16:58:17 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.0.ab044acd-af73-449a-a8e2-31b66f88fc9c.tmp to file:/tmp/spark-checkpoint-enrichment/commits/0
26/01/04 16:58:17 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T16:58:08.910Z",
  "batchId" : 0,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "addBatch" : 4214,
    "commitOffsets" : 235,
    "getBatch" : 234,
    "latestOffset" : 2872,
    "queryPlanning" : 913,
    "triggerExecution" : 8846,
    "walCommit" : 302
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : null,
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 2326,
        "1" : 2603,
        "0" : 2035
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 2326,
        "1" : 2603,
        "0" : 2035
      }
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 0
  }
}
26/01/04 16:58:20 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/1 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.1.b228770d-6b11-45fe-9295-2f3294521647.tmp
26/01/04 16:58:20 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.1.b228770d-6b11-45fe-9295-2f3294521647.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/1
26/01/04 16:58:20 INFO MicroBatchExecution: Committed offsets for batch 1. Metadata OffsetSeqMetadata(0,1767545899990,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 16:58:20 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 16:58:20 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 16:58:20 INFO BlockManagerInfo: Removed broadcast_5_piece0 on spark-master:33535 in memory (size: 2.6 KiB, free: 434.4 MiB)
26/01/04 16:58:20 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 172.18.0.14:43523 in memory (size: 2.6 KiB, free: 434.4 MiB)
26/01/04 16:58:20 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 16:58:20 INFO BlockManagerInfo: Removed broadcast_3_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 16:58:20 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 16:58:20 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 16:58:21 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 16:58:21 INFO BlockManagerInfo: Removed broadcast_4_piece0 on spark-master:33535 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 16:58:21 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 16:58:21 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 16:58:21 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#2287 - airline_prefix.nullCount#2286) > 0)
26/01/04 16:58:21 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#2322 - min_flight_num.nullCount#2321) > 0)
26/01/04 16:58:21 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#2317 - max_flight_num.nullCount#2316) > 0)
26/01/04 16:58:21 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 16:58:21 INFO DAGScheduler: Got job 5 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 16:58:21 INFO DAGScheduler: Final stage: ResultStage 6 (start at NativeMethodAccessorImpl.java:0)
26/01/04 16:58:21 INFO DAGScheduler: Parents of final stage: List()
26/01/04 16:58:21 INFO DAGScheduler: Missing parents: List()
26/01/04 16:58:21 INFO DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[31] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 16:58:21 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 38.5 KiB, free 434.4 MiB)
26/01/04 16:58:21 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.3 MiB)
26/01/04 16:58:21 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 16:58:21 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1585
26/01/04 16:58:21 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 6 (MapPartitionsRDD[31] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 16:58:21 INFO TaskSchedulerImpl: Adding task set 6.0 with 2 tasks resource profile 0
26/01/04 16:58:21 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 8) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 16:58:21 INFO TaskSetManager: Starting task 1.0 in stage 6.0 (TID 9) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 16:58:21 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 16:58:21 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 16:58:21 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 8) in 315 ms on 172.18.0.14 (executor 1) (1/2)
26/01/04 16:58:21 INFO TaskSetManager: Finished task 1.0 in stage 6.0 (TID 9) in 316 ms on 172.18.0.13 (executor 0) (2/2)
26/01/04 16:58:21 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool 
26/01/04 16:58:22 INFO DAGScheduler: ResultStage 6 (start at NativeMethodAccessorImpl.java:0) finished in 0.375 s
26/01/04 16:58:22 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 16:58:22 INFO TaskSchedulerImpl: Killing all running tasks in stage 6: Stage finished
26/01/04 16:58:22 INFO DAGScheduler: Job 5 finished: start at NativeMethodAccessorImpl.java:0, took 0.399971 s
26/01/04 16:58:22 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.3 MiB)
26/01/04 16:58:22 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 16:58:22 INFO SparkContext: Created broadcast 7 from start at NativeMethodAccessorImpl.java:0
26/01/04 16:58:22 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 1, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@11405a93]. The input RDD has 3 partitions.
26/01/04 16:58:22 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 16:58:22 INFO DAGScheduler: Got job 6 (start at NativeMethodAccessorImpl.java:0) with 3 output partitions
26/01/04 16:58:22 INFO DAGScheduler: Final stage: ResultStage 7 (start at NativeMethodAccessorImpl.java:0)
26/01/04 16:58:22 INFO DAGScheduler: Parents of final stage: List()
26/01/04 16:58:22 INFO DAGScheduler: Missing parents: List()
26/01/04 16:58:22 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[36] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 16:58:22 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 52.9 KiB, free 434.3 MiB)
26/01/04 16:58:22 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 20.2 KiB, free 434.3 MiB)
26/01/04 16:58:22 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on spark-master:33535 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 16:58:22 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1585
26/01/04 16:58:22 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 7 (MapPartitionsRDD[36] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
26/01/04 16:58:22 INFO TaskSchedulerImpl: Adding task set 7.0 with 3 tasks resource profile 0
26/01/04 16:58:22 INFO TaskSetManager: Starting task 1.0 in stage 7.0 (TID 10) (172.18.0.14, executor 1, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/01/04 16:58:22 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 11) (172.18.0.13, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/01/04 16:58:22 INFO TaskSetManager: Starting task 2.0 in stage 7.0 (TID 12) (172.18.0.13, executor 0, partition 2, PROCESS_LOCAL, 8594 bytes) 
26/01/04 16:58:22 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 172.18.0.14:43523 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 16:58:22 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 172.18.0.13:44401 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 16:58:23 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 172.18.0.14:43523 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 16:58:23 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 172.18.0.13:44401 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 16:58:29 INFO TaskSetManager: Finished task 2.0 in stage 7.0 (TID 12) in 7326 ms on 172.18.0.13 (executor 0) (1/3)
26/01/04 16:58:29 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 11) in 7342 ms on 172.18.0.13 (executor 0) (2/3)
26/01/04 16:58:29 INFO TaskSetManager: Finished task 1.0 in stage 7.0 (TID 10) in 7373 ms on 172.18.0.14 (executor 1) (3/3)
26/01/04 16:58:29 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool 
26/01/04 16:58:29 INFO DAGScheduler: ResultStage 7 (start at NativeMethodAccessorImpl.java:0) finished in 7.525 s
26/01/04 16:58:29 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 16:58:29 INFO TaskSchedulerImpl: Killing all running tasks in stage 7: Stage finished
26/01/04 16:58:29 INFO DAGScheduler: Job 6 finished: start at NativeMethodAccessorImpl.java:0, took 7.557300 s
26/01/04 16:58:29 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 1, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@11405a93] is committing.
26/01/04 16:58:29 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 1, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@11405a93] committed.
26/01/04 16:58:29 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/1 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.1.22711e52-6ebc-4b68-ad93-bc5e2be49938.tmp
26/01/04 16:58:29 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.1.22711e52-6ebc-4b68-ad93-bc5e2be49938.tmp to file:/tmp/spark-checkpoint-enrichment/commits/1
26/01/04 16:58:29 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T16:58:19.949Z",
  "batchId" : 1,
  "numInputRows" : 30,
  "inputRowsPerSecond" : 566.0377358490566,
  "processedRowsPerSecond" : 2.9916234543278817,
  "durationMs" : {
    "addBatch" : 8981,
    "commitOffsets" : 305,
    "getBatch" : 5,
    "latestOffset" : 41,
    "queryPlanning" : 477,
    "triggerExecution" : 10028,
    "walCommit" : 210
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 2326,
        "1" : 2603,
        "0" : 2035
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 2337,
        "1" : 2613,
        "0" : 2044
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 2337,
        "1" : 2613,
        "0" : 2044
      }
    },
    "numInputRows" : 30,
    "inputRowsPerSecond" : 566.0377358490566,
    "processedRowsPerSecond" : 2.9916234543278817,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 31
  }
}
26/01/04 16:58:31 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/2 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.2.3c9e186e-4155-4e26-a939-0889c3e04e32.tmp
26/01/04 16:58:31 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.2.3c9e186e-4155-4e26-a939-0889c3e04e32.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/2
26/01/04 16:58:31 INFO MicroBatchExecution: Committed offsets for batch 2. Metadata OffsetSeqMetadata(0,1767545910992,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 16:58:31 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 16:58:31 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 16:58:31 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 16:58:31 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 16:58:31 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 16:58:31 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 16:58:31 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#3091 - airline_prefix.nullCount#3090) > 0)
26/01/04 16:58:31 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#3126 - min_flight_num.nullCount#3125) > 0)
26/01/04 16:58:31 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#3121 - max_flight_num.nullCount#3120) > 0)
26/01/04 16:58:31 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 16:58:31 INFO DAGScheduler: Got job 7 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 16:58:31 INFO DAGScheduler: Final stage: ResultStage 8 (start at NativeMethodAccessorImpl.java:0)
26/01/04 16:58:31 INFO DAGScheduler: Parents of final stage: List()
26/01/04 16:58:31 INFO DAGScheduler: Missing parents: List()
26/01/04 16:58:31 INFO DAGScheduler: Submitting ResultStage 8 (MapPartitionsRDD[41] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 16:58:31 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 38.5 KiB, free 434.2 MiB)
26/01/04 16:58:31 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.2 MiB)
26/01/04 16:58:31 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 16:58:31 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1585
26/01/04 16:58:31 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 8 (MapPartitionsRDD[41] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 16:58:31 INFO TaskSchedulerImpl: Adding task set 8.0 with 2 tasks resource profile 0
26/01/04 16:58:31 INFO TaskSetManager: Starting task 1.0 in stage 8.0 (TID 13) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 16:58:31 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 14) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 16:58:31 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 16:58:31 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 16:58:32 INFO TaskSetManager: Finished task 1.0 in stage 8.0 (TID 13) in 214 ms on 172.18.0.13 (executor 0) (1/2)
26/01/04 16:58:32 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 14) in 265 ms on 172.18.0.14 (executor 1) (2/2)
26/01/04 16:58:32 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool 
26/01/04 16:58:32 INFO DAGScheduler: ResultStage 8 (start at NativeMethodAccessorImpl.java:0) finished in 0.308 s
26/01/04 16:58:32 INFO DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 16:58:32 INFO TaskSchedulerImpl: Killing all running tasks in stage 8: Stage finished
26/01/04 16:58:32 INFO DAGScheduler: Job 7 finished: start at NativeMethodAccessorImpl.java:0, took 0.344792 s
26/01/04 16:58:32 INFO BlockManagerInfo: Removed broadcast_6_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 16:58:32 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.3 MiB)
26/01/04 16:58:32 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 16:58:32 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 16:58:32 INFO SparkContext: Created broadcast 10 from start at NativeMethodAccessorImpl.java:0
26/01/04 16:58:32 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 16:58:32 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 2, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@5b829c03]. The input RDD has 3 partitions.
26/01/04 16:58:32 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 16:58:32 INFO DAGScheduler: Got job 8 (start at NativeMethodAccessorImpl.java:0) with 3 output partitions
26/01/04 16:58:32 INFO DAGScheduler: Final stage: ResultStage 9 (start at NativeMethodAccessorImpl.java:0)
26/01/04 16:58:32 INFO DAGScheduler: Parents of final stage: List()
26/01/04 16:58:32 INFO DAGScheduler: Missing parents: List()
26/01/04 16:58:32 INFO BlockManagerInfo: Removed broadcast_8_piece0 on spark-master:33535 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 16:58:32 INFO DAGScheduler: Submitting ResultStage 9 (MapPartitionsRDD[46] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 16:58:32 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 52.9 KiB, free 434.3 MiB)
26/01/04 16:58:32 INFO BlockManagerInfo: Removed broadcast_8_piece0 on 172.18.0.13:44401 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 16:58:32 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 20.2 KiB, free 434.3 MiB)
26/01/04 16:58:32 INFO BlockManagerInfo: Removed broadcast_8_piece0 on 172.18.0.14:43523 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 16:58:32 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on spark-master:33535 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 16:58:32 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1585
26/01/04 16:58:32 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 9 (MapPartitionsRDD[46] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
26/01/04 16:58:32 INFO TaskSchedulerImpl: Adding task set 9.0 with 3 tasks resource profile 0
26/01/04 16:58:32 INFO TaskSetManager: Starting task 1.0 in stage 9.0 (TID 15) (172.18.0.14, executor 1, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/01/04 16:58:32 INFO BlockManagerInfo: Removed broadcast_9_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 16:58:32 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 16) (172.18.0.13, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/01/04 16:58:32 INFO TaskSetManager: Starting task 2.0 in stage 9.0 (TID 17) (172.18.0.13, executor 0, partition 2, PROCESS_LOCAL, 8594 bytes) 
26/01/04 16:58:32 INFO BlockManagerInfo: Removed broadcast_9_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 16:58:32 INFO BlockManagerInfo: Removed broadcast_9_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 16:58:32 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 172.18.0.14:43523 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 16:58:32 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 172.18.0.14:43523 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 16:58:32 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 172.18.0.13:44401 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 16:58:32 INFO BlockManagerInfo: Removed broadcast_7_piece0 on spark-master:33535 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 16:58:33 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 172.18.0.13:44401 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 16:58:33 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 172.18.0.14:43523 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 16:58:33 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 172.18.0.13:44401 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 16:58:34 INFO TaskSetManager: Finished task 1.0 in stage 9.0 (TID 15) in 1531 ms on 172.18.0.14 (executor 1) (1/3)
26/01/04 16:58:34 INFO TaskSetManager: Finished task 2.0 in stage 9.0 (TID 17) in 1803 ms on 172.18.0.13 (executor 0) (2/3)
26/01/04 16:58:34 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 16) in 1829 ms on 172.18.0.13 (executor 0) (3/3)
26/01/04 16:58:34 INFO TaskSchedulerImpl: Removed TaskSet 9.0, whose tasks have all completed, from pool 
26/01/04 16:58:34 INFO DAGScheduler: ResultStage 9 (start at NativeMethodAccessorImpl.java:0) finished in 2.004 s
26/01/04 16:58:34 INFO DAGScheduler: Job 8 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 16:58:34 INFO TaskSchedulerImpl: Killing all running tasks in stage 9: Stage finished
26/01/04 16:58:34 INFO DAGScheduler: Job 8 finished: start at NativeMethodAccessorImpl.java:0, took 2.025807 s
26/01/04 16:58:34 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 2, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@5b829c03] is committing.
26/01/04 16:58:34 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 2, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@5b829c03] committed.
26/01/04 16:58:34 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/2 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.2.363cf043-be40-4a0b-84a7-e75341abfd64.tmp
26/01/04 16:58:34 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.2.363cf043-be40-4a0b-84a7-e75341abfd64.tmp to file:/tmp/spark-checkpoint-enrichment/commits/2
26/01/04 16:58:34 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T16:58:30.977Z",
  "batchId" : 2,
  "numInputRows" : 30,
  "inputRowsPerSecond" : 1200.0,
  "processedRowsPerSecond" : 8.19224467504096,
  "durationMs" : {
    "addBatch" : 3048,
    "commitOffsets" : 264,
    "getBatch" : 0,
    "latestOffset" : 13,
    "queryPlanning" : 102,
    "triggerExecution" : 3661,
    "walCommit" : 231
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 2337,
        "1" : 2613,
        "0" : 2044
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 2348,
        "1" : 2623,
        "0" : 2053
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 2348,
        "1" : 2623,
        "0" : 2053
      }
    },
    "numInputRows" : 30,
    "inputRowsPerSecond" : 1200.0,
    "processedRowsPerSecond" : 8.19224467504096,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 31
  }
}
26/01/04 16:58:42 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/3 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.3.67c31b7e-2dbd-452e-8772-71a6cf065afb.tmp
26/01/04 16:58:42 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.3.67c31b7e-2dbd-452e-8772-71a6cf065afb.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/3
26/01/04 16:58:42 INFO MicroBatchExecution: Committed offsets for batch 3. Metadata OffsetSeqMetadata(0,1767545921984,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 16:58:42 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 16:58:42 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 16:58:42 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 16:58:42 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 16:58:42 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 16:58:42 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 16:58:42 INFO BlockManagerInfo: Removed broadcast_11_piece0 on spark-master:33535 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 16:58:42 INFO BlockManagerInfo: Removed broadcast_11_piece0 on 172.18.0.14:43523 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 16:58:42 INFO BlockManagerInfo: Removed broadcast_11_piece0 on 172.18.0.13:44401 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 16:58:42 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#3895 - airline_prefix.nullCount#3894) > 0)
26/01/04 16:58:42 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#3930 - min_flight_num.nullCount#3929) > 0)
26/01/04 16:58:42 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#3925 - max_flight_num.nullCount#3924) > 0)
26/01/04 16:58:42 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 16:58:42 INFO DAGScheduler: Got job 9 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 16:58:42 INFO DAGScheduler: Final stage: ResultStage 10 (start at NativeMethodAccessorImpl.java:0)
26/01/04 16:58:42 INFO DAGScheduler: Parents of final stage: List()
26/01/04 16:58:42 INFO DAGScheduler: Missing parents: List()
26/01/04 16:58:42 INFO DAGScheduler: Submitting ResultStage 10 (MapPartitionsRDD[51] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 16:58:42 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 38.5 KiB, free 434.4 MiB)
26/01/04 16:58:42 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.3 MiB)
26/01/04 16:58:42 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 16:58:42 INFO SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:1585
26/01/04 16:58:42 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 10 (MapPartitionsRDD[51] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 16:58:42 INFO TaskSchedulerImpl: Adding task set 10.0 with 2 tasks resource profile 0
26/01/04 16:58:42 INFO TaskSetManager: Starting task 1.0 in stage 10.0 (TID 18) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 16:58:42 INFO TaskSetManager: Starting task 0.0 in stage 10.0 (TID 19) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 16:58:42 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 16:58:43 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 16:58:43 INFO BlockManagerInfo: Removed broadcast_10_piece0 on 172.18.0.13:44401 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 16:58:43 INFO BlockManagerInfo: Removed broadcast_10_piece0 on spark-master:33535 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 16:58:43 INFO BlockManagerInfo: Removed broadcast_10_piece0 on 172.18.0.14:43523 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 16:58:43 INFO TaskSetManager: Finished task 0.0 in stage 10.0 (TID 19) in 224 ms on 172.18.0.14 (executor 1) (1/2)
26/01/04 16:58:43 INFO TaskSetManager: Finished task 1.0 in stage 10.0 (TID 18) in 371 ms on 172.18.0.13 (executor 0) (2/2)
26/01/04 16:58:43 INFO TaskSchedulerImpl: Removed TaskSet 10.0, whose tasks have all completed, from pool 
26/01/04 16:58:43 INFO DAGScheduler: ResultStage 10 (start at NativeMethodAccessorImpl.java:0) finished in 0.426 s
26/01/04 16:58:43 INFO DAGScheduler: Job 9 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 16:58:43 INFO TaskSchedulerImpl: Killing all running tasks in stage 10: Stage finished
26/01/04 16:58:43 INFO DAGScheduler: Job 9 finished: start at NativeMethodAccessorImpl.java:0, took 0.455429 s
26/01/04 16:58:43 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.3 MiB)
26/01/04 16:58:43 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 16:58:43 INFO SparkContext: Created broadcast 13 from start at NativeMethodAccessorImpl.java:0
26/01/04 16:58:43 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 3, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@65c03ffb]. The input RDD has 1 partitions.
26/01/04 16:58:43 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 16:58:43 INFO DAGScheduler: Got job 10 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/01/04 16:58:43 INFO DAGScheduler: Final stage: ResultStage 11 (start at NativeMethodAccessorImpl.java:0)
26/01/04 16:58:43 INFO DAGScheduler: Parents of final stage: List()
26/01/04 16:58:43 INFO DAGScheduler: Missing parents: List()
26/01/04 16:58:43 INFO DAGScheduler: Submitting ResultStage 11 (MapPartitionsRDD[56] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 16:58:43 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 52.9 KiB, free 434.3 MiB)
26/01/04 16:58:43 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 20.2 KiB, free 434.3 MiB)
26/01/04 16:58:43 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on spark-master:33535 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 16:58:43 INFO SparkContext: Created broadcast 14 from broadcast at DAGScheduler.scala:1585
26/01/04 16:58:43 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 11 (MapPartitionsRDD[56] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/01/04 16:58:43 INFO TaskSchedulerImpl: Adding task set 11.0 with 1 tasks resource profile 0
26/01/04 16:58:43 INFO TaskSetManager: Starting task 0.0 in stage 11.0 (TID 20) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/01/04 16:58:43 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on 172.18.0.14:43523 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 16:58:43 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on 172.18.0.14:43523 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 16:58:44 INFO TaskSetManager: Finished task 0.0 in stage 11.0 (TID 20) in 959 ms on 172.18.0.14 (executor 1) (1/1)
26/01/04 16:58:44 INFO TaskSchedulerImpl: Removed TaskSet 11.0, whose tasks have all completed, from pool 
26/01/04 16:58:44 INFO DAGScheduler: ResultStage 11 (start at NativeMethodAccessorImpl.java:0) finished in 0.990 s
26/01/04 16:58:44 INFO DAGScheduler: Job 10 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 16:58:44 INFO TaskSchedulerImpl: Killing all running tasks in stage 11: Stage finished
26/01/04 16:58:44 INFO DAGScheduler: Job 10 finished: start at NativeMethodAccessorImpl.java:0, took 1.037531 s
26/01/04 16:58:44 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 3, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@65c03ffb] is committing.
26/01/04 16:58:44 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 3, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@65c03ffb] committed.
26/01/04 16:58:44 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/3 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.3.05938f98-54eb-4f43-864c-b7c40341613c.tmp
26/01/04 16:58:44 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.3.05938f98-54eb-4f43-864c-b7c40341613c.tmp to file:/tmp/spark-checkpoint-enrichment/commits/3
26/01/04 16:58:44 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T16:58:41.964Z",
  "batchId" : 3,
  "numInputRows" : 2,
  "inputRowsPerSecond" : 74.07407407407408,
  "processedRowsPerSecond" : 0.7527286413248024,
  "durationMs" : {
    "addBatch" : 2087,
    "commitOffsets" : 154,
    "getBatch" : 1,
    "latestOffset" : 18,
    "queryPlanning" : 206,
    "triggerExecution" : 2657,
    "walCommit" : 177
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 2348,
        "1" : 2623,
        "0" : 2053
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 2348,
        "1" : 2625,
        "0" : 2053
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 2348,
        "1" : 2625,
        "0" : 2053
      }
    },
    "numInputRows" : 2,
    "inputRowsPerSecond" : 74.07407407407408,
    "processedRowsPerSecond" : 0.7527286413248024,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 2
  }
}
26/01/04 16:58:44 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/4 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.4.c764e027-cb69-4390-b01a-801e6dc97988.tmp
26/01/04 16:58:44 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.4.c764e027-cb69-4390-b01a-801e6dc97988.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/4
26/01/04 16:58:44 INFO MicroBatchExecution: Committed offsets for batch 4. Metadata OffsetSeqMetadata(0,1767545924642,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 16:58:44 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 16:58:44 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 16:58:44 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 16:58:44 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 16:58:44 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 16:58:44 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 16:58:45 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#4699 - airline_prefix.nullCount#4698) > 0)
26/01/04 16:58:45 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#4734 - min_flight_num.nullCount#4733) > 0)
26/01/04 16:58:45 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#4729 - max_flight_num.nullCount#4728) > 0)
26/01/04 16:58:45 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 16:58:45 INFO DAGScheduler: Got job 11 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 16:58:45 INFO DAGScheduler: Final stage: ResultStage 12 (start at NativeMethodAccessorImpl.java:0)
26/01/04 16:58:45 INFO DAGScheduler: Parents of final stage: List()
26/01/04 16:58:45 INFO DAGScheduler: Missing parents: List()
26/01/04 16:58:45 INFO DAGScheduler: Submitting ResultStage 12 (MapPartitionsRDD[61] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 16:58:45 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 38.5 KiB, free 434.2 MiB)
26/01/04 16:58:45 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.2 MiB)
26/01/04 16:58:45 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 16:58:45 INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1585
26/01/04 16:58:45 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 12 (MapPartitionsRDD[61] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 16:58:45 INFO TaskSchedulerImpl: Adding task set 12.0 with 2 tasks resource profile 0
26/01/04 16:58:45 INFO TaskSetManager: Starting task 1.0 in stage 12.0 (TID 21) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 16:58:45 INFO TaskSetManager: Starting task 0.0 in stage 12.0 (TID 22) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 16:58:45 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 16:58:45 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 16:58:45 INFO TaskSetManager: Finished task 0.0 in stage 12.0 (TID 22) in 125 ms on 172.18.0.14 (executor 1) (1/2)
26/01/04 16:58:45 INFO TaskSetManager: Finished task 1.0 in stage 12.0 (TID 21) in 135 ms on 172.18.0.13 (executor 0) (2/2)
26/01/04 16:58:45 INFO TaskSchedulerImpl: Removed TaskSet 12.0, whose tasks have all completed, from pool 
26/01/04 16:58:45 INFO DAGScheduler: ResultStage 12 (start at NativeMethodAccessorImpl.java:0) finished in 0.153 s
26/01/04 16:58:45 INFO DAGScheduler: Job 11 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 16:58:45 INFO TaskSchedulerImpl: Killing all running tasks in stage 12: Stage finished
26/01/04 16:58:45 INFO DAGScheduler: Job 11 finished: start at NativeMethodAccessorImpl.java:0, took 0.162825 s
26/01/04 16:58:45 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.2 MiB)
26/01/04 16:58:45 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 16:58:45 INFO SparkContext: Created broadcast 16 from start at NativeMethodAccessorImpl.java:0
26/01/04 16:58:45 INFO BlockManagerInfo: Removed broadcast_12_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 16:58:45 INFO TorrentBroadcast: Started reading broadcast variable 16 with 1 pieces (estimated total size 4.0 MiB)
26/01/04 16:58:45 INFO BlockManagerInfo: Removed broadcast_12_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 16:58:45 INFO BlockManagerInfo: Removed broadcast_12_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 16:58:45 INFO BlockManagerInfo: Removed broadcast_13_piece0 on spark-master:33535 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 16:58:45 INFO BlockManagerInfo: Removed broadcast_13_piece0 on 172.18.0.14:43523 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 16:58:45 INFO BlockManagerInfo: Removed broadcast_15_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 16:58:45 INFO TorrentBroadcast: Reading broadcast variable 16 took 67 ms
26/01/04 16:58:45 INFO BlockManagerInfo: Removed broadcast_15_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 16:58:45 INFO BlockManagerInfo: Removed broadcast_15_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 16:58:45 INFO BlockManagerInfo: Removed broadcast_14_piece0 on spark-master:33535 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 16:58:45 INFO BlockManagerInfo: Removed broadcast_14_piece0 on 172.18.0.14:43523 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 16:58:45 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 4, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@1d1afd24]. The input RDD has 3 partitions.
26/01/04 16:58:45 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 16:58:45 INFO DAGScheduler: Got job 12 (start at NativeMethodAccessorImpl.java:0) with 3 output partitions
26/01/04 16:58:45 INFO DAGScheduler: Final stage: ResultStage 13 (start at NativeMethodAccessorImpl.java:0)
26/01/04 16:58:45 INFO DAGScheduler: Parents of final stage: List()
26/01/04 16:58:45 INFO DAGScheduler: Missing parents: List()
26/01/04 16:58:45 INFO DAGScheduler: Submitting ResultStage 13 (MapPartitionsRDD[66] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 16:58:45 INFO MemoryStore: Block broadcast_17 stored as values in memory (estimated size 52.9 KiB, free 434.3 MiB)
26/01/04 16:58:45 INFO MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 20.2 KiB, free 434.3 MiB)
26/01/04 16:58:45 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on spark-master:33535 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 16:58:45 INFO SparkContext: Created broadcast 17 from broadcast at DAGScheduler.scala:1585
26/01/04 16:58:45 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 13 (MapPartitionsRDD[66] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
26/01/04 16:58:45 INFO TaskSchedulerImpl: Adding task set 13.0 with 3 tasks resource profile 0
26/01/04 16:58:45 INFO TaskSetManager: Starting task 1.0 in stage 13.0 (TID 23) (172.18.0.14, executor 1, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/01/04 16:58:45 INFO TaskSetManager: Starting task 0.0 in stage 13.0 (TID 24) (172.18.0.13, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/01/04 16:58:45 INFO TaskSetManager: Starting task 2.0 in stage 13.0 (TID 25) (172.18.0.13, executor 0, partition 2, PROCESS_LOCAL, 8594 bytes) 
26/01/04 16:58:45 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on 172.18.0.14:43523 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 16:58:45 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on 172.18.0.13:44401 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 16:58:45 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on 172.18.0.14:43523 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 16:58:45 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on 172.18.0.13:44401 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 16:58:45 INFO TaskSetManager: Finished task 1.0 in stage 13.0 (TID 23) in 437 ms on 172.18.0.14 (executor 1) (1/3)
26/01/04 16:58:46 INFO TaskSetManager: Finished task 0.0 in stage 13.0 (TID 24) in 987 ms on 172.18.0.13 (executor 0) (2/3)
26/01/04 16:58:46 INFO TaskSetManager: Finished task 2.0 in stage 13.0 (TID 25) in 1063 ms on 172.18.0.13 (executor 0) (3/3)
26/01/04 16:58:46 INFO TaskSchedulerImpl: Removed TaskSet 13.0, whose tasks have all completed, from pool 
26/01/04 16:58:46 INFO DAGScheduler: ResultStage 13 (start at NativeMethodAccessorImpl.java:0) finished in 1.104 s
26/01/04 16:58:46 INFO DAGScheduler: Job 12 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 16:58:46 INFO TaskSchedulerImpl: Killing all running tasks in stage 13: Stage finished
26/01/04 16:58:46 INFO DAGScheduler: Job 12 finished: start at NativeMethodAccessorImpl.java:0, took 1.119042 s
26/01/04 16:58:46 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 4, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@1d1afd24] is committing.
26/01/04 16:58:46 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 4, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@1d1afd24] committed.
26/01/04 16:58:46 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/4 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.4.3cfd7fe3-22ac-4747-82e8-8b7f378e8714.tmp
26/01/04 16:58:46 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.4.3cfd7fe3-22ac-4747-82e8-8b7f378e8714.tmp to file:/tmp/spark-checkpoint-enrichment/commits/4
26/01/04 16:58:46 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T16:58:44.633Z",
  "batchId" : 4,
  "numInputRows" : 28,
  "inputRowsPerSecond" : 10.49082053203447,
  "processedRowsPerSecond" : 13.671875,
  "durationMs" : {
    "addBatch" : 1634,
    "commitOffsets" : 163,
    "getBatch" : 1,
    "latestOffset" : 9,
    "queryPlanning" : 129,
    "triggerExecution" : 2048,
    "walCommit" : 105
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 2348,
        "1" : 2625,
        "0" : 2053
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 2359,
        "1" : 2633,
        "0" : 2062
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 2359,
        "1" : 2633,
        "0" : 2062
      }
    },
    "numInputRows" : 28,
    "inputRowsPerSecond" : 10.49082053203447,
    "processedRowsPerSecond" : 13.671875,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 29
  }
}
26/01/04 16:58:53 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/5 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.5.ee30fe8a-de18-4474-b9f8-a799f9ce10ab.tmp
26/01/04 16:58:53 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.5.ee30fe8a-de18-4474-b9f8-a799f9ce10ab.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/5
26/01/04 16:58:53 INFO MicroBatchExecution: Committed offsets for batch 5. Metadata OffsetSeqMetadata(0,1767545933063,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 16:58:53 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 16:58:53 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 16:58:53 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 16:58:53 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 16:58:54 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 16:58:54 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 16:58:54 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#5503 - airline_prefix.nullCount#5502) > 0)
26/01/04 16:58:54 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#5538 - min_flight_num.nullCount#5537) > 0)
26/01/04 16:58:54 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#5533 - max_flight_num.nullCount#5532) > 0)
26/01/04 16:58:54 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 16:58:54 INFO DAGScheduler: Got job 13 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 16:58:54 INFO DAGScheduler: Final stage: ResultStage 14 (start at NativeMethodAccessorImpl.java:0)
26/01/04 16:58:54 INFO DAGScheduler: Parents of final stage: List()
26/01/04 16:58:54 INFO DAGScheduler: Missing parents: List()
26/01/04 16:58:54 INFO DAGScheduler: Submitting ResultStage 14 (MapPartitionsRDD[71] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 16:58:54 INFO MemoryStore: Block broadcast_18 stored as values in memory (estimated size 38.5 KiB, free 434.3 MiB)
26/01/04 16:58:54 INFO MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.3 MiB)
26/01/04 16:58:54 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 16:58:54 INFO SparkContext: Created broadcast 18 from broadcast at DAGScheduler.scala:1585
26/01/04 16:58:54 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 14 (MapPartitionsRDD[71] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 16:58:54 INFO TaskSchedulerImpl: Adding task set 14.0 with 2 tasks resource profile 0
26/01/04 16:58:54 INFO TaskSetManager: Starting task 0.0 in stage 14.0 (TID 26) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 16:58:54 INFO TaskSetManager: Starting task 1.0 in stage 14.0 (TID 27) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 16:58:54 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 16:58:54 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 16:58:54 INFO TaskSetManager: Finished task 1.0 in stage 14.0 (TID 27) in 233 ms on 172.18.0.13 (executor 0) (1/2)
26/01/04 16:58:54 INFO TaskSetManager: Finished task 0.0 in stage 14.0 (TID 26) in 241 ms on 172.18.0.14 (executor 1) (2/2)
26/01/04 16:58:54 INFO TaskSchedulerImpl: Removed TaskSet 14.0, whose tasks have all completed, from pool 
26/01/04 16:58:54 INFO DAGScheduler: ResultStage 14 (start at NativeMethodAccessorImpl.java:0) finished in 0.300 s
26/01/04 16:58:54 INFO DAGScheduler: Job 13 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 16:58:54 INFO TaskSchedulerImpl: Killing all running tasks in stage 14: Stage finished
26/01/04 16:58:54 INFO DAGScheduler: Job 13 finished: start at NativeMethodAccessorImpl.java:0, took 0.341914 s
26/01/04 16:58:54 INFO MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.3 MiB)
26/01/04 16:58:54 INFO BlockManagerInfo: Added broadcast_19_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 16:58:54 INFO SparkContext: Created broadcast 19 from start at NativeMethodAccessorImpl.java:0
26/01/04 16:58:54 INFO TorrentBroadcast: Started reading broadcast variable 19 with 1 pieces (estimated total size 4.0 MiB)
26/01/04 16:58:54 INFO TorrentBroadcast: Reading broadcast variable 19 took 0 ms
26/01/04 16:58:54 INFO BlockManagerInfo: Removed broadcast_17_piece0 on spark-master:33535 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 16:58:54 INFO BlockManagerInfo: Removed broadcast_17_piece0 on 172.18.0.13:44401 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 16:58:54 INFO BlockManagerInfo: Removed broadcast_17_piece0 on 172.18.0.14:43523 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 16:58:54 INFO BlockManagerInfo: Removed broadcast_18_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 16:58:54 INFO BlockManagerInfo: Removed broadcast_18_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 16:58:54 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 5, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@31080bf7]. The input RDD has 3 partitions.
26/01/04 16:58:54 INFO BlockManagerInfo: Removed broadcast_18_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 16:58:54 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 16:58:54 INFO DAGScheduler: Got job 14 (start at NativeMethodAccessorImpl.java:0) with 3 output partitions
26/01/04 16:58:54 INFO DAGScheduler: Final stage: ResultStage 15 (start at NativeMethodAccessorImpl.java:0)
26/01/04 16:58:54 INFO DAGScheduler: Parents of final stage: List()
26/01/04 16:58:54 INFO DAGScheduler: Missing parents: List()
26/01/04 16:58:54 INFO DAGScheduler: Submitting ResultStage 15 (MapPartitionsRDD[76] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 16:58:54 INFO MemoryStore: Block broadcast_20 stored as values in memory (estimated size 52.9 KiB, free 434.3 MiB)
26/01/04 16:58:54 INFO MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 20.2 KiB, free 434.3 MiB)
26/01/04 16:58:55 INFO BlockManagerInfo: Added broadcast_20_piece0 in memory on spark-master:33535 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 16:58:55 INFO SparkContext: Created broadcast 20 from broadcast at DAGScheduler.scala:1585
26/01/04 16:58:55 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 15 (MapPartitionsRDD[76] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
26/01/04 16:58:55 INFO TaskSchedulerImpl: Adding task set 15.0 with 3 tasks resource profile 0
26/01/04 16:58:55 INFO TaskSetManager: Starting task 0.0 in stage 15.0 (TID 28) (172.18.0.13, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/01/04 16:58:55 INFO TaskSetManager: Starting task 1.0 in stage 15.0 (TID 29) (172.18.0.14, executor 1, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/01/04 16:58:55 INFO TaskSetManager: Starting task 2.0 in stage 15.0 (TID 30) (172.18.0.13, executor 0, partition 2, PROCESS_LOCAL, 8594 bytes) 
26/01/04 16:58:55 INFO BlockManagerInfo: Added broadcast_20_piece0 in memory on 172.18.0.14:43523 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 16:58:55 INFO BlockManagerInfo: Added broadcast_20_piece0 in memory on 172.18.0.13:44401 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 16:58:55 INFO BlockManagerInfo: Added broadcast_19_piece0 in memory on 172.18.0.13:44401 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 16:58:55 INFO BlockManagerInfo: Added broadcast_19_piece0 in memory on 172.18.0.14:43523 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 16:58:56 INFO TaskSetManager: Finished task 1.0 in stage 15.0 (TID 29) in 1296 ms on 172.18.0.14 (executor 1) (1/3)
26/01/04 16:58:56 INFO TaskSetManager: Finished task 0.0 in stage 15.0 (TID 28) in 1357 ms on 172.18.0.13 (executor 0) (2/3)
26/01/04 16:58:56 INFO TaskSetManager: Finished task 2.0 in stage 15.0 (TID 30) in 1353 ms on 172.18.0.13 (executor 0) (3/3)
26/01/04 16:58:56 INFO TaskSchedulerImpl: Removed TaskSet 15.0, whose tasks have all completed, from pool 
26/01/04 16:58:56 INFO DAGScheduler: ResultStage 15 (start at NativeMethodAccessorImpl.java:0) finished in 1.499 s
26/01/04 16:58:56 INFO DAGScheduler: Job 14 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 16:58:56 INFO TaskSchedulerImpl: Killing all running tasks in stage 15: Stage finished
26/01/04 16:58:56 INFO DAGScheduler: Job 14 finished: start at NativeMethodAccessorImpl.java:0, took 1.516651 s
26/01/04 16:58:56 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 5, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@31080bf7] is committing.
26/01/04 16:58:56 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 5, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@31080bf7] committed.
26/01/04 16:58:56 INFO BlockManagerInfo: Removed broadcast_20_piece0 on spark-master:33535 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 16:58:56 INFO BlockManagerInfo: Removed broadcast_20_piece0 on 172.18.0.14:43523 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 16:58:56 INFO BlockManagerInfo: Removed broadcast_20_piece0 on 172.18.0.13:44401 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 16:58:56 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/5 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.5.ad024858-810f-4773-bbff-69c1f5aaeb54.tmp
26/01/04 16:58:56 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.5.ad024858-810f-4773-bbff-69c1f5aaeb54.tmp to file:/tmp/spark-checkpoint-enrichment/commits/5
26/01/04 16:58:56 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T16:58:53.020Z",
  "batchId" : 5,
  "numInputRows" : 30,
  "inputRowsPerSecond" : 1111.111111111111,
  "processedRowsPerSecond" : 8.116883116883116,
  "durationMs" : {
    "addBatch" : 2601,
    "commitOffsets" : 258,
    "getBatch" : 0,
    "latestOffset" : 43,
    "queryPlanning" : 533,
    "triggerExecution" : 3696,
    "walCommit" : 242
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 2359,
        "1" : 2633,
        "0" : 2062
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 2370,
        "1" : 2643,
        "0" : 2071
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 2370,
        "1" : 2643,
        "0" : 2071
      }
    },
    "numInputRows" : 30,
    "inputRowsPerSecond" : 1111.111111111111,
    "processedRowsPerSecond" : 8.116883116883116,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 31
  }
}
26/01/04 16:59:04 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/6 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.6.070f55bb-5de6-4189-8c38-42c32599d881.tmp
26/01/04 16:59:04 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.6.070f55bb-5de6-4189-8c38-42c32599d881.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/6
26/01/04 16:59:04 INFO MicroBatchExecution: Committed offsets for batch 6. Metadata OffsetSeqMetadata(0,1767545944103,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 16:59:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 16:59:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 16:59:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 16:59:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 16:59:05 INFO BlockManagerInfo: Removed broadcast_16_piece0 on spark-master:33535 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 16:59:05 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 16:59:05 INFO BlockManagerInfo: Removed broadcast_16_piece0 on 172.18.0.13:44401 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 16:59:05 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 16:59:05 INFO BlockManagerInfo: Removed broadcast_16_piece0 on 172.18.0.14:43523 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 16:59:05 INFO BlockManagerInfo: Removed broadcast_19_piece0 on spark-master:33535 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 16:59:05 INFO BlockManagerInfo: Removed broadcast_19_piece0 on 172.18.0.13:44401 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 16:59:05 INFO BlockManagerInfo: Removed broadcast_19_piece0 on 172.18.0.14:43523 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 16:59:05 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#6307 - airline_prefix.nullCount#6306) > 0)
26/01/04 16:59:05 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#6342 - min_flight_num.nullCount#6341) > 0)
26/01/04 16:59:05 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#6337 - max_flight_num.nullCount#6336) > 0)
26/01/04 16:59:05 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 16:59:05 INFO DAGScheduler: Got job 15 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 16:59:05 INFO DAGScheduler: Final stage: ResultStage 16 (start at NativeMethodAccessorImpl.java:0)
26/01/04 16:59:05 INFO DAGScheduler: Parents of final stage: List()
26/01/04 16:59:05 INFO DAGScheduler: Missing parents: List()
26/01/04 16:59:05 INFO DAGScheduler: Submitting ResultStage 16 (MapPartitionsRDD[81] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 16:59:05 INFO MemoryStore: Block broadcast_21 stored as values in memory (estimated size 38.5 KiB, free 434.4 MiB)
26/01/04 16:59:05 INFO MemoryStore: Block broadcast_21_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.3 MiB)
26/01/04 16:59:05 INFO BlockManagerInfo: Added broadcast_21_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 16:59:05 INFO SparkContext: Created broadcast 21 from broadcast at DAGScheduler.scala:1585
26/01/04 16:59:05 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 16 (MapPartitionsRDD[81] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 16:59:05 INFO TaskSchedulerImpl: Adding task set 16.0 with 2 tasks resource profile 0
26/01/04 16:59:05 INFO TaskSetManager: Starting task 1.0 in stage 16.0 (TID 31) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 16:59:05 INFO TaskSetManager: Starting task 0.0 in stage 16.0 (TID 32) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 16:59:05 INFO BlockManagerInfo: Added broadcast_21_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 16:59:05 INFO BlockManagerInfo: Added broadcast_21_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 16:59:05 INFO TaskSetManager: Finished task 0.0 in stage 16.0 (TID 32) in 174 ms on 172.18.0.14 (executor 1) (1/2)
26/01/04 16:59:06 INFO TaskSetManager: Finished task 1.0 in stage 16.0 (TID 31) in 322 ms on 172.18.0.13 (executor 0) (2/2)
26/01/04 16:59:06 INFO TaskSchedulerImpl: Removed TaskSet 16.0, whose tasks have all completed, from pool 
26/01/04 16:59:06 INFO DAGScheduler: ResultStage 16 (start at NativeMethodAccessorImpl.java:0) finished in 0.387 s
26/01/04 16:59:06 INFO DAGScheduler: Job 15 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 16:59:06 INFO TaskSchedulerImpl: Killing all running tasks in stage 16: Stage finished
26/01/04 16:59:06 INFO DAGScheduler: Job 15 finished: start at NativeMethodAccessorImpl.java:0, took 0.416602 s
26/01/04 16:59:06 INFO MemoryStore: Block broadcast_22_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.3 MiB)
26/01/04 16:59:06 INFO BlockManagerInfo: Added broadcast_22_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 16:59:06 INFO SparkContext: Created broadcast 22 from start at NativeMethodAccessorImpl.java:0
26/01/04 16:59:06 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 6, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@4c39ea91]. The input RDD has 1 partitions.
26/01/04 16:59:06 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 16:59:06 INFO DAGScheduler: Got job 16 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/01/04 16:59:06 INFO DAGScheduler: Final stage: ResultStage 17 (start at NativeMethodAccessorImpl.java:0)
26/01/04 16:59:06 INFO DAGScheduler: Parents of final stage: List()
26/01/04 16:59:06 INFO DAGScheduler: Missing parents: List()
26/01/04 16:59:06 INFO DAGScheduler: Submitting ResultStage 17 (MapPartitionsRDD[86] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 16:59:06 INFO MemoryStore: Block broadcast_23 stored as values in memory (estimated size 52.9 KiB, free 434.3 MiB)
26/01/04 16:59:06 INFO MemoryStore: Block broadcast_23_piece0 stored as bytes in memory (estimated size 20.2 KiB, free 434.3 MiB)
26/01/04 16:59:06 INFO BlockManagerInfo: Added broadcast_23_piece0 in memory on spark-master:33535 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 16:59:06 INFO SparkContext: Created broadcast 23 from broadcast at DAGScheduler.scala:1585
26/01/04 16:59:06 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 17 (MapPartitionsRDD[86] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/01/04 16:59:06 INFO TaskSchedulerImpl: Adding task set 17.0 with 1 tasks resource profile 0
26/01/04 16:59:06 INFO TaskSetManager: Starting task 0.0 in stage 17.0 (TID 33) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/01/04 16:59:06 INFO BlockManagerInfo: Added broadcast_23_piece0 in memory on 172.18.0.14:43523 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 16:59:06 INFO BlockManagerInfo: Added broadcast_22_piece0 in memory on 172.18.0.14:43523 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 16:59:07 INFO TaskSetManager: Finished task 0.0 in stage 17.0 (TID 33) in 1135 ms on 172.18.0.14 (executor 1) (1/1)
26/01/04 16:59:07 INFO TaskSchedulerImpl: Removed TaskSet 17.0, whose tasks have all completed, from pool 
26/01/04 16:59:07 INFO DAGScheduler: ResultStage 17 (start at NativeMethodAccessorImpl.java:0) finished in 1.158 s
26/01/04 16:59:07 INFO DAGScheduler: Job 16 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 16:59:07 INFO TaskSchedulerImpl: Killing all running tasks in stage 17: Stage finished
26/01/04 16:59:07 INFO DAGScheduler: Job 16 finished: start at NativeMethodAccessorImpl.java:0, took 1.168954 s
26/01/04 16:59:07 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 6, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@4c39ea91] is committing.
26/01/04 16:59:07 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 6, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@4c39ea91] committed.
26/01/04 16:59:07 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/6 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.6.33ad7765-0bda-4cf9-9f98-9e9b5e23ceb5.tmp
26/01/04 16:59:07 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.6.33ad7765-0bda-4cf9-9f98-9e9b5e23ceb5.tmp to file:/tmp/spark-checkpoint-enrichment/commits/6
26/01/04 16:59:07 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T16:59:04.063Z",
  "batchId" : 6,
  "numInputRows" : 2,
  "inputRowsPerSecond" : 66.66666666666667,
  "processedRowsPerSecond" : 0.562429696287964,
  "durationMs" : {
    "addBatch" : 2756,
    "commitOffsets" : 260,
    "getBatch" : 0,
    "latestOffset" : 40,
    "queryPlanning" : 287,
    "triggerExecution" : 3556,
    "walCommit" : 209
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 2370,
        "1" : 2643,
        "0" : 2071
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 2370,
        "1" : 2645,
        "0" : 2071
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 2370,
        "1" : 2645,
        "0" : 2071
      }
    },
    "numInputRows" : 2,
    "inputRowsPerSecond" : 66.66666666666667,
    "processedRowsPerSecond" : 0.562429696287964,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 2
  }
}
26/01/04 16:59:07 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/7 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.7.fb58113f-577c-40a1-8735-e340077cc851.tmp
26/01/04 16:59:07 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.7.fb58113f-577c-40a1-8735-e340077cc851.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/7
26/01/04 16:59:07 INFO MicroBatchExecution: Committed offsets for batch 7. Metadata OffsetSeqMetadata(0,1767545947636,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 16:59:08 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 16:59:08 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 16:59:08 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 16:59:08 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 16:59:08 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 16:59:08 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 16:59:08 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#7111 - airline_prefix.nullCount#7110) > 0)
26/01/04 16:59:08 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#7146 - min_flight_num.nullCount#7145) > 0)
26/01/04 16:59:08 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#7141 - max_flight_num.nullCount#7140) > 0)
26/01/04 16:59:08 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 16:59:08 INFO DAGScheduler: Got job 17 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 16:59:08 INFO DAGScheduler: Final stage: ResultStage 18 (start at NativeMethodAccessorImpl.java:0)
26/01/04 16:59:08 INFO DAGScheduler: Parents of final stage: List()
26/01/04 16:59:08 INFO DAGScheduler: Missing parents: List()
26/01/04 16:59:08 INFO DAGScheduler: Submitting ResultStage 18 (MapPartitionsRDD[91] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 16:59:08 INFO MemoryStore: Block broadcast_24 stored as values in memory (estimated size 38.5 KiB, free 434.2 MiB)
26/01/04 16:59:08 INFO MemoryStore: Block broadcast_24_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.2 MiB)
26/01/04 16:59:08 INFO BlockManagerInfo: Added broadcast_24_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 16:59:08 INFO SparkContext: Created broadcast 24 from broadcast at DAGScheduler.scala:1585
26/01/04 16:59:08 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 18 (MapPartitionsRDD[91] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 16:59:08 INFO TaskSchedulerImpl: Adding task set 18.0 with 2 tasks resource profile 0
26/01/04 16:59:08 INFO TaskSetManager: Starting task 0.0 in stage 18.0 (TID 34) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 16:59:08 INFO TaskSetManager: Starting task 1.0 in stage 18.0 (TID 35) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 16:59:08 INFO BlockManagerInfo: Removed broadcast_23_piece0 on spark-master:33535 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 16:59:08 INFO BlockManagerInfo: Removed broadcast_23_piece0 on 172.18.0.14:43523 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 16:59:08 INFO BlockManagerInfo: Added broadcast_24_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 16:59:09 INFO BlockManagerInfo: Removed broadcast_22_piece0 on spark-master:33535 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 16:59:09 INFO BlockManagerInfo: Removed broadcast_22_piece0 on 172.18.0.14:43523 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 16:59:09 INFO BlockManagerInfo: Added broadcast_24_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 16:59:09 INFO BlockManagerInfo: Removed broadcast_21_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 16:59:09 INFO BlockManagerInfo: Removed broadcast_21_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 16:59:09 INFO TaskSetManager: Finished task 0.0 in stage 18.0 (TID 34) in 358 ms on 172.18.0.14 (executor 1) (1/2)
26/01/04 16:59:09 INFO BlockManagerInfo: Removed broadcast_21_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 16:59:09 INFO TaskSetManager: Finished task 1.0 in stage 18.0 (TID 35) in 388 ms on 172.18.0.13 (executor 0) (2/2)
26/01/04 16:59:09 INFO TaskSchedulerImpl: Removed TaskSet 18.0, whose tasks have all completed, from pool 
26/01/04 16:59:09 INFO DAGScheduler: ResultStage 18 (start at NativeMethodAccessorImpl.java:0) finished in 0.526 s
26/01/04 16:59:09 INFO DAGScheduler: Job 17 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 16:59:09 INFO TaskSchedulerImpl: Killing all running tasks in stage 18: Stage finished
26/01/04 16:59:09 INFO DAGScheduler: Job 17 finished: start at NativeMethodAccessorImpl.java:0, took 0.630018 s
26/01/04 16:59:09 INFO MemoryStore: Block broadcast_25_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.3 MiB)
26/01/04 16:59:09 INFO BlockManagerInfo: Added broadcast_25_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 16:59:09 INFO SparkContext: Created broadcast 25 from start at NativeMethodAccessorImpl.java:0
26/01/04 16:59:09 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 7, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@6f6b5db9]. The input RDD has 3 partitions.
26/01/04 16:59:09 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 16:59:09 INFO DAGScheduler: Got job 18 (start at NativeMethodAccessorImpl.java:0) with 3 output partitions
26/01/04 16:59:09 INFO DAGScheduler: Final stage: ResultStage 19 (start at NativeMethodAccessorImpl.java:0)
26/01/04 16:59:09 INFO DAGScheduler: Parents of final stage: List()
26/01/04 16:59:09 INFO DAGScheduler: Missing parents: List()
26/01/04 16:59:09 INFO DAGScheduler: Submitting ResultStage 19 (MapPartitionsRDD[96] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 16:59:09 INFO MemoryStore: Block broadcast_26 stored as values in memory (estimated size 52.9 KiB, free 434.3 MiB)
26/01/04 16:59:09 INFO MemoryStore: Block broadcast_26_piece0 stored as bytes in memory (estimated size 20.2 KiB, free 434.3 MiB)
26/01/04 16:59:09 INFO BlockManagerInfo: Added broadcast_26_piece0 in memory on spark-master:33535 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 16:59:09 INFO SparkContext: Created broadcast 26 from broadcast at DAGScheduler.scala:1585
26/01/04 16:59:09 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 19 (MapPartitionsRDD[96] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
26/01/04 16:59:09 INFO TaskSchedulerImpl: Adding task set 19.0 with 3 tasks resource profile 0
26/01/04 16:59:09 INFO TaskSetManager: Starting task 0.0 in stage 19.0 (TID 36) (172.18.0.13, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/01/04 16:59:09 INFO TaskSetManager: Starting task 1.0 in stage 19.0 (TID 37) (172.18.0.14, executor 1, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/01/04 16:59:09 INFO TaskSetManager: Starting task 2.0 in stage 19.0 (TID 38) (172.18.0.13, executor 0, partition 2, PROCESS_LOCAL, 8594 bytes) 
26/01/04 16:59:09 INFO BlockManagerInfo: Added broadcast_26_piece0 in memory on 172.18.0.14:43523 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 16:59:09 INFO BlockManagerInfo: Removed broadcast_24_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 16:59:09 INFO BlockManagerInfo: Removed broadcast_24_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 16:59:09 INFO BlockManagerInfo: Removed broadcast_24_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 16:59:09 INFO BlockManagerInfo: Added broadcast_26_piece0 in memory on 172.18.0.13:44401 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 16:59:09 INFO BlockManagerInfo: Added broadcast_25_piece0 in memory on 172.18.0.13:44401 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 16:59:09 INFO BlockManagerInfo: Added broadcast_25_piece0 in memory on 172.18.0.14:43523 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 16:59:10 INFO TaskSetManager: Finished task 1.0 in stage 19.0 (TID 37) in 843 ms on 172.18.0.14 (executor 1) (1/3)
26/01/04 16:59:11 INFO TaskSetManager: Finished task 2.0 in stage 19.0 (TID 38) in 1792 ms on 172.18.0.13 (executor 0) (2/3)
26/01/04 16:59:11 INFO TaskSetManager: Finished task 0.0 in stage 19.0 (TID 36) in 1805 ms on 172.18.0.13 (executor 0) (3/3)
26/01/04 16:59:11 INFO TaskSchedulerImpl: Removed TaskSet 19.0, whose tasks have all completed, from pool 
26/01/04 16:59:11 INFO DAGScheduler: ResultStage 19 (start at NativeMethodAccessorImpl.java:0) finished in 1.866 s
26/01/04 16:59:11 INFO DAGScheduler: Job 18 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 16:59:11 INFO TaskSchedulerImpl: Killing all running tasks in stage 19: Stage finished
26/01/04 16:59:11 INFO DAGScheduler: Job 18 finished: start at NativeMethodAccessorImpl.java:0, took 1.886743 s
26/01/04 16:59:11 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 7, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@6f6b5db9] is committing.
26/01/04 16:59:11 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 7, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@6f6b5db9] committed.
26/01/04 16:59:11 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/7 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.7.928cfb8c-d05a-414f-bfd0-d582c24ec4fa.tmp
26/01/04 16:59:11 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.7.928cfb8c-d05a-414f-bfd0-d582c24ec4fa.tmp to file:/tmp/spark-checkpoint-enrichment/commits/7
26/01/04 16:59:11 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T16:59:07.625Z",
  "batchId" : 7,
  "numInputRows" : 28,
  "inputRowsPerSecond" : 7.860752386299832,
  "processedRowsPerSecond" : 7.04048277596178,
  "durationMs" : {
    "addBatch" : 3096,
    "commitOffsets" : 332,
    "getBatch" : 3,
    "latestOffset" : 11,
    "queryPlanning" : 265,
    "triggerExecution" : 3977,
    "walCommit" : 261
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 2370,
        "1" : 2645,
        "0" : 2071
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 2381,
        "1" : 2653,
        "0" : 2080
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 2381,
        "1" : 2653,
        "0" : 2080
      }
    },
    "numInputRows" : 28,
    "inputRowsPerSecond" : 7.860752386299832,
    "processedRowsPerSecond" : 7.04048277596178,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 29
  }
}
26/01/04 16:59:15 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/8 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.8.f1c2207f-0083-48fc-8179-965dcc4779c5.tmp
26/01/04 16:59:15 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.8.f1c2207f-0083-48fc-8179-965dcc4779c5.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/8
26/01/04 16:59:15 INFO MicroBatchExecution: Committed offsets for batch 8. Metadata OffsetSeqMetadata(0,1767545955178,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 16:59:15 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 16:59:15 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 16:59:15 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 16:59:15 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 16:59:16 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 16:59:16 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 16:59:16 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#7915 - airline_prefix.nullCount#7914) > 0)
26/01/04 16:59:16 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#7950 - min_flight_num.nullCount#7949) > 0)
26/01/04 16:59:16 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#7945 - max_flight_num.nullCount#7944) > 0)
26/01/04 16:59:16 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 16:59:16 INFO DAGScheduler: Got job 19 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 16:59:16 INFO DAGScheduler: Final stage: ResultStage 20 (start at NativeMethodAccessorImpl.java:0)
26/01/04 16:59:16 INFO DAGScheduler: Parents of final stage: List()
26/01/04 16:59:16 INFO DAGScheduler: Missing parents: List()
26/01/04 16:59:16 INFO DAGScheduler: Submitting ResultStage 20 (MapPartitionsRDD[101] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 16:59:16 INFO MemoryStore: Block broadcast_27 stored as values in memory (estimated size 38.5 KiB, free 434.3 MiB)
26/01/04 16:59:16 INFO MemoryStore: Block broadcast_27_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.3 MiB)
26/01/04 16:59:16 INFO BlockManagerInfo: Added broadcast_27_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 16:59:16 INFO SparkContext: Created broadcast 27 from broadcast at DAGScheduler.scala:1585
26/01/04 16:59:16 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 20 (MapPartitionsRDD[101] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 16:59:16 INFO TaskSchedulerImpl: Adding task set 20.0 with 2 tasks resource profile 0
26/01/04 16:59:16 INFO TaskSetManager: Starting task 0.0 in stage 20.0 (TID 39) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 16:59:16 INFO TaskSetManager: Starting task 1.0 in stage 20.0 (TID 40) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 16:59:16 INFO BlockManagerInfo: Added broadcast_27_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 16:59:16 INFO BlockManagerInfo: Added broadcast_27_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 16:59:16 INFO TaskSetManager: Finished task 0.0 in stage 20.0 (TID 39) in 179 ms on 172.18.0.14 (executor 1) (1/2)
26/01/04 16:59:16 INFO TaskSetManager: Finished task 1.0 in stage 20.0 (TID 40) in 255 ms on 172.18.0.13 (executor 0) (2/2)
26/01/04 16:59:16 INFO TaskSchedulerImpl: Removed TaskSet 20.0, whose tasks have all completed, from pool 
26/01/04 16:59:16 INFO DAGScheduler: ResultStage 20 (start at NativeMethodAccessorImpl.java:0) finished in 0.310 s
26/01/04 16:59:16 INFO DAGScheduler: Job 19 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 16:59:16 INFO TaskSchedulerImpl: Killing all running tasks in stage 20: Stage finished
26/01/04 16:59:16 INFO DAGScheduler: Job 19 finished: start at NativeMethodAccessorImpl.java:0, took 0.345423 s
26/01/04 16:59:16 INFO MemoryStore: Block broadcast_28_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.3 MiB)
26/01/04 16:59:16 INFO BlockManagerInfo: Added broadcast_28_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 16:59:16 INFO SparkContext: Created broadcast 28 from start at NativeMethodAccessorImpl.java:0
26/01/04 16:59:16 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 8, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@27c636f0]. The input RDD has 3 partitions.
26/01/04 16:59:16 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 16:59:16 INFO DAGScheduler: Got job 20 (start at NativeMethodAccessorImpl.java:0) with 3 output partitions
26/01/04 16:59:16 INFO DAGScheduler: Final stage: ResultStage 21 (start at NativeMethodAccessorImpl.java:0)
26/01/04 16:59:16 INFO DAGScheduler: Parents of final stage: List()
26/01/04 16:59:16 INFO DAGScheduler: Missing parents: List()
26/01/04 16:59:16 INFO DAGScheduler: Submitting ResultStage 21 (MapPartitionsRDD[106] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 16:59:16 INFO MemoryStore: Block broadcast_29 stored as values in memory (estimated size 52.9 KiB, free 434.2 MiB)
26/01/04 16:59:16 INFO MemoryStore: Block broadcast_29_piece0 stored as bytes in memory (estimated size 20.2 KiB, free 434.2 MiB)
26/01/04 16:59:16 INFO BlockManagerInfo: Added broadcast_29_piece0 in memory on spark-master:33535 (size: 20.2 KiB, free: 434.3 MiB)
26/01/04 16:59:16 INFO SparkContext: Created broadcast 29 from broadcast at DAGScheduler.scala:1585
26/01/04 16:59:16 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 21 (MapPartitionsRDD[106] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
26/01/04 16:59:16 INFO TaskSchedulerImpl: Adding task set 21.0 with 3 tasks resource profile 0
26/01/04 16:59:16 INFO TaskSetManager: Starting task 1.0 in stage 21.0 (TID 41) (172.18.0.14, executor 1, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/01/04 16:59:16 INFO TaskSetManager: Starting task 0.0 in stage 21.0 (TID 42) (172.18.0.13, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/01/04 16:59:16 INFO TaskSetManager: Starting task 2.0 in stage 21.0 (TID 43) (172.18.0.13, executor 0, partition 2, PROCESS_LOCAL, 8594 bytes) 
26/01/04 16:59:16 INFO BlockManagerInfo: Removed broadcast_27_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 16:59:16 INFO BlockManagerInfo: Removed broadcast_27_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 16:59:16 INFO BlockManagerInfo: Removed broadcast_27_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 16:59:16 INFO BlockManagerInfo: Added broadcast_29_piece0 in memory on 172.18.0.14:43523 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 16:59:16 INFO BlockManagerInfo: Removed broadcast_26_piece0 on 172.18.0.13:44401 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 16:59:16 INFO BlockManagerInfo: Removed broadcast_26_piece0 on spark-master:33535 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 16:59:16 INFO BlockManagerInfo: Added broadcast_29_piece0 in memory on 172.18.0.13:44401 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 16:59:16 INFO BlockManagerInfo: Removed broadcast_26_piece0 on 172.18.0.14:43523 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 16:59:16 INFO BlockManagerInfo: Added broadcast_28_piece0 in memory on 172.18.0.14:43523 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 16:59:17 INFO BlockManagerInfo: Added broadcast_28_piece0 in memory on 172.18.0.13:44401 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 16:59:17 INFO TaskSetManager: Finished task 1.0 in stage 21.0 (TID 41) in 1218 ms on 172.18.0.14 (executor 1) (1/3)
26/01/04 16:59:17 INFO TaskSetManager: Finished task 2.0 in stage 21.0 (TID 43) in 1235 ms on 172.18.0.13 (executor 0) (2/3)
26/01/04 16:59:18 INFO TaskSetManager: Finished task 0.0 in stage 21.0 (TID 42) in 1386 ms on 172.18.0.13 (executor 0) (3/3)
26/01/04 16:59:18 INFO TaskSchedulerImpl: Removed TaskSet 21.0, whose tasks have all completed, from pool 
26/01/04 16:59:18 INFO DAGScheduler: ResultStage 21 (start at NativeMethodAccessorImpl.java:0) finished in 1.461 s
26/01/04 16:59:18 INFO DAGScheduler: Job 20 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 16:59:18 INFO TaskSchedulerImpl: Killing all running tasks in stage 21: Stage finished
26/01/04 16:59:18 INFO DAGScheduler: Job 20 finished: start at NativeMethodAccessorImpl.java:0, took 1.480058 s
26/01/04 16:59:18 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 8, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@27c636f0] is committing.
26/01/04 16:59:18 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 8, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@27c636f0] committed.
26/01/04 16:59:18 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/8 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.8.0337f1bd-6ec8-4eec-8a76-9004b06c49ea.tmp
26/01/04 16:59:18 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.8.0337f1bd-6ec8-4eec-8a76-9004b06c49ea.tmp to file:/tmp/spark-checkpoint-enrichment/commits/8
26/01/04 16:59:18 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T16:59:15.171Z",
  "batchId" : 8,
  "numInputRows" : 30,
  "inputRowsPerSecond" : 1764.705882352941,
  "processedRowsPerSecond" : 9.59079283887468,
  "durationMs" : {
    "addBatch" : 2558,
    "commitOffsets" : 171,
    "getBatch" : 0,
    "latestOffset" : 7,
    "queryPlanning" : 231,
    "triggerExecution" : 3128,
    "walCommit" : 159
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 2381,
        "1" : 2653,
        "0" : 2080
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 2392,
        "1" : 2663,
        "0" : 2089
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 2392,
        "1" : 2663,
        "0" : 2089
      }
    },
    "numInputRows" : 30,
    "inputRowsPerSecond" : 1764.705882352941,
    "processedRowsPerSecond" : 9.59079283887468,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 31
  }
}
26/01/04 16:59:26 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/9 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.9.39334b48-c03f-47cd-91cc-23807f45b73c.tmp
26/01/04 16:59:26 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.9.39334b48-c03f-47cd-91cc-23807f45b73c.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/9
26/01/04 16:59:26 INFO MicroBatchExecution: Committed offsets for batch 9. Metadata OffsetSeqMetadata(0,1767545966420,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 16:59:26 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 16:59:26 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 16:59:26 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 16:59:26 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 16:59:27 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 16:59:27 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 16:59:27 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#8719 - airline_prefix.nullCount#8718) > 0)
26/01/04 16:59:27 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#8754 - min_flight_num.nullCount#8753) > 0)
26/01/04 16:59:27 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#8749 - max_flight_num.nullCount#8748) > 0)
26/01/04 16:59:27 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 16:59:27 INFO DAGScheduler: Got job 21 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 16:59:27 INFO DAGScheduler: Final stage: ResultStage 22 (start at NativeMethodAccessorImpl.java:0)
26/01/04 16:59:27 INFO DAGScheduler: Parents of final stage: List()
26/01/04 16:59:27 INFO DAGScheduler: Missing parents: List()
26/01/04 16:59:27 INFO DAGScheduler: Submitting ResultStage 22 (MapPartitionsRDD[111] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 16:59:27 INFO MemoryStore: Block broadcast_30 stored as values in memory (estimated size 38.5 KiB, free 434.3 MiB)
26/01/04 16:59:27 INFO MemoryStore: Block broadcast_30_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.3 MiB)
26/01/04 16:59:27 INFO BlockManagerInfo: Added broadcast_30_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 16:59:27 INFO SparkContext: Created broadcast 30 from broadcast at DAGScheduler.scala:1585
26/01/04 16:59:27 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 22 (MapPartitionsRDD[111] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 16:59:27 INFO TaskSchedulerImpl: Adding task set 22.0 with 2 tasks resource profile 0
26/01/04 16:59:27 INFO TaskSetManager: Starting task 0.0 in stage 22.0 (TID 44) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 16:59:27 INFO TaskSetManager: Starting task 1.0 in stage 22.0 (TID 45) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 16:59:27 INFO BlockManagerInfo: Added broadcast_30_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 16:59:27 INFO BlockManagerInfo: Added broadcast_30_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 16:59:27 INFO TaskSetManager: Finished task 1.0 in stage 22.0 (TID 45) in 166 ms on 172.18.0.13 (executor 0) (1/2)
26/01/04 16:59:27 INFO TaskSetManager: Finished task 0.0 in stage 22.0 (TID 44) in 254 ms on 172.18.0.14 (executor 1) (2/2)
26/01/04 16:59:27 INFO BlockManagerInfo: Removed broadcast_28_piece0 on spark-master:33535 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 16:59:27 INFO DAGScheduler: ResultStage 22 (start at NativeMethodAccessorImpl.java:0) finished in 0.340 s
26/01/04 16:59:27 INFO DAGScheduler: Job 21 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 16:59:27 INFO BlockManagerInfo: Removed broadcast_28_piece0 on 172.18.0.14:43523 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 16:59:27 INFO BlockManagerInfo: Removed broadcast_28_piece0 on 172.18.0.13:44401 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 16:59:27 INFO TaskSchedulerImpl: Removed TaskSet 22.0, whose tasks have all completed, from pool 
26/01/04 16:59:27 INFO TaskSchedulerImpl: Killing all running tasks in stage 22: Stage finished
26/01/04 16:59:27 INFO DAGScheduler: Job 21 finished: start at NativeMethodAccessorImpl.java:0, took 0.433295 s
26/01/04 16:59:27 INFO MemoryStore: Block broadcast_31_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.3 MiB)
26/01/04 16:59:27 INFO BlockManagerInfo: Added broadcast_31_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 16:59:27 INFO SparkContext: Created broadcast 31 from start at NativeMethodAccessorImpl.java:0
26/01/04 16:59:27 INFO BlockManagerInfo: Removed broadcast_25_piece0 on 172.18.0.13:44401 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 16:59:27 INFO BlockManagerInfo: Removed broadcast_25_piece0 on spark-master:33535 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 16:59:27 INFO BlockManagerInfo: Removed broadcast_25_piece0 on 172.18.0.14:43523 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 16:59:27 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 9, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@c831e91]. The input RDD has 3 partitions.
26/01/04 16:59:27 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 16:59:27 INFO DAGScheduler: Got job 22 (start at NativeMethodAccessorImpl.java:0) with 3 output partitions
26/01/04 16:59:27 INFO DAGScheduler: Final stage: ResultStage 23 (start at NativeMethodAccessorImpl.java:0)
26/01/04 16:59:27 INFO DAGScheduler: Parents of final stage: List()
26/01/04 16:59:27 INFO DAGScheduler: Missing parents: List()
26/01/04 16:59:27 INFO DAGScheduler: Submitting ResultStage 23 (MapPartitionsRDD[116] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 16:59:27 INFO MemoryStore: Block broadcast_32 stored as values in memory (estimated size 52.9 KiB, free 434.2 MiB)
26/01/04 16:59:27 INFO BlockManagerInfo: Removed broadcast_29_piece0 on spark-master:33535 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 16:59:27 INFO MemoryStore: Block broadcast_32_piece0 stored as bytes in memory (estimated size 20.2 KiB, free 434.3 MiB)
26/01/04 16:59:27 INFO BlockManagerInfo: Added broadcast_32_piece0 in memory on spark-master:33535 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 16:59:27 INFO SparkContext: Created broadcast 32 from broadcast at DAGScheduler.scala:1585
26/01/04 16:59:27 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 23 (MapPartitionsRDD[116] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
26/01/04 16:59:27 INFO TaskSchedulerImpl: Adding task set 23.0 with 3 tasks resource profile 0
26/01/04 16:59:27 INFO TaskSetManager: Starting task 1.0 in stage 23.0 (TID 46) (172.18.0.14, executor 1, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/01/04 16:59:27 INFO TaskSetManager: Starting task 0.0 in stage 23.0 (TID 47) (172.18.0.13, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/01/04 16:59:27 INFO TaskSetManager: Starting task 2.0 in stage 23.0 (TID 48) (172.18.0.13, executor 0, partition 2, PROCESS_LOCAL, 8594 bytes) 
26/01/04 16:59:27 INFO BlockManagerInfo: Removed broadcast_29_piece0 on 172.18.0.14:43523 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 16:59:27 INFO BlockManagerInfo: Removed broadcast_29_piece0 on 172.18.0.13:44401 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 16:59:27 INFO BlockManagerInfo: Added broadcast_32_piece0 in memory on 172.18.0.13:44401 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 16:59:27 INFO BlockManagerInfo: Added broadcast_32_piece0 in memory on 172.18.0.14:43523 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 16:59:27 INFO BlockManagerInfo: Added broadcast_31_piece0 in memory on 172.18.0.13:44401 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 16:59:27 INFO BlockManagerInfo: Added broadcast_31_piece0 in memory on 172.18.0.14:43523 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 16:59:28 INFO TaskSetManager: Finished task 1.0 in stage 23.0 (TID 46) in 989 ms on 172.18.0.14 (executor 1) (1/3)
26/01/04 16:59:28 INFO TaskSetManager: Finished task 2.0 in stage 23.0 (TID 48) in 1180 ms on 172.18.0.13 (executor 0) (2/3)
26/01/04 16:59:28 INFO TaskSetManager: Finished task 0.0 in stage 23.0 (TID 47) in 1183 ms on 172.18.0.13 (executor 0) (3/3)
26/01/04 16:59:28 INFO TaskSchedulerImpl: Removed TaskSet 23.0, whose tasks have all completed, from pool 
26/01/04 16:59:28 INFO DAGScheduler: ResultStage 23 (start at NativeMethodAccessorImpl.java:0) finished in 1.223 s
26/01/04 16:59:28 INFO DAGScheduler: Job 22 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 16:59:28 INFO TaskSchedulerImpl: Killing all running tasks in stage 23: Stage finished
26/01/04 16:59:29 INFO DAGScheduler: Job 22 finished: start at NativeMethodAccessorImpl.java:0, took 1.247833 s
26/01/04 16:59:29 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 9, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@c831e91] is committing.
26/01/04 16:59:29 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 9, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@c831e91] committed.
26/01/04 16:59:29 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/9 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.9.578912e2-be12-4415-b81a-d44d36ceb5d8.tmp
26/01/04 16:59:29 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.9.578912e2-be12-4415-b81a-d44d36ceb5d8.tmp to file:/tmp/spark-checkpoint-enrichment/commits/9
26/01/04 16:59:29 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T16:59:26.391Z",
  "batchId" : 9,
  "numInputRows" : 30,
  "inputRowsPerSecond" : 1034.4827586206895,
  "processedRowsPerSecond" : 10.559662090813093,
  "durationMs" : {
    "addBatch" : 2241,
    "commitOffsets" : 228,
    "getBatch" : 1,
    "latestOffset" : 29,
    "queryPlanning" : 174,
    "triggerExecution" : 2841,
    "walCommit" : 168
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 2392,
        "1" : 2663,
        "0" : 2089
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 2403,
        "1" : 2673,
        "0" : 2098
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 2403,
        "1" : 2673,
        "0" : 2098
      }
    },
    "numInputRows" : 30,
    "inputRowsPerSecond" : 1034.4827586206895,
    "processedRowsPerSecond" : 10.559662090813093,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 31
  }
}
26/01/04 16:59:37 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/10 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.10.0cbe77f7-c5f8-4a12-88cb-87ff5761fe71.tmp
26/01/04 16:59:37 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.10.0cbe77f7-c5f8-4a12-88cb-87ff5761fe71.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/10
26/01/04 16:59:37 INFO MicroBatchExecution: Committed offsets for batch 10. Metadata OffsetSeqMetadata(0,1767545977427,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 16:59:37 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 16:59:37 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 16:59:37 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 16:59:37 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 16:59:37 INFO BlockManagerInfo: Removed broadcast_32_piece0 on spark-master:33535 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 16:59:37 INFO BlockManagerInfo: Removed broadcast_32_piece0 on 172.18.0.13:44401 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 16:59:37 INFO BlockManagerInfo: Removed broadcast_32_piece0 on 172.18.0.14:43523 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 16:59:37 INFO BlockManagerInfo: Removed broadcast_31_piece0 on spark-master:33535 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 16:59:37 INFO BlockManagerInfo: Removed broadcast_31_piece0 on 172.18.0.13:44401 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 16:59:37 INFO BlockManagerInfo: Removed broadcast_31_piece0 on 172.18.0.14:43523 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 16:59:37 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 16:59:37 INFO BlockManagerInfo: Removed broadcast_30_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 16:59:37 INFO BlockManagerInfo: Removed broadcast_30_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 16:59:37 INFO BlockManagerInfo: Removed broadcast_30_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 16:59:37 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 16:59:38 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#9523 - airline_prefix.nullCount#9522) > 0)
26/01/04 16:59:38 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#9558 - min_flight_num.nullCount#9557) > 0)
26/01/04 16:59:38 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#9553 - max_flight_num.nullCount#9552) > 0)
26/01/04 16:59:38 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 16:59:38 INFO DAGScheduler: Got job 23 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 16:59:38 INFO DAGScheduler: Final stage: ResultStage 24 (start at NativeMethodAccessorImpl.java:0)
26/01/04 16:59:38 INFO DAGScheduler: Parents of final stage: List()
26/01/04 16:59:38 INFO DAGScheduler: Missing parents: List()
26/01/04 16:59:38 INFO DAGScheduler: Submitting ResultStage 24 (MapPartitionsRDD[121] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 16:59:38 INFO MemoryStore: Block broadcast_33 stored as values in memory (estimated size 38.5 KiB, free 434.4 MiB)
26/01/04 16:59:38 INFO MemoryStore: Block broadcast_33_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.3 MiB)
26/01/04 16:59:38 INFO BlockManagerInfo: Added broadcast_33_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 16:59:38 INFO SparkContext: Created broadcast 33 from broadcast at DAGScheduler.scala:1585
26/01/04 16:59:38 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 24 (MapPartitionsRDD[121] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 16:59:38 INFO TaskSchedulerImpl: Adding task set 24.0 with 2 tasks resource profile 0
26/01/04 16:59:38 INFO TaskSetManager: Starting task 1.0 in stage 24.0 (TID 49) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 16:59:38 INFO TaskSetManager: Starting task 0.0 in stage 24.0 (TID 50) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 16:59:38 INFO BlockManagerInfo: Added broadcast_33_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 16:59:38 INFO BlockManagerInfo: Added broadcast_33_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 16:59:38 INFO TaskSetManager: Finished task 1.0 in stage 24.0 (TID 49) in 224 ms on 172.18.0.13 (executor 0) (1/2)
26/01/04 16:59:38 INFO TaskSetManager: Finished task 0.0 in stage 24.0 (TID 50) in 219 ms on 172.18.0.14 (executor 1) (2/2)
26/01/04 16:59:38 INFO TaskSchedulerImpl: Removed TaskSet 24.0, whose tasks have all completed, from pool 
26/01/04 16:59:38 INFO DAGScheduler: ResultStage 24 (start at NativeMethodAccessorImpl.java:0) finished in 0.282 s
26/01/04 16:59:38 INFO DAGScheduler: Job 23 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 16:59:38 INFO TaskSchedulerImpl: Killing all running tasks in stage 24: Stage finished
26/01/04 16:59:38 INFO DAGScheduler: Job 23 finished: start at NativeMethodAccessorImpl.java:0, took 0.314094 s
26/01/04 16:59:38 INFO MemoryStore: Block broadcast_34_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.3 MiB)
26/01/04 16:59:38 INFO BlockManagerInfo: Added broadcast_34_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 16:59:38 INFO SparkContext: Created broadcast 34 from start at NativeMethodAccessorImpl.java:0
26/01/04 16:59:38 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 10, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@5246d8b3]. The input RDD has 3 partitions.
26/01/04 16:59:38 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 16:59:38 INFO DAGScheduler: Got job 24 (start at NativeMethodAccessorImpl.java:0) with 3 output partitions
26/01/04 16:59:38 INFO DAGScheduler: Final stage: ResultStage 25 (start at NativeMethodAccessorImpl.java:0)
26/01/04 16:59:38 INFO DAGScheduler: Parents of final stage: List()
26/01/04 16:59:38 INFO DAGScheduler: Missing parents: List()
26/01/04 16:59:38 INFO DAGScheduler: Submitting ResultStage 25 (MapPartitionsRDD[126] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 16:59:38 INFO MemoryStore: Block broadcast_35 stored as values in memory (estimated size 52.9 KiB, free 434.3 MiB)
26/01/04 16:59:38 INFO MemoryStore: Block broadcast_35_piece0 stored as bytes in memory (estimated size 20.2 KiB, free 434.3 MiB)
26/01/04 16:59:38 INFO BlockManagerInfo: Added broadcast_35_piece0 in memory on spark-master:33535 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 16:59:38 INFO SparkContext: Created broadcast 35 from broadcast at DAGScheduler.scala:1585
26/01/04 16:59:38 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 25 (MapPartitionsRDD[126] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
26/01/04 16:59:38 INFO TaskSchedulerImpl: Adding task set 25.0 with 3 tasks resource profile 0
26/01/04 16:59:38 INFO TaskSetManager: Starting task 1.0 in stage 25.0 (TID 51) (172.18.0.14, executor 1, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/01/04 16:59:38 INFO TaskSetManager: Starting task 0.0 in stage 25.0 (TID 52) (172.18.0.13, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/01/04 16:59:38 INFO TaskSetManager: Starting task 2.0 in stage 25.0 (TID 53) (172.18.0.13, executor 0, partition 2, PROCESS_LOCAL, 8594 bytes) 
26/01/04 16:59:38 INFO BlockManagerInfo: Added broadcast_35_piece0 in memory on 172.18.0.13:44401 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 16:59:38 INFO BlockManagerInfo: Added broadcast_35_piece0 in memory on 172.18.0.14:43523 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 16:59:38 INFO BlockManagerInfo: Added broadcast_34_piece0 in memory on 172.18.0.13:44401 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 16:59:38 INFO BlockManagerInfo: Added broadcast_34_piece0 in memory on 172.18.0.14:43523 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 16:59:39 INFO TaskSetManager: Finished task 0.0 in stage 25.0 (TID 52) in 1150 ms on 172.18.0.13 (executor 0) (1/3)
26/01/04 16:59:39 INFO TaskSetManager: Finished task 1.0 in stage 25.0 (TID 51) in 1174 ms on 172.18.0.14 (executor 1) (2/3)
26/01/04 16:59:39 INFO TaskSetManager: Finished task 2.0 in stage 25.0 (TID 53) in 1277 ms on 172.18.0.13 (executor 0) (3/3)
26/01/04 16:59:39 INFO TaskSchedulerImpl: Removed TaskSet 25.0, whose tasks have all completed, from pool 
26/01/04 16:59:39 INFO DAGScheduler: ResultStage 25 (start at NativeMethodAccessorImpl.java:0) finished in 1.311 s
26/01/04 16:59:39 INFO DAGScheduler: Job 24 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 16:59:39 INFO TaskSchedulerImpl: Killing all running tasks in stage 25: Stage finished
26/01/04 16:59:39 INFO DAGScheduler: Job 24 finished: start at NativeMethodAccessorImpl.java:0, took 1.329006 s
26/01/04 16:59:39 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 10, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@5246d8b3] is committing.
26/01/04 16:59:39 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 10, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@5246d8b3] committed.
26/01/04 16:59:39 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/10 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.10.ac91ca25-cf7a-42d9-9ea7-54ab4dc41e9f.tmp
26/01/04 16:59:40 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.10.ac91ca25-cf7a-42d9-9ea7-54ab4dc41e9f.tmp to file:/tmp/spark-checkpoint-enrichment/commits/10
26/01/04 16:59:40 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T16:59:37.405Z",
  "batchId" : 10,
  "numInputRows" : 30,
  "inputRowsPerSecond" : 1250.0,
  "processedRowsPerSecond" : 11.363636363636363,
  "durationMs" : {
    "addBatch" : 2085,
    "commitOffsets" : 233,
    "getBatch" : 0,
    "latestOffset" : 22,
    "queryPlanning" : 109,
    "triggerExecution" : 2640,
    "walCommit" : 183
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 2403,
        "1" : 2673,
        "0" : 2098
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 2414,
        "1" : 2683,
        "0" : 2107
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 2414,
        "1" : 2683,
        "0" : 2107
      }
    },
    "numInputRows" : 30,
    "inputRowsPerSecond" : 1250.0,
    "processedRowsPerSecond" : 11.363636363636363,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 31
  }
}
26/01/04 16:59:48 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/11 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.11.0337f854-ea93-44f6-b7d9-516254bf79c4.tmp
26/01/04 16:59:48 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.11.0337f854-ea93-44f6-b7d9-516254bf79c4.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/11
26/01/04 16:59:48 INFO MicroBatchExecution: Committed offsets for batch 11. Metadata OffsetSeqMetadata(0,1767545988455,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 16:59:48 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 16:59:48 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 16:59:48 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 16:59:48 INFO BlockManagerInfo: Removed broadcast_33_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 16:59:48 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 16:59:48 INFO BlockManagerInfo: Removed broadcast_33_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 16:59:48 INFO BlockManagerInfo: Removed broadcast_33_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 16:59:48 INFO BlockManagerInfo: Removed broadcast_35_piece0 on spark-master:33535 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 16:59:48 INFO BlockManagerInfo: Removed broadcast_35_piece0 on 172.18.0.13:44401 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 16:59:48 INFO BlockManagerInfo: Removed broadcast_35_piece0 on 172.18.0.14:43523 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 16:59:48 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 16:59:48 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 16:59:49 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#10327 - airline_prefix.nullCount#10326) > 0)
26/01/04 16:59:49 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#10362 - min_flight_num.nullCount#10361) > 0)
26/01/04 16:59:49 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#10357 - max_flight_num.nullCount#10356) > 0)
26/01/04 16:59:49 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 16:59:49 INFO DAGScheduler: Got job 25 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 16:59:49 INFO DAGScheduler: Final stage: ResultStage 26 (start at NativeMethodAccessorImpl.java:0)
26/01/04 16:59:49 INFO DAGScheduler: Parents of final stage: List()
26/01/04 16:59:49 INFO DAGScheduler: Missing parents: List()
26/01/04 16:59:49 INFO DAGScheduler: Submitting ResultStage 26 (MapPartitionsRDD[131] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 16:59:49 INFO MemoryStore: Block broadcast_36 stored as values in memory (estimated size 38.5 KiB, free 434.4 MiB)
26/01/04 16:59:49 INFO MemoryStore: Block broadcast_36_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.3 MiB)
26/01/04 16:59:49 INFO BlockManagerInfo: Added broadcast_36_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 16:59:49 INFO SparkContext: Created broadcast 36 from broadcast at DAGScheduler.scala:1585
26/01/04 16:59:49 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 26 (MapPartitionsRDD[131] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 16:59:49 INFO TaskSchedulerImpl: Adding task set 26.0 with 2 tasks resource profile 0
26/01/04 16:59:49 INFO TaskSetManager: Starting task 1.0 in stage 26.0 (TID 54) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 16:59:49 INFO TaskSetManager: Starting task 0.0 in stage 26.0 (TID 55) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 16:59:49 INFO BlockManagerInfo: Added broadcast_36_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 16:59:49 INFO BlockManagerInfo: Added broadcast_36_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 16:59:49 INFO TaskSetManager: Finished task 0.0 in stage 26.0 (TID 55) in 243 ms on 172.18.0.14 (executor 1) (1/2)
26/01/04 16:59:49 INFO TaskSetManager: Finished task 1.0 in stage 26.0 (TID 54) in 246 ms on 172.18.0.13 (executor 0) (2/2)
26/01/04 16:59:49 INFO TaskSchedulerImpl: Removed TaskSet 26.0, whose tasks have all completed, from pool 
26/01/04 16:59:49 INFO DAGScheduler: ResultStage 26 (start at NativeMethodAccessorImpl.java:0) finished in 0.398 s
26/01/04 16:59:49 INFO DAGScheduler: Job 25 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 16:59:49 INFO TaskSchedulerImpl: Killing all running tasks in stage 26: Stage finished
26/01/04 16:59:49 INFO DAGScheduler: Job 25 finished: start at NativeMethodAccessorImpl.java:0, took 0.416018 s
26/01/04 16:59:49 INFO BlockManagerInfo: Removed broadcast_34_piece0 on spark-master:33535 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 16:59:49 INFO BlockManagerInfo: Removed broadcast_34_piece0 on 172.18.0.14:43523 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 16:59:49 INFO MemoryStore: Block broadcast_37_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.3 MiB)
26/01/04 16:59:49 INFO BlockManagerInfo: Added broadcast_37_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 16:59:49 INFO SparkContext: Created broadcast 37 from start at NativeMethodAccessorImpl.java:0
26/01/04 16:59:49 INFO BlockManagerInfo: Removed broadcast_34_piece0 on 172.18.0.13:44401 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 16:59:49 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 11, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@2c629c27]. The input RDD has 2 partitions.
26/01/04 16:59:49 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 16:59:49 INFO DAGScheduler: Got job 26 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 16:59:49 INFO DAGScheduler: Final stage: ResultStage 27 (start at NativeMethodAccessorImpl.java:0)
26/01/04 16:59:49 INFO DAGScheduler: Parents of final stage: List()
26/01/04 16:59:49 INFO DAGScheduler: Missing parents: List()
26/01/04 16:59:49 INFO DAGScheduler: Submitting ResultStage 27 (MapPartitionsRDD[136] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 16:59:49 INFO MemoryStore: Block broadcast_38 stored as values in memory (estimated size 52.9 KiB, free 434.3 MiB)
26/01/04 16:59:49 INFO MemoryStore: Block broadcast_38_piece0 stored as bytes in memory (estimated size 20.2 KiB, free 434.3 MiB)
26/01/04 16:59:49 INFO BlockManagerInfo: Added broadcast_38_piece0 in memory on spark-master:33535 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 16:59:49 INFO SparkContext: Created broadcast 38 from broadcast at DAGScheduler.scala:1585
26/01/04 16:59:49 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 27 (MapPartitionsRDD[136] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 16:59:49 INFO TaskSchedulerImpl: Adding task set 27.0 with 2 tasks resource profile 0
26/01/04 16:59:49 INFO TaskSetManager: Starting task 0.0 in stage 27.0 (TID 56) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/01/04 16:59:49 INFO TaskSetManager: Starting task 1.0 in stage 27.0 (TID 57) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/01/04 16:59:49 INFO BlockManagerInfo: Added broadcast_38_piece0 in memory on 172.18.0.13:44401 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 16:59:49 INFO BlockManagerInfo: Added broadcast_38_piece0 in memory on 172.18.0.14:43523 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 16:59:49 INFO BlockManagerInfo: Added broadcast_37_piece0 in memory on 172.18.0.13:44401 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 16:59:49 INFO BlockManagerInfo: Added broadcast_37_piece0 in memory on 172.18.0.14:43523 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 16:59:50 INFO TaskSetManager: Finished task 0.0 in stage 27.0 (TID 56) in 937 ms on 172.18.0.14 (executor 1) (1/2)
26/01/04 16:59:50 INFO TaskSetManager: Finished task 1.0 in stage 27.0 (TID 57) in 956 ms on 172.18.0.13 (executor 0) (2/2)
26/01/04 16:59:50 INFO TaskSchedulerImpl: Removed TaskSet 27.0, whose tasks have all completed, from pool 
26/01/04 16:59:50 INFO DAGScheduler: ResultStage 27 (start at NativeMethodAccessorImpl.java:0) finished in 0.993 s
26/01/04 16:59:50 INFO DAGScheduler: Job 26 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 16:59:50 INFO TaskSchedulerImpl: Killing all running tasks in stage 27: Stage finished
26/01/04 16:59:50 INFO DAGScheduler: Job 26 finished: start at NativeMethodAccessorImpl.java:0, took 1.012646 s
26/01/04 16:59:50 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 11, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@2c629c27] is committing.
26/01/04 16:59:50 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 11, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@2c629c27] committed.
26/01/04 16:59:50 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/11 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.11.b20cb44e-7882-4eca-aabf-e8336af5ab87.tmp
26/01/04 16:59:50 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.11.b20cb44e-7882-4eca-aabf-e8336af5ab87.tmp to file:/tmp/spark-checkpoint-enrichment/commits/11
26/01/04 16:59:50 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T16:59:48.436Z",
  "batchId" : 11,
  "numInputRows" : 3,
  "inputRowsPerSecond" : 157.89473684210526,
  "processedRowsPerSecond" : 1.2599748005039901,
  "durationMs" : {
    "addBatch" : 1843,
    "commitOffsets" : 182,
    "getBatch" : 0,
    "latestOffset" : 18,
    "queryPlanning" : 143,
    "triggerExecution" : 2381,
    "walCommit" : 188
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 2414,
        "1" : 2683,
        "0" : 2107
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 2414,
        "1" : 2685,
        "0" : 2108
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 2414,
        "1" : 2685,
        "0" : 2108
      }
    },
    "numInputRows" : 3,
    "inputRowsPerSecond" : 157.89473684210526,
    "processedRowsPerSecond" : 1.2599748005039901,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 3
  }
}
26/01/04 16:59:50 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/12 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.12.638addbb-964b-48b7-a0ca-7925e2dbb983.tmp
26/01/04 16:59:51 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.12.638addbb-964b-48b7-a0ca-7925e2dbb983.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/12
26/01/04 16:59:51 INFO MicroBatchExecution: Committed offsets for batch 12. Metadata OffsetSeqMetadata(0,1767545990834,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 16:59:51 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 16:59:51 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 16:59:51 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 16:59:51 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 16:59:51 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 16:59:51 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 16:59:51 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#11131 - airline_prefix.nullCount#11130) > 0)
26/01/04 16:59:51 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#11166 - min_flight_num.nullCount#11165) > 0)
26/01/04 16:59:51 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#11161 - max_flight_num.nullCount#11160) > 0)
26/01/04 16:59:51 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 16:59:51 INFO DAGScheduler: Got job 27 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 16:59:51 INFO DAGScheduler: Final stage: ResultStage 28 (start at NativeMethodAccessorImpl.java:0)
26/01/04 16:59:51 INFO DAGScheduler: Parents of final stage: List()
26/01/04 16:59:51 INFO DAGScheduler: Missing parents: List()
26/01/04 16:59:51 INFO DAGScheduler: Submitting ResultStage 28 (MapPartitionsRDD[141] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 16:59:51 INFO MemoryStore: Block broadcast_39 stored as values in memory (estimated size 38.5 KiB, free 434.2 MiB)
26/01/04 16:59:51 INFO MemoryStore: Block broadcast_39_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.2 MiB)
26/01/04 16:59:51 INFO BlockManagerInfo: Added broadcast_39_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 16:59:51 INFO SparkContext: Created broadcast 39 from broadcast at DAGScheduler.scala:1585
26/01/04 16:59:51 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 28 (MapPartitionsRDD[141] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 16:59:51 INFO TaskSchedulerImpl: Adding task set 28.0 with 2 tasks resource profile 0
26/01/04 16:59:51 INFO TaskSetManager: Starting task 0.0 in stage 28.0 (TID 58) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 16:59:51 INFO TaskSetManager: Starting task 1.0 in stage 28.0 (TID 59) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 16:59:51 INFO BlockManagerInfo: Added broadcast_39_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 16:59:51 INFO BlockManagerInfo: Added broadcast_39_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 16:59:51 INFO TaskSetManager: Finished task 0.0 in stage 28.0 (TID 58) in 103 ms on 172.18.0.14 (executor 1) (1/2)
26/01/04 16:59:51 INFO TaskSetManager: Finished task 1.0 in stage 28.0 (TID 59) in 132 ms on 172.18.0.13 (executor 0) (2/2)
26/01/04 16:59:51 INFO TaskSchedulerImpl: Removed TaskSet 28.0, whose tasks have all completed, from pool 
26/01/04 16:59:51 INFO DAGScheduler: ResultStage 28 (start at NativeMethodAccessorImpl.java:0) finished in 0.168 s
26/01/04 16:59:51 INFO DAGScheduler: Job 27 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 16:59:51 INFO TaskSchedulerImpl: Killing all running tasks in stage 28: Stage finished
26/01/04 16:59:51 INFO DAGScheduler: Job 27 finished: start at NativeMethodAccessorImpl.java:0, took 0.186721 s
26/01/04 16:59:51 INFO MemoryStore: Block broadcast_40_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.2 MiB)
26/01/04 16:59:51 INFO BlockManagerInfo: Added broadcast_40_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 16:59:51 INFO SparkContext: Created broadcast 40 from start at NativeMethodAccessorImpl.java:0
26/01/04 16:59:51 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 12, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@21c05d26]. The input RDD has 3 partitions.
26/01/04 16:59:51 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 16:59:51 INFO BlockManagerInfo: Removed broadcast_36_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 16:59:51 INFO DAGScheduler: Got job 28 (start at NativeMethodAccessorImpl.java:0) with 3 output partitions
26/01/04 16:59:51 INFO DAGScheduler: Final stage: ResultStage 29 (start at NativeMethodAccessorImpl.java:0)
26/01/04 16:59:51 INFO DAGScheduler: Parents of final stage: List()
26/01/04 16:59:51 INFO DAGScheduler: Missing parents: List()
26/01/04 16:59:51 INFO DAGScheduler: Submitting ResultStage 29 (MapPartitionsRDD[146] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 16:59:51 INFO BlockManagerInfo: Removed broadcast_36_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 16:59:51 INFO MemoryStore: Block broadcast_41 stored as values in memory (estimated size 52.9 KiB, free 434.2 MiB)
26/01/04 16:59:51 INFO BlockManagerInfo: Removed broadcast_36_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 16:59:51 INFO MemoryStore: Block broadcast_41_piece0 stored as bytes in memory (estimated size 20.2 KiB, free 434.2 MiB)
26/01/04 16:59:51 INFO BlockManagerInfo: Added broadcast_41_piece0 in memory on spark-master:33535 (size: 20.2 KiB, free: 434.3 MiB)
26/01/04 16:59:51 INFO SparkContext: Created broadcast 41 from broadcast at DAGScheduler.scala:1585
26/01/04 16:59:51 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 29 (MapPartitionsRDD[146] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
26/01/04 16:59:51 INFO TaskSchedulerImpl: Adding task set 29.0 with 3 tasks resource profile 0
26/01/04 16:59:51 INFO TaskSetManager: Starting task 1.0 in stage 29.0 (TID 60) (172.18.0.14, executor 1, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/01/04 16:59:51 INFO TaskSetManager: Starting task 0.0 in stage 29.0 (TID 61) (172.18.0.13, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/01/04 16:59:51 INFO TaskSetManager: Starting task 2.0 in stage 29.0 (TID 62) (172.18.0.13, executor 0, partition 2, PROCESS_LOCAL, 8594 bytes) 
26/01/04 16:59:51 INFO BlockManagerInfo: Removed broadcast_38_piece0 on spark-master:33535 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 16:59:51 INFO BlockManagerInfo: Removed broadcast_38_piece0 on 172.18.0.14:43523 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 16:59:51 INFO BlockManagerInfo: Removed broadcast_38_piece0 on 172.18.0.13:44401 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 16:59:51 INFO BlockManagerInfo: Added broadcast_41_piece0 in memory on 172.18.0.14:43523 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 16:59:51 INFO BlockManagerInfo: Removed broadcast_39_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 16:59:51 INFO BlockManagerInfo: Added broadcast_41_piece0 in memory on 172.18.0.13:44401 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 16:59:51 INFO BlockManagerInfo: Removed broadcast_39_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 16:59:51 INFO BlockManagerInfo: Removed broadcast_39_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 16:59:51 INFO BlockManagerInfo: Added broadcast_40_piece0 in memory on 172.18.0.14:43523 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 16:59:51 INFO BlockManagerInfo: Added broadcast_40_piece0 in memory on 172.18.0.13:44401 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 16:59:52 INFO TaskSetManager: Finished task 1.0 in stage 29.0 (TID 60) in 542 ms on 172.18.0.14 (executor 1) (1/3)
26/01/04 16:59:52 INFO TaskSetManager: Finished task 2.0 in stage 29.0 (TID 62) in 544 ms on 172.18.0.13 (executor 0) (2/3)
26/01/04 16:59:52 INFO TaskSetManager: Finished task 0.0 in stage 29.0 (TID 61) in 1081 ms on 172.18.0.13 (executor 0) (3/3)
26/01/04 16:59:52 INFO TaskSchedulerImpl: Removed TaskSet 29.0, whose tasks have all completed, from pool 
26/01/04 16:59:52 INFO DAGScheduler: ResultStage 29 (start at NativeMethodAccessorImpl.java:0) finished in 1.116 s
26/01/04 16:59:52 INFO DAGScheduler: Job 28 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 16:59:52 INFO TaskSchedulerImpl: Killing all running tasks in stage 29: Stage finished
26/01/04 16:59:52 INFO DAGScheduler: Job 28 finished: start at NativeMethodAccessorImpl.java:0, took 1.135235 s
26/01/04 16:59:52 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 12, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@21c05d26] is committing.
26/01/04 16:59:52 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 12, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@21c05d26] committed.
26/01/04 16:59:52 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/12 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.12.27f8591f-43fd-429c-8644-45eb15f5500f.tmp
26/01/04 16:59:53 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.12.27f8591f-43fd-429c-8644-45eb15f5500f.tmp to file:/tmp/spark-checkpoint-enrichment/commits/12
26/01/04 16:59:53 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T16:59:50.827Z",
  "batchId" : 12,
  "numInputRows" : 27,
  "inputRowsPerSecond" : 11.292346298619824,
  "processedRowsPerSecond" : 12.010676156583628,
  "durationMs" : {
    "addBatch" : 1587,
    "commitOffsets" : 377,
    "getBatch" : 0,
    "latestOffset" : 7,
    "queryPlanning" : 87,
    "triggerExecution" : 2248,
    "walCommit" : 188
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 2414,
        "1" : 2685,
        "0" : 2108
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 2425,
        "1" : 2693,
        "0" : 2116
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 2425,
        "1" : 2693,
        "0" : 2116
      }
    },
    "numInputRows" : 27,
    "inputRowsPerSecond" : 11.292346298619824,
    "processedRowsPerSecond" : 12.010676156583628,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 28
  }
}
26/01/04 16:59:53 INFO BlockManagerInfo: Removed broadcast_41_piece0 on spark-master:33535 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 16:59:53 INFO BlockManagerInfo: Removed broadcast_41_piece0 on 172.18.0.14:43523 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 16:59:53 INFO BlockManagerInfo: Removed broadcast_41_piece0 on 172.18.0.13:44401 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 16:59:59 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/13 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.13.39a4a215-88cb-4b05-9ca3-d37cd1e9295f.tmp
26/01/04 16:59:59 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.13.39a4a215-88cb-4b05-9ca3-d37cd1e9295f.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/13
26/01/04 16:59:59 INFO MicroBatchExecution: Committed offsets for batch 13. Metadata OffsetSeqMetadata(0,1767545999489,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 16:59:59 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 16:59:59 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 16:59:59 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 16:59:59 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 16:59:59 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:00:00 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:00:00 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#11935 - airline_prefix.nullCount#11934) > 0)
26/01/04 17:00:00 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#11970 - min_flight_num.nullCount#11969) > 0)
26/01/04 17:00:00 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#11965 - max_flight_num.nullCount#11964) > 0)
26/01/04 17:00:00 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:00:00 INFO DAGScheduler: Got job 29 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 17:00:00 INFO DAGScheduler: Final stage: ResultStage 30 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:00:00 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:00:00 INFO DAGScheduler: Missing parents: List()
26/01/04 17:00:00 INFO DAGScheduler: Submitting ResultStage 30 (MapPartitionsRDD[151] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:00:00 INFO MemoryStore: Block broadcast_42 stored as values in memory (estimated size 38.5 KiB, free 434.4 MiB)
26/01/04 17:00:00 INFO MemoryStore: Block broadcast_42_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.3 MiB)
26/01/04 17:00:00 INFO BlockManagerInfo: Added broadcast_42_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:00:00 INFO SparkContext: Created broadcast 42 from broadcast at DAGScheduler.scala:1585
26/01/04 17:00:00 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 30 (MapPartitionsRDD[151] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 17:00:00 INFO TaskSchedulerImpl: Adding task set 30.0 with 2 tasks resource profile 0
26/01/04 17:00:00 INFO TaskSetManager: Starting task 0.0 in stage 30.0 (TID 63) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 17:00:00 INFO TaskSetManager: Starting task 1.0 in stage 30.0 (TID 64) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 17:00:00 INFO BlockManagerInfo: Removed broadcast_37_piece0 on spark-master:33535 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:00:00 INFO BlockManagerInfo: Removed broadcast_37_piece0 on 172.18.0.13:44401 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:00:00 INFO BlockManagerInfo: Removed broadcast_37_piece0 on 172.18.0.14:43523 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:00:00 INFO BlockManagerInfo: Removed broadcast_40_piece0 on spark-master:33535 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:00:00 INFO BlockManagerInfo: Removed broadcast_40_piece0 on 172.18.0.14:43523 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:00:00 INFO BlockManagerInfo: Added broadcast_42_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:00:00 INFO BlockManagerInfo: Added broadcast_42_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:00:00 INFO BlockManagerInfo: Removed broadcast_40_piece0 on 172.18.0.13:44401 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:00:00 INFO TaskSetManager: Finished task 0.0 in stage 30.0 (TID 63) in 321 ms on 172.18.0.14 (executor 1) (1/2)
26/01/04 17:00:00 INFO TaskSetManager: Finished task 1.0 in stage 30.0 (TID 64) in 410 ms on 172.18.0.13 (executor 0) (2/2)
26/01/04 17:00:00 INFO TaskSchedulerImpl: Removed TaskSet 30.0, whose tasks have all completed, from pool 
26/01/04 17:00:00 INFO DAGScheduler: ResultStage 30 (start at NativeMethodAccessorImpl.java:0) finished in 0.520 s
26/01/04 17:00:00 INFO DAGScheduler: Job 29 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:00:00 INFO TaskSchedulerImpl: Killing all running tasks in stage 30: Stage finished
26/01/04 17:00:00 INFO DAGScheduler: Job 29 finished: start at NativeMethodAccessorImpl.java:0, took 0.548960 s
26/01/04 17:00:00 INFO MemoryStore: Block broadcast_43_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.3 MiB)
26/01/04 17:00:00 INFO BlockManagerInfo: Added broadcast_43_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:00:00 INFO SparkContext: Created broadcast 43 from start at NativeMethodAccessorImpl.java:0
26/01/04 17:00:00 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 13, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@7892481a]. The input RDD has 3 partitions.
26/01/04 17:00:00 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:00:00 INFO DAGScheduler: Got job 30 (start at NativeMethodAccessorImpl.java:0) with 3 output partitions
26/01/04 17:00:00 INFO DAGScheduler: Final stage: ResultStage 31 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:00:00 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:00:00 INFO DAGScheduler: Missing parents: List()
26/01/04 17:00:00 INFO DAGScheduler: Submitting ResultStage 31 (MapPartitionsRDD[156] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:00:00 INFO MemoryStore: Block broadcast_44 stored as values in memory (estimated size 52.9 KiB, free 434.3 MiB)
26/01/04 17:00:00 INFO MemoryStore: Block broadcast_44_piece0 stored as bytes in memory (estimated size 20.2 KiB, free 434.3 MiB)
26/01/04 17:00:00 INFO BlockManagerInfo: Added broadcast_44_piece0 in memory on spark-master:33535 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:00:00 INFO SparkContext: Created broadcast 44 from broadcast at DAGScheduler.scala:1585
26/01/04 17:00:00 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 31 (MapPartitionsRDD[156] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
26/01/04 17:00:00 INFO TaskSchedulerImpl: Adding task set 31.0 with 3 tasks resource profile 0
26/01/04 17:00:00 INFO TaskSetManager: Starting task 0.0 in stage 31.0 (TID 65) (172.18.0.13, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:00:00 INFO TaskSetManager: Starting task 1.0 in stage 31.0 (TID 66) (172.18.0.14, executor 1, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:00:00 INFO TaskSetManager: Starting task 2.0 in stage 31.0 (TID 67) (172.18.0.13, executor 0, partition 2, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:00:00 INFO BlockManagerInfo: Added broadcast_44_piece0 in memory on 172.18.0.13:44401 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:00:00 INFO BlockManagerInfo: Added broadcast_44_piece0 in memory on 172.18.0.14:43523 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:00:01 INFO BlockManagerInfo: Added broadcast_43_piece0 in memory on 172.18.0.14:43523 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:00:01 INFO BlockManagerInfo: Added broadcast_43_piece0 in memory on 172.18.0.13:44401 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:00:01 INFO TaskSetManager: Finished task 1.0 in stage 31.0 (TID 66) in 1058 ms on 172.18.0.14 (executor 1) (1/3)
26/01/04 17:00:02 INFO TaskSetManager: Finished task 0.0 in stage 31.0 (TID 65) in 1461 ms on 172.18.0.13 (executor 0) (2/3)
26/01/04 17:00:02 INFO TaskSetManager: Finished task 2.0 in stage 31.0 (TID 67) in 1455 ms on 172.18.0.13 (executor 0) (3/3)
26/01/04 17:00:02 INFO TaskSchedulerImpl: Removed TaskSet 31.0, whose tasks have all completed, from pool 
26/01/04 17:00:02 INFO DAGScheduler: ResultStage 31 (start at NativeMethodAccessorImpl.java:0) finished in 1.504 s
26/01/04 17:00:02 INFO DAGScheduler: Job 30 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:00:02 INFO TaskSchedulerImpl: Killing all running tasks in stage 31: Stage finished
26/01/04 17:00:02 INFO DAGScheduler: Job 30 finished: start at NativeMethodAccessorImpl.java:0, took 1.513420 s
26/01/04 17:00:02 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 13, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@7892481a] is committing.
26/01/04 17:00:02 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 13, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@7892481a] committed.
26/01/04 17:00:02 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/13 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.13.3c08c8eb-a611-43dd-822f-71a112758442.tmp
26/01/04 17:00:02 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.13.3c08c8eb-a611-43dd-822f-71a112758442.tmp to file:/tmp/spark-checkpoint-enrichment/commits/13
26/01/04 17:00:02 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T16:59:59.472Z",
  "batchId" : 13,
  "numInputRows" : 30,
  "inputRowsPerSecond" : 2000.0,
  "processedRowsPerSecond" : 9.787928221859707,
  "durationMs" : {
    "addBatch" : 2507,
    "commitOffsets" : 274,
    "getBatch" : 0,
    "latestOffset" : 17,
    "queryPlanning" : 118,
    "triggerExecution" : 3065,
    "walCommit" : 141
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 2425,
        "1" : 2693,
        "0" : 2116
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 2436,
        "1" : 2703,
        "0" : 2125
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 2436,
        "1" : 2703,
        "0" : 2125
      }
    },
    "numInputRows" : 30,
    "inputRowsPerSecond" : 2000.0,
    "processedRowsPerSecond" : 9.787928221859707,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 31
  }
}
26/01/04 17:00:10 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/14 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.14.ad3f5cbd-be48-4f49-bb3e-50ee135f0d4d.tmp
26/01/04 17:00:10 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.14.ad3f5cbd-be48-4f49-bb3e-50ee135f0d4d.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/14
26/01/04 17:00:10 INFO MicroBatchExecution: Committed offsets for batch 14. Metadata OffsetSeqMetadata(0,1767546010518,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 17:00:10 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:00:10 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:00:10 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:00:10 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:00:10 INFO BlockManagerInfo: Removed broadcast_42_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:00:10 INFO BlockManagerInfo: Removed broadcast_42_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:00:10 INFO BlockManagerInfo: Removed broadcast_42_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:00:11 INFO BlockManagerInfo: Removed broadcast_44_piece0 on spark-master:33535 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:00:11 INFO BlockManagerInfo: Removed broadcast_44_piece0 on 172.18.0.13:44401 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:00:11 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:00:11 INFO BlockManagerInfo: Removed broadcast_44_piece0 on 172.18.0.14:43523 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:00:11 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:00:11 INFO BlockManagerInfo: Removed broadcast_43_piece0 on spark-master:33535 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:00:11 INFO BlockManagerInfo: Removed broadcast_43_piece0 on 172.18.0.14:43523 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:00:11 INFO BlockManagerInfo: Removed broadcast_43_piece0 on 172.18.0.13:44401 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:00:11 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#12739 - airline_prefix.nullCount#12738) > 0)
26/01/04 17:00:11 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#12774 - min_flight_num.nullCount#12773) > 0)
26/01/04 17:00:11 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#12769 - max_flight_num.nullCount#12768) > 0)
26/01/04 17:00:11 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:00:11 INFO DAGScheduler: Got job 31 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 17:00:11 INFO DAGScheduler: Final stage: ResultStage 32 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:00:11 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:00:11 INFO DAGScheduler: Missing parents: List()
26/01/04 17:00:11 INFO DAGScheduler: Submitting ResultStage 32 (MapPartitionsRDD[161] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:00:11 INFO MemoryStore: Block broadcast_45 stored as values in memory (estimated size 38.5 KiB, free 434.4 MiB)
26/01/04 17:00:11 INFO MemoryStore: Block broadcast_45_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.3 MiB)
26/01/04 17:00:11 INFO BlockManagerInfo: Added broadcast_45_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:00:11 INFO SparkContext: Created broadcast 45 from broadcast at DAGScheduler.scala:1585
26/01/04 17:00:11 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 32 (MapPartitionsRDD[161] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 17:00:11 INFO TaskSchedulerImpl: Adding task set 32.0 with 2 tasks resource profile 0
26/01/04 17:00:11 INFO TaskSetManager: Starting task 0.0 in stage 32.0 (TID 68) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 17:00:11 INFO TaskSetManager: Starting task 1.0 in stage 32.0 (TID 69) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 17:00:11 INFO BlockManagerInfo: Added broadcast_45_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:00:11 INFO BlockManagerInfo: Added broadcast_45_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:00:11 INFO TaskSetManager: Finished task 0.0 in stage 32.0 (TID 68) in 291 ms on 172.18.0.14 (executor 1) (1/2)
26/01/04 17:00:11 INFO TaskSetManager: Finished task 1.0 in stage 32.0 (TID 69) in 301 ms on 172.18.0.13 (executor 0) (2/2)
26/01/04 17:00:11 INFO TaskSchedulerImpl: Removed TaskSet 32.0, whose tasks have all completed, from pool 
26/01/04 17:00:11 INFO DAGScheduler: ResultStage 32 (start at NativeMethodAccessorImpl.java:0) finished in 0.391 s
26/01/04 17:00:11 INFO DAGScheduler: Job 31 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:00:11 INFO TaskSchedulerImpl: Killing all running tasks in stage 32: Stage finished
26/01/04 17:00:11 INFO DAGScheduler: Job 31 finished: start at NativeMethodAccessorImpl.java:0, took 0.424925 s
26/01/04 17:00:11 INFO MemoryStore: Block broadcast_46_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.3 MiB)
26/01/04 17:00:11 INFO BlockManagerInfo: Added broadcast_46_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:00:11 INFO SparkContext: Created broadcast 46 from start at NativeMethodAccessorImpl.java:0
26/01/04 17:00:11 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 14, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@26061889]. The input RDD has 3 partitions.
26/01/04 17:00:11 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:00:11 INFO DAGScheduler: Got job 32 (start at NativeMethodAccessorImpl.java:0) with 3 output partitions
26/01/04 17:00:11 INFO DAGScheduler: Final stage: ResultStage 33 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:00:11 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:00:11 INFO DAGScheduler: Missing parents: List()
26/01/04 17:00:11 INFO DAGScheduler: Submitting ResultStage 33 (MapPartitionsRDD[166] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:00:11 INFO MemoryStore: Block broadcast_47 stored as values in memory (estimated size 52.9 KiB, free 434.3 MiB)
26/01/04 17:00:11 INFO MemoryStore: Block broadcast_47_piece0 stored as bytes in memory (estimated size 20.2 KiB, free 434.3 MiB)
26/01/04 17:00:11 INFO BlockManagerInfo: Added broadcast_47_piece0 in memory on spark-master:33535 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:00:11 INFO SparkContext: Created broadcast 47 from broadcast at DAGScheduler.scala:1585
26/01/04 17:00:11 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 33 (MapPartitionsRDD[166] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
26/01/04 17:00:11 INFO TaskSchedulerImpl: Adding task set 33.0 with 3 tasks resource profile 0
26/01/04 17:00:11 INFO TaskSetManager: Starting task 0.0 in stage 33.0 (TID 70) (172.18.0.13, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:00:11 INFO TaskSetManager: Starting task 1.0 in stage 33.0 (TID 71) (172.18.0.14, executor 1, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:00:11 INFO TaskSetManager: Starting task 2.0 in stage 33.0 (TID 72) (172.18.0.13, executor 0, partition 2, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:00:11 INFO BlockManagerInfo: Added broadcast_47_piece0 in memory on 172.18.0.13:44401 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:00:11 INFO BlockManagerInfo: Added broadcast_47_piece0 in memory on 172.18.0.14:43523 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:00:11 INFO BlockManagerInfo: Added broadcast_46_piece0 in memory on 172.18.0.13:44401 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:00:12 INFO BlockManagerInfo: Added broadcast_46_piece0 in memory on 172.18.0.14:43523 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:00:12 INFO TaskSetManager: Finished task 0.0 in stage 33.0 (TID 70) in 785 ms on 172.18.0.13 (executor 0) (1/3)
26/01/04 17:00:12 INFO TaskSetManager: Finished task 2.0 in stage 33.0 (TID 72) in 784 ms on 172.18.0.13 (executor 0) (2/3)
26/01/04 17:00:12 INFO TaskSetManager: Finished task 1.0 in stage 33.0 (TID 71) in 864 ms on 172.18.0.14 (executor 1) (3/3)
26/01/04 17:00:12 INFO DAGScheduler: ResultStage 33 (start at NativeMethodAccessorImpl.java:0) finished in 0.881 s
26/01/04 17:00:12 INFO DAGScheduler: Job 32 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:00:12 INFO TaskSchedulerImpl: Removed TaskSet 33.0, whose tasks have all completed, from pool 
26/01/04 17:00:12 INFO TaskSchedulerImpl: Killing all running tasks in stage 33: Stage finished
26/01/04 17:00:12 INFO DAGScheduler: Job 32 finished: start at NativeMethodAccessorImpl.java:0, took 0.892717 s
26/01/04 17:00:12 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 14, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@26061889] is committing.
26/01/04 17:00:12 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 14, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@26061889] committed.
26/01/04 17:00:12 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/14 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.14.c7f6002e-2873-4a7b-a467-305e55fc2055.tmp
26/01/04 17:00:12 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.14.c7f6002e-2873-4a7b-a467-305e55fc2055.tmp to file:/tmp/spark-checkpoint-enrichment/commits/14
26/01/04 17:00:12 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T17:00:10.507Z",
  "batchId" : 14,
  "numInputRows" : 30,
  "inputRowsPerSecond" : 2000.0,
  "processedRowsPerSecond" : 12.376237623762377,
  "durationMs" : {
    "addBatch" : 1971,
    "commitOffsets" : 187,
    "getBatch" : 0,
    "latestOffset" : 11,
    "queryPlanning" : 98,
    "triggerExecution" : 2424,
    "walCommit" : 155
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 2436,
        "1" : 2703,
        "0" : 2125
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 2447,
        "1" : 2713,
        "0" : 2134
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 2447,
        "1" : 2713,
        "0" : 2134
      }
    },
    "numInputRows" : 30,
    "inputRowsPerSecond" : 2000.0,
    "processedRowsPerSecond" : 12.376237623762377,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 31
  }
}
26/01/04 17:00:21 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/15 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.15.ef500f0e-7fd5-4bf2-b06b-1ecc3c3d83f6.tmp
26/01/04 17:00:21 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.15.ef500f0e-7fd5-4bf2-b06b-1ecc3c3d83f6.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/15
26/01/04 17:00:21 INFO MicroBatchExecution: Committed offsets for batch 15. Metadata OffsetSeqMetadata(0,1767546021565,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 17:00:22 INFO BlockManagerInfo: Removed broadcast_47_piece0 on spark-master:33535 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:00:22 INFO BlockManagerInfo: Removed broadcast_47_piece0 on 172.18.0.14:43523 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:00:22 INFO BlockManagerInfo: Removed broadcast_47_piece0 on 172.18.0.13:44401 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:00:22 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:00:22 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:00:22 INFO BlockManagerInfo: Removed broadcast_45_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:00:22 INFO BlockManagerInfo: Removed broadcast_45_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:00:22 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:00:22 INFO BlockManagerInfo: Removed broadcast_45_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:00:22 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:00:22 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:00:22 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:00:22 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#13543 - airline_prefix.nullCount#13542) > 0)
26/01/04 17:00:22 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#13578 - min_flight_num.nullCount#13577) > 0)
26/01/04 17:00:22 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#13573 - max_flight_num.nullCount#13572) > 0)
26/01/04 17:00:22 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:00:22 INFO DAGScheduler: Got job 33 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 17:00:22 INFO DAGScheduler: Final stage: ResultStage 34 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:00:22 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:00:22 INFO DAGScheduler: Missing parents: List()
26/01/04 17:00:22 INFO DAGScheduler: Submitting ResultStage 34 (MapPartitionsRDD[171] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:00:22 INFO MemoryStore: Block broadcast_48 stored as values in memory (estimated size 38.5 KiB, free 434.4 MiB)
26/01/04 17:00:22 INFO MemoryStore: Block broadcast_48_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.3 MiB)
26/01/04 17:00:22 INFO BlockManagerInfo: Added broadcast_48_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:00:22 INFO SparkContext: Created broadcast 48 from broadcast at DAGScheduler.scala:1585
26/01/04 17:00:22 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 34 (MapPartitionsRDD[171] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 17:00:22 INFO TaskSchedulerImpl: Adding task set 34.0 with 2 tasks resource profile 0
26/01/04 17:00:22 INFO TaskSetManager: Starting task 0.0 in stage 34.0 (TID 73) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 17:00:22 INFO TaskSetManager: Starting task 1.0 in stage 34.0 (TID 74) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 17:00:22 INFO BlockManagerInfo: Added broadcast_48_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:00:22 INFO BlockManagerInfo: Added broadcast_48_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:00:22 INFO TaskSetManager: Finished task 1.0 in stage 34.0 (TID 74) in 128 ms on 172.18.0.13 (executor 0) (1/2)
26/01/04 17:00:22 INFO TaskSetManager: Finished task 0.0 in stage 34.0 (TID 73) in 218 ms on 172.18.0.14 (executor 1) (2/2)
26/01/04 17:00:22 INFO TaskSchedulerImpl: Removed TaskSet 34.0, whose tasks have all completed, from pool 
26/01/04 17:00:22 INFO DAGScheduler: ResultStage 34 (start at NativeMethodAccessorImpl.java:0) finished in 0.264 s
26/01/04 17:00:22 INFO DAGScheduler: Job 33 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:00:22 INFO TaskSchedulerImpl: Killing all running tasks in stage 34: Stage finished
26/01/04 17:00:22 INFO DAGScheduler: Job 33 finished: start at NativeMethodAccessorImpl.java:0, took 0.364491 s
26/01/04 17:00:22 INFO MemoryStore: Block broadcast_49_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.3 MiB)
26/01/04 17:00:22 INFO BlockManagerInfo: Added broadcast_49_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:00:22 INFO SparkContext: Created broadcast 49 from start at NativeMethodAccessorImpl.java:0
26/01/04 17:00:22 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 15, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@617a30b9]. The input RDD has 3 partitions.
26/01/04 17:00:22 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:00:22 INFO DAGScheduler: Got job 34 (start at NativeMethodAccessorImpl.java:0) with 3 output partitions
26/01/04 17:00:22 INFO DAGScheduler: Final stage: ResultStage 35 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:00:22 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:00:22 INFO DAGScheduler: Missing parents: List()
26/01/04 17:00:22 INFO DAGScheduler: Submitting ResultStage 35 (MapPartitionsRDD[176] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:00:22 INFO MemoryStore: Block broadcast_50 stored as values in memory (estimated size 52.9 KiB, free 434.3 MiB)
26/01/04 17:00:22 INFO MemoryStore: Block broadcast_50_piece0 stored as bytes in memory (estimated size 20.2 KiB, free 434.3 MiB)
26/01/04 17:00:22 INFO BlockManagerInfo: Added broadcast_50_piece0 in memory on spark-master:33535 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:00:22 INFO SparkContext: Created broadcast 50 from broadcast at DAGScheduler.scala:1585
26/01/04 17:00:22 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 35 (MapPartitionsRDD[176] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
26/01/04 17:00:22 INFO TaskSchedulerImpl: Adding task set 35.0 with 3 tasks resource profile 0
26/01/04 17:00:22 INFO TaskSetManager: Starting task 0.0 in stage 35.0 (TID 75) (172.18.0.13, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:00:22 INFO TaskSetManager: Starting task 1.0 in stage 35.0 (TID 76) (172.18.0.14, executor 1, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:00:22 INFO TaskSetManager: Starting task 2.0 in stage 35.0 (TID 77) (172.18.0.13, executor 0, partition 2, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:00:22 INFO BlockManagerInfo: Added broadcast_50_piece0 in memory on 172.18.0.14:43523 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:00:22 INFO BlockManagerInfo: Added broadcast_50_piece0 in memory on 172.18.0.13:44401 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:00:23 INFO BlockManagerInfo: Added broadcast_49_piece0 in memory on 172.18.0.14:43523 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:00:23 INFO BlockManagerInfo: Added broadcast_49_piece0 in memory on 172.18.0.13:44401 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:00:23 INFO TaskSetManager: Finished task 1.0 in stage 35.0 (TID 76) in 876 ms on 172.18.0.14 (executor 1) (1/3)
26/01/04 17:00:23 INFO TaskSetManager: Finished task 2.0 in stage 35.0 (TID 77) in 968 ms on 172.18.0.13 (executor 0) (2/3)
26/01/04 17:00:23 INFO TaskSetManager: Finished task 0.0 in stage 35.0 (TID 75) in 1005 ms on 172.18.0.13 (executor 0) (3/3)
26/01/04 17:00:23 INFO TaskSchedulerImpl: Removed TaskSet 35.0, whose tasks have all completed, from pool 
26/01/04 17:00:23 INFO DAGScheduler: ResultStage 35 (start at NativeMethodAccessorImpl.java:0) finished in 1.038 s
26/01/04 17:00:23 INFO DAGScheduler: Job 34 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:00:23 INFO TaskSchedulerImpl: Killing all running tasks in stage 35: Stage finished
26/01/04 17:00:23 INFO DAGScheduler: Job 34 finished: start at NativeMethodAccessorImpl.java:0, took 1.053613 s
26/01/04 17:00:23 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 15, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@617a30b9] is committing.
26/01/04 17:00:23 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 15, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@617a30b9] committed.
26/01/04 17:00:23 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/15 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.15.2b8e4385-023a-4e5d-bc9d-fefb0d167450.tmp
26/01/04 17:00:24 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.15.2b8e4385-023a-4e5d-bc9d-fefb0d167450.tmp to file:/tmp/spark-checkpoint-enrichment/commits/15
26/01/04 17:00:24 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T17:00:21.545Z",
  "batchId" : 15,
  "numInputRows" : 30,
  "inputRowsPerSecond" : 1304.3478260869565,
  "processedRowsPerSecond" : 11.66407465007776,
  "durationMs" : {
    "addBatch" : 1900,
    "commitOffsets" : 171,
    "getBatch" : 0,
    "latestOffset" : 20,
    "queryPlanning" : 231,
    "triggerExecution" : 2572,
    "walCommit" : 248
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 2447,
        "1" : 2713,
        "0" : 2134
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 2458,
        "1" : 2723,
        "0" : 2143
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 2458,
        "1" : 2723,
        "0" : 2143
      }
    },
    "numInputRows" : 30,
    "inputRowsPerSecond" : 1304.3478260869565,
    "processedRowsPerSecond" : 11.66407465007776,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 31
  }
}
26/01/04 17:00:32 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/16 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.16.e7957b4d-f22c-43a2-afda-d0d08ee6848f.tmp
26/01/04 17:00:32 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.16.e7957b4d-f22c-43a2-afda-d0d08ee6848f.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/16
26/01/04 17:00:32 INFO MicroBatchExecution: Committed offsets for batch 16. Metadata OffsetSeqMetadata(0,1767546032651,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 17:00:32 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:00:32 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:00:33 INFO BlockManagerInfo: Removed broadcast_48_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:00:33 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:00:33 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:00:33 INFO BlockManagerInfo: Removed broadcast_48_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:00:33 INFO BlockManagerInfo: Removed broadcast_48_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:00:33 INFO BlockManagerInfo: Removed broadcast_49_piece0 on spark-master:33535 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:00:33 INFO BlockManagerInfo: Removed broadcast_49_piece0 on 172.18.0.13:44401 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:00:33 INFO BlockManagerInfo: Removed broadcast_49_piece0 on 172.18.0.14:43523 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:00:33 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:00:33 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:00:33 INFO BlockManagerInfo: Removed broadcast_50_piece0 on spark-master:33535 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:00:33 INFO BlockManagerInfo: Removed broadcast_50_piece0 on 172.18.0.14:43523 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:00:33 INFO BlockManagerInfo: Removed broadcast_50_piece0 on 172.18.0.13:44401 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:00:33 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#14347 - airline_prefix.nullCount#14346) > 0)
26/01/04 17:00:33 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#14382 - min_flight_num.nullCount#14381) > 0)
26/01/04 17:00:33 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#14377 - max_flight_num.nullCount#14376) > 0)
26/01/04 17:00:33 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:00:33 INFO DAGScheduler: Got job 35 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 17:00:33 INFO DAGScheduler: Final stage: ResultStage 36 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:00:33 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:00:33 INFO DAGScheduler: Missing parents: List()
26/01/04 17:00:33 INFO DAGScheduler: Submitting ResultStage 36 (MapPartitionsRDD[181] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:00:33 INFO MemoryStore: Block broadcast_51 stored as values in memory (estimated size 38.5 KiB, free 434.4 MiB)
26/01/04 17:00:33 INFO MemoryStore: Block broadcast_51_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.3 MiB)
26/01/04 17:00:33 INFO BlockManagerInfo: Added broadcast_51_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:00:33 INFO SparkContext: Created broadcast 51 from broadcast at DAGScheduler.scala:1585
26/01/04 17:00:33 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 36 (MapPartitionsRDD[181] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 17:00:33 INFO TaskSchedulerImpl: Adding task set 36.0 with 2 tasks resource profile 0
26/01/04 17:00:33 INFO TaskSetManager: Starting task 1.0 in stage 36.0 (TID 78) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 17:00:33 INFO TaskSetManager: Starting task 0.0 in stage 36.0 (TID 79) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 17:00:33 INFO BlockManagerInfo: Added broadcast_51_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:00:33 INFO BlockManagerInfo: Removed broadcast_46_piece0 on spark-master:33535 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:00:33 INFO BlockManagerInfo: Added broadcast_51_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:00:33 INFO BlockManagerInfo: Removed broadcast_46_piece0 on 172.18.0.14:43523 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:00:33 INFO BlockManagerInfo: Removed broadcast_46_piece0 on 172.18.0.13:44401 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:00:33 INFO TaskSetManager: Finished task 1.0 in stage 36.0 (TID 78) in 226 ms on 172.18.0.13 (executor 0) (1/2)
26/01/04 17:00:33 INFO TaskSetManager: Finished task 0.0 in stage 36.0 (TID 79) in 274 ms on 172.18.0.14 (executor 1) (2/2)
26/01/04 17:00:33 INFO TaskSchedulerImpl: Removed TaskSet 36.0, whose tasks have all completed, from pool 
26/01/04 17:00:33 INFO DAGScheduler: ResultStage 36 (start at NativeMethodAccessorImpl.java:0) finished in 0.330 s
26/01/04 17:00:33 INFO DAGScheduler: Job 35 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:00:33 INFO TaskSchedulerImpl: Killing all running tasks in stage 36: Stage finished
26/01/04 17:00:33 INFO DAGScheduler: Job 35 finished: start at NativeMethodAccessorImpl.java:0, took 0.346707 s
26/01/04 17:00:33 INFO MemoryStore: Block broadcast_52_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.3 MiB)
26/01/04 17:00:33 INFO BlockManagerInfo: Added broadcast_52_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:00:33 INFO SparkContext: Created broadcast 52 from start at NativeMethodAccessorImpl.java:0
26/01/04 17:00:33 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 16, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@5fdd487]. The input RDD has 2 partitions.
26/01/04 17:00:33 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:00:33 INFO DAGScheduler: Got job 36 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 17:00:33 INFO DAGScheduler: Final stage: ResultStage 37 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:00:33 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:00:33 INFO DAGScheduler: Missing parents: List()
26/01/04 17:00:33 INFO DAGScheduler: Submitting ResultStage 37 (MapPartitionsRDD[186] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:00:33 INFO MemoryStore: Block broadcast_53 stored as values in memory (estimated size 52.9 KiB, free 434.3 MiB)
26/01/04 17:00:33 INFO MemoryStore: Block broadcast_53_piece0 stored as bytes in memory (estimated size 20.2 KiB, free 434.3 MiB)
26/01/04 17:00:33 INFO BlockManagerInfo: Added broadcast_53_piece0 in memory on spark-master:33535 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:00:33 INFO SparkContext: Created broadcast 53 from broadcast at DAGScheduler.scala:1585
26/01/04 17:00:33 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 37 (MapPartitionsRDD[186] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 17:00:33 INFO TaskSchedulerImpl: Adding task set 37.0 with 2 tasks resource profile 0
26/01/04 17:00:33 INFO TaskSetManager: Starting task 0.0 in stage 37.0 (TID 80) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:00:33 INFO TaskSetManager: Starting task 1.0 in stage 37.0 (TID 81) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:00:33 INFO BlockManagerInfo: Added broadcast_53_piece0 in memory on 172.18.0.14:43523 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:00:33 INFO BlockManagerInfo: Added broadcast_53_piece0 in memory on 172.18.0.13:44401 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:00:33 INFO BlockManagerInfo: Added broadcast_52_piece0 in memory on 172.18.0.14:43523 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:00:33 INFO BlockManagerInfo: Added broadcast_52_piece0 in memory on 172.18.0.13:44401 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:00:34 INFO TaskSetManager: Finished task 0.0 in stage 37.0 (TID 80) in 899 ms on 172.18.0.14 (executor 1) (1/2)
26/01/04 17:00:34 INFO TaskSetManager: Finished task 1.0 in stage 37.0 (TID 81) in 915 ms on 172.18.0.13 (executor 0) (2/2)
26/01/04 17:00:34 INFO TaskSchedulerImpl: Removed TaskSet 37.0, whose tasks have all completed, from pool 
26/01/04 17:00:34 INFO DAGScheduler: ResultStage 37 (start at NativeMethodAccessorImpl.java:0) finished in 0.931 s
26/01/04 17:00:34 INFO DAGScheduler: Job 36 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:00:34 INFO TaskSchedulerImpl: Killing all running tasks in stage 37: Stage finished
26/01/04 17:00:34 INFO DAGScheduler: Job 36 finished: start at NativeMethodAccessorImpl.java:0, took 0.938413 s
26/01/04 17:00:34 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 16, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@5fdd487] is committing.
26/01/04 17:00:34 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 16, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@5fdd487] committed.
26/01/04 17:00:34 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/16 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.16.57434c5e-9a86-4e86-b048-e3f5c91ac4b8.tmp
26/01/04 17:00:35 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.16.57434c5e-9a86-4e86-b048-e3f5c91ac4b8.tmp to file:/tmp/spark-checkpoint-enrichment/commits/16
26/01/04 17:00:35 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T17:00:32.623Z",
  "batchId" : 16,
  "numInputRows" : 15,
  "inputRowsPerSecond" : 1000.0,
  "processedRowsPerSecond" : 5.919494869771113,
  "durationMs" : {
    "addBatch" : 1796,
    "commitOffsets" : 378,
    "getBatch" : 1,
    "latestOffset" : 28,
    "queryPlanning" : 119,
    "triggerExecution" : 2534,
    "walCommit" : 211
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 2458,
        "1" : 2723,
        "0" : 2143
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 2458,
        "1" : 2732,
        "0" : 2149
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 2458,
        "1" : 2732,
        "0" : 2149
      }
    },
    "numInputRows" : 15,
    "inputRowsPerSecond" : 1000.0,
    "processedRowsPerSecond" : 5.919494869771113,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 16
  }
}
26/01/04 17:00:35 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/17 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.17.47da8588-6842-426f-b128-fef87e3a9910.tmp
26/01/04 17:00:35 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.17.47da8588-6842-426f-b128-fef87e3a9910.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/17
26/01/04 17:00:35 INFO MicroBatchExecution: Committed offsets for batch 17. Metadata OffsetSeqMetadata(0,1767546035164,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 17:00:35 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:00:35 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:00:35 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:00:35 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:00:35 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:00:35 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:00:35 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#15151 - airline_prefix.nullCount#15150) > 0)
26/01/04 17:00:35 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#15186 - min_flight_num.nullCount#15185) > 0)
26/01/04 17:00:35 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#15181 - max_flight_num.nullCount#15180) > 0)
26/01/04 17:00:35 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:00:35 INFO DAGScheduler: Got job 37 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 17:00:35 INFO DAGScheduler: Final stage: ResultStage 38 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:00:35 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:00:35 INFO DAGScheduler: Missing parents: List()
26/01/04 17:00:35 INFO DAGScheduler: Submitting ResultStage 38 (MapPartitionsRDD[191] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:00:35 INFO MemoryStore: Block broadcast_54 stored as values in memory (estimated size 38.5 KiB, free 434.2 MiB)
26/01/04 17:00:35 INFO MemoryStore: Block broadcast_54_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.2 MiB)
26/01/04 17:00:35 INFO BlockManagerInfo: Added broadcast_54_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:00:35 INFO SparkContext: Created broadcast 54 from broadcast at DAGScheduler.scala:1585
26/01/04 17:00:35 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 38 (MapPartitionsRDD[191] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 17:00:35 INFO TaskSchedulerImpl: Adding task set 38.0 with 2 tasks resource profile 0
26/01/04 17:00:35 INFO TaskSetManager: Starting task 0.0 in stage 38.0 (TID 82) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 17:00:35 INFO TaskSetManager: Starting task 1.0 in stage 38.0 (TID 83) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 17:00:36 INFO BlockManagerInfo: Added broadcast_54_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:00:36 INFO BlockManagerInfo: Added broadcast_54_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:00:36 INFO TaskSetManager: Finished task 1.0 in stage 38.0 (TID 83) in 121 ms on 172.18.0.13 (executor 0) (1/2)
26/01/04 17:00:36 INFO TaskSetManager: Finished task 0.0 in stage 38.0 (TID 82) in 186 ms on 172.18.0.14 (executor 1) (2/2)
26/01/04 17:00:36 INFO TaskSchedulerImpl: Removed TaskSet 38.0, whose tasks have all completed, from pool 
26/01/04 17:00:36 INFO DAGScheduler: ResultStage 38 (start at NativeMethodAccessorImpl.java:0) finished in 0.207 s
26/01/04 17:00:36 INFO DAGScheduler: Job 37 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:00:36 INFO TaskSchedulerImpl: Killing all running tasks in stage 38: Stage finished
26/01/04 17:00:36 INFO DAGScheduler: Job 37 finished: start at NativeMethodAccessorImpl.java:0, took 0.227146 s
26/01/04 17:00:36 INFO MemoryStore: Block broadcast_55_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.2 MiB)
26/01/04 17:00:36 INFO BlockManagerInfo: Added broadcast_55_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:00:36 INFO SparkContext: Created broadcast 55 from start at NativeMethodAccessorImpl.java:0
26/01/04 17:00:36 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 17, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@2908fb2e]. The input RDD has 3 partitions.
26/01/04 17:00:36 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:00:36 INFO DAGScheduler: Got job 38 (start at NativeMethodAccessorImpl.java:0) with 3 output partitions
26/01/04 17:00:36 INFO DAGScheduler: Final stage: ResultStage 39 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:00:36 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:00:36 INFO DAGScheduler: Missing parents: List()
26/01/04 17:00:36 INFO DAGScheduler: Submitting ResultStage 39 (MapPartitionsRDD[196] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:00:36 INFO MemoryStore: Block broadcast_56 stored as values in memory (estimated size 52.9 KiB, free 434.2 MiB)
26/01/04 17:00:36 INFO MemoryStore: Block broadcast_56_piece0 stored as bytes in memory (estimated size 20.2 KiB, free 434.1 MiB)
26/01/04 17:00:36 INFO BlockManagerInfo: Added broadcast_56_piece0 in memory on spark-master:33535 (size: 20.2 KiB, free: 434.3 MiB)
26/01/04 17:00:36 INFO SparkContext: Created broadcast 56 from broadcast at DAGScheduler.scala:1585
26/01/04 17:00:36 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 39 (MapPartitionsRDD[196] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
26/01/04 17:00:36 INFO TaskSchedulerImpl: Adding task set 39.0 with 3 tasks resource profile 0
26/01/04 17:00:36 INFO TaskSetManager: Starting task 0.0 in stage 39.0 (TID 84) (172.18.0.13, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:00:36 INFO TaskSetManager: Starting task 1.0 in stage 39.0 (TID 85) (172.18.0.14, executor 1, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:00:36 INFO TaskSetManager: Starting task 2.0 in stage 39.0 (TID 86) (172.18.0.13, executor 0, partition 2, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:00:36 INFO BlockManagerInfo: Removed broadcast_51_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:00:36 INFO BlockManagerInfo: Removed broadcast_51_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:00:36 INFO BlockManagerInfo: Added broadcast_56_piece0 in memory on 172.18.0.14:43523 (size: 20.2 KiB, free: 434.3 MiB)
26/01/04 17:00:36 INFO BlockManagerInfo: Removed broadcast_51_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:00:36 INFO BlockManagerInfo: Removed broadcast_54_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:00:36 INFO BlockManagerInfo: Removed broadcast_54_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:00:36 INFO BlockManagerInfo: Added broadcast_56_piece0 in memory on 172.18.0.13:44401 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:00:36 INFO BlockManagerInfo: Removed broadcast_54_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:00:36 INFO BlockManagerInfo: Added broadcast_55_piece0 in memory on 172.18.0.14:43523 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:00:36 INFO BlockManagerInfo: Removed broadcast_53_piece0 on 172.18.0.14:43523 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:00:36 INFO BlockManagerInfo: Removed broadcast_53_piece0 on spark-master:33535 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:00:36 INFO BlockManagerInfo: Removed broadcast_53_piece0 on 172.18.0.13:44401 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:00:36 INFO BlockManagerInfo: Added broadcast_55_piece0 in memory on 172.18.0.13:44401 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:00:37 INFO TaskSetManager: Finished task 2.0 in stage 39.0 (TID 86) in 803 ms on 172.18.0.13 (executor 0) (1/3)
26/01/04 17:00:37 INFO TaskSetManager: Finished task 1.0 in stage 39.0 (TID 85) in 806 ms on 172.18.0.14 (executor 1) (2/3)
26/01/04 17:00:37 INFO TaskSetManager: Finished task 0.0 in stage 39.0 (TID 84) in 1246 ms on 172.18.0.13 (executor 0) (3/3)
26/01/04 17:00:37 INFO TaskSchedulerImpl: Removed TaskSet 39.0, whose tasks have all completed, from pool 
26/01/04 17:00:37 INFO DAGScheduler: ResultStage 39 (start at NativeMethodAccessorImpl.java:0) finished in 1.268 s
26/01/04 17:00:37 INFO DAGScheduler: Job 38 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:00:37 INFO TaskSchedulerImpl: Killing all running tasks in stage 39: Stage finished
26/01/04 17:00:37 INFO DAGScheduler: Job 38 finished: start at NativeMethodAccessorImpl.java:0, took 1.281758 s
26/01/04 17:00:37 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 17, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@2908fb2e] is committing.
26/01/04 17:00:37 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 17, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@2908fb2e] committed.
26/01/04 17:00:37 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/17 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.17.4534816e-2768-4eac-a594-f9b30f009804.tmp
26/01/04 17:00:37 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.17.4534816e-2768-4eac-a594-f9b30f009804.tmp to file:/tmp/spark-checkpoint-enrichment/commits/17
26/01/04 17:00:37 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T17:00:35.159Z",
  "batchId" : 17,
  "numInputRows" : 15,
  "inputRowsPerSecond" : 5.914826498422713,
  "processedRowsPerSecond" : 5.841121495327102,
  "durationMs" : {
    "addBatch" : 1993,
    "commitOffsets" : 172,
    "getBatch" : 0,
    "latestOffset" : 5,
    "queryPlanning" : 162,
    "triggerExecution" : 2568,
    "walCommit" : 230
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 2458,
        "1" : 2732,
        "0" : 2149
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 2469,
        "1" : 2733,
        "0" : 2152
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 2469,
        "1" : 2733,
        "0" : 2152
      }
    },
    "numInputRows" : 15,
    "inputRowsPerSecond" : 5.914826498422713,
    "processedRowsPerSecond" : 5.841121495327102,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 15
  }
}
26/01/04 17:00:39 INFO BlockManagerInfo: Removed broadcast_56_piece0 on spark-master:33535 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:00:39 INFO BlockManagerInfo: Removed broadcast_56_piece0 on 172.18.0.14:43523 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:00:39 INFO BlockManagerInfo: Removed broadcast_56_piece0 on 172.18.0.13:44401 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:00:43 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/18 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.18.53bf453c-8dbe-43eb-a536-3260d05dea93.tmp
26/01/04 17:00:43 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.18.53bf453c-8dbe-43eb-a536-3260d05dea93.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/18
26/01/04 17:00:43 INFO MicroBatchExecution: Committed offsets for batch 18. Metadata OffsetSeqMetadata(0,1767546043673,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 17:00:43 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:00:43 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:00:43 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:00:43 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:00:44 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:00:44 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:00:44 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#15955 - airline_prefix.nullCount#15954) > 0)
26/01/04 17:00:44 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#15990 - min_flight_num.nullCount#15989) > 0)
26/01/04 17:00:44 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#15985 - max_flight_num.nullCount#15984) > 0)
26/01/04 17:00:44 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:00:44 INFO DAGScheduler: Got job 39 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 17:00:44 INFO DAGScheduler: Final stage: ResultStage 40 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:00:44 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:00:44 INFO DAGScheduler: Missing parents: List()
26/01/04 17:00:44 INFO DAGScheduler: Submitting ResultStage 40 (MapPartitionsRDD[201] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:00:44 INFO MemoryStore: Block broadcast_57 stored as values in memory (estimated size 38.5 KiB, free 434.4 MiB)
26/01/04 17:00:44 INFO MemoryStore: Block broadcast_57_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.3 MiB)
26/01/04 17:00:44 INFO BlockManagerInfo: Added broadcast_57_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:00:44 INFO SparkContext: Created broadcast 57 from broadcast at DAGScheduler.scala:1585
26/01/04 17:00:44 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 40 (MapPartitionsRDD[201] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 17:00:44 INFO TaskSchedulerImpl: Adding task set 40.0 with 2 tasks resource profile 0
26/01/04 17:00:44 INFO TaskSetManager: Starting task 0.0 in stage 40.0 (TID 87) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 17:00:44 INFO TaskSetManager: Starting task 1.0 in stage 40.0 (TID 88) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 17:00:44 INFO BlockManagerInfo: Added broadcast_57_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:00:44 INFO BlockManagerInfo: Added broadcast_57_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:00:44 INFO TaskSetManager: Finished task 0.0 in stage 40.0 (TID 87) in 115 ms on 172.18.0.14 (executor 1) (1/2)
26/01/04 17:00:44 INFO TaskSetManager: Finished task 1.0 in stage 40.0 (TID 88) in 116 ms on 172.18.0.13 (executor 0) (2/2)
26/01/04 17:00:44 INFO TaskSchedulerImpl: Removed TaskSet 40.0, whose tasks have all completed, from pool 
26/01/04 17:00:44 INFO DAGScheduler: ResultStage 40 (start at NativeMethodAccessorImpl.java:0) finished in 0.130 s
26/01/04 17:00:44 INFO DAGScheduler: Job 39 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:00:44 INFO TaskSchedulerImpl: Killing all running tasks in stage 40: Stage finished
26/01/04 17:00:44 INFO DAGScheduler: Job 39 finished: start at NativeMethodAccessorImpl.java:0, took 0.137939 s
26/01/04 17:00:44 INFO MemoryStore: Block broadcast_58_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.3 MiB)
26/01/04 17:00:44 INFO BlockManagerInfo: Added broadcast_58_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:00:44 INFO SparkContext: Created broadcast 58 from start at NativeMethodAccessorImpl.java:0
26/01/04 17:00:44 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 18, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@1ad8348b]. The input RDD has 1 partitions.
26/01/04 17:00:44 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:00:44 INFO DAGScheduler: Got job 40 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/01/04 17:00:44 INFO DAGScheduler: Final stage: ResultStage 41 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:00:44 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:00:44 INFO DAGScheduler: Missing parents: List()
26/01/04 17:00:44 INFO DAGScheduler: Submitting ResultStage 41 (MapPartitionsRDD[206] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:00:44 INFO MemoryStore: Block broadcast_59 stored as values in memory (estimated size 52.9 KiB, free 434.3 MiB)
26/01/04 17:00:44 INFO MemoryStore: Block broadcast_59_piece0 stored as bytes in memory (estimated size 20.2 KiB, free 434.3 MiB)
26/01/04 17:00:44 INFO BlockManagerInfo: Added broadcast_59_piece0 in memory on spark-master:33535 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:00:44 INFO SparkContext: Created broadcast 59 from broadcast at DAGScheduler.scala:1585
26/01/04 17:00:44 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 41 (MapPartitionsRDD[206] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/01/04 17:00:44 INFO TaskSchedulerImpl: Adding task set 41.0 with 1 tasks resource profile 0
26/01/04 17:00:44 INFO TaskSetManager: Starting task 0.0 in stage 41.0 (TID 89) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:00:44 INFO BlockManagerInfo: Added broadcast_59_piece0 in memory on 172.18.0.14:43523 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:00:44 INFO BlockManagerInfo: Added broadcast_58_piece0 in memory on 172.18.0.14:43523 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:00:45 INFO TaskSetManager: Finished task 0.0 in stage 41.0 (TID 89) in 678 ms on 172.18.0.14 (executor 1) (1/1)
26/01/04 17:00:45 INFO TaskSchedulerImpl: Removed TaskSet 41.0, whose tasks have all completed, from pool 
26/01/04 17:00:45 INFO DAGScheduler: ResultStage 41 (start at NativeMethodAccessorImpl.java:0) finished in 0.694 s
26/01/04 17:00:45 INFO DAGScheduler: Job 40 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:00:45 INFO TaskSchedulerImpl: Killing all running tasks in stage 41: Stage finished
26/01/04 17:00:45 INFO DAGScheduler: Job 40 finished: start at NativeMethodAccessorImpl.java:0, took 0.699184 s
26/01/04 17:00:45 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 18, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@1ad8348b] is committing.
26/01/04 17:00:45 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 18, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@1ad8348b] committed.
26/01/04 17:00:45 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/18 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.18.b00d81f0-a010-4d0b-82ad-c2ca36d8ecaf.tmp
26/01/04 17:00:45 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.18.b00d81f0-a010-4d0b-82ad-c2ca36d8ecaf.tmp to file:/tmp/spark-checkpoint-enrichment/commits/18
26/01/04 17:00:45 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T17:00:43.663Z",
  "batchId" : 18,
  "numInputRows" : 2,
  "inputRowsPerSecond" : 117.6470588235294,
  "processedRowsPerSecond" : 1.3513513513513513,
  "durationMs" : {
    "addBatch" : 1112,
    "commitOffsets" : 113,
    "getBatch" : 1,
    "latestOffset" : 10,
    "queryPlanning" : 88,
    "triggerExecution" : 1480,
    "walCommit" : 155
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 2469,
        "1" : 2733,
        "0" : 2152
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 2469,
        "1" : 2735,
        "0" : 2152
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 2469,
        "1" : 2735,
        "0" : 2152
      }
    },
    "numInputRows" : 2,
    "inputRowsPerSecond" : 117.6470588235294,
    "processedRowsPerSecond" : 1.3513513513513513,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 2
  }
}
26/01/04 17:00:45 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/19 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.19.ac5a32f3-67a2-4245-b2cb-18f4de1a8abb.tmp
26/01/04 17:00:45 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.19.ac5a32f3-67a2-4245-b2cb-18f4de1a8abb.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/19
26/01/04 17:00:45 INFO MicroBatchExecution: Committed offsets for batch 19. Metadata OffsetSeqMetadata(0,1767546045148,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 17:00:45 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:00:45 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:00:45 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:00:45 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:00:45 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:00:45 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:00:45 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#16759 - airline_prefix.nullCount#16758) > 0)
26/01/04 17:00:45 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#16794 - min_flight_num.nullCount#16793) > 0)
26/01/04 17:00:45 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#16789 - max_flight_num.nullCount#16788) > 0)
26/01/04 17:00:45 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:00:45 INFO DAGScheduler: Got job 41 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 17:00:45 INFO DAGScheduler: Final stage: ResultStage 42 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:00:45 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:00:45 INFO DAGScheduler: Missing parents: List()
26/01/04 17:00:45 INFO DAGScheduler: Submitting ResultStage 42 (MapPartitionsRDD[211] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:00:45 INFO MemoryStore: Block broadcast_60 stored as values in memory (estimated size 38.5 KiB, free 434.2 MiB)
26/01/04 17:00:45 INFO MemoryStore: Block broadcast_60_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.2 MiB)
26/01/04 17:00:45 INFO BlockManagerInfo: Added broadcast_60_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:00:45 INFO SparkContext: Created broadcast 60 from broadcast at DAGScheduler.scala:1585
26/01/04 17:00:45 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 42 (MapPartitionsRDD[211] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 17:00:45 INFO TaskSchedulerImpl: Adding task set 42.0 with 2 tasks resource profile 0
26/01/04 17:00:45 INFO TaskSetManager: Starting task 1.0 in stage 42.0 (TID 90) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 17:00:45 INFO TaskSetManager: Starting task 0.0 in stage 42.0 (TID 91) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 17:00:45 INFO BlockManagerInfo: Added broadcast_60_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:00:45 INFO BlockManagerInfo: Added broadcast_60_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:00:45 INFO TaskSetManager: Finished task 1.0 in stage 42.0 (TID 90) in 162 ms on 172.18.0.13 (executor 0) (1/2)
26/01/04 17:00:45 INFO TaskSetManager: Finished task 0.0 in stage 42.0 (TID 91) in 193 ms on 172.18.0.14 (executor 1) (2/2)
26/01/04 17:00:45 INFO TaskSchedulerImpl: Removed TaskSet 42.0, whose tasks have all completed, from pool 
26/01/04 17:00:45 INFO DAGScheduler: ResultStage 42 (start at NativeMethodAccessorImpl.java:0) finished in 0.273 s
26/01/04 17:00:45 INFO DAGScheduler: Job 41 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:00:45 INFO TaskSchedulerImpl: Killing all running tasks in stage 42: Stage finished
26/01/04 17:00:45 INFO DAGScheduler: Job 41 finished: start at NativeMethodAccessorImpl.java:0, took 0.286871 s
26/01/04 17:00:45 INFO MemoryStore: Block broadcast_61_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.2 MiB)
26/01/04 17:00:45 INFO BlockManagerInfo: Added broadcast_61_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:00:45 INFO SparkContext: Created broadcast 61 from start at NativeMethodAccessorImpl.java:0
26/01/04 17:00:45 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 19, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@28273d3]. The input RDD has 3 partitions.
26/01/04 17:00:45 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:00:45 INFO DAGScheduler: Got job 42 (start at NativeMethodAccessorImpl.java:0) with 3 output partitions
26/01/04 17:00:45 INFO DAGScheduler: Final stage: ResultStage 43 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:00:45 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:00:45 INFO DAGScheduler: Missing parents: List()
26/01/04 17:00:45 INFO DAGScheduler: Submitting ResultStage 43 (MapPartitionsRDD[216] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:00:45 INFO MemoryStore: Block broadcast_62 stored as values in memory (estimated size 52.9 KiB, free 434.2 MiB)
26/01/04 17:00:46 INFO MemoryStore: Block broadcast_62_piece0 stored as bytes in memory (estimated size 20.2 KiB, free 434.1 MiB)
26/01/04 17:00:46 INFO BlockManagerInfo: Added broadcast_62_piece0 in memory on spark-master:33535 (size: 20.2 KiB, free: 434.3 MiB)
26/01/04 17:00:46 INFO SparkContext: Created broadcast 62 from broadcast at DAGScheduler.scala:1585
26/01/04 17:00:46 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 43 (MapPartitionsRDD[216] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
26/01/04 17:00:46 INFO TaskSchedulerImpl: Adding task set 43.0 with 3 tasks resource profile 0
26/01/04 17:00:46 INFO TaskSetManager: Starting task 1.0 in stage 43.0 (TID 92) (172.18.0.14, executor 1, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:00:46 INFO TaskSetManager: Starting task 0.0 in stage 43.0 (TID 93) (172.18.0.13, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:00:46 INFO TaskSetManager: Starting task 2.0 in stage 43.0 (TID 94) (172.18.0.13, executor 0, partition 2, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:00:46 INFO BlockManagerInfo: Added broadcast_62_piece0 in memory on 172.18.0.14:43523 (size: 20.2 KiB, free: 434.3 MiB)
26/01/04 17:00:46 INFO BlockManagerInfo: Added broadcast_62_piece0 in memory on 172.18.0.13:44401 (size: 20.2 KiB, free: 434.3 MiB)
26/01/04 17:00:46 INFO BlockManagerInfo: Added broadcast_61_piece0 in memory on 172.18.0.13:44401 (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:00:46 INFO BlockManagerInfo: Added broadcast_61_piece0 in memory on 172.18.0.14:43523 (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:00:46 INFO TaskSetManager: Finished task 1.0 in stage 43.0 (TID 92) in 426 ms on 172.18.0.14 (executor 1) (1/3)
26/01/04 17:00:47 INFO TaskSetManager: Finished task 0.0 in stage 43.0 (TID 93) in 1029 ms on 172.18.0.13 (executor 0) (2/3)
26/01/04 17:00:47 INFO TaskSetManager: Finished task 2.0 in stage 43.0 (TID 94) in 1088 ms on 172.18.0.13 (executor 0) (3/3)
26/01/04 17:00:47 INFO TaskSchedulerImpl: Removed TaskSet 43.0, whose tasks have all completed, from pool 
26/01/04 17:00:47 INFO DAGScheduler: ResultStage 43 (start at NativeMethodAccessorImpl.java:0) finished in 1.121 s
26/01/04 17:00:47 INFO DAGScheduler: Job 42 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:00:47 INFO TaskSchedulerImpl: Killing all running tasks in stage 43: Stage finished
26/01/04 17:00:47 INFO DAGScheduler: Job 42 finished: start at NativeMethodAccessorImpl.java:0, took 1.135266 s
26/01/04 17:00:47 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 19, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@28273d3] is committing.
26/01/04 17:00:47 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 19, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@28273d3] committed.
26/01/04 17:00:47 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/19 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.19.2df7596c-3b0b-4005-8a6e-ed9b041e1656.tmp
26/01/04 17:00:47 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.19.2df7596c-3b0b-4005-8a6e-ed9b041e1656.tmp to file:/tmp/spark-checkpoint-enrichment/commits/19
26/01/04 17:00:47 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T17:00:45.145Z",
  "batchId" : 19,
  "numInputRows" : 28,
  "inputRowsPerSecond" : 18.893387314439945,
  "processedRowsPerSecond" : 13.17027281279398,
  "durationMs" : {
    "addBatch" : 1745,
    "commitOffsets" : 156,
    "getBatch" : 0,
    "latestOffset" : 3,
    "queryPlanning" : 100,
    "triggerExecution" : 2126,
    "walCommit" : 121
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 2469,
        "1" : 2735,
        "0" : 2152
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 2480,
        "1" : 2743,
        "0" : 2161
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 2480,
        "1" : 2743,
        "0" : 2161
      }
    },
    "numInputRows" : 28,
    "inputRowsPerSecond" : 18.893387314439945,
    "processedRowsPerSecond" : 13.17027281279398,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 29
  }
}
26/01/04 17:00:51 INFO BlockManagerInfo: Removed broadcast_57_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:00:51 INFO BlockManagerInfo: Removed broadcast_57_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:00:51 INFO BlockManagerInfo: Removed broadcast_57_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:00:51 INFO BlockManagerInfo: Removed broadcast_58_piece0 on spark-master:33535 in memory (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:00:51 INFO BlockManagerInfo: Removed broadcast_58_piece0 on 172.18.0.14:43523 in memory (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:00:51 INFO BlockManagerInfo: Removed broadcast_62_piece0 on spark-master:33535 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:00:51 INFO BlockManagerInfo: Removed broadcast_62_piece0 on 172.18.0.13:44401 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:00:51 INFO BlockManagerInfo: Removed broadcast_62_piece0 on 172.18.0.14:43523 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:00:51 INFO BlockManagerInfo: Removed broadcast_59_piece0 on spark-master:33535 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:00:51 INFO BlockManagerInfo: Removed broadcast_59_piece0 on 172.18.0.14:43523 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:00:51 INFO BlockManagerInfo: Removed broadcast_60_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:00:51 INFO BlockManagerInfo: Removed broadcast_60_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:00:51 INFO BlockManagerInfo: Removed broadcast_60_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:00:51 INFO BlockManagerInfo: Removed broadcast_55_piece0 on spark-master:33535 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:00:51 INFO BlockManagerInfo: Removed broadcast_55_piece0 on 172.18.0.14:43523 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:00:51 INFO BlockManagerInfo: Removed broadcast_55_piece0 on 172.18.0.13:44401 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:00:54 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/20 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.20.0c962339-7b5f-45c6-82a2-6384a973f506.tmp
26/01/04 17:00:54 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.20.0c962339-7b5f-45c6-82a2-6384a973f506.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/20
26/01/04 17:00:54 INFO MicroBatchExecution: Committed offsets for batch 20. Metadata OffsetSeqMetadata(0,1767546054734,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 17:00:54 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:00:54 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:00:54 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:00:54 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:00:55 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:00:55 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:00:55 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#17563 - airline_prefix.nullCount#17562) > 0)
26/01/04 17:00:55 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#17598 - min_flight_num.nullCount#17597) > 0)
26/01/04 17:00:55 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#17593 - max_flight_num.nullCount#17592) > 0)
26/01/04 17:00:55 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:00:55 INFO DAGScheduler: Got job 43 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 17:00:55 INFO DAGScheduler: Final stage: ResultStage 44 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:00:55 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:00:55 INFO DAGScheduler: Missing parents: List()
26/01/04 17:00:55 INFO DAGScheduler: Submitting ResultStage 44 (MapPartitionsRDD[221] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:00:55 INFO MemoryStore: Block broadcast_63 stored as values in memory (estimated size 38.5 KiB, free 434.4 MiB)
26/01/04 17:00:55 INFO MemoryStore: Block broadcast_63_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.3 MiB)
26/01/04 17:00:55 INFO BlockManagerInfo: Added broadcast_63_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:00:55 INFO SparkContext: Created broadcast 63 from broadcast at DAGScheduler.scala:1585
26/01/04 17:00:55 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 44 (MapPartitionsRDD[221] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 17:00:55 INFO TaskSchedulerImpl: Adding task set 44.0 with 2 tasks resource profile 0
26/01/04 17:00:55 INFO BlockManagerInfo: Removed broadcast_61_piece0 on 172.18.0.14:43523 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:00:55 INFO TaskSetManager: Starting task 0.0 in stage 44.0 (TID 95) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 17:00:55 INFO BlockManagerInfo: Removed broadcast_61_piece0 on spark-master:33535 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:00:55 INFO TaskSetManager: Starting task 1.0 in stage 44.0 (TID 96) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 17:00:55 INFO BlockManagerInfo: Removed broadcast_61_piece0 on 172.18.0.13:44401 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:00:55 INFO BlockManagerInfo: Added broadcast_63_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:00:55 INFO BlockManagerInfo: Added broadcast_63_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:00:55 INFO TaskSetManager: Finished task 1.0 in stage 44.0 (TID 96) in 131 ms on 172.18.0.13 (executor 0) (1/2)
26/01/04 17:00:55 INFO TaskSetManager: Finished task 0.0 in stage 44.0 (TID 95) in 145 ms on 172.18.0.14 (executor 1) (2/2)
26/01/04 17:00:55 INFO TaskSchedulerImpl: Removed TaskSet 44.0, whose tasks have all completed, from pool 
26/01/04 17:00:55 INFO DAGScheduler: ResultStage 44 (start at NativeMethodAccessorImpl.java:0) finished in 0.187 s
26/01/04 17:00:55 INFO DAGScheduler: Job 43 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:00:55 INFO TaskSchedulerImpl: Killing all running tasks in stage 44: Stage finished
26/01/04 17:00:55 INFO DAGScheduler: Job 43 finished: start at NativeMethodAccessorImpl.java:0, took 0.199106 s
26/01/04 17:00:55 INFO MemoryStore: Block broadcast_64_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.3 MiB)
26/01/04 17:00:55 INFO BlockManagerInfo: Added broadcast_64_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:00:55 INFO SparkContext: Created broadcast 64 from start at NativeMethodAccessorImpl.java:0
26/01/04 17:00:55 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 20, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@714ed244]. The input RDD has 3 partitions.
26/01/04 17:00:55 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:00:55 INFO DAGScheduler: Got job 44 (start at NativeMethodAccessorImpl.java:0) with 3 output partitions
26/01/04 17:00:55 INFO DAGScheduler: Final stage: ResultStage 45 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:00:55 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:00:55 INFO DAGScheduler: Missing parents: List()
26/01/04 17:00:55 INFO DAGScheduler: Submitting ResultStage 45 (MapPartitionsRDD[226] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:00:55 INFO MemoryStore: Block broadcast_65 stored as values in memory (estimated size 52.9 KiB, free 434.3 MiB)
26/01/04 17:00:55 INFO MemoryStore: Block broadcast_65_piece0 stored as bytes in memory (estimated size 20.2 KiB, free 434.3 MiB)
26/01/04 17:00:55 INFO BlockManagerInfo: Added broadcast_65_piece0 in memory on spark-master:33535 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:00:55 INFO SparkContext: Created broadcast 65 from broadcast at DAGScheduler.scala:1585
26/01/04 17:00:55 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 45 (MapPartitionsRDD[226] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
26/01/04 17:00:55 INFO TaskSchedulerImpl: Adding task set 45.0 with 3 tasks resource profile 0
26/01/04 17:00:55 INFO TaskSetManager: Starting task 1.0 in stage 45.0 (TID 97) (172.18.0.14, executor 1, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:00:55 INFO TaskSetManager: Starting task 0.0 in stage 45.0 (TID 98) (172.18.0.13, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:00:55 INFO TaskSetManager: Starting task 2.0 in stage 45.0 (TID 99) (172.18.0.13, executor 0, partition 2, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:00:55 INFO BlockManagerInfo: Added broadcast_65_piece0 in memory on 172.18.0.13:44401 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:00:55 INFO BlockManagerInfo: Added broadcast_65_piece0 in memory on 172.18.0.14:43523 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:00:55 INFO BlockManagerInfo: Added broadcast_64_piece0 in memory on 172.18.0.13:44401 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:00:55 INFO BlockManagerInfo: Added broadcast_64_piece0 in memory on 172.18.0.14:43523 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:00:56 INFO TaskSetManager: Finished task 2.0 in stage 45.0 (TID 99) in 1010 ms on 172.18.0.13 (executor 0) (1/3)
26/01/04 17:00:56 INFO TaskSetManager: Finished task 1.0 in stage 45.0 (TID 97) in 1036 ms on 172.18.0.14 (executor 1) (2/3)
26/01/04 17:00:56 INFO TaskSetManager: Finished task 0.0 in stage 45.0 (TID 98) in 1041 ms on 172.18.0.13 (executor 0) (3/3)
26/01/04 17:00:56 INFO TaskSchedulerImpl: Removed TaskSet 45.0, whose tasks have all completed, from pool 
26/01/04 17:00:56 INFO DAGScheduler: ResultStage 45 (start at NativeMethodAccessorImpl.java:0) finished in 1.064 s
26/01/04 17:00:56 INFO DAGScheduler: Job 44 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:00:56 INFO TaskSchedulerImpl: Killing all running tasks in stage 45: Stage finished
26/01/04 17:00:56 INFO DAGScheduler: Job 44 finished: start at NativeMethodAccessorImpl.java:0, took 1.078427 s
26/01/04 17:00:56 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 20, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@714ed244] is committing.
26/01/04 17:00:56 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 20, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@714ed244] committed.
26/01/04 17:00:56 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/20 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.20.c5890c7c-fc8b-4c51-91ea-41ae804c17ae.tmp
26/01/04 17:00:56 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.20.c5890c7c-fc8b-4c51-91ea-41ae804c17ae.tmp to file:/tmp/spark-checkpoint-enrichment/commits/20
26/01/04 17:00:56 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T17:00:54.732Z",
  "batchId" : 20,
  "numInputRows" : 30,
  "inputRowsPerSecond" : 2142.8571428571427,
  "processedRowsPerSecond" : 15.847860538827257,
  "durationMs" : {
    "addBatch" : 1529,
    "commitOffsets" : 157,
    "getBatch" : 0,
    "latestOffset" : 2,
    "queryPlanning" : 92,
    "triggerExecution" : 1893,
    "walCommit" : 111
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 2480,
        "1" : 2743,
        "0" : 2161
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 2491,
        "1" : 2753,
        "0" : 2170
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 2491,
        "1" : 2753,
        "0" : 2170
      }
    },
    "numInputRows" : 30,
    "inputRowsPerSecond" : 2142.8571428571427,
    "processedRowsPerSecond" : 15.847860538827257,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 31
  }
}
26/01/04 17:01:05 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/21 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.21.475e7787-f524-49d8-b956-20a8a56766b8.tmp
26/01/04 17:01:05 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.21.475e7787-f524-49d8-b956-20a8a56766b8.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/21
26/01/04 17:01:05 INFO MicroBatchExecution: Committed offsets for batch 21. Metadata OffsetSeqMetadata(0,1767546065753,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 17:01:05 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:01:05 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:01:05 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:01:05 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:01:06 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:01:06 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:01:06 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#18367 - airline_prefix.nullCount#18366) > 0)
26/01/04 17:01:06 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#18402 - min_flight_num.nullCount#18401) > 0)
26/01/04 17:01:06 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#18397 - max_flight_num.nullCount#18396) > 0)
26/01/04 17:01:06 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:01:06 INFO DAGScheduler: Got job 45 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 17:01:06 INFO DAGScheduler: Final stage: ResultStage 46 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:01:06 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:01:06 INFO DAGScheduler: Missing parents: List()
26/01/04 17:01:06 INFO DAGScheduler: Submitting ResultStage 46 (MapPartitionsRDD[231] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:01:06 INFO MemoryStore: Block broadcast_66 stored as values in memory (estimated size 38.5 KiB, free 434.2 MiB)
26/01/04 17:01:06 INFO MemoryStore: Block broadcast_66_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.2 MiB)
26/01/04 17:01:06 INFO BlockManagerInfo: Added broadcast_66_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:01:06 INFO SparkContext: Created broadcast 66 from broadcast at DAGScheduler.scala:1585
26/01/04 17:01:06 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 46 (MapPartitionsRDD[231] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 17:01:06 INFO TaskSchedulerImpl: Adding task set 46.0 with 2 tasks resource profile 0
26/01/04 17:01:06 INFO TaskSetManager: Starting task 0.0 in stage 46.0 (TID 100) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 17:01:06 INFO TaskSetManager: Starting task 1.0 in stage 46.0 (TID 101) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 17:01:06 INFO BlockManagerInfo: Added broadcast_66_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:01:06 INFO BlockManagerInfo: Added broadcast_66_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:01:06 INFO TaskSetManager: Finished task 0.0 in stage 46.0 (TID 100) in 65 ms on 172.18.0.14 (executor 1) (1/2)
26/01/04 17:01:06 INFO TaskSetManager: Finished task 1.0 in stage 46.0 (TID 101) in 87 ms on 172.18.0.13 (executor 0) (2/2)
26/01/04 17:01:06 INFO TaskSchedulerImpl: Removed TaskSet 46.0, whose tasks have all completed, from pool 
26/01/04 17:01:06 INFO DAGScheduler: ResultStage 46 (start at NativeMethodAccessorImpl.java:0) finished in 0.100 s
26/01/04 17:01:06 INFO DAGScheduler: Job 45 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:01:06 INFO TaskSchedulerImpl: Killing all running tasks in stage 46: Stage finished
26/01/04 17:01:06 INFO DAGScheduler: Job 45 finished: start at NativeMethodAccessorImpl.java:0, took 0.106364 s
26/01/04 17:01:06 INFO MemoryStore: Block broadcast_67_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.2 MiB)
26/01/04 17:01:06 INFO BlockManagerInfo: Added broadcast_67_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:01:06 INFO SparkContext: Created broadcast 67 from start at NativeMethodAccessorImpl.java:0
26/01/04 17:01:06 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 21, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@7533955b]. The input RDD has 3 partitions.
26/01/04 17:01:06 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:01:06 INFO DAGScheduler: Got job 46 (start at NativeMethodAccessorImpl.java:0) with 3 output partitions
26/01/04 17:01:06 INFO DAGScheduler: Final stage: ResultStage 47 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:01:06 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:01:06 INFO DAGScheduler: Missing parents: List()
26/01/04 17:01:06 INFO DAGScheduler: Submitting ResultStage 47 (MapPartitionsRDD[236] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:01:06 INFO MemoryStore: Block broadcast_68 stored as values in memory (estimated size 52.9 KiB, free 434.2 MiB)
26/01/04 17:01:06 INFO MemoryStore: Block broadcast_68_piece0 stored as bytes in memory (estimated size 20.2 KiB, free 434.1 MiB)
26/01/04 17:01:06 INFO BlockManagerInfo: Added broadcast_68_piece0 in memory on spark-master:33535 (size: 20.2 KiB, free: 434.3 MiB)
26/01/04 17:01:06 INFO SparkContext: Created broadcast 68 from broadcast at DAGScheduler.scala:1585
26/01/04 17:01:06 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 47 (MapPartitionsRDD[236] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
26/01/04 17:01:06 INFO TaskSchedulerImpl: Adding task set 47.0 with 3 tasks resource profile 0
26/01/04 17:01:06 INFO TaskSetManager: Starting task 0.0 in stage 47.0 (TID 102) (172.18.0.13, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:01:06 INFO TaskSetManager: Starting task 1.0 in stage 47.0 (TID 103) (172.18.0.14, executor 1, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:01:06 INFO TaskSetManager: Starting task 2.0 in stage 47.0 (TID 104) (172.18.0.13, executor 0, partition 2, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:01:06 INFO BlockManagerInfo: Added broadcast_68_piece0 in memory on 172.18.0.13:44401 (size: 20.2 KiB, free: 434.3 MiB)
26/01/04 17:01:06 INFO BlockManagerInfo: Added broadcast_68_piece0 in memory on 172.18.0.14:43523 (size: 20.2 KiB, free: 434.3 MiB)
26/01/04 17:01:06 INFO BlockManagerInfo: Added broadcast_67_piece0 in memory on 172.18.0.13:44401 (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:01:06 INFO BlockManagerInfo: Added broadcast_67_piece0 in memory on 172.18.0.14:43523 (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:01:06 INFO TaskSetManager: Finished task 1.0 in stage 47.0 (TID 103) in 783 ms on 172.18.0.14 (executor 1) (1/3)
26/01/04 17:01:07 INFO TaskSetManager: Finished task 0.0 in stage 47.0 (TID 102) in 835 ms on 172.18.0.13 (executor 0) (2/3)
26/01/04 17:01:07 INFO TaskSetManager: Finished task 2.0 in stage 47.0 (TID 104) in 841 ms on 172.18.0.13 (executor 0) (3/3)
26/01/04 17:01:07 INFO TaskSchedulerImpl: Removed TaskSet 47.0, whose tasks have all completed, from pool 
26/01/04 17:01:07 INFO DAGScheduler: ResultStage 47 (start at NativeMethodAccessorImpl.java:0) finished in 0.865 s
26/01/04 17:01:07 INFO DAGScheduler: Job 46 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:01:07 INFO TaskSchedulerImpl: Killing all running tasks in stage 47: Stage finished
26/01/04 17:01:07 INFO DAGScheduler: Job 46 finished: start at NativeMethodAccessorImpl.java:0, took 0.871771 s
26/01/04 17:01:07 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 21, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@7533955b] is committing.
26/01/04 17:01:07 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 21, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@7533955b] committed.
26/01/04 17:01:07 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/21 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.21.2b563e7a-6b9f-4ab7-bcfd-eff530c0dce8.tmp
26/01/04 17:01:07 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.21.2b563e7a-6b9f-4ab7-bcfd-eff530c0dce8.tmp to file:/tmp/spark-checkpoint-enrichment/commits/21
26/01/04 17:01:07 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T17:01:05.750Z",
  "batchId" : 21,
  "numInputRows" : 6,
  "inputRowsPerSecond" : 375.0,
  "processedRowsPerSecond" : 4.051316677920324,
  "durationMs" : {
    "addBatch" : 1114,
    "commitOffsets" : 179,
    "getBatch" : 0,
    "latestOffset" : 3,
    "queryPlanning" : 41,
    "triggerExecution" : 1481,
    "walCommit" : 143
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 2491,
        "1" : 2753,
        "0" : 2170
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 2493,
        "1" : 2756,
        "0" : 2171
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 2493,
        "1" : 2756,
        "0" : 2171
      }
    },
    "numInputRows" : 6,
    "inputRowsPerSecond" : 375.0,
    "processedRowsPerSecond" : 4.051316677920324,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 6
  }
}
26/01/04 17:01:07 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/22 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.22.29b5ef27-298a-493b-9983-2c1f967ffdcd.tmp
26/01/04 17:01:07 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.22.29b5ef27-298a-493b-9983-2c1f967ffdcd.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/22
26/01/04 17:01:07 INFO MicroBatchExecution: Committed offsets for batch 22. Metadata OffsetSeqMetadata(0,1767546067242,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 17:01:07 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:01:07 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:01:07 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:01:07 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:01:07 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:01:07 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:01:07 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#19171 - airline_prefix.nullCount#19170) > 0)
26/01/04 17:01:07 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#19206 - min_flight_num.nullCount#19205) > 0)
26/01/04 17:01:07 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#19201 - max_flight_num.nullCount#19200) > 0)
26/01/04 17:01:07 INFO BlockManagerInfo: Removed broadcast_63_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:01:07 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:01:07 INFO DAGScheduler: Got job 47 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 17:01:07 INFO DAGScheduler: Final stage: ResultStage 48 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:01:07 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:01:07 INFO DAGScheduler: Missing parents: List()
26/01/04 17:01:07 INFO DAGScheduler: Submitting ResultStage 48 (MapPartitionsRDD[241] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:01:07 INFO BlockManagerInfo: Removed broadcast_63_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:01:07 INFO BlockManagerInfo: Removed broadcast_63_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:01:07 INFO MemoryStore: Block broadcast_69 stored as values in memory (estimated size 38.5 KiB, free 434.2 MiB)
26/01/04 17:01:07 INFO MemoryStore: Block broadcast_69_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.1 MiB)
26/01/04 17:01:07 INFO BlockManagerInfo: Added broadcast_69_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:01:07 INFO SparkContext: Created broadcast 69 from broadcast at DAGScheduler.scala:1585
26/01/04 17:01:07 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 48 (MapPartitionsRDD[241] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 17:01:07 INFO TaskSchedulerImpl: Adding task set 48.0 with 2 tasks resource profile 0
26/01/04 17:01:07 INFO TaskSetManager: Starting task 1.0 in stage 48.0 (TID 105) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 17:01:07 INFO TaskSetManager: Starting task 0.0 in stage 48.0 (TID 106) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 17:01:07 INFO BlockManagerInfo: Removed broadcast_67_piece0 on spark-master:33535 in memory (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:01:07 INFO BlockManagerInfo: Removed broadcast_67_piece0 on 172.18.0.13:44401 in memory (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:01:07 INFO BlockManagerInfo: Removed broadcast_67_piece0 on 172.18.0.14:43523 in memory (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:01:07 INFO BlockManagerInfo: Added broadcast_69_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:01:07 INFO BlockManagerInfo: Removed broadcast_52_piece0 on spark-master:33535 in memory (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:01:07 INFO BlockManagerInfo: Added broadcast_69_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:01:07 INFO BlockManagerInfo: Removed broadcast_52_piece0 on 172.18.0.14:43523 in memory (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:01:07 INFO BlockManagerInfo: Removed broadcast_52_piece0 on 172.18.0.13:44401 in memory (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:01:07 INFO BlockManagerInfo: Removed broadcast_66_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:01:07 INFO BlockManagerInfo: Removed broadcast_66_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:01:07 INFO BlockManagerInfo: Removed broadcast_66_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:01:07 INFO TaskSetManager: Finished task 0.0 in stage 48.0 (TID 106) in 96 ms on 172.18.0.14 (executor 1) (1/2)
26/01/04 17:01:07 INFO TaskSetManager: Finished task 1.0 in stage 48.0 (TID 105) in 105 ms on 172.18.0.13 (executor 0) (2/2)
26/01/04 17:01:07 INFO TaskSchedulerImpl: Removed TaskSet 48.0, whose tasks have all completed, from pool 
26/01/04 17:01:07 INFO DAGScheduler: ResultStage 48 (start at NativeMethodAccessorImpl.java:0) finished in 0.139 s
26/01/04 17:01:07 INFO DAGScheduler: Job 47 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:01:07 INFO TaskSchedulerImpl: Killing all running tasks in stage 48: Stage finished
26/01/04 17:01:07 INFO DAGScheduler: Job 47 finished: start at NativeMethodAccessorImpl.java:0, took 0.150036 s
26/01/04 17:01:07 INFO BlockManagerInfo: Removed broadcast_64_piece0 on spark-master:33535 in memory (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:01:07 INFO BlockManagerInfo: Removed broadcast_64_piece0 on 172.18.0.14:43523 in memory (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:01:07 INFO MemoryStore: Block broadcast_70_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.2 MiB)
26/01/04 17:01:07 INFO BlockManagerInfo: Added broadcast_70_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:01:07 INFO BlockManagerInfo: Removed broadcast_64_piece0 on 172.18.0.13:44401 in memory (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:01:07 INFO SparkContext: Created broadcast 70 from start at NativeMethodAccessorImpl.java:0
26/01/04 17:01:07 INFO BlockManagerInfo: Removed broadcast_65_piece0 on spark-master:33535 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:01:07 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 22, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@ac7d3f2]. The input RDD has 3 partitions.
26/01/04 17:01:07 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:01:07 INFO DAGScheduler: Got job 48 (start at NativeMethodAccessorImpl.java:0) with 3 output partitions
26/01/04 17:01:07 INFO DAGScheduler: Final stage: ResultStage 49 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:01:07 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:01:07 INFO DAGScheduler: Missing parents: List()
26/01/04 17:01:07 INFO DAGScheduler: Submitting ResultStage 49 (MapPartitionsRDD[246] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:01:07 INFO BlockManagerInfo: Removed broadcast_65_piece0 on 172.18.0.14:43523 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:01:07 INFO BlockManagerInfo: Removed broadcast_65_piece0 on 172.18.0.13:44401 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:01:07 INFO MemoryStore: Block broadcast_71 stored as values in memory (estimated size 52.9 KiB, free 434.2 MiB)
26/01/04 17:01:07 INFO MemoryStore: Block broadcast_71_piece0 stored as bytes in memory (estimated size 20.2 KiB, free 434.2 MiB)
26/01/04 17:01:07 INFO BlockManagerInfo: Added broadcast_71_piece0 in memory on spark-master:33535 (size: 20.2 KiB, free: 434.3 MiB)
26/01/04 17:01:07 INFO SparkContext: Created broadcast 71 from broadcast at DAGScheduler.scala:1585
26/01/04 17:01:07 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 49 (MapPartitionsRDD[246] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
26/01/04 17:01:07 INFO TaskSchedulerImpl: Adding task set 49.0 with 3 tasks resource profile 0
26/01/04 17:01:07 INFO BlockManagerInfo: Removed broadcast_69_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:01:07 INFO BlockManagerInfo: Removed broadcast_69_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:01:07 INFO TaskSetManager: Starting task 0.0 in stage 49.0 (TID 107) (172.18.0.13, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:01:07 INFO BlockManagerInfo: Removed broadcast_69_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:01:07 INFO TaskSetManager: Starting task 1.0 in stage 49.0 (TID 108) (172.18.0.14, executor 1, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:01:07 INFO TaskSetManager: Starting task 2.0 in stage 49.0 (TID 109) (172.18.0.13, executor 0, partition 2, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:01:07 INFO BlockManagerInfo: Removed broadcast_68_piece0 on spark-master:33535 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:01:07 INFO BlockManagerInfo: Removed broadcast_68_piece0 on 172.18.0.13:44401 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:01:07 INFO BlockManagerInfo: Removed broadcast_68_piece0 on 172.18.0.14:43523 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:01:08 INFO BlockManagerInfo: Added broadcast_71_piece0 in memory on 172.18.0.13:44401 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:01:08 INFO BlockManagerInfo: Added broadcast_71_piece0 in memory on 172.18.0.14:43523 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:01:08 INFO BlockManagerInfo: Added broadcast_70_piece0 in memory on 172.18.0.13:44401 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:01:08 INFO BlockManagerInfo: Added broadcast_70_piece0 in memory on 172.18.0.14:43523 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:01:08 INFO TaskSetManager: Finished task 0.0 in stage 49.0 (TID 107) in 292 ms on 172.18.0.13 (executor 0) (1/3)
26/01/04 17:01:08 INFO TaskSetManager: Finished task 2.0 in stage 49.0 (TID 109) in 266 ms on 172.18.0.13 (executor 0) (2/3)
26/01/04 17:01:08 INFO TaskSetManager: Finished task 1.0 in stage 49.0 (TID 108) in 278 ms on 172.18.0.14 (executor 1) (3/3)
26/01/04 17:01:08 INFO DAGScheduler: ResultStage 49 (start at NativeMethodAccessorImpl.java:0) finished in 0.331 s
26/01/04 17:01:08 INFO DAGScheduler: Job 48 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:01:08 INFO TaskSchedulerImpl: Removed TaskSet 49.0, whose tasks have all completed, from pool 
26/01/04 17:01:08 INFO TaskSchedulerImpl: Killing all running tasks in stage 49: Stage finished
26/01/04 17:01:08 INFO DAGScheduler: Job 48 finished: start at NativeMethodAccessorImpl.java:0, took 0.336498 s
26/01/04 17:01:08 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 22, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@ac7d3f2] is committing.
26/01/04 17:01:08 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 22, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@ac7d3f2] committed.
26/01/04 17:01:08 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/22 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.22.96cfbc61-3bff-4caa-a6c6-77011c88dd8b.tmp
26/01/04 17:01:08 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.22.96cfbc61-3bff-4caa-a6c6-77011c88dd8b.tmp to file:/tmp/spark-checkpoint-enrichment/commits/22
26/01/04 17:01:08 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T17:01:07.237Z",
  "batchId" : 22,
  "numInputRows" : 24,
  "inputRowsPerSecond" : 16.139878950907868,
  "processedRowsPerSecond" : 21.641118124436428,
  "durationMs" : {
    "addBatch" : 736,
    "commitOffsets" : 138,
    "getBatch" : 0,
    "latestOffset" : 5,
    "queryPlanning" : 72,
    "triggerExecution" : 1109,
    "walCommit" : 156
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 2493,
        "1" : 2756,
        "0" : 2171
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 2502,
        "1" : 2763,
        "0" : 2179
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 2502,
        "1" : 2763,
        "0" : 2179
      }
    },
    "numInputRows" : 24,
    "inputRowsPerSecond" : 16.139878950907868,
    "processedRowsPerSecond" : 21.641118124436428,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 25
  }
}
26/01/04 17:01:16 WARN KafkaOffsetReaderAdmin: Error in attempt 1 getting Kafka offsets: 
java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
	at java.base/java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:396)
	at java.base/java.util.concurrent.CompletableFuture.get(CompletableFuture.java:2073)
	at org.apache.kafka.common.internals.KafkaFutureImpl.get(KafkaFutureImpl.java:165)
	at org.apache.spark.sql.kafka010.ConsumerStrategy.retrieveAllPartitions(ConsumerStrategy.scala:66)
	at org.apache.spark.sql.kafka010.ConsumerStrategy.retrieveAllPartitions$(ConsumerStrategy.scala:65)
	at org.apache.spark.sql.kafka010.SubscribeStrategy.retrieveAllPartitions(ConsumerStrategy.scala:102)
	at org.apache.spark.sql.kafka010.SubscribeStrategy.assignedTopicPartitions(ConsumerStrategy.scala:113)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.$anonfun$partitionsAssignedToAdmin$1(KafkaOffsetReaderAdmin.scala:499)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.withRetries(KafkaOffsetReaderAdmin.scala:518)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.partitionsAssignedToAdmin(KafkaOffsetReaderAdmin.scala:498)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.fetchLatestOffsets(KafkaOffsetReaderAdmin.scala:297)
	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.latestOffset(KafkaMicroBatchStream.scala:130)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at scala.collection.TraversableLike.map(TraversableLike.scala:286)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:279)
	at scala.collection.AbstractTraversable.map(Traversable.scala:108)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)
	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)
	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)
Caused by: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
26/01/04 17:01:17 INFO AppInfoParser: App info kafka.admin.client for adminclient-1 unregistered
26/01/04 17:01:17 INFO Metrics: Metrics scheduler closed
26/01/04 17:01:17 INFO Metrics: Closing reporter org.apache.kafka.common.metrics.JmxReporter
26/01/04 17:01:17 INFO Metrics: Metrics reporters closed
26/01/04 17:01:17 INFO AdminClientConfig: AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.servers = [kafka:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

26/01/04 17:01:17 INFO AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
26/01/04 17:01:17 INFO AppInfoParser: Kafka version: 3.5.1
26/01/04 17:01:17 INFO AppInfoParser: Kafka commitId: 2c6fb6c54472e90a
26/01/04 17:01:17 INFO AppInfoParser: Kafka startTimeMs: 1767546077434
26/01/04 17:01:17 WARN KafkaOffsetReaderAdmin: Found incorrect offsets in some partitions (partition, previous offset, fetched offset): ArrayBuffer((aviation-india-states-0,2179,9))
26/01/04 17:01:17 WARN KafkaOffsetReaderAdmin: Retrying to fetch latest offsets because of incorrect offsets
26/01/04 17:01:18 WARN KafkaOffsetReaderAdmin: Found incorrect offsets in some partitions (partition, previous offset, fetched offset): ArrayBuffer((aviation-india-states-0,2179,9))
26/01/04 17:01:18 WARN KafkaOffsetReaderAdmin: Retrying to fetch latest offsets because of incorrect offsets
26/01/04 17:01:19 WARN KafkaOffsetReaderAdmin: Found incorrect offsets in some partitions (partition, previous offset, fetched offset): ArrayBuffer((aviation-india-states-0,2179,9))
26/01/04 17:01:19 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/23 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.23.e4bf05ed-ac90-4673-97b5-45c7f096b7b2.tmp
26/01/04 17:01:19 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.23.e4bf05ed-ac90-4673-97b5-45c7f096b7b2.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/23
26/01/04 17:01:19 INFO MicroBatchExecution: Committed offsets for batch 23. Metadata OffsetSeqMetadata(0,1767546079489,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 17:01:19 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:01:19 WARN KafkaMicroBatchStream: Set(aviation-india-states-2, aviation-india-states-1) are gone. Some data may have been missed.. 
Some data may have been lost because they are not available in Kafka any more; either the
 data was aged out by Kafka or the topic may have been deleted before all the data in the
 topic was processed. If you want your streaming query to fail on such cases, set the source
 option "failOnDataLoss" to "true".
    
26/01/04 17:01:19 WARN KafkaMicroBatchStream: Partition aviation-india-states-0's offset was changed from 2179 to 9, some data may have been missed. 
Some data may have been lost because they are not available in Kafka any more; either the
 data was aged out by Kafka or the topic may have been deleted before all the data in the
 topic was processed. If you want your streaming query to fail on such cases, set the source
 option "failOnDataLoss" to "true".
    
26/01/04 17:01:19 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:01:19 WARN KafkaMicroBatchStream: Set(aviation-india-states-2, aviation-india-states-1) are gone. Some data may have been missed.. 
Some data may have been lost because they are not available in Kafka any more; either the
 data was aged out by Kafka or the topic may have been deleted before all the data in the
 topic was processed. If you want your streaming query to fail on such cases, set the source
 option "failOnDataLoss" to "true".
    
26/01/04 17:01:19 WARN KafkaMicroBatchStream: Partition aviation-india-states-0's offset was changed from 2179 to 9, some data may have been missed. 
Some data may have been lost because they are not available in Kafka any more; either the
 data was aged out by Kafka or the topic may have been deleted before all the data in the
 topic was processed. If you want your streaming query to fail on such cases, set the source
 option "failOnDataLoss" to "true".
    
26/01/04 17:01:19 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:01:19 WARN KafkaMicroBatchStream: Set(aviation-india-states-2, aviation-india-states-1) are gone. Some data may have been missed.. 
Some data may have been lost because they are not available in Kafka any more; either the
 data was aged out by Kafka or the topic may have been deleted before all the data in the
 topic was processed. If you want your streaming query to fail on such cases, set the source
 option "failOnDataLoss" to "true".
    
26/01/04 17:01:19 WARN KafkaMicroBatchStream: Partition aviation-india-states-0's offset was changed from 2179 to 9, some data may have been missed. 
Some data may have been lost because they are not available in Kafka any more; either the
 data was aged out by Kafka or the topic may have been deleted before all the data in the
 topic was processed. If you want your streaming query to fail on such cases, set the source
 option "failOnDataLoss" to "true".
    
26/01/04 17:01:19 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:01:19 WARN KafkaMicroBatchStream: Set(aviation-india-states-2, aviation-india-states-1) are gone. Some data may have been missed.. 
Some data may have been lost because they are not available in Kafka any more; either the
 data was aged out by Kafka or the topic may have been deleted before all the data in the
 topic was processed. If you want your streaming query to fail on such cases, set the source
 option "failOnDataLoss" to "true".
    
26/01/04 17:01:19 WARN KafkaMicroBatchStream: Partition aviation-india-states-0's offset was changed from 2179 to 9, some data may have been missed. 
Some data may have been lost because they are not available in Kafka any more; either the
 data was aged out by Kafka or the topic may have been deleted before all the data in the
 topic was processed. If you want your streaming query to fail on such cases, set the source
 option "failOnDataLoss" to "true".
    
26/01/04 17:01:19 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:01:19 WARN KafkaMicroBatchStream: Set(aviation-india-states-2, aviation-india-states-1) are gone. Some data may have been missed.. 
Some data may have been lost because they are not available in Kafka any more; either the
 data was aged out by Kafka or the topic may have been deleted before all the data in the
 topic was processed. If you want your streaming query to fail on such cases, set the source
 option "failOnDataLoss" to "true".
    
26/01/04 17:01:19 WARN KafkaMicroBatchStream: Partition aviation-india-states-0's offset was changed from 2179 to 9, some data may have been missed. 
Some data may have been lost because they are not available in Kafka any more; either the
 data was aged out by Kafka or the topic may have been deleted before all the data in the
 topic was processed. If you want your streaming query to fail on such cases, set the source
 option "failOnDataLoss" to "true".
    
26/01/04 17:01:19 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:01:19 WARN KafkaMicroBatchStream: Set(aviation-india-states-2, aviation-india-states-1) are gone. Some data may have been missed.. 
Some data may have been lost because they are not available in Kafka any more; either the
 data was aged out by Kafka or the topic may have been deleted before all the data in the
 topic was processed. If you want your streaming query to fail on such cases, set the source
 option "failOnDataLoss" to "true".
    
26/01/04 17:01:19 WARN KafkaMicroBatchStream: Partition aviation-india-states-0's offset was changed from 2179 to 9, some data may have been missed. 
Some data may have been lost because they are not available in Kafka any more; either the
 data was aged out by Kafka or the topic may have been deleted before all the data in the
 topic was processed. If you want your streaming query to fail on such cases, set the source
 option "failOnDataLoss" to "true".
    
26/01/04 17:01:19 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#19975 - airline_prefix.nullCount#19974) > 0)
26/01/04 17:01:19 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#20010 - min_flight_num.nullCount#20009) > 0)
26/01/04 17:01:19 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#20005 - max_flight_num.nullCount#20004) > 0)
26/01/04 17:01:19 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:01:19 INFO DAGScheduler: Got job 49 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 17:01:19 INFO DAGScheduler: Final stage: ResultStage 50 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:01:19 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:01:19 INFO DAGScheduler: Missing parents: List()
26/01/04 17:01:19 INFO DAGScheduler: Submitting ResultStage 50 (MapPartitionsRDD[251] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:01:19 INFO MemoryStore: Block broadcast_72 stored as values in memory (estimated size 38.5 KiB, free 434.3 MiB)
26/01/04 17:01:19 INFO MemoryStore: Block broadcast_72_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.3 MiB)
26/01/04 17:01:19 INFO BlockManagerInfo: Added broadcast_72_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:01:19 INFO SparkContext: Created broadcast 72 from broadcast at DAGScheduler.scala:1585
26/01/04 17:01:19 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 50 (MapPartitionsRDD[251] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 17:01:19 INFO TaskSchedulerImpl: Adding task set 50.0 with 2 tasks resource profile 0
26/01/04 17:01:19 INFO TaskSetManager: Starting task 1.0 in stage 50.0 (TID 110) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 17:01:19 INFO TaskSetManager: Starting task 0.0 in stage 50.0 (TID 111) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 17:01:19 INFO BlockManagerInfo: Added broadcast_72_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:01:19 INFO BlockManagerInfo: Added broadcast_72_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:01:19 INFO TaskSetManager: Finished task 1.0 in stage 50.0 (TID 110) in 35 ms on 172.18.0.13 (executor 0) (1/2)
26/01/04 17:01:19 INFO TaskSetManager: Finished task 0.0 in stage 50.0 (TID 111) in 36 ms on 172.18.0.14 (executor 1) (2/2)
26/01/04 17:01:19 INFO TaskSchedulerImpl: Removed TaskSet 50.0, whose tasks have all completed, from pool 
26/01/04 17:01:19 INFO DAGScheduler: ResultStage 50 (start at NativeMethodAccessorImpl.java:0) finished in 0.043 s
26/01/04 17:01:19 INFO DAGScheduler: Job 49 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:01:19 INFO TaskSchedulerImpl: Killing all running tasks in stage 50: Stage finished
26/01/04 17:01:19 INFO DAGScheduler: Job 49 finished: start at NativeMethodAccessorImpl.java:0, took 0.045023 s
26/01/04 17:01:19 INFO MemoryStore: Block broadcast_73_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.3 MiB)
26/01/04 17:01:19 INFO BlockManagerInfo: Added broadcast_73_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:01:19 INFO SparkContext: Created broadcast 73 from start at NativeMethodAccessorImpl.java:0
26/01/04 17:01:19 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 23, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@5d503570]. The input RDD has 1 partitions.
26/01/04 17:01:19 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:01:19 INFO DAGScheduler: Got job 50 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/01/04 17:01:19 INFO DAGScheduler: Final stage: ResultStage 51 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:01:19 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:01:19 INFO DAGScheduler: Missing parents: List()
26/01/04 17:01:19 INFO DAGScheduler: Submitting ResultStage 51 (ParallelCollectionRDD[257] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:01:19 INFO MemoryStore: Block broadcast_74 stored as values in memory (estimated size 4.4 KiB, free 434.3 MiB)
26/01/04 17:01:19 INFO MemoryStore: Block broadcast_74_piece0 stored as bytes in memory (estimated size 2.6 KiB, free 434.3 MiB)
26/01/04 17:01:19 INFO BlockManagerInfo: Added broadcast_74_piece0 in memory on spark-master:33535 (size: 2.6 KiB, free: 434.4 MiB)
26/01/04 17:01:19 INFO SparkContext: Created broadcast 74 from broadcast at DAGScheduler.scala:1585
26/01/04 17:01:19 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 51 (ParallelCollectionRDD[257] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/01/04 17:01:19 INFO TaskSchedulerImpl: Adding task set 51.0 with 1 tasks resource profile 0
26/01/04 17:01:19 INFO TaskSetManager: Starting task 0.0 in stage 51.0 (TID 112) (172.18.0.13, executor 0, partition 0, PROCESS_LOCAL, 7640 bytes) 
26/01/04 17:01:19 INFO BlockManagerInfo: Added broadcast_74_piece0 in memory on 172.18.0.13:44401 (size: 2.6 KiB, free: 434.4 MiB)
26/01/04 17:01:19 INFO TaskSetManager: Finished task 0.0 in stage 51.0 (TID 112) in 43 ms on 172.18.0.13 (executor 0) (1/1)
26/01/04 17:01:19 INFO TaskSchedulerImpl: Removed TaskSet 51.0, whose tasks have all completed, from pool 
26/01/04 17:01:19 INFO DAGScheduler: ResultStage 51 (start at NativeMethodAccessorImpl.java:0) finished in 0.051 s
26/01/04 17:01:19 INFO DAGScheduler: Job 50 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:01:19 INFO TaskSchedulerImpl: Killing all running tasks in stage 51: Stage finished
26/01/04 17:01:19 INFO DAGScheduler: Job 50 finished: start at NativeMethodAccessorImpl.java:0, took 0.057922 s
26/01/04 17:01:19 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 23, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@5d503570] is committing.
26/01/04 17:01:19 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 23, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@5d503570] committed.
26/01/04 17:01:19 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/23 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.23.9362dc85-562a-4d5a-86f6-c2057e8282a4.tmp
26/01/04 17:01:19 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.23.9362dc85-562a-4d5a-86f6-c2057e8282a4.tmp to file:/tmp/spark-checkpoint-enrichment/commits/23
26/01/04 17:01:19 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T17:01:16.340Z",
  "batchId" : 23,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "addBatch" : 210,
    "commitOffsets" : 96,
    "getBatch" : 0,
    "latestOffset" : 3149,
    "queryPlanning" : 51,
    "triggerExecution" : 3595,
    "walCommit" : 86
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 2502,
        "1" : 2763,
        "0" : 2179
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "0" : 9
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "0" : 9
      }
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 0
  }
}
26/01/04 17:01:27 INFO BlockManagerInfo: Removed broadcast_71_piece0 on spark-master:33535 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:01:27 INFO BlockManagerInfo: Removed broadcast_71_piece0 on 172.18.0.13:44401 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:01:27 INFO BlockManagerInfo: Removed broadcast_71_piece0 on 172.18.0.14:43523 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:01:27 INFO BlockManagerInfo: Removed broadcast_74_piece0 on spark-master:33535 in memory (size: 2.6 KiB, free: 434.4 MiB)
26/01/04 17:01:27 INFO BlockManagerInfo: Removed broadcast_74_piece0 on 172.18.0.13:44401 in memory (size: 2.6 KiB, free: 434.4 MiB)
26/01/04 17:01:27 INFO BlockManagerInfo: Removed broadcast_72_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:01:27 INFO BlockManagerInfo: Removed broadcast_72_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:01:27 INFO BlockManagerInfo: Removed broadcast_72_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:01:29 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/01/04 17:01:39 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/01/04 17:01:49 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/01/04 17:01:58 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/24 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.24.9de6d1ef-2b49-4241-b771-a33e7cbccd60.tmp
26/01/04 17:01:58 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.24.9de6d1ef-2b49-4241-b771-a33e7cbccd60.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/24
26/01/04 17:01:58 INFO MicroBatchExecution: Committed offsets for batch 24. Metadata OffsetSeqMetadata(0,1767546118131,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 17:01:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:01:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:01:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:01:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:01:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:01:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:01:58 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#20779 - airline_prefix.nullCount#20778) > 0)
26/01/04 17:01:58 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#20814 - min_flight_num.nullCount#20813) > 0)
26/01/04 17:01:58 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#20809 - max_flight_num.nullCount#20808) > 0)
26/01/04 17:01:58 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:01:58 INFO DAGScheduler: Got job 51 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 17:01:58 INFO DAGScheduler: Final stage: ResultStage 52 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:01:58 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:01:58 INFO DAGScheduler: Missing parents: List()
26/01/04 17:01:58 INFO DAGScheduler: Submitting ResultStage 52 (MapPartitionsRDD[262] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:01:58 INFO MemoryStore: Block broadcast_75 stored as values in memory (estimated size 38.5 KiB, free 434.4 MiB)
26/01/04 17:01:58 INFO MemoryStore: Block broadcast_75_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.3 MiB)
26/01/04 17:01:58 INFO BlockManagerInfo: Added broadcast_75_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:01:58 INFO BlockManagerInfo: Removed broadcast_73_piece0 on spark-master:33535 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:01:58 INFO SparkContext: Created broadcast 75 from broadcast at DAGScheduler.scala:1585
26/01/04 17:01:58 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 52 (MapPartitionsRDD[262] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 17:01:58 INFO TaskSchedulerImpl: Adding task set 52.0 with 2 tasks resource profile 0
26/01/04 17:01:58 INFO TaskSetManager: Starting task 1.0 in stage 52.0 (TID 113) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 17:01:58 INFO TaskSetManager: Starting task 0.0 in stage 52.0 (TID 114) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 17:01:58 INFO BlockManagerInfo: Added broadcast_75_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:01:58 INFO BlockManagerInfo: Added broadcast_75_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:01:58 INFO TaskSetManager: Finished task 0.0 in stage 52.0 (TID 114) in 42 ms on 172.18.0.14 (executor 1) (1/2)
26/01/04 17:01:58 INFO TaskSetManager: Finished task 1.0 in stage 52.0 (TID 113) in 50 ms on 172.18.0.13 (executor 0) (2/2)
26/01/04 17:01:58 INFO TaskSchedulerImpl: Removed TaskSet 52.0, whose tasks have all completed, from pool 
26/01/04 17:01:58 INFO DAGScheduler: ResultStage 52 (start at NativeMethodAccessorImpl.java:0) finished in 0.063 s
26/01/04 17:01:58 INFO DAGScheduler: Job 51 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:01:58 INFO TaskSchedulerImpl: Killing all running tasks in stage 52: Stage finished
26/01/04 17:01:58 INFO DAGScheduler: Job 51 finished: start at NativeMethodAccessorImpl.java:0, took 0.065465 s
26/01/04 17:01:58 INFO MemoryStore: Block broadcast_76_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.3 MiB)
26/01/04 17:01:58 INFO BlockManagerInfo: Added broadcast_76_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:01:58 INFO SparkContext: Created broadcast 76 from start at NativeMethodAccessorImpl.java:0
26/01/04 17:01:58 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 24, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@721ba628]. The input RDD has 1 partitions.
26/01/04 17:01:58 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:01:58 INFO DAGScheduler: Got job 52 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/01/04 17:01:58 INFO DAGScheduler: Final stage: ResultStage 53 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:01:58 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:01:58 INFO DAGScheduler: Missing parents: List()
26/01/04 17:01:58 INFO DAGScheduler: Submitting ResultStage 53 (MapPartitionsRDD[267] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:01:58 INFO MemoryStore: Block broadcast_77 stored as values in memory (estimated size 52.9 KiB, free 434.3 MiB)
26/01/04 17:01:58 INFO MemoryStore: Block broadcast_77_piece0 stored as bytes in memory (estimated size 20.2 KiB, free 434.3 MiB)
26/01/04 17:01:58 INFO BlockManagerInfo: Added broadcast_77_piece0 in memory on spark-master:33535 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:01:58 INFO SparkContext: Created broadcast 77 from broadcast at DAGScheduler.scala:1585
26/01/04 17:01:58 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 53 (MapPartitionsRDD[267] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/01/04 17:01:58 INFO TaskSchedulerImpl: Adding task set 53.0 with 1 tasks resource profile 0
26/01/04 17:01:58 INFO TaskSetManager: Starting task 0.0 in stage 53.0 (TID 115) (172.18.0.13, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:01:58 INFO BlockManagerInfo: Added broadcast_77_piece0 in memory on 172.18.0.13:44401 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:01:58 INFO BlockManagerInfo: Added broadcast_76_piece0 in memory on 172.18.0.13:44401 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:01:59 INFO TaskSetManager: Finished task 0.0 in stage 53.0 (TID 115) in 648 ms on 172.18.0.13 (executor 0) (1/1)
26/01/04 17:01:59 INFO TaskSchedulerImpl: Removed TaskSet 53.0, whose tasks have all completed, from pool 
26/01/04 17:01:59 INFO DAGScheduler: ResultStage 53 (start at NativeMethodAccessorImpl.java:0) finished in 0.653 s
26/01/04 17:01:59 INFO DAGScheduler: Job 52 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:01:59 INFO TaskSchedulerImpl: Killing all running tasks in stage 53: Stage finished
26/01/04 17:01:59 INFO DAGScheduler: Job 52 finished: start at NativeMethodAccessorImpl.java:0, took 0.655838 s
26/01/04 17:01:59 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 24, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@721ba628] is committing.
26/01/04 17:01:59 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 24, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@721ba628] committed.
26/01/04 17:01:59 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/24 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.24.0b6bf049-521c-471d-8e08-563e309c8e98.tmp
26/01/04 17:01:59 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.24.0b6bf049-521c-471d-8e08-563e309c8e98.tmp to file:/tmp/spark-checkpoint-enrichment/commits/24
26/01/04 17:01:59 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T17:01:58.129Z",
  "batchId" : 24,
  "numInputRows" : 30,
  "inputRowsPerSecond" : 2500.0,
  "processedRowsPerSecond" : 29.498525073746315,
  "durationMs" : {
    "addBatch" : 803,
    "commitOffsets" : 71,
    "getBatch" : 0,
    "latestOffset" : 2,
    "queryPlanning" : 46,
    "triggerExecution" : 1017,
    "walCommit" : 94
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "0" : 9
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "0" : 39
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "0" : 39
      }
    },
    "numInputRows" : 30,
    "inputRowsPerSecond" : 2500.0,
    "processedRowsPerSecond" : 29.498525073746315,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 31
  }
}
26/01/04 17:02:09 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/25 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.25.fb67f046-2fcd-48c5-8292-4bd81c51e577.tmp
26/01/04 17:02:09 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.25.fb67f046-2fcd-48c5-8292-4bd81c51e577.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/25
26/01/04 17:02:09 INFO MicroBatchExecution: Committed offsets for batch 25. Metadata OffsetSeqMetadata(0,1767546129144,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 17:02:09 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:02:09 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:02:09 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:02:09 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:02:09 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:02:09 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:02:09 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#21583 - airline_prefix.nullCount#21582) > 0)
26/01/04 17:02:09 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#21618 - min_flight_num.nullCount#21617) > 0)
26/01/04 17:02:09 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#21613 - max_flight_num.nullCount#21612) > 0)
26/01/04 17:02:09 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:02:09 INFO DAGScheduler: Got job 53 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 17:02:09 INFO DAGScheduler: Final stage: ResultStage 54 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:02:09 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:02:09 INFO DAGScheduler: Missing parents: List()
26/01/04 17:02:09 INFO DAGScheduler: Submitting ResultStage 54 (MapPartitionsRDD[272] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:02:09 INFO MemoryStore: Block broadcast_78 stored as values in memory (estimated size 38.5 KiB, free 434.2 MiB)
26/01/04 17:02:09 INFO MemoryStore: Block broadcast_78_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.2 MiB)
26/01/04 17:02:09 INFO BlockManagerInfo: Added broadcast_78_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:02:09 INFO SparkContext: Created broadcast 78 from broadcast at DAGScheduler.scala:1585
26/01/04 17:02:09 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 54 (MapPartitionsRDD[272] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 17:02:09 INFO TaskSchedulerImpl: Adding task set 54.0 with 2 tasks resource profile 0
26/01/04 17:02:09 INFO TaskSetManager: Starting task 1.0 in stage 54.0 (TID 116) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 17:02:09 INFO TaskSetManager: Starting task 0.0 in stage 54.0 (TID 117) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 17:02:09 INFO BlockManagerInfo: Added broadcast_78_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:02:09 INFO BlockManagerInfo: Added broadcast_78_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:02:09 INFO TaskSetManager: Finished task 0.0 in stage 54.0 (TID 117) in 30 ms on 172.18.0.14 (executor 1) (1/2)
26/01/04 17:02:09 INFO TaskSetManager: Finished task 1.0 in stage 54.0 (TID 116) in 32 ms on 172.18.0.13 (executor 0) (2/2)
26/01/04 17:02:09 INFO TaskSchedulerImpl: Removed TaskSet 54.0, whose tasks have all completed, from pool 
26/01/04 17:02:09 INFO DAGScheduler: ResultStage 54 (start at NativeMethodAccessorImpl.java:0) finished in 0.038 s
26/01/04 17:02:09 INFO DAGScheduler: Job 53 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:02:09 INFO TaskSchedulerImpl: Killing all running tasks in stage 54: Stage finished
26/01/04 17:02:09 INFO DAGScheduler: Job 53 finished: start at NativeMethodAccessorImpl.java:0, took 0.041354 s
26/01/04 17:02:09 INFO MemoryStore: Block broadcast_79_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.2 MiB)
26/01/04 17:02:09 INFO BlockManagerInfo: Added broadcast_79_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:02:09 INFO SparkContext: Created broadcast 79 from start at NativeMethodAccessorImpl.java:0
26/01/04 17:02:09 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 25, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@3877b8fe]. The input RDD has 1 partitions.
26/01/04 17:02:09 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:02:09 INFO DAGScheduler: Got job 54 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/01/04 17:02:09 INFO DAGScheduler: Final stage: ResultStage 55 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:02:09 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:02:09 INFO DAGScheduler: Missing parents: List()
26/01/04 17:02:09 INFO DAGScheduler: Submitting ResultStage 55 (MapPartitionsRDD[277] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:02:09 INFO MemoryStore: Block broadcast_80 stored as values in memory (estimated size 52.9 KiB, free 434.2 MiB)
26/01/04 17:02:09 INFO MemoryStore: Block broadcast_80_piece0 stored as bytes in memory (estimated size 20.2 KiB, free 434.1 MiB)
26/01/04 17:02:09 INFO BlockManagerInfo: Added broadcast_80_piece0 in memory on spark-master:33535 (size: 20.2 KiB, free: 434.3 MiB)
26/01/04 17:02:09 INFO SparkContext: Created broadcast 80 from broadcast at DAGScheduler.scala:1585
26/01/04 17:02:09 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 55 (MapPartitionsRDD[277] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/01/04 17:02:09 INFO TaskSchedulerImpl: Adding task set 55.0 with 1 tasks resource profile 0
26/01/04 17:02:09 INFO TaskSetManager: Starting task 0.0 in stage 55.0 (TID 118) (172.18.0.13, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:02:09 INFO BlockManagerInfo: Added broadcast_80_piece0 in memory on 172.18.0.13:44401 (size: 20.2 KiB, free: 434.3 MiB)
26/01/04 17:02:09 INFO BlockManagerInfo: Added broadcast_79_piece0 in memory on 172.18.0.13:44401 (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:02:09 INFO TaskSetManager: Finished task 0.0 in stage 55.0 (TID 118) in 606 ms on 172.18.0.13 (executor 0) (1/1)
26/01/04 17:02:09 INFO TaskSchedulerImpl: Removed TaskSet 55.0, whose tasks have all completed, from pool 
26/01/04 17:02:09 INFO DAGScheduler: ResultStage 55 (start at NativeMethodAccessorImpl.java:0) finished in 0.612 s
26/01/04 17:02:09 INFO DAGScheduler: Job 54 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:02:09 INFO TaskSchedulerImpl: Killing all running tasks in stage 55: Stage finished
26/01/04 17:02:09 INFO DAGScheduler: Job 54 finished: start at NativeMethodAccessorImpl.java:0, took 0.614664 s
26/01/04 17:02:09 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 25, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@3877b8fe] is committing.
26/01/04 17:02:09 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 25, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@3877b8fe] committed.
26/01/04 17:02:09 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/25 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.25.1e87fe73-0a62-4d85-9f50-c4761af1a62f.tmp
26/01/04 17:02:10 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.25.1e87fe73-0a62-4d85-9f50-c4761af1a62f.tmp to file:/tmp/spark-checkpoint-enrichment/commits/25
26/01/04 17:02:10 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T17:02:09.142Z",
  "batchId" : 25,
  "numInputRows" : 30,
  "inputRowsPerSecond" : 2500.0,
  "processedRowsPerSecond" : 33.149171270718234,
  "durationMs" : {
    "addBatch" : 736,
    "commitOffsets" : 64,
    "getBatch" : 0,
    "latestOffset" : 2,
    "queryPlanning" : 32,
    "triggerExecution" : 905,
    "walCommit" : 70
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "0" : 39
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "0" : 69
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "0" : 69
      }
    },
    "numInputRows" : 30,
    "inputRowsPerSecond" : 2500.0,
    "processedRowsPerSecond" : 33.149171270718234,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 31
  }
}
26/01/04 17:02:12 INFO BlockManagerInfo: Removed broadcast_70_piece0 on spark-master:33535 in memory (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:02:12 INFO BlockManagerInfo: Removed broadcast_70_piece0 on 172.18.0.13:44401 in memory (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:02:12 INFO BlockManagerInfo: Removed broadcast_70_piece0 on 172.18.0.14:43523 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:02:12 INFO BlockManagerInfo: Removed broadcast_78_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:02:12 INFO BlockManagerInfo: Removed broadcast_78_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:02:12 INFO BlockManagerInfo: Removed broadcast_78_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:02:12 INFO BlockManagerInfo: Removed broadcast_76_piece0 on spark-master:33535 in memory (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:02:12 INFO BlockManagerInfo: Removed broadcast_76_piece0 on 172.18.0.13:44401 in memory (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:02:12 INFO BlockManagerInfo: Removed broadcast_75_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:02:12 INFO BlockManagerInfo: Removed broadcast_75_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:02:12 INFO BlockManagerInfo: Removed broadcast_75_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:02:12 INFO BlockManagerInfo: Removed broadcast_77_piece0 on spark-master:33535 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:02:12 INFO BlockManagerInfo: Removed broadcast_77_piece0 on 172.18.0.13:44401 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:02:12 INFO BlockManagerInfo: Removed broadcast_80_piece0 on spark-master:33535 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:02:12 INFO BlockManagerInfo: Removed broadcast_80_piece0 on 172.18.0.13:44401 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:02:20 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/01/04 17:02:21 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/26 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.26.71a434e6-2495-4074-9e12-4f9ec31d196a.tmp
26/01/04 17:02:21 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.26.71a434e6-2495-4074-9e12-4f9ec31d196a.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/26
26/01/04 17:02:21 INFO MicroBatchExecution: Committed offsets for batch 26. Metadata OffsetSeqMetadata(0,1767546141664,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 17:02:21 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:02:21 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:02:21 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:02:21 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:02:21 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:02:21 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:02:21 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#22387 - airline_prefix.nullCount#22386) > 0)
26/01/04 17:02:21 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#22422 - min_flight_num.nullCount#22421) > 0)
26/01/04 17:02:21 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#22417 - max_flight_num.nullCount#22416) > 0)
26/01/04 17:02:21 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:02:21 INFO DAGScheduler: Got job 55 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 17:02:21 INFO DAGScheduler: Final stage: ResultStage 56 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:02:21 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:02:21 INFO DAGScheduler: Missing parents: List()
26/01/04 17:02:21 INFO DAGScheduler: Submitting ResultStage 56 (MapPartitionsRDD[282] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:02:21 INFO MemoryStore: Block broadcast_81 stored as values in memory (estimated size 38.5 KiB, free 434.4 MiB)
26/01/04 17:02:21 INFO MemoryStore: Block broadcast_81_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.3 MiB)
26/01/04 17:02:21 INFO BlockManagerInfo: Added broadcast_81_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:02:21 INFO SparkContext: Created broadcast 81 from broadcast at DAGScheduler.scala:1585
26/01/04 17:02:21 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 56 (MapPartitionsRDD[282] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 17:02:21 INFO TaskSchedulerImpl: Adding task set 56.0 with 2 tasks resource profile 0
26/01/04 17:02:21 INFO TaskSetManager: Starting task 0.0 in stage 56.0 (TID 119) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 17:02:21 INFO TaskSetManager: Starting task 1.0 in stage 56.0 (TID 120) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 17:02:21 INFO BlockManagerInfo: Added broadcast_81_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:02:21 INFO BlockManagerInfo: Added broadcast_81_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:02:21 INFO TaskSetManager: Finished task 1.0 in stage 56.0 (TID 120) in 24 ms on 172.18.0.13 (executor 0) (1/2)
26/01/04 17:02:21 INFO TaskSetManager: Finished task 0.0 in stage 56.0 (TID 119) in 28 ms on 172.18.0.14 (executor 1) (2/2)
26/01/04 17:02:21 INFO TaskSchedulerImpl: Removed TaskSet 56.0, whose tasks have all completed, from pool 
26/01/04 17:02:21 INFO DAGScheduler: ResultStage 56 (start at NativeMethodAccessorImpl.java:0) finished in 0.036 s
26/01/04 17:02:21 INFO DAGScheduler: Job 55 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:02:21 INFO TaskSchedulerImpl: Killing all running tasks in stage 56: Stage finished
26/01/04 17:02:21 INFO DAGScheduler: Job 55 finished: start at NativeMethodAccessorImpl.java:0, took 0.039828 s
26/01/04 17:02:21 INFO MemoryStore: Block broadcast_82_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.3 MiB)
26/01/04 17:02:21 INFO BlockManagerInfo: Added broadcast_82_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:02:21 INFO SparkContext: Created broadcast 82 from start at NativeMethodAccessorImpl.java:0
26/01/04 17:02:21 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 26, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@31f5aa92]. The input RDD has 1 partitions.
26/01/04 17:02:21 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:02:21 INFO DAGScheduler: Got job 56 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/01/04 17:02:21 INFO DAGScheduler: Final stage: ResultStage 57 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:02:21 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:02:21 INFO DAGScheduler: Missing parents: List()
26/01/04 17:02:21 INFO DAGScheduler: Submitting ResultStage 57 (MapPartitionsRDD[287] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:02:21 INFO MemoryStore: Block broadcast_83 stored as values in memory (estimated size 52.9 KiB, free 434.3 MiB)
26/01/04 17:02:21 INFO MemoryStore: Block broadcast_83_piece0 stored as bytes in memory (estimated size 20.2 KiB, free 434.3 MiB)
26/01/04 17:02:21 INFO BlockManagerInfo: Added broadcast_83_piece0 in memory on spark-master:33535 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:02:21 INFO SparkContext: Created broadcast 83 from broadcast at DAGScheduler.scala:1585
26/01/04 17:02:21 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 57 (MapPartitionsRDD[287] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/01/04 17:02:21 INFO TaskSchedulerImpl: Adding task set 57.0 with 1 tasks resource profile 0
26/01/04 17:02:21 INFO TaskSetManager: Starting task 0.0 in stage 57.0 (TID 121) (172.18.0.13, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:02:21 INFO BlockManagerInfo: Added broadcast_83_piece0 in memory on 172.18.0.13:44401 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:02:21 INFO BlockManagerInfo: Added broadcast_82_piece0 in memory on 172.18.0.13:44401 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:02:22 INFO TaskSetManager: Finished task 0.0 in stage 57.0 (TID 121) in 589 ms on 172.18.0.13 (executor 0) (1/1)
26/01/04 17:02:22 INFO TaskSchedulerImpl: Removed TaskSet 57.0, whose tasks have all completed, from pool 
26/01/04 17:02:22 INFO DAGScheduler: ResultStage 57 (start at NativeMethodAccessorImpl.java:0) finished in 0.598 s
26/01/04 17:02:22 INFO DAGScheduler: Job 56 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:02:22 INFO TaskSchedulerImpl: Killing all running tasks in stage 57: Stage finished
26/01/04 17:02:22 INFO DAGScheduler: Job 56 finished: start at NativeMethodAccessorImpl.java:0, took 0.600453 s
26/01/04 17:02:22 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 26, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@31f5aa92] is committing.
26/01/04 17:02:22 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 26, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@31f5aa92] committed.
26/01/04 17:02:22 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/26 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.26.5c8cd68a-85d3-42c3-bd7c-117314af1fd5.tmp
26/01/04 17:02:22 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.26.5c8cd68a-85d3-42c3-bd7c-117314af1fd5.tmp to file:/tmp/spark-checkpoint-enrichment/commits/26
26/01/04 17:02:22 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T17:02:21.663Z",
  "batchId" : 26,
  "numInputRows" : 30,
  "inputRowsPerSecond" : 2727.2727272727275,
  "processedRowsPerSecond" : 33.25942350332594,
  "durationMs" : {
    "addBatch" : 722,
    "commitOffsets" : 59,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 49,
    "triggerExecution" : 902,
    "walCommit" : 69
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "0" : 69
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "0" : 99
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "0" : 99
      }
    },
    "numInputRows" : 30,
    "inputRowsPerSecond" : 2727.2727272727275,
    "processedRowsPerSecond" : 33.25942350332594,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 31
  }
}
26/01/04 17:02:31 INFO BlockManagerInfo: Removed broadcast_81_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:02:31 INFO BlockManagerInfo: Removed broadcast_81_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:02:31 INFO BlockManagerInfo: Removed broadcast_81_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:02:31 INFO BlockManagerInfo: Removed broadcast_83_piece0 on spark-master:33535 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:02:31 INFO BlockManagerInfo: Removed broadcast_83_piece0 on 172.18.0.13:44401 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:02:31 INFO BlockManagerInfo: Removed broadcast_79_piece0 on spark-master:33535 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:02:31 INFO BlockManagerInfo: Removed broadcast_79_piece0 on 172.18.0.13:44401 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:02:32 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/01/04 17:02:32 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/27 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.27.0727f111-e9e5-47d1-96f5-611b9fe7b009.tmp
26/01/04 17:02:32 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.27.0727f111-e9e5-47d1-96f5-611b9fe7b009.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/27
26/01/04 17:02:32 INFO MicroBatchExecution: Committed offsets for batch 27. Metadata OffsetSeqMetadata(0,1767546152680,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 17:02:32 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:02:32 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:02:32 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:02:32 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:02:32 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:02:32 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:02:32 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#23191 - airline_prefix.nullCount#23190) > 0)
26/01/04 17:02:32 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#23226 - min_flight_num.nullCount#23225) > 0)
26/01/04 17:02:32 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#23221 - max_flight_num.nullCount#23220) > 0)
26/01/04 17:02:32 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:02:32 INFO DAGScheduler: Got job 57 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 17:02:32 INFO DAGScheduler: Final stage: ResultStage 58 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:02:32 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:02:32 INFO DAGScheduler: Missing parents: List()
26/01/04 17:02:32 INFO DAGScheduler: Submitting ResultStage 58 (MapPartitionsRDD[292] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:02:32 INFO MemoryStore: Block broadcast_84 stored as values in memory (estimated size 38.5 KiB, free 434.4 MiB)
26/01/04 17:02:32 INFO MemoryStore: Block broadcast_84_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.3 MiB)
26/01/04 17:02:32 INFO BlockManagerInfo: Added broadcast_84_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:02:32 INFO BlockManagerInfo: Removed broadcast_82_piece0 on spark-master:33535 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:02:32 INFO SparkContext: Created broadcast 84 from broadcast at DAGScheduler.scala:1585
26/01/04 17:02:32 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 58 (MapPartitionsRDD[292] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 17:02:32 INFO TaskSchedulerImpl: Adding task set 58.0 with 2 tasks resource profile 0
26/01/04 17:02:32 INFO BlockManagerInfo: Removed broadcast_82_piece0 on 172.18.0.13:44401 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:02:32 INFO TaskSetManager: Starting task 1.0 in stage 58.0 (TID 122) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 17:02:32 INFO TaskSetManager: Starting task 0.0 in stage 58.0 (TID 123) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 17:02:32 INFO BlockManagerInfo: Added broadcast_84_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:02:32 INFO BlockManagerInfo: Added broadcast_84_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:02:32 INFO TaskSetManager: Finished task 1.0 in stage 58.0 (TID 122) in 29 ms on 172.18.0.13 (executor 0) (1/2)
26/01/04 17:02:32 INFO TaskSetManager: Finished task 0.0 in stage 58.0 (TID 123) in 29 ms on 172.18.0.14 (executor 1) (2/2)
26/01/04 17:02:32 INFO TaskSchedulerImpl: Removed TaskSet 58.0, whose tasks have all completed, from pool 
26/01/04 17:02:32 INFO DAGScheduler: ResultStage 58 (start at NativeMethodAccessorImpl.java:0) finished in 0.040 s
26/01/04 17:02:32 INFO DAGScheduler: Job 57 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:02:32 INFO TaskSchedulerImpl: Killing all running tasks in stage 58: Stage finished
26/01/04 17:02:32 INFO DAGScheduler: Job 57 finished: start at NativeMethodAccessorImpl.java:0, took 0.042639 s
26/01/04 17:02:32 INFO MemoryStore: Block broadcast_85_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.3 MiB)
26/01/04 17:02:32 INFO BlockManagerInfo: Added broadcast_85_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:02:32 INFO SparkContext: Created broadcast 85 from start at NativeMethodAccessorImpl.java:0
26/01/04 17:02:32 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 27, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@1d55ec89]. The input RDD has 1 partitions.
26/01/04 17:02:32 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:02:32 INFO DAGScheduler: Got job 58 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/01/04 17:02:32 INFO DAGScheduler: Final stage: ResultStage 59 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:02:32 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:02:32 INFO DAGScheduler: Missing parents: List()
26/01/04 17:02:32 INFO DAGScheduler: Submitting ResultStage 59 (MapPartitionsRDD[297] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:02:32 INFO MemoryStore: Block broadcast_86 stored as values in memory (estimated size 52.9 KiB, free 434.3 MiB)
26/01/04 17:02:32 INFO MemoryStore: Block broadcast_86_piece0 stored as bytes in memory (estimated size 20.2 KiB, free 434.3 MiB)
26/01/04 17:02:32 INFO BlockManagerInfo: Added broadcast_86_piece0 in memory on spark-master:33535 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:02:32 INFO SparkContext: Created broadcast 86 from broadcast at DAGScheduler.scala:1585
26/01/04 17:02:32 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 59 (MapPartitionsRDD[297] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/01/04 17:02:32 INFO TaskSchedulerImpl: Adding task set 59.0 with 1 tasks resource profile 0
26/01/04 17:02:32 INFO TaskSetManager: Starting task 0.0 in stage 59.0 (TID 124) (172.18.0.13, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:02:32 INFO BlockManagerInfo: Added broadcast_86_piece0 in memory on 172.18.0.13:44401 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:02:32 INFO BlockManagerInfo: Added broadcast_85_piece0 in memory on 172.18.0.13:44401 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:02:33 INFO TaskSetManager: Finished task 0.0 in stage 59.0 (TID 124) in 595 ms on 172.18.0.13 (executor 0) (1/1)
26/01/04 17:02:33 INFO TaskSchedulerImpl: Removed TaskSet 59.0, whose tasks have all completed, from pool 
26/01/04 17:02:33 INFO DAGScheduler: ResultStage 59 (start at NativeMethodAccessorImpl.java:0) finished in 0.601 s
26/01/04 17:02:33 INFO DAGScheduler: Job 58 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:02:33 INFO TaskSchedulerImpl: Killing all running tasks in stage 59: Stage finished
26/01/04 17:02:33 INFO DAGScheduler: Job 58 finished: start at NativeMethodAccessorImpl.java:0, took 0.602130 s
26/01/04 17:02:33 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 27, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@1d55ec89] is committing.
26/01/04 17:02:33 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 27, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@1d55ec89] committed.
26/01/04 17:02:33 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/27 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.27.31228273-76de-4f3f-b7b1-da3a7b3424ca.tmp
26/01/04 17:02:33 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.27.31228273-76de-4f3f-b7b1-da3a7b3424ca.tmp to file:/tmp/spark-checkpoint-enrichment/commits/27
26/01/04 17:02:33 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T17:02:32.679Z",
  "batchId" : 27,
  "numInputRows" : 30,
  "inputRowsPerSecond" : 2500.0,
  "processedRowsPerSecond" : 34.36426116838488,
  "durationMs" : {
    "addBatch" : 726,
    "commitOffsets" : 62,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 24,
    "triggerExecution" : 873,
    "walCommit" : 59
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "0" : 99
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "0" : 129
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "0" : 129
      }
    },
    "numInputRows" : 30,
    "inputRowsPerSecond" : 2500.0,
    "processedRowsPerSecond" : 34.36426116838488,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 31
  }
}
26/01/04 17:02:43 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/01/04 17:02:43 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/28 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.28.29400dd9-7e96-4362-9021-0b16cc4ef335.tmp
26/01/04 17:02:43 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.28.29400dd9-7e96-4362-9021-0b16cc4ef335.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/28
26/01/04 17:02:43 INFO MicroBatchExecution: Committed offsets for batch 28. Metadata OffsetSeqMetadata(0,1767546163696,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 17:02:43 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:02:43 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:02:43 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:02:43 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:02:43 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:02:43 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:02:43 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#23995 - airline_prefix.nullCount#23994) > 0)
26/01/04 17:02:43 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#24030 - min_flight_num.nullCount#24029) > 0)
26/01/04 17:02:43 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#24025 - max_flight_num.nullCount#24024) > 0)
26/01/04 17:02:43 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:02:43 INFO DAGScheduler: Got job 59 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 17:02:43 INFO DAGScheduler: Final stage: ResultStage 60 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:02:43 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:02:43 INFO DAGScheduler: Missing parents: List()
26/01/04 17:02:43 INFO DAGScheduler: Submitting ResultStage 60 (MapPartitionsRDD[302] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:02:43 INFO MemoryStore: Block broadcast_87 stored as values in memory (estimated size 38.5 KiB, free 434.2 MiB)
26/01/04 17:02:43 INFO MemoryStore: Block broadcast_87_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.2 MiB)
26/01/04 17:02:43 INFO BlockManagerInfo: Added broadcast_87_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:02:43 INFO SparkContext: Created broadcast 87 from broadcast at DAGScheduler.scala:1585
26/01/04 17:02:43 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 60 (MapPartitionsRDD[302] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 17:02:43 INFO TaskSchedulerImpl: Adding task set 60.0 with 2 tasks resource profile 0
26/01/04 17:02:44 INFO TaskSetManager: Starting task 1.0 in stage 60.0 (TID 125) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 17:02:44 INFO TaskSetManager: Starting task 0.0 in stage 60.0 (TID 126) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 17:02:44 INFO BlockManagerInfo: Added broadcast_87_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:02:44 INFO BlockManagerInfo: Added broadcast_87_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:02:44 INFO TaskSetManager: Finished task 1.0 in stage 60.0 (TID 125) in 45 ms on 172.18.0.13 (executor 0) (1/2)
26/01/04 17:02:44 INFO TaskSetManager: Finished task 0.0 in stage 60.0 (TID 126) in 44 ms on 172.18.0.14 (executor 1) (2/2)
26/01/04 17:02:44 INFO TaskSchedulerImpl: Removed TaskSet 60.0, whose tasks have all completed, from pool 
26/01/04 17:02:44 INFO DAGScheduler: ResultStage 60 (start at NativeMethodAccessorImpl.java:0) finished in 0.057 s
26/01/04 17:02:44 INFO DAGScheduler: Job 59 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:02:44 INFO TaskSchedulerImpl: Killing all running tasks in stage 60: Stage finished
26/01/04 17:02:44 INFO DAGScheduler: Job 59 finished: start at NativeMethodAccessorImpl.java:0, took 0.065116 s
26/01/04 17:02:44 INFO MemoryStore: Block broadcast_88_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.2 MiB)
26/01/04 17:02:44 INFO BlockManagerInfo: Added broadcast_88_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:02:44 INFO SparkContext: Created broadcast 88 from start at NativeMethodAccessorImpl.java:0
26/01/04 17:02:44 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 28, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@feaa05]. The input RDD has 1 partitions.
26/01/04 17:02:44 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:02:44 INFO DAGScheduler: Got job 60 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/01/04 17:02:44 INFO DAGScheduler: Final stage: ResultStage 61 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:02:44 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:02:44 INFO DAGScheduler: Missing parents: List()
26/01/04 17:02:44 INFO DAGScheduler: Submitting ResultStage 61 (MapPartitionsRDD[307] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:02:44 INFO MemoryStore: Block broadcast_89 stored as values in memory (estimated size 52.9 KiB, free 434.2 MiB)
26/01/04 17:02:44 INFO MemoryStore: Block broadcast_89_piece0 stored as bytes in memory (estimated size 20.2 KiB, free 434.1 MiB)
26/01/04 17:02:44 INFO BlockManagerInfo: Added broadcast_89_piece0 in memory on spark-master:33535 (size: 20.2 KiB, free: 434.3 MiB)
26/01/04 17:02:44 INFO SparkContext: Created broadcast 89 from broadcast at DAGScheduler.scala:1585
26/01/04 17:02:44 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 61 (MapPartitionsRDD[307] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/01/04 17:02:44 INFO TaskSchedulerImpl: Adding task set 61.0 with 1 tasks resource profile 0
26/01/04 17:02:44 INFO TaskSetManager: Starting task 0.0 in stage 61.0 (TID 127) (172.18.0.13, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:02:44 INFO BlockManagerInfo: Added broadcast_89_piece0 in memory on 172.18.0.13:44401 (size: 20.2 KiB, free: 434.3 MiB)
26/01/04 17:02:44 INFO BlockManagerInfo: Added broadcast_88_piece0 in memory on 172.18.0.13:44401 (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:02:44 INFO TaskSetManager: Finished task 0.0 in stage 61.0 (TID 127) in 610 ms on 172.18.0.13 (executor 0) (1/1)
26/01/04 17:02:44 INFO TaskSchedulerImpl: Removed TaskSet 61.0, whose tasks have all completed, from pool 
26/01/04 17:02:44 INFO DAGScheduler: ResultStage 61 (start at NativeMethodAccessorImpl.java:0) finished in 0.615 s
26/01/04 17:02:44 INFO DAGScheduler: Job 60 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:02:44 INFO TaskSchedulerImpl: Killing all running tasks in stage 61: Stage finished
26/01/04 17:02:44 INFO DAGScheduler: Job 60 finished: start at NativeMethodAccessorImpl.java:0, took 0.618460 s
26/01/04 17:02:44 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 28, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@feaa05] is committing.
26/01/04 17:02:44 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 28, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@feaa05] committed.
26/01/04 17:02:44 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/28 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.28.a2299133-018c-4ea4-b834-0ecf6c4f8216.tmp
26/01/04 17:02:44 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.28.a2299133-018c-4ea4-b834-0ecf6c4f8216.tmp to file:/tmp/spark-checkpoint-enrichment/commits/28
26/01/04 17:02:44 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T17:02:43.694Z",
  "batchId" : 28,
  "numInputRows" : 30,
  "inputRowsPerSecond" : 2500.0,
  "processedRowsPerSecond" : 28.19548872180451,
  "durationMs" : {
    "addBatch" : 824,
    "commitOffsets" : 74,
    "getBatch" : 0,
    "latestOffset" : 2,
    "queryPlanning" : 53,
    "triggerExecution" : 1064,
    "walCommit" : 111
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "0" : 129
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "0" : 159
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "0" : 159
      }
    },
    "numInputRows" : 30,
    "inputRowsPerSecond" : 2500.0,
    "processedRowsPerSecond" : 28.19548872180451,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 31
  }
}
26/01/04 17:02:46 INFO BlockManagerInfo: Removed broadcast_85_piece0 on spark-master:33535 in memory (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:02:46 INFO BlockManagerInfo: Removed broadcast_85_piece0 on 172.18.0.13:44401 in memory (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:02:46 INFO BlockManagerInfo: Removed broadcast_84_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:02:46 INFO BlockManagerInfo: Removed broadcast_84_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:02:46 INFO BlockManagerInfo: Removed broadcast_84_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:02:46 INFO BlockManagerInfo: Removed broadcast_87_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:02:46 INFO BlockManagerInfo: Removed broadcast_87_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:02:46 INFO BlockManagerInfo: Removed broadcast_87_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:02:46 INFO BlockManagerInfo: Removed broadcast_89_piece0 on spark-master:33535 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:02:46 INFO BlockManagerInfo: Removed broadcast_89_piece0 on 172.18.0.13:44401 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:02:46 INFO BlockManagerInfo: Removed broadcast_86_piece0 on spark-master:33535 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:02:47 INFO BlockManagerInfo: Removed broadcast_86_piece0 on 172.18.0.13:44401 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:02:54 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/29 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.29.2b5bf75b-45ba-4e35-b166-75aa3137e1b4.tmp
26/01/04 17:02:54 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.29.2b5bf75b-45ba-4e35-b166-75aa3137e1b4.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/29
26/01/04 17:02:54 INFO MicroBatchExecution: Committed offsets for batch 29. Metadata OffsetSeqMetadata(0,1767546174720,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 17:02:54 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:02:54 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:02:54 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:02:54 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:02:54 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:02:54 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:02:54 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#24799 - airline_prefix.nullCount#24798) > 0)
26/01/04 17:02:54 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#24834 - min_flight_num.nullCount#24833) > 0)
26/01/04 17:02:54 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#24829 - max_flight_num.nullCount#24828) > 0)
26/01/04 17:02:54 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:02:54 INFO DAGScheduler: Got job 61 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 17:02:54 INFO DAGScheduler: Final stage: ResultStage 62 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:02:54 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:02:54 INFO DAGScheduler: Missing parents: List()
26/01/04 17:02:54 INFO DAGScheduler: Submitting ResultStage 62 (MapPartitionsRDD[312] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:02:54 INFO MemoryStore: Block broadcast_90 stored as values in memory (estimated size 38.5 KiB, free 434.4 MiB)
26/01/04 17:02:54 INFO MemoryStore: Block broadcast_90_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.3 MiB)
26/01/04 17:02:54 INFO BlockManagerInfo: Added broadcast_90_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:02:54 INFO SparkContext: Created broadcast 90 from broadcast at DAGScheduler.scala:1585
26/01/04 17:02:54 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 62 (MapPartitionsRDD[312] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 17:02:54 INFO TaskSchedulerImpl: Adding task set 62.0 with 2 tasks resource profile 0
26/01/04 17:02:54 INFO TaskSetManager: Starting task 1.0 in stage 62.0 (TID 128) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 17:02:54 INFO TaskSetManager: Starting task 0.0 in stage 62.0 (TID 129) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 17:02:54 INFO BlockManagerInfo: Added broadcast_90_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:02:54 INFO BlockManagerInfo: Added broadcast_90_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:02:54 INFO TaskSetManager: Finished task 1.0 in stage 62.0 (TID 128) in 28 ms on 172.18.0.13 (executor 0) (1/2)
26/01/04 17:02:54 INFO TaskSetManager: Finished task 0.0 in stage 62.0 (TID 129) in 31 ms on 172.18.0.14 (executor 1) (2/2)
26/01/04 17:02:54 INFO TaskSchedulerImpl: Removed TaskSet 62.0, whose tasks have all completed, from pool 
26/01/04 17:02:54 INFO DAGScheduler: ResultStage 62 (start at NativeMethodAccessorImpl.java:0) finished in 0.039 s
26/01/04 17:02:54 INFO DAGScheduler: Job 61 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:02:54 INFO TaskSchedulerImpl: Killing all running tasks in stage 62: Stage finished
26/01/04 17:02:54 INFO DAGScheduler: Job 61 finished: start at NativeMethodAccessorImpl.java:0, took 0.041882 s
26/01/04 17:02:54 INFO MemoryStore: Block broadcast_91_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.3 MiB)
26/01/04 17:02:54 INFO BlockManagerInfo: Added broadcast_91_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:02:54 INFO SparkContext: Created broadcast 91 from start at NativeMethodAccessorImpl.java:0
26/01/04 17:02:54 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 29, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@37aa3208]. The input RDD has 1 partitions.
26/01/04 17:02:54 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:02:54 INFO DAGScheduler: Got job 62 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/01/04 17:02:54 INFO DAGScheduler: Final stage: ResultStage 63 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:02:54 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:02:54 INFO DAGScheduler: Missing parents: List()
26/01/04 17:02:54 INFO DAGScheduler: Submitting ResultStage 63 (MapPartitionsRDD[317] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:02:54 INFO MemoryStore: Block broadcast_92 stored as values in memory (estimated size 52.9 KiB, free 434.3 MiB)
26/01/04 17:02:54 INFO MemoryStore: Block broadcast_92_piece0 stored as bytes in memory (estimated size 20.2 KiB, free 434.3 MiB)
26/01/04 17:02:54 INFO BlockManagerInfo: Added broadcast_92_piece0 in memory on spark-master:33535 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:02:54 INFO SparkContext: Created broadcast 92 from broadcast at DAGScheduler.scala:1585
26/01/04 17:02:54 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 63 (MapPartitionsRDD[317] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/01/04 17:02:54 INFO TaskSchedulerImpl: Adding task set 63.0 with 1 tasks resource profile 0
26/01/04 17:02:54 INFO TaskSetManager: Starting task 0.0 in stage 63.0 (TID 130) (172.18.0.13, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:02:54 INFO BlockManagerInfo: Added broadcast_92_piece0 in memory on 172.18.0.13:44401 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:02:55 INFO BlockManagerInfo: Added broadcast_91_piece0 in memory on 172.18.0.13:44401 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:02:55 INFO TaskSetManager: Finished task 0.0 in stage 63.0 (TID 130) in 597 ms on 172.18.0.13 (executor 0) (1/1)
26/01/04 17:02:55 INFO TaskSchedulerImpl: Removed TaskSet 63.0, whose tasks have all completed, from pool 
26/01/04 17:02:55 INFO DAGScheduler: ResultStage 63 (start at NativeMethodAccessorImpl.java:0) finished in 0.602 s
26/01/04 17:02:55 INFO DAGScheduler: Job 62 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:02:55 INFO TaskSchedulerImpl: Killing all running tasks in stage 63: Stage finished
26/01/04 17:02:55 INFO DAGScheduler: Job 62 finished: start at NativeMethodAccessorImpl.java:0, took 0.603723 s
26/01/04 17:02:55 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 29, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@37aa3208] is committing.
26/01/04 17:02:55 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 29, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@37aa3208] committed.
26/01/04 17:02:55 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/29 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.29.51542626-75e1-4d8e-b2ae-aca79b515669.tmp
26/01/04 17:02:55 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.29.51542626-75e1-4d8e-b2ae-aca79b515669.tmp to file:/tmp/spark-checkpoint-enrichment/commits/29
26/01/04 17:02:55 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T17:02:54.718Z",
  "batchId" : 29,
  "numInputRows" : 30,
  "inputRowsPerSecond" : 2500.0,
  "processedRowsPerSecond" : 31.81336161187699,
  "durationMs" : {
    "addBatch" : 718,
    "commitOffsets" : 73,
    "getBatch" : 0,
    "latestOffset" : 2,
    "queryPlanning" : 41,
    "triggerExecution" : 943,
    "walCommit" : 108
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "0" : 159
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "0" : 189
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "0" : 189
      }
    },
    "numInputRows" : 30,
    "inputRowsPerSecond" : 2500.0,
    "processedRowsPerSecond" : 31.81336161187699,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 31
  }
}
26/01/04 17:03:05 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/01/04 17:03:05 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/30 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.30.6005b8de-4d39-44fc-aea8-1269252a00b4.tmp
26/01/04 17:03:05 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.30.6005b8de-4d39-44fc-aea8-1269252a00b4.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/30
26/01/04 17:03:05 INFO MicroBatchExecution: Committed offsets for batch 30. Metadata OffsetSeqMetadata(0,1767546185733,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 17:03:05 INFO BlockManagerInfo: Removed broadcast_91_piece0 on spark-master:33535 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:03:05 INFO BlockManagerInfo: Removed broadcast_91_piece0 on 172.18.0.13:44401 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:03:05 INFO BlockManagerInfo: Removed broadcast_90_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:03:05 INFO BlockManagerInfo: Removed broadcast_90_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:03:05 INFO BlockManagerInfo: Removed broadcast_90_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:03:05 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:03:05 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:03:05 INFO BlockManagerInfo: Removed broadcast_92_piece0 on spark-master:33535 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:03:05 INFO BlockManagerInfo: Removed broadcast_92_piece0 on 172.18.0.13:44401 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:03:05 INFO BlockManagerInfo: Removed broadcast_88_piece0 on spark-master:33535 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:03:05 INFO BlockManagerInfo: Removed broadcast_88_piece0 on 172.18.0.13:44401 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:03:05 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:03:05 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:03:05 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:03:05 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:03:05 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#25603 - airline_prefix.nullCount#25602) > 0)
26/01/04 17:03:05 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#25638 - min_flight_num.nullCount#25637) > 0)
26/01/04 17:03:05 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#25633 - max_flight_num.nullCount#25632) > 0)
26/01/04 17:03:05 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:03:05 INFO DAGScheduler: Got job 63 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 17:03:05 INFO DAGScheduler: Final stage: ResultStage 64 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:03:05 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:03:05 INFO DAGScheduler: Missing parents: List()
26/01/04 17:03:05 INFO DAGScheduler: Submitting ResultStage 64 (MapPartitionsRDD[322] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:03:05 INFO MemoryStore: Block broadcast_93 stored as values in memory (estimated size 38.5 KiB, free 434.4 MiB)
26/01/04 17:03:05 INFO MemoryStore: Block broadcast_93_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.3 MiB)
26/01/04 17:03:05 INFO BlockManagerInfo: Added broadcast_93_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:03:05 INFO SparkContext: Created broadcast 93 from broadcast at DAGScheduler.scala:1585
26/01/04 17:03:05 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 64 (MapPartitionsRDD[322] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 17:03:05 INFO TaskSchedulerImpl: Adding task set 64.0 with 2 tasks resource profile 0
26/01/04 17:03:05 INFO TaskSetManager: Starting task 0.0 in stage 64.0 (TID 131) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 17:03:05 INFO TaskSetManager: Starting task 1.0 in stage 64.0 (TID 132) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 17:03:05 INFO BlockManagerInfo: Added broadcast_93_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:03:05 INFO BlockManagerInfo: Added broadcast_93_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:03:05 INFO TaskSetManager: Finished task 1.0 in stage 64.0 (TID 132) in 25 ms on 172.18.0.13 (executor 0) (1/2)
26/01/04 17:03:05 INFO TaskSetManager: Finished task 0.0 in stage 64.0 (TID 131) in 29 ms on 172.18.0.14 (executor 1) (2/2)
26/01/04 17:03:05 INFO TaskSchedulerImpl: Removed TaskSet 64.0, whose tasks have all completed, from pool 
26/01/04 17:03:05 INFO DAGScheduler: ResultStage 64 (start at NativeMethodAccessorImpl.java:0) finished in 0.038 s
26/01/04 17:03:05 INFO DAGScheduler: Job 63 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:03:05 INFO TaskSchedulerImpl: Killing all running tasks in stage 64: Stage finished
26/01/04 17:03:05 INFO DAGScheduler: Job 63 finished: start at NativeMethodAccessorImpl.java:0, took 0.041065 s
26/01/04 17:03:05 INFO MemoryStore: Block broadcast_94_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.3 MiB)
26/01/04 17:03:05 INFO BlockManagerInfo: Added broadcast_94_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:03:05 INFO SparkContext: Created broadcast 94 from start at NativeMethodAccessorImpl.java:0
26/01/04 17:03:05 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 30, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@203d7252]. The input RDD has 1 partitions.
26/01/04 17:03:05 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:03:05 INFO DAGScheduler: Got job 64 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/01/04 17:03:05 INFO DAGScheduler: Final stage: ResultStage 65 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:03:05 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:03:05 INFO DAGScheduler: Missing parents: List()
26/01/04 17:03:05 INFO DAGScheduler: Submitting ResultStage 65 (MapPartitionsRDD[327] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:03:05 INFO MemoryStore: Block broadcast_95 stored as values in memory (estimated size 52.9 KiB, free 434.3 MiB)
26/01/04 17:03:05 INFO MemoryStore: Block broadcast_95_piece0 stored as bytes in memory (estimated size 20.2 KiB, free 434.3 MiB)
26/01/04 17:03:05 INFO BlockManagerInfo: Added broadcast_95_piece0 in memory on spark-master:33535 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:03:05 INFO SparkContext: Created broadcast 95 from broadcast at DAGScheduler.scala:1585
26/01/04 17:03:05 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 65 (MapPartitionsRDD[327] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/01/04 17:03:05 INFO TaskSchedulerImpl: Adding task set 65.0 with 1 tasks resource profile 0
26/01/04 17:03:05 INFO TaskSetManager: Starting task 0.0 in stage 65.0 (TID 133) (172.18.0.13, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:03:05 INFO BlockManagerInfo: Added broadcast_95_piece0 in memory on 172.18.0.13:44401 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:03:05 INFO BlockManagerInfo: Added broadcast_94_piece0 in memory on 172.18.0.13:44401 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:03:06 INFO TaskSetManager: Finished task 0.0 in stage 65.0 (TID 133) in 594 ms on 172.18.0.13 (executor 0) (1/1)
26/01/04 17:03:06 INFO TaskSchedulerImpl: Removed TaskSet 65.0, whose tasks have all completed, from pool 
26/01/04 17:03:06 INFO DAGScheduler: ResultStage 65 (start at NativeMethodAccessorImpl.java:0) finished in 0.600 s
26/01/04 17:03:06 INFO DAGScheduler: Job 64 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:03:06 INFO TaskSchedulerImpl: Killing all running tasks in stage 65: Stage finished
26/01/04 17:03:06 INFO DAGScheduler: Job 64 finished: start at NativeMethodAccessorImpl.java:0, took 0.602576 s
26/01/04 17:03:06 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 30, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@203d7252] is committing.
26/01/04 17:03:06 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 30, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@203d7252] committed.
26/01/04 17:03:06 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/30 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.30.67129479-43c1-44de-b575-bf522f8367eb.tmp
26/01/04 17:03:06 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.30.67129479-43c1-44de-b575-bf522f8367eb.tmp to file:/tmp/spark-checkpoint-enrichment/commits/30
26/01/04 17:03:06 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T17:03:05.732Z",
  "batchId" : 30,
  "numInputRows" : 30,
  "inputRowsPerSecond" : 2500.0,
  "processedRowsPerSecond" : 33.40757238307349,
  "durationMs" : {
    "addBatch" : 723,
    "commitOffsets" : 66,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 40,
    "triggerExecution" : 898,
    "walCommit" : 68
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "0" : 189
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "0" : 219
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "0" : 219
      }
    },
    "numInputRows" : 30,
    "inputRowsPerSecond" : 2500.0,
    "processedRowsPerSecond" : 33.40757238307349,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 31
  }
}
26/01/04 17:03:16 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/01/04 17:03:16 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/31 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.31.1faf28aa-32b0-43da-9cb3-2358a41b6685.tmp
26/01/04 17:03:16 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.31.1faf28aa-32b0-43da-9cb3-2358a41b6685.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/31
26/01/04 17:03:16 INFO MicroBatchExecution: Committed offsets for batch 31. Metadata OffsetSeqMetadata(0,1767546196747,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 17:03:16 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:03:16 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:03:16 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:03:16 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:03:16 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:03:16 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:03:16 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#26407 - airline_prefix.nullCount#26406) > 0)
26/01/04 17:03:16 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#26442 - min_flight_num.nullCount#26441) > 0)
26/01/04 17:03:16 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#26437 - max_flight_num.nullCount#26436) > 0)
26/01/04 17:03:16 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:03:16 INFO DAGScheduler: Got job 65 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 17:03:16 INFO DAGScheduler: Final stage: ResultStage 66 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:03:16 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:03:16 INFO DAGScheduler: Missing parents: List()
26/01/04 17:03:16 INFO DAGScheduler: Submitting ResultStage 66 (MapPartitionsRDD[332] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:03:16 INFO MemoryStore: Block broadcast_96 stored as values in memory (estimated size 38.5 KiB, free 434.2 MiB)
26/01/04 17:03:16 INFO MemoryStore: Block broadcast_96_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.2 MiB)
26/01/04 17:03:16 INFO BlockManagerInfo: Added broadcast_96_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:03:16 INFO SparkContext: Created broadcast 96 from broadcast at DAGScheduler.scala:1585
26/01/04 17:03:16 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 66 (MapPartitionsRDD[332] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 17:03:16 INFO TaskSchedulerImpl: Adding task set 66.0 with 2 tasks resource profile 0
26/01/04 17:03:16 INFO TaskSetManager: Starting task 1.0 in stage 66.0 (TID 134) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 17:03:16 INFO TaskSetManager: Starting task 0.0 in stage 66.0 (TID 135) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 17:03:16 INFO BlockManagerInfo: Added broadcast_96_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:03:16 INFO BlockManagerInfo: Added broadcast_96_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:03:16 INFO TaskSetManager: Finished task 1.0 in stage 66.0 (TID 134) in 27 ms on 172.18.0.13 (executor 0) (1/2)
26/01/04 17:03:16 INFO TaskSetManager: Finished task 0.0 in stage 66.0 (TID 135) in 28 ms on 172.18.0.14 (executor 1) (2/2)
26/01/04 17:03:16 INFO TaskSchedulerImpl: Removed TaskSet 66.0, whose tasks have all completed, from pool 
26/01/04 17:03:16 INFO DAGScheduler: ResultStage 66 (start at NativeMethodAccessorImpl.java:0) finished in 0.034 s
26/01/04 17:03:16 INFO DAGScheduler: Job 65 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:03:16 INFO TaskSchedulerImpl: Killing all running tasks in stage 66: Stage finished
26/01/04 17:03:16 INFO DAGScheduler: Job 65 finished: start at NativeMethodAccessorImpl.java:0, took 0.036932 s
26/01/04 17:03:16 INFO MemoryStore: Block broadcast_97_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.2 MiB)
26/01/04 17:03:16 INFO BlockManagerInfo: Added broadcast_97_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:03:16 INFO SparkContext: Created broadcast 97 from start at NativeMethodAccessorImpl.java:0
26/01/04 17:03:17 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 31, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@5dabe180]. The input RDD has 1 partitions.
26/01/04 17:03:17 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:03:17 INFO DAGScheduler: Got job 66 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/01/04 17:03:17 INFO DAGScheduler: Final stage: ResultStage 67 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:03:17 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:03:17 INFO DAGScheduler: Missing parents: List()
26/01/04 17:03:17 INFO DAGScheduler: Submitting ResultStage 67 (MapPartitionsRDD[337] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:03:17 INFO MemoryStore: Block broadcast_98 stored as values in memory (estimated size 52.9 KiB, free 434.2 MiB)
26/01/04 17:03:17 INFO MemoryStore: Block broadcast_98_piece0 stored as bytes in memory (estimated size 20.2 KiB, free 434.1 MiB)
26/01/04 17:03:17 INFO BlockManagerInfo: Added broadcast_98_piece0 in memory on spark-master:33535 (size: 20.2 KiB, free: 434.3 MiB)
26/01/04 17:03:17 INFO SparkContext: Created broadcast 98 from broadcast at DAGScheduler.scala:1585
26/01/04 17:03:17 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 67 (MapPartitionsRDD[337] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/01/04 17:03:17 INFO TaskSchedulerImpl: Adding task set 67.0 with 1 tasks resource profile 0
26/01/04 17:03:17 INFO TaskSetManager: Starting task 0.0 in stage 67.0 (TID 136) (172.18.0.13, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:03:17 INFO BlockManagerInfo: Added broadcast_98_piece0 in memory on 172.18.0.13:44401 (size: 20.2 KiB, free: 434.3 MiB)
26/01/04 17:03:17 INFO BlockManagerInfo: Added broadcast_97_piece0 in memory on 172.18.0.13:44401 (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:03:17 INFO TaskSetManager: Finished task 0.0 in stage 67.0 (TID 136) in 597 ms on 172.18.0.13 (executor 0) (1/1)
26/01/04 17:03:17 INFO TaskSchedulerImpl: Removed TaskSet 67.0, whose tasks have all completed, from pool 
26/01/04 17:03:17 INFO DAGScheduler: ResultStage 67 (start at NativeMethodAccessorImpl.java:0) finished in 0.603 s
26/01/04 17:03:17 INFO DAGScheduler: Job 66 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:03:17 INFO TaskSchedulerImpl: Killing all running tasks in stage 67: Stage finished
26/01/04 17:03:17 INFO DAGScheduler: Job 66 finished: start at NativeMethodAccessorImpl.java:0, took 0.605513 s
26/01/04 17:03:17 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 31, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@5dabe180] is committing.
26/01/04 17:03:17 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 31, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@5dabe180] committed.
26/01/04 17:03:17 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/31 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.31.bf6712a4-1206-4038-a46e-defc8d4c0835.tmp
26/01/04 17:03:17 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.31.bf6712a4-1206-4038-a46e-defc8d4c0835.tmp to file:/tmp/spark-checkpoint-enrichment/commits/31
26/01/04 17:03:17 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T17:03:16.746Z",
  "batchId" : 31,
  "numInputRows" : 30,
  "inputRowsPerSecond" : 2500.0,
  "processedRowsPerSecond" : 32.43243243243243,
  "durationMs" : {
    "addBatch" : 739,
    "commitOffsets" : 63,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 31,
    "triggerExecution" : 925,
    "walCommit" : 89
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "0" : 219
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "0" : 249
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "0" : 249
      }
    },
    "numInputRows" : 30,
    "inputRowsPerSecond" : 2500.0,
    "processedRowsPerSecond" : 32.43243243243243,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 31
  }
}
26/01/04 17:03:19 INFO BlockManagerInfo: Removed broadcast_93_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:03:19 INFO BlockManagerInfo: Removed broadcast_93_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:03:19 INFO BlockManagerInfo: Removed broadcast_93_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:03:19 INFO BlockManagerInfo: Removed broadcast_98_piece0 on spark-master:33535 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:03:19 INFO BlockManagerInfo: Removed broadcast_98_piece0 on 172.18.0.13:44401 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:03:19 INFO BlockManagerInfo: Removed broadcast_96_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:03:19 INFO BlockManagerInfo: Removed broadcast_96_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:03:19 INFO BlockManagerInfo: Removed broadcast_96_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:03:19 INFO BlockManagerInfo: Removed broadcast_94_piece0 on spark-master:33535 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:03:19 INFO BlockManagerInfo: Removed broadcast_94_piece0 on 172.18.0.13:44401 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:03:19 INFO BlockManagerInfo: Removed broadcast_95_piece0 on spark-master:33535 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:03:19 INFO BlockManagerInfo: Removed broadcast_95_piece0 on 172.18.0.13:44401 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:03:27 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/01/04 17:03:27 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/32 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.32.c14631a0-8ba9-4e05-a4f8-dc0109a0f83f.tmp
26/01/04 17:03:27 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.32.c14631a0-8ba9-4e05-a4f8-dc0109a0f83f.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/32
26/01/04 17:03:27 INFO MicroBatchExecution: Committed offsets for batch 32. Metadata OffsetSeqMetadata(0,1767546207767,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 17:03:27 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:03:27 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:03:27 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:03:27 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:03:27 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:03:27 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:03:27 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#27211 - airline_prefix.nullCount#27210) > 0)
26/01/04 17:03:27 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#27246 - min_flight_num.nullCount#27245) > 0)
26/01/04 17:03:27 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#27241 - max_flight_num.nullCount#27240) > 0)
26/01/04 17:03:27 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:03:27 INFO DAGScheduler: Got job 67 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 17:03:27 INFO DAGScheduler: Final stage: ResultStage 68 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:03:27 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:03:27 INFO DAGScheduler: Missing parents: List()
26/01/04 17:03:27 INFO DAGScheduler: Submitting ResultStage 68 (MapPartitionsRDD[342] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:03:27 INFO MemoryStore: Block broadcast_99 stored as values in memory (estimated size 38.5 KiB, free 434.4 MiB)
26/01/04 17:03:27 INFO MemoryStore: Block broadcast_99_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.3 MiB)
26/01/04 17:03:27 INFO BlockManagerInfo: Added broadcast_99_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:03:27 INFO SparkContext: Created broadcast 99 from broadcast at DAGScheduler.scala:1585
26/01/04 17:03:27 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 68 (MapPartitionsRDD[342] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 17:03:27 INFO TaskSchedulerImpl: Adding task set 68.0 with 2 tasks resource profile 0
26/01/04 17:03:27 INFO TaskSetManager: Starting task 1.0 in stage 68.0 (TID 137) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 17:03:27 INFO TaskSetManager: Starting task 0.0 in stage 68.0 (TID 138) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 17:03:27 INFO BlockManagerInfo: Added broadcast_99_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:03:27 INFO BlockManagerInfo: Added broadcast_99_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:03:27 INFO TaskSetManager: Finished task 1.0 in stage 68.0 (TID 137) in 25 ms on 172.18.0.13 (executor 0) (1/2)
26/01/04 17:03:27 INFO TaskSetManager: Finished task 0.0 in stage 68.0 (TID 138) in 27 ms on 172.18.0.14 (executor 1) (2/2)
26/01/04 17:03:27 INFO TaskSchedulerImpl: Removed TaskSet 68.0, whose tasks have all completed, from pool 
26/01/04 17:03:27 INFO DAGScheduler: ResultStage 68 (start at NativeMethodAccessorImpl.java:0) finished in 0.033 s
26/01/04 17:03:27 INFO DAGScheduler: Job 67 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:03:27 INFO TaskSchedulerImpl: Killing all running tasks in stage 68: Stage finished
26/01/04 17:03:27 INFO DAGScheduler: Job 67 finished: start at NativeMethodAccessorImpl.java:0, took 0.035911 s
26/01/04 17:03:27 INFO MemoryStore: Block broadcast_100_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.3 MiB)
26/01/04 17:03:27 INFO BlockManagerInfo: Added broadcast_100_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:03:27 INFO SparkContext: Created broadcast 100 from start at NativeMethodAccessorImpl.java:0
26/01/04 17:03:27 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 32, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@44b778f8]. The input RDD has 1 partitions.
26/01/04 17:03:27 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:03:27 INFO DAGScheduler: Got job 68 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/01/04 17:03:27 INFO DAGScheduler: Final stage: ResultStage 69 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:03:27 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:03:27 INFO DAGScheduler: Missing parents: List()
26/01/04 17:03:27 INFO DAGScheduler: Submitting ResultStage 69 (MapPartitionsRDD[347] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:03:27 INFO MemoryStore: Block broadcast_101 stored as values in memory (estimated size 52.9 KiB, free 434.3 MiB)
26/01/04 17:03:27 INFO MemoryStore: Block broadcast_101_piece0 stored as bytes in memory (estimated size 20.2 KiB, free 434.3 MiB)
26/01/04 17:03:27 INFO BlockManagerInfo: Added broadcast_101_piece0 in memory on spark-master:33535 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:03:27 INFO SparkContext: Created broadcast 101 from broadcast at DAGScheduler.scala:1585
26/01/04 17:03:27 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 69 (MapPartitionsRDD[347] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/01/04 17:03:27 INFO TaskSchedulerImpl: Adding task set 69.0 with 1 tasks resource profile 0
26/01/04 17:03:27 INFO TaskSetManager: Starting task 0.0 in stage 69.0 (TID 139) (172.18.0.13, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:03:28 INFO BlockManagerInfo: Added broadcast_101_piece0 in memory on 172.18.0.13:44401 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:03:28 INFO BlockManagerInfo: Added broadcast_100_piece0 in memory on 172.18.0.13:44401 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:03:28 INFO TaskSetManager: Finished task 0.0 in stage 69.0 (TID 139) in 587 ms on 172.18.0.13 (executor 0) (1/1)
26/01/04 17:03:28 INFO TaskSchedulerImpl: Removed TaskSet 69.0, whose tasks have all completed, from pool 
26/01/04 17:03:28 INFO DAGScheduler: ResultStage 69 (start at NativeMethodAccessorImpl.java:0) finished in 0.594 s
26/01/04 17:03:28 INFO DAGScheduler: Job 68 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:03:28 INFO TaskSchedulerImpl: Killing all running tasks in stage 69: Stage finished
26/01/04 17:03:28 INFO DAGScheduler: Job 68 finished: start at NativeMethodAccessorImpl.java:0, took 0.596655 s
26/01/04 17:03:28 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 32, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@44b778f8] is committing.
26/01/04 17:03:28 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 32, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@44b778f8] committed.
26/01/04 17:03:28 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/32 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.32.44d9534b-5901-4996-9a88-eb29b92fcd20.tmp
26/01/04 17:03:28 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.32.44d9534b-5901-4996-9a88-eb29b92fcd20.tmp to file:/tmp/spark-checkpoint-enrichment/commits/32
26/01/04 17:03:28 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T17:03:27.766Z",
  "batchId" : 32,
  "numInputRows" : 30,
  "inputRowsPerSecond" : 2500.0,
  "processedRowsPerSecond" : 33.898305084745765,
  "durationMs" : {
    "addBatch" : 714,
    "commitOffsets" : 66,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 26,
    "triggerExecution" : 885,
    "walCommit" : 77
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "0" : 249
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "0" : 279
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "0" : 279
      }
    },
    "numInputRows" : 30,
    "inputRowsPerSecond" : 2500.0,
    "processedRowsPerSecond" : 33.898305084745765,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 31
  }
}
26/01/04 17:03:37 INFO BlockManagerInfo: Removed broadcast_97_piece0 on spark-master:33535 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:03:37 INFO BlockManagerInfo: Removed broadcast_97_piece0 on 172.18.0.13:44401 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:03:37 INFO BlockManagerInfo: Removed broadcast_101_piece0 on spark-master:33535 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:03:37 INFO BlockManagerInfo: Removed broadcast_101_piece0 on 172.18.0.13:44401 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:03:37 INFO BlockManagerInfo: Removed broadcast_99_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:03:37 INFO BlockManagerInfo: Removed broadcast_99_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:03:37 INFO BlockManagerInfo: Removed broadcast_99_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:03:38 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/01/04 17:03:38 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/33 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.33.d4f8fb20-fb0b-416b-b931-e4fa707aba9b.tmp
26/01/04 17:03:38 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.33.d4f8fb20-fb0b-416b-b931-e4fa707aba9b.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/33
26/01/04 17:03:38 INFO MicroBatchExecution: Committed offsets for batch 33. Metadata OffsetSeqMetadata(0,1767546218777,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 17:03:38 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:03:38 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:03:38 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:03:38 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:03:38 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:03:38 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:03:38 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#28015 - airline_prefix.nullCount#28014) > 0)
26/01/04 17:03:38 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#28050 - min_flight_num.nullCount#28049) > 0)
26/01/04 17:03:38 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#28045 - max_flight_num.nullCount#28044) > 0)
26/01/04 17:03:38 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:03:38 INFO DAGScheduler: Got job 69 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 17:03:38 INFO DAGScheduler: Final stage: ResultStage 70 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:03:38 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:03:38 INFO DAGScheduler: Missing parents: List()
26/01/04 17:03:38 INFO DAGScheduler: Submitting ResultStage 70 (MapPartitionsRDD[352] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:03:38 INFO MemoryStore: Block broadcast_102 stored as values in memory (estimated size 38.5 KiB, free 434.4 MiB)
26/01/04 17:03:38 INFO MemoryStore: Block broadcast_102_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.3 MiB)
26/01/04 17:03:38 INFO BlockManagerInfo: Added broadcast_102_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:03:38 INFO BlockManagerInfo: Removed broadcast_100_piece0 on spark-master:33535 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:03:38 INFO SparkContext: Created broadcast 102 from broadcast at DAGScheduler.scala:1585
26/01/04 17:03:38 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 70 (MapPartitionsRDD[352] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 17:03:38 INFO TaskSchedulerImpl: Adding task set 70.0 with 2 tasks resource profile 0
26/01/04 17:03:38 INFO BlockManagerInfo: Removed broadcast_100_piece0 on 172.18.0.13:44401 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:03:38 INFO TaskSetManager: Starting task 1.0 in stage 70.0 (TID 140) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 17:03:38 INFO TaskSetManager: Starting task 0.0 in stage 70.0 (TID 141) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 17:03:39 INFO BlockManagerInfo: Added broadcast_102_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:03:39 INFO BlockManagerInfo: Added broadcast_102_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:03:39 INFO TaskSetManager: Finished task 1.0 in stage 70.0 (TID 140) in 24 ms on 172.18.0.13 (executor 0) (1/2)
26/01/04 17:03:39 INFO TaskSetManager: Finished task 0.0 in stage 70.0 (TID 141) in 26 ms on 172.18.0.14 (executor 1) (2/2)
26/01/04 17:03:39 INFO TaskSchedulerImpl: Removed TaskSet 70.0, whose tasks have all completed, from pool 
26/01/04 17:03:39 INFO DAGScheduler: ResultStage 70 (start at NativeMethodAccessorImpl.java:0) finished in 0.037 s
26/01/04 17:03:39 INFO DAGScheduler: Job 69 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:03:39 INFO TaskSchedulerImpl: Killing all running tasks in stage 70: Stage finished
26/01/04 17:03:39 INFO DAGScheduler: Job 69 finished: start at NativeMethodAccessorImpl.java:0, took 0.039040 s
26/01/04 17:03:39 INFO MemoryStore: Block broadcast_103_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.3 MiB)
26/01/04 17:03:39 INFO BlockManagerInfo: Added broadcast_103_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:03:39 INFO SparkContext: Created broadcast 103 from start at NativeMethodAccessorImpl.java:0
26/01/04 17:03:39 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 33, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@6585987]. The input RDD has 1 partitions.
26/01/04 17:03:39 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:03:39 INFO DAGScheduler: Got job 70 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/01/04 17:03:39 INFO DAGScheduler: Final stage: ResultStage 71 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:03:39 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:03:39 INFO DAGScheduler: Missing parents: List()
26/01/04 17:03:39 INFO DAGScheduler: Submitting ResultStage 71 (MapPartitionsRDD[357] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:03:39 INFO MemoryStore: Block broadcast_104 stored as values in memory (estimated size 52.9 KiB, free 434.3 MiB)
26/01/04 17:03:39 INFO MemoryStore: Block broadcast_104_piece0 stored as bytes in memory (estimated size 20.2 KiB, free 434.3 MiB)
26/01/04 17:03:39 INFO BlockManagerInfo: Added broadcast_104_piece0 in memory on spark-master:33535 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:03:39 INFO SparkContext: Created broadcast 104 from broadcast at DAGScheduler.scala:1585
26/01/04 17:03:39 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 71 (MapPartitionsRDD[357] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/01/04 17:03:39 INFO TaskSchedulerImpl: Adding task set 71.0 with 1 tasks resource profile 0
26/01/04 17:03:39 INFO TaskSetManager: Starting task 0.0 in stage 71.0 (TID 142) (172.18.0.13, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:03:39 INFO BlockManagerInfo: Added broadcast_104_piece0 in memory on 172.18.0.13:44401 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:03:39 INFO BlockManagerInfo: Added broadcast_103_piece0 in memory on 172.18.0.13:44401 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:03:39 INFO TaskSetManager: Finished task 0.0 in stage 71.0 (TID 142) in 596 ms on 172.18.0.13 (executor 0) (1/1)
26/01/04 17:03:39 INFO TaskSchedulerImpl: Removed TaskSet 71.0, whose tasks have all completed, from pool 
26/01/04 17:03:39 INFO DAGScheduler: ResultStage 71 (start at NativeMethodAccessorImpl.java:0) finished in 0.603 s
26/01/04 17:03:39 INFO DAGScheduler: Job 70 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:03:39 INFO TaskSchedulerImpl: Killing all running tasks in stage 71: Stage finished
26/01/04 17:03:39 INFO DAGScheduler: Job 70 finished: start at NativeMethodAccessorImpl.java:0, took 0.605092 s
26/01/04 17:03:39 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 33, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@6585987] is committing.
26/01/04 17:03:39 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 33, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@6585987] committed.
26/01/04 17:03:39 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/33 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.33.e0721efa-3ebd-41d7-b975-fa3328448501.tmp
26/01/04 17:03:39 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.33.e0721efa-3ebd-41d7-b975-fa3328448501.tmp to file:/tmp/spark-checkpoint-enrichment/commits/33
26/01/04 17:03:39 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T17:03:38.775Z",
  "batchId" : 33,
  "numInputRows" : 30,
  "inputRowsPerSecond" : 2500.0,
  "processedRowsPerSecond" : 32.25806451612903,
  "durationMs" : {
    "addBatch" : 722,
    "commitOffsets" : 60,
    "getBatch" : 0,
    "latestOffset" : 2,
    "queryPlanning" : 46,
    "triggerExecution" : 930,
    "walCommit" : 100
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "0" : 279
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "0" : 309
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "0" : 309
      }
    },
    "numInputRows" : 30,
    "inputRowsPerSecond" : 2500.0,
    "processedRowsPerSecond" : 32.25806451612903,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 31
  }
}
26/01/04 17:03:49 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/01/04 17:03:49 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/34 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.34.bfab989f-75d3-4edc-aaee-8a1099bdb18e.tmp
26/01/04 17:03:49 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.34.bfab989f-75d3-4edc-aaee-8a1099bdb18e.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/34
26/01/04 17:03:49 INFO MicroBatchExecution: Committed offsets for batch 34. Metadata OffsetSeqMetadata(0,1767546229786,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 17:03:49 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:03:49 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:03:49 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:03:49 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:03:49 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:03:49 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:03:49 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#28819 - airline_prefix.nullCount#28818) > 0)
26/01/04 17:03:49 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#28854 - min_flight_num.nullCount#28853) > 0)
26/01/04 17:03:49 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#28849 - max_flight_num.nullCount#28848) > 0)
26/01/04 17:03:49 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:03:49 INFO DAGScheduler: Got job 71 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 17:03:49 INFO DAGScheduler: Final stage: ResultStage 72 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:03:49 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:03:49 INFO DAGScheduler: Missing parents: List()
26/01/04 17:03:49 INFO DAGScheduler: Submitting ResultStage 72 (MapPartitionsRDD[362] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:03:49 INFO MemoryStore: Block broadcast_105 stored as values in memory (estimated size 38.5 KiB, free 434.2 MiB)
26/01/04 17:03:49 INFO MemoryStore: Block broadcast_105_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.2 MiB)
26/01/04 17:03:49 INFO BlockManagerInfo: Added broadcast_105_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:03:49 INFO SparkContext: Created broadcast 105 from broadcast at DAGScheduler.scala:1585
26/01/04 17:03:49 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 72 (MapPartitionsRDD[362] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 17:03:49 INFO TaskSchedulerImpl: Adding task set 72.0 with 2 tasks resource profile 0
26/01/04 17:03:49 INFO TaskSetManager: Starting task 0.0 in stage 72.0 (TID 143) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 17:03:49 INFO TaskSetManager: Starting task 1.0 in stage 72.0 (TID 144) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 17:03:49 INFO BlockManagerInfo: Added broadcast_105_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:03:49 INFO BlockManagerInfo: Added broadcast_105_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:03:49 INFO TaskSetManager: Finished task 0.0 in stage 72.0 (TID 143) in 30 ms on 172.18.0.14 (executor 1) (1/2)
26/01/04 17:03:49 INFO TaskSetManager: Finished task 1.0 in stage 72.0 (TID 144) in 31 ms on 172.18.0.13 (executor 0) (2/2)
26/01/04 17:03:49 INFO TaskSchedulerImpl: Removed TaskSet 72.0, whose tasks have all completed, from pool 
26/01/04 17:03:49 INFO DAGScheduler: ResultStage 72 (start at NativeMethodAccessorImpl.java:0) finished in 0.039 s
26/01/04 17:03:49 INFO DAGScheduler: Job 71 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:03:49 INFO TaskSchedulerImpl: Killing all running tasks in stage 72: Stage finished
26/01/04 17:03:49 INFO DAGScheduler: Job 71 finished: start at NativeMethodAccessorImpl.java:0, took 0.042128 s
26/01/04 17:03:49 INFO MemoryStore: Block broadcast_106_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.2 MiB)
26/01/04 17:03:49 INFO BlockManagerInfo: Added broadcast_106_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:03:49 INFO SparkContext: Created broadcast 106 from start at NativeMethodAccessorImpl.java:0
26/01/04 17:03:50 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 34, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@35db96dc]. The input RDD has 1 partitions.
26/01/04 17:03:50 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:03:50 INFO DAGScheduler: Got job 72 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/01/04 17:03:50 INFO DAGScheduler: Final stage: ResultStage 73 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:03:50 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:03:50 INFO DAGScheduler: Missing parents: List()
26/01/04 17:03:50 INFO DAGScheduler: Submitting ResultStage 73 (MapPartitionsRDD[367] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:03:50 INFO MemoryStore: Block broadcast_107 stored as values in memory (estimated size 52.9 KiB, free 434.2 MiB)
26/01/04 17:03:50 INFO MemoryStore: Block broadcast_107_piece0 stored as bytes in memory (estimated size 20.2 KiB, free 434.1 MiB)
26/01/04 17:03:50 INFO BlockManagerInfo: Added broadcast_107_piece0 in memory on spark-master:33535 (size: 20.2 KiB, free: 434.3 MiB)
26/01/04 17:03:50 INFO SparkContext: Created broadcast 107 from broadcast at DAGScheduler.scala:1585
26/01/04 17:03:50 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 73 (MapPartitionsRDD[367] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/01/04 17:03:50 INFO TaskSchedulerImpl: Adding task set 73.0 with 1 tasks resource profile 0
26/01/04 17:03:50 INFO TaskSetManager: Starting task 0.0 in stage 73.0 (TID 145) (172.18.0.13, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:03:50 INFO BlockManagerInfo: Added broadcast_107_piece0 in memory on 172.18.0.13:44401 (size: 20.2 KiB, free: 434.3 MiB)
26/01/04 17:03:50 INFO BlockManagerInfo: Added broadcast_106_piece0 in memory on 172.18.0.13:44401 (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:03:50 INFO TaskSetManager: Finished task 0.0 in stage 73.0 (TID 145) in 594 ms on 172.18.0.13 (executor 0) (1/1)
26/01/04 17:03:50 INFO TaskSchedulerImpl: Removed TaskSet 73.0, whose tasks have all completed, from pool 
26/01/04 17:03:50 INFO DAGScheduler: ResultStage 73 (start at NativeMethodAccessorImpl.java:0) finished in 0.601 s
26/01/04 17:03:50 INFO DAGScheduler: Job 72 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:03:50 INFO TaskSchedulerImpl: Killing all running tasks in stage 73: Stage finished
26/01/04 17:03:50 INFO DAGScheduler: Job 72 finished: start at NativeMethodAccessorImpl.java:0, took 0.605155 s
26/01/04 17:03:50 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 34, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@35db96dc] is committing.
26/01/04 17:03:50 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 34, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@35db96dc] committed.
26/01/04 17:03:50 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/34 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.34.b6c9c7a0-8d20-4585-85c1-486bef364a57.tmp
26/01/04 17:03:50 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.34.b6c9c7a0-8d20-4585-85c1-486bef364a57.tmp to file:/tmp/spark-checkpoint-enrichment/commits/34
26/01/04 17:03:50 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T17:03:49.783Z",
  "batchId" : 34,
  "numInputRows" : 30,
  "inputRowsPerSecond" : 2500.0,
  "processedRowsPerSecond" : 33.82187147688839,
  "durationMs" : {
    "addBatch" : 725,
    "commitOffsets" : 60,
    "getBatch" : 0,
    "latestOffset" : 3,
    "queryPlanning" : 26,
    "triggerExecution" : 887,
    "walCommit" : 72
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "0" : 309
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "0" : 339
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "0" : 339
      }
    },
    "numInputRows" : 30,
    "inputRowsPerSecond" : 2500.0,
    "processedRowsPerSecond" : 33.82187147688839,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 31
  }
}
26/01/04 17:03:52 INFO BlockManagerInfo: Removed broadcast_103_piece0 on spark-master:33535 in memory (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:03:52 INFO BlockManagerInfo: Removed broadcast_103_piece0 on 172.18.0.13:44401 in memory (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:03:52 INFO BlockManagerInfo: Removed broadcast_105_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:03:52 INFO BlockManagerInfo: Removed broadcast_105_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:03:52 INFO BlockManagerInfo: Removed broadcast_105_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:03:52 INFO BlockManagerInfo: Removed broadcast_107_piece0 on spark-master:33535 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:03:52 INFO BlockManagerInfo: Removed broadcast_107_piece0 on 172.18.0.13:44401 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:03:52 INFO BlockManagerInfo: Removed broadcast_104_piece0 on spark-master:33535 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:03:52 INFO BlockManagerInfo: Removed broadcast_104_piece0 on 172.18.0.13:44401 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:03:52 INFO BlockManagerInfo: Removed broadcast_102_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:03:52 INFO BlockManagerInfo: Removed broadcast_102_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:03:52 INFO BlockManagerInfo: Removed broadcast_102_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:04:00 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/01/04 17:04:00 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/35 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.35.2a11e17f-d6b7-4a2c-95be-f1e397cf5fd0.tmp
26/01/04 17:04:00 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.35.2a11e17f-d6b7-4a2c-95be-f1e397cf5fd0.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/35
26/01/04 17:04:00 INFO MicroBatchExecution: Committed offsets for batch 35. Metadata OffsetSeqMetadata(0,1767546240807,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 17:04:00 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:04:00 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:04:00 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:04:00 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:04:00 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:04:00 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:04:00 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#29623 - airline_prefix.nullCount#29622) > 0)
26/01/04 17:04:00 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#29658 - min_flight_num.nullCount#29657) > 0)
26/01/04 17:04:00 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#29653 - max_flight_num.nullCount#29652) > 0)
26/01/04 17:04:00 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:04:00 INFO DAGScheduler: Got job 73 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 17:04:00 INFO DAGScheduler: Final stage: ResultStage 74 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:04:00 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:04:00 INFO DAGScheduler: Missing parents: List()
26/01/04 17:04:00 INFO DAGScheduler: Submitting ResultStage 74 (MapPartitionsRDD[372] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:04:00 INFO MemoryStore: Block broadcast_108 stored as values in memory (estimated size 38.5 KiB, free 434.4 MiB)
26/01/04 17:04:01 INFO MemoryStore: Block broadcast_108_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.3 MiB)
26/01/04 17:04:01 INFO BlockManagerInfo: Added broadcast_108_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:04:01 INFO SparkContext: Created broadcast 108 from broadcast at DAGScheduler.scala:1585
26/01/04 17:04:01 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 74 (MapPartitionsRDD[372] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 17:04:01 INFO TaskSchedulerImpl: Adding task set 74.0 with 2 tasks resource profile 0
26/01/04 17:04:01 INFO TaskSetManager: Starting task 0.0 in stage 74.0 (TID 146) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 17:04:01 INFO TaskSetManager: Starting task 1.0 in stage 74.0 (TID 147) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 17:04:01 INFO BlockManagerInfo: Added broadcast_108_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:04:01 INFO BlockManagerInfo: Added broadcast_108_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:04:01 INFO TaskSetManager: Finished task 0.0 in stage 74.0 (TID 146) in 25 ms on 172.18.0.14 (executor 1) (1/2)
26/01/04 17:04:01 INFO TaskSetManager: Finished task 1.0 in stage 74.0 (TID 147) in 25 ms on 172.18.0.13 (executor 0) (2/2)
26/01/04 17:04:01 INFO TaskSchedulerImpl: Removed TaskSet 74.0, whose tasks have all completed, from pool 
26/01/04 17:04:01 INFO DAGScheduler: ResultStage 74 (start at NativeMethodAccessorImpl.java:0) finished in 0.033 s
26/01/04 17:04:01 INFO DAGScheduler: Job 73 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:04:01 INFO TaskSchedulerImpl: Killing all running tasks in stage 74: Stage finished
26/01/04 17:04:01 INFO DAGScheduler: Job 73 finished: start at NativeMethodAccessorImpl.java:0, took 0.035992 s
26/01/04 17:04:01 INFO MemoryStore: Block broadcast_109_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.3 MiB)
26/01/04 17:04:01 INFO BlockManagerInfo: Added broadcast_109_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:04:01 INFO SparkContext: Created broadcast 109 from start at NativeMethodAccessorImpl.java:0
26/01/04 17:04:01 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 35, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@7dfefbe9]. The input RDD has 1 partitions.
26/01/04 17:04:01 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:04:01 INFO DAGScheduler: Got job 74 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/01/04 17:04:01 INFO DAGScheduler: Final stage: ResultStage 75 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:04:01 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:04:01 INFO DAGScheduler: Missing parents: List()
26/01/04 17:04:01 INFO DAGScheduler: Submitting ResultStage 75 (MapPartitionsRDD[377] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:04:01 INFO MemoryStore: Block broadcast_110 stored as values in memory (estimated size 52.9 KiB, free 434.3 MiB)
26/01/04 17:04:01 INFO MemoryStore: Block broadcast_110_piece0 stored as bytes in memory (estimated size 20.2 KiB, free 434.3 MiB)
26/01/04 17:04:01 INFO BlockManagerInfo: Added broadcast_110_piece0 in memory on spark-master:33535 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:04:01 INFO SparkContext: Created broadcast 110 from broadcast at DAGScheduler.scala:1585
26/01/04 17:04:01 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 75 (MapPartitionsRDD[377] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/01/04 17:04:01 INFO TaskSchedulerImpl: Adding task set 75.0 with 1 tasks resource profile 0
26/01/04 17:04:01 INFO TaskSetManager: Starting task 0.0 in stage 75.0 (TID 148) (172.18.0.13, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:04:01 INFO BlockManagerInfo: Added broadcast_110_piece0 in memory on 172.18.0.13:44401 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:04:01 INFO BlockManagerInfo: Added broadcast_109_piece0 in memory on 172.18.0.13:44401 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:04:01 INFO TaskSetManager: Finished task 0.0 in stage 75.0 (TID 148) in 580 ms on 172.18.0.13 (executor 0) (1/1)
26/01/04 17:04:01 INFO TaskSchedulerImpl: Removed TaskSet 75.0, whose tasks have all completed, from pool 
26/01/04 17:04:01 INFO DAGScheduler: ResultStage 75 (start at NativeMethodAccessorImpl.java:0) finished in 0.586 s
26/01/04 17:04:01 INFO DAGScheduler: Job 74 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:04:01 INFO TaskSchedulerImpl: Killing all running tasks in stage 75: Stage finished
26/01/04 17:04:01 INFO DAGScheduler: Job 74 finished: start at NativeMethodAccessorImpl.java:0, took 0.587692 s
26/01/04 17:04:01 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 35, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@7dfefbe9] is committing.
26/01/04 17:04:01 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 35, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@7dfefbe9] committed.
26/01/04 17:04:01 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/35 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.35.612e00be-fd7f-4f2f-8964-ed3796da414b.tmp
26/01/04 17:04:01 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.35.612e00be-fd7f-4f2f-8964-ed3796da414b.tmp to file:/tmp/spark-checkpoint-enrichment/commits/35
26/01/04 17:04:01 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T17:04:00.806Z",
  "batchId" : 35,
  "numInputRows" : 30,
  "inputRowsPerSecond" : 2500.0,
  "processedRowsPerSecond" : 33.557046979865774,
  "durationMs" : {
    "addBatch" : 715,
    "commitOffsets" : 67,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 25,
    "triggerExecution" : 894,
    "walCommit" : 83
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "0" : 339
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "0" : 369
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "0" : 369
      }
    },
    "numInputRows" : 30,
    "inputRowsPerSecond" : 2500.0,
    "processedRowsPerSecond" : 33.557046979865774,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 31
  }
}
26/01/04 17:04:10 INFO BlockManagerInfo: Removed broadcast_110_piece0 on spark-master:33535 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:04:10 INFO BlockManagerInfo: Removed broadcast_110_piece0 on 172.18.0.13:44401 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:04:10 INFO BlockManagerInfo: Removed broadcast_108_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:04:10 INFO BlockManagerInfo: Removed broadcast_108_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:04:10 INFO BlockManagerInfo: Removed broadcast_108_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:04:10 INFO BlockManagerInfo: Removed broadcast_106_piece0 on spark-master:33535 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:04:10 INFO BlockManagerInfo: Removed broadcast_106_piece0 on 172.18.0.13:44401 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:04:11 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/01/04 17:04:11 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/36 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.36.5014c1da-84a7-4035-8278-ce6e57c212d9.tmp
26/01/04 17:04:11 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.36.5014c1da-84a7-4035-8278-ce6e57c212d9.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/36
26/01/04 17:04:11 INFO MicroBatchExecution: Committed offsets for batch 36. Metadata OffsetSeqMetadata(0,1767546251818,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 17:04:11 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:04:11 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:04:11 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:04:11 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:04:11 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:04:11 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:04:11 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#30427 - airline_prefix.nullCount#30426) > 0)
26/01/04 17:04:11 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#30462 - min_flight_num.nullCount#30461) > 0)
26/01/04 17:04:11 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#30457 - max_flight_num.nullCount#30456) > 0)
26/01/04 17:04:11 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:04:11 INFO DAGScheduler: Got job 75 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 17:04:11 INFO DAGScheduler: Final stage: ResultStage 76 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:04:11 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:04:11 INFO DAGScheduler: Missing parents: List()
26/01/04 17:04:11 INFO DAGScheduler: Submitting ResultStage 76 (MapPartitionsRDD[382] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:04:11 INFO MemoryStore: Block broadcast_111 stored as values in memory (estimated size 38.5 KiB, free 434.4 MiB)
26/01/04 17:04:11 INFO MemoryStore: Block broadcast_111_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.3 MiB)
26/01/04 17:04:11 INFO BlockManagerInfo: Added broadcast_111_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:04:11 INFO SparkContext: Created broadcast 111 from broadcast at DAGScheduler.scala:1585
26/01/04 17:04:11 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 76 (MapPartitionsRDD[382] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 17:04:11 INFO TaskSchedulerImpl: Adding task set 76.0 with 2 tasks resource profile 0
26/01/04 17:04:11 INFO BlockManagerInfo: Removed broadcast_109_piece0 on spark-master:33535 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:04:11 INFO BlockManagerInfo: Removed broadcast_109_piece0 on 172.18.0.13:44401 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:04:11 INFO TaskSetManager: Starting task 0.0 in stage 76.0 (TID 149) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 17:04:11 INFO TaskSetManager: Starting task 1.0 in stage 76.0 (TID 150) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 17:04:12 INFO BlockManagerInfo: Added broadcast_111_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:04:12 INFO BlockManagerInfo: Added broadcast_111_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:04:12 INFO TaskSetManager: Finished task 1.0 in stage 76.0 (TID 150) in 61 ms on 172.18.0.13 (executor 0) (1/2)
26/01/04 17:04:12 INFO TaskSetManager: Finished task 0.0 in stage 76.0 (TID 149) in 64 ms on 172.18.0.14 (executor 1) (2/2)
26/01/04 17:04:12 INFO TaskSchedulerImpl: Removed TaskSet 76.0, whose tasks have all completed, from pool 
26/01/04 17:04:12 INFO DAGScheduler: ResultStage 76 (start at NativeMethodAccessorImpl.java:0) finished in 0.079 s
26/01/04 17:04:12 INFO DAGScheduler: Job 75 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:04:12 INFO TaskSchedulerImpl: Killing all running tasks in stage 76: Stage finished
26/01/04 17:04:12 INFO DAGScheduler: Job 75 finished: start at NativeMethodAccessorImpl.java:0, took 0.083191 s
26/01/04 17:04:12 INFO MemoryStore: Block broadcast_112_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.3 MiB)
26/01/04 17:04:12 INFO BlockManagerInfo: Added broadcast_112_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:04:12 INFO SparkContext: Created broadcast 112 from start at NativeMethodAccessorImpl.java:0
26/01/04 17:04:12 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 36, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@22644257]. The input RDD has 1 partitions.
26/01/04 17:04:12 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:04:12 INFO DAGScheduler: Got job 76 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/01/04 17:04:12 INFO DAGScheduler: Final stage: ResultStage 77 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:04:12 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:04:12 INFO DAGScheduler: Missing parents: List()
26/01/04 17:04:12 INFO DAGScheduler: Submitting ResultStage 77 (MapPartitionsRDD[387] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:04:12 INFO MemoryStore: Block broadcast_113 stored as values in memory (estimated size 52.9 KiB, free 434.3 MiB)
26/01/04 17:04:12 INFO MemoryStore: Block broadcast_113_piece0 stored as bytes in memory (estimated size 20.2 KiB, free 434.3 MiB)
26/01/04 17:04:12 INFO BlockManagerInfo: Added broadcast_113_piece0 in memory on spark-master:33535 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:04:12 INFO SparkContext: Created broadcast 113 from broadcast at DAGScheduler.scala:1585
26/01/04 17:04:12 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 77 (MapPartitionsRDD[387] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/01/04 17:04:12 INFO TaskSchedulerImpl: Adding task set 77.0 with 1 tasks resource profile 0
26/01/04 17:04:12 INFO TaskSetManager: Starting task 0.0 in stage 77.0 (TID 151) (172.18.0.13, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:04:12 INFO BlockManagerInfo: Added broadcast_113_piece0 in memory on 172.18.0.13:44401 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:04:12 INFO BlockManagerInfo: Added broadcast_112_piece0 in memory on 172.18.0.13:44401 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:04:12 INFO TaskSetManager: Finished task 0.0 in stage 77.0 (TID 151) in 593 ms on 172.18.0.13 (executor 0) (1/1)
26/01/04 17:04:12 INFO TaskSchedulerImpl: Removed TaskSet 77.0, whose tasks have all completed, from pool 
26/01/04 17:04:12 INFO DAGScheduler: ResultStage 77 (start at NativeMethodAccessorImpl.java:0) finished in 0.600 s
26/01/04 17:04:12 INFO DAGScheduler: Job 76 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:04:12 INFO TaskSchedulerImpl: Killing all running tasks in stage 77: Stage finished
26/01/04 17:04:12 INFO DAGScheduler: Job 76 finished: start at NativeMethodAccessorImpl.java:0, took 0.602635 s
26/01/04 17:04:12 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 36, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@22644257] is committing.
26/01/04 17:04:12 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 36, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@22644257] committed.
26/01/04 17:04:12 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/36 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.36.c34c5ce8-fcc2-4fcd-90d1-4fb4d931478f.tmp
26/01/04 17:04:12 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.36.c34c5ce8-fcc2-4fcd-90d1-4fb4d931478f.tmp to file:/tmp/spark-checkpoint-enrichment/commits/36
26/01/04 17:04:12 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T17:04:11.817Z",
  "batchId" : 36,
  "numInputRows" : 30,
  "inputRowsPerSecond" : 2307.6923076923076,
  "processedRowsPerSecond" : 32.362459546925564,
  "durationMs" : {
    "addBatch" : 766,
    "commitOffsets" : 65,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 24,
    "triggerExecution" : 927,
    "walCommit" : 70
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "0" : 369
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "0" : 399
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "0" : 399
      }
    },
    "numInputRows" : 30,
    "inputRowsPerSecond" : 2307.6923076923076,
    "processedRowsPerSecond" : 32.362459546925564,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 31
  }
}
26/01/04 17:04:22 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/01/04 17:04:22 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/37 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.37.75036433-870c-4745-b2ed-e8bccca98fed.tmp
26/01/04 17:04:22 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.37.75036433-870c-4745-b2ed-e8bccca98fed.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/37
26/01/04 17:04:22 INFO MicroBatchExecution: Committed offsets for batch 37. Metadata OffsetSeqMetadata(0,1767546262839,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 17:04:22 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:04:22 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:04:22 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:04:22 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:04:22 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:04:22 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:04:23 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#31231 - airline_prefix.nullCount#31230) > 0)
26/01/04 17:04:23 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#31266 - min_flight_num.nullCount#31265) > 0)
26/01/04 17:04:23 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#31261 - max_flight_num.nullCount#31260) > 0)
26/01/04 17:04:23 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:04:23 INFO DAGScheduler: Got job 77 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 17:04:23 INFO DAGScheduler: Final stage: ResultStage 78 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:04:23 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:04:23 INFO DAGScheduler: Missing parents: List()
26/01/04 17:04:23 INFO DAGScheduler: Submitting ResultStage 78 (MapPartitionsRDD[392] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:04:23 INFO MemoryStore: Block broadcast_114 stored as values in memory (estimated size 38.5 KiB, free 434.2 MiB)
26/01/04 17:04:23 INFO MemoryStore: Block broadcast_114_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.2 MiB)
26/01/04 17:04:23 INFO BlockManagerInfo: Added broadcast_114_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:04:23 INFO SparkContext: Created broadcast 114 from broadcast at DAGScheduler.scala:1585
26/01/04 17:04:23 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 78 (MapPartitionsRDD[392] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 17:04:23 INFO TaskSchedulerImpl: Adding task set 78.0 with 2 tasks resource profile 0
26/01/04 17:04:23 INFO TaskSetManager: Starting task 0.0 in stage 78.0 (TID 152) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 17:04:23 INFO TaskSetManager: Starting task 1.0 in stage 78.0 (TID 153) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 17:04:23 INFO BlockManagerInfo: Added broadcast_114_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:04:23 INFO BlockManagerInfo: Added broadcast_114_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:04:23 INFO TaskSetManager: Finished task 1.0 in stage 78.0 (TID 153) in 28 ms on 172.18.0.13 (executor 0) (1/2)
26/01/04 17:04:23 INFO TaskSetManager: Finished task 0.0 in stage 78.0 (TID 152) in 30 ms on 172.18.0.14 (executor 1) (2/2)
26/01/04 17:04:23 INFO TaskSchedulerImpl: Removed TaskSet 78.0, whose tasks have all completed, from pool 
26/01/04 17:04:23 INFO DAGScheduler: ResultStage 78 (start at NativeMethodAccessorImpl.java:0) finished in 0.037 s
26/01/04 17:04:23 INFO DAGScheduler: Job 77 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:04:23 INFO TaskSchedulerImpl: Killing all running tasks in stage 78: Stage finished
26/01/04 17:04:23 INFO DAGScheduler: Job 77 finished: start at NativeMethodAccessorImpl.java:0, took 0.040479 s
26/01/04 17:04:23 INFO MemoryStore: Block broadcast_115_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.2 MiB)
26/01/04 17:04:23 INFO BlockManagerInfo: Added broadcast_115_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:04:23 INFO SparkContext: Created broadcast 115 from start at NativeMethodAccessorImpl.java:0
26/01/04 17:04:23 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 37, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@3067fac9]. The input RDD has 1 partitions.
26/01/04 17:04:23 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:04:23 INFO DAGScheduler: Got job 78 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/01/04 17:04:23 INFO DAGScheduler: Final stage: ResultStage 79 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:04:23 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:04:23 INFO DAGScheduler: Missing parents: List()
26/01/04 17:04:23 INFO DAGScheduler: Submitting ResultStage 79 (MapPartitionsRDD[397] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:04:23 INFO MemoryStore: Block broadcast_116 stored as values in memory (estimated size 52.9 KiB, free 434.2 MiB)
26/01/04 17:04:23 INFO MemoryStore: Block broadcast_116_piece0 stored as bytes in memory (estimated size 20.2 KiB, free 434.1 MiB)
26/01/04 17:04:23 INFO BlockManagerInfo: Added broadcast_116_piece0 in memory on spark-master:33535 (size: 20.2 KiB, free: 434.3 MiB)
26/01/04 17:04:23 INFO SparkContext: Created broadcast 116 from broadcast at DAGScheduler.scala:1585
26/01/04 17:04:23 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 79 (MapPartitionsRDD[397] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/01/04 17:04:23 INFO TaskSchedulerImpl: Adding task set 79.0 with 1 tasks resource profile 0
26/01/04 17:04:23 INFO TaskSetManager: Starting task 0.0 in stage 79.0 (TID 154) (172.18.0.13, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:04:23 INFO BlockManagerInfo: Added broadcast_116_piece0 in memory on 172.18.0.13:44401 (size: 20.2 KiB, free: 434.3 MiB)
26/01/04 17:04:23 INFO BlockManagerInfo: Added broadcast_115_piece0 in memory on 172.18.0.13:44401 (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:04:23 INFO TaskSetManager: Finished task 0.0 in stage 79.0 (TID 154) in 580 ms on 172.18.0.13 (executor 0) (1/1)
26/01/04 17:04:23 INFO TaskSchedulerImpl: Removed TaskSet 79.0, whose tasks have all completed, from pool 
26/01/04 17:04:23 INFO DAGScheduler: ResultStage 79 (start at NativeMethodAccessorImpl.java:0) finished in 0.585 s
26/01/04 17:04:23 INFO DAGScheduler: Job 78 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:04:23 INFO TaskSchedulerImpl: Killing all running tasks in stage 79: Stage finished
26/01/04 17:04:23 INFO DAGScheduler: Job 78 finished: start at NativeMethodAccessorImpl.java:0, took 0.586998 s
26/01/04 17:04:23 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 37, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@3067fac9] is committing.
26/01/04 17:04:23 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 37, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@3067fac9] committed.
26/01/04 17:04:23 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/37 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.37.955589e6-c233-490a-9679-67fff51cc812.tmp
26/01/04 17:04:23 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.37.955589e6-c233-490a-9679-67fff51cc812.tmp to file:/tmp/spark-checkpoint-enrichment/commits/37
26/01/04 17:04:23 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T17:04:22.837Z",
  "batchId" : 37,
  "numInputRows" : 30,
  "inputRowsPerSecond" : 2500.0,
  "processedRowsPerSecond" : 33.70786516853933,
  "durationMs" : {
    "addBatch" : 728,
    "commitOffsets" : 61,
    "getBatch" : 0,
    "latestOffset" : 2,
    "queryPlanning" : 31,
    "triggerExecution" : 890,
    "walCommit" : 68
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "0" : 399
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "0" : 429
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "0" : 429
      }
    },
    "numInputRows" : 30,
    "inputRowsPerSecond" : 2500.0,
    "processedRowsPerSecond" : 33.70786516853933,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 31
  }
}
26/01/04 17:04:25 INFO BlockManagerInfo: Removed broadcast_114_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:04:25 INFO BlockManagerInfo: Removed broadcast_114_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:04:25 INFO BlockManagerInfo: Removed broadcast_114_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:04:25 INFO BlockManagerInfo: Removed broadcast_111_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:04:25 INFO BlockManagerInfo: Removed broadcast_111_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:04:25 INFO BlockManagerInfo: Removed broadcast_111_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:04:25 INFO BlockManagerInfo: Removed broadcast_113_piece0 on spark-master:33535 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:04:25 INFO BlockManagerInfo: Removed broadcast_113_piece0 on 172.18.0.13:44401 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:04:25 INFO BlockManagerInfo: Removed broadcast_116_piece0 on spark-master:33535 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:04:25 INFO BlockManagerInfo: Removed broadcast_116_piece0 on 172.18.0.13:44401 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:04:25 INFO BlockManagerInfo: Removed broadcast_112_piece0 on spark-master:33535 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:04:25 INFO BlockManagerInfo: Removed broadcast_112_piece0 on 172.18.0.13:44401 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:04:33 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/01/04 17:04:33 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/38 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.38.184e6577-13a5-453f-a8b9-74acda8e4a21.tmp
26/01/04 17:04:33 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.38.184e6577-13a5-453f-a8b9-74acda8e4a21.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/38
26/01/04 17:04:33 INFO MicroBatchExecution: Committed offsets for batch 38. Metadata OffsetSeqMetadata(0,1767546273854,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 17:04:33 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:04:33 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:04:33 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:04:33 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:04:34 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:04:34 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:04:34 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#32035 - airline_prefix.nullCount#32034) > 0)
26/01/04 17:04:34 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#32070 - min_flight_num.nullCount#32069) > 0)
26/01/04 17:04:34 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#32065 - max_flight_num.nullCount#32064) > 0)
26/01/04 17:04:34 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:04:34 INFO DAGScheduler: Got job 79 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 17:04:34 INFO DAGScheduler: Final stage: ResultStage 80 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:04:34 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:04:34 INFO DAGScheduler: Missing parents: List()
26/01/04 17:04:34 INFO DAGScheduler: Submitting ResultStage 80 (MapPartitionsRDD[402] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:04:34 INFO MemoryStore: Block broadcast_117 stored as values in memory (estimated size 38.5 KiB, free 434.4 MiB)
26/01/04 17:04:34 INFO MemoryStore: Block broadcast_117_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.3 MiB)
26/01/04 17:04:34 INFO BlockManagerInfo: Added broadcast_117_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:04:34 INFO SparkContext: Created broadcast 117 from broadcast at DAGScheduler.scala:1585
26/01/04 17:04:34 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 80 (MapPartitionsRDD[402] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 17:04:34 INFO TaskSchedulerImpl: Adding task set 80.0 with 2 tasks resource profile 0
26/01/04 17:04:34 INFO TaskSetManager: Starting task 0.0 in stage 80.0 (TID 155) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 17:04:34 INFO TaskSetManager: Starting task 1.0 in stage 80.0 (TID 156) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 17:04:34 INFO BlockManagerInfo: Added broadcast_117_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:04:34 INFO BlockManagerInfo: Added broadcast_117_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:04:34 INFO TaskSetManager: Finished task 1.0 in stage 80.0 (TID 156) in 27 ms on 172.18.0.13 (executor 0) (1/2)
26/01/04 17:04:34 INFO TaskSetManager: Finished task 0.0 in stage 80.0 (TID 155) in 30 ms on 172.18.0.14 (executor 1) (2/2)
26/01/04 17:04:34 INFO TaskSchedulerImpl: Removed TaskSet 80.0, whose tasks have all completed, from pool 
26/01/04 17:04:34 INFO DAGScheduler: ResultStage 80 (start at NativeMethodAccessorImpl.java:0) finished in 0.038 s
26/01/04 17:04:34 INFO DAGScheduler: Job 79 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:04:34 INFO TaskSchedulerImpl: Killing all running tasks in stage 80: Stage finished
26/01/04 17:04:34 INFO DAGScheduler: Job 79 finished: start at NativeMethodAccessorImpl.java:0, took 0.040737 s
26/01/04 17:04:34 INFO MemoryStore: Block broadcast_118_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.3 MiB)
26/01/04 17:04:34 INFO BlockManagerInfo: Added broadcast_118_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:04:34 INFO SparkContext: Created broadcast 118 from start at NativeMethodAccessorImpl.java:0
26/01/04 17:04:34 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 38, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@70d7a14b]. The input RDD has 1 partitions.
26/01/04 17:04:34 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:04:34 INFO DAGScheduler: Got job 80 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/01/04 17:04:34 INFO DAGScheduler: Final stage: ResultStage 81 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:04:34 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:04:34 INFO DAGScheduler: Missing parents: List()
26/01/04 17:04:34 INFO DAGScheduler: Submitting ResultStage 81 (MapPartitionsRDD[407] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:04:34 INFO MemoryStore: Block broadcast_119 stored as values in memory (estimated size 52.9 KiB, free 434.3 MiB)
26/01/04 17:04:34 INFO MemoryStore: Block broadcast_119_piece0 stored as bytes in memory (estimated size 20.2 KiB, free 434.3 MiB)
26/01/04 17:04:34 INFO BlockManagerInfo: Added broadcast_119_piece0 in memory on spark-master:33535 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:04:34 INFO SparkContext: Created broadcast 119 from broadcast at DAGScheduler.scala:1585
26/01/04 17:04:34 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 81 (MapPartitionsRDD[407] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/01/04 17:04:34 INFO TaskSchedulerImpl: Adding task set 81.0 with 1 tasks resource profile 0
26/01/04 17:04:34 INFO TaskSetManager: Starting task 0.0 in stage 81.0 (TID 157) (172.18.0.13, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:04:34 INFO BlockManagerInfo: Added broadcast_119_piece0 in memory on 172.18.0.13:44401 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:04:34 INFO BlockManagerInfo: Added broadcast_118_piece0 in memory on 172.18.0.13:44401 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:04:34 INFO TaskSetManager: Finished task 0.0 in stage 81.0 (TID 157) in 593 ms on 172.18.0.13 (executor 0) (1/1)
26/01/04 17:04:34 INFO TaskSchedulerImpl: Removed TaskSet 81.0, whose tasks have all completed, from pool 
26/01/04 17:04:34 INFO DAGScheduler: ResultStage 81 (start at NativeMethodAccessorImpl.java:0) finished in 0.600 s
26/01/04 17:04:34 INFO DAGScheduler: Job 80 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:04:34 INFO TaskSchedulerImpl: Killing all running tasks in stage 81: Stage finished
26/01/04 17:04:34 INFO DAGScheduler: Job 80 finished: start at NativeMethodAccessorImpl.java:0, took 0.601796 s
26/01/04 17:04:34 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 38, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@70d7a14b] is committing.
26/01/04 17:04:34 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 38, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@70d7a14b] committed.
26/01/04 17:04:34 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/38 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.38.5f06b50c-7ea4-4f8d-99c4-fb225c17481c.tmp
26/01/04 17:04:34 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.38.5f06b50c-7ea4-4f8d-99c4-fb225c17481c.tmp to file:/tmp/spark-checkpoint-enrichment/commits/38
26/01/04 17:04:34 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T17:04:33.853Z",
  "batchId" : 38,
  "numInputRows" : 30,
  "inputRowsPerSecond" : 2500.0,
  "processedRowsPerSecond" : 33.40757238307349,
  "durationMs" : {
    "addBatch" : 724,
    "commitOffsets" : 70,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 31,
    "triggerExecution" : 898,
    "walCommit" : 70
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "0" : 429
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "0" : 459
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "0" : 459
      }
    },
    "numInputRows" : 30,
    "inputRowsPerSecond" : 2500.0,
    "processedRowsPerSecond" : 33.40757238307349,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 31
  }
}
26/01/04 17:04:44 INFO BlockManagerInfo: Removed broadcast_115_piece0 on spark-master:33535 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:04:44 INFO BlockManagerInfo: Removed broadcast_115_piece0 on 172.18.0.13:44401 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:04:44 INFO BlockManagerInfo: Removed broadcast_119_piece0 on spark-master:33535 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:04:44 INFO BlockManagerInfo: Removed broadcast_119_piece0 on 172.18.0.13:44401 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:04:44 INFO BlockManagerInfo: Removed broadcast_117_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:04:44 INFO BlockManagerInfo: Removed broadcast_117_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:04:44 INFO BlockManagerInfo: Removed broadcast_117_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:04:44 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/01/04 17:04:44 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/39 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.39.1cc9e38d-6b07-482f-af31-a7eb6397eb70.tmp
26/01/04 17:04:44 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.39.1cc9e38d-6b07-482f-af31-a7eb6397eb70.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/39
26/01/04 17:04:44 INFO MicroBatchExecution: Committed offsets for batch 39. Metadata OffsetSeqMetadata(0,1767546284866,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 17:04:44 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:04:44 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:04:45 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:04:45 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:04:45 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:04:45 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:04:45 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#32839 - airline_prefix.nullCount#32838) > 0)
26/01/04 17:04:45 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#32874 - min_flight_num.nullCount#32873) > 0)
26/01/04 17:04:45 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#32869 - max_flight_num.nullCount#32868) > 0)
26/01/04 17:04:45 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:04:45 INFO DAGScheduler: Got job 81 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 17:04:45 INFO DAGScheduler: Final stage: ResultStage 82 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:04:45 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:04:45 INFO DAGScheduler: Missing parents: List()
26/01/04 17:04:45 INFO DAGScheduler: Submitting ResultStage 82 (MapPartitionsRDD[412] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:04:45 INFO MemoryStore: Block broadcast_120 stored as values in memory (estimated size 38.5 KiB, free 434.4 MiB)
26/01/04 17:04:45 INFO MemoryStore: Block broadcast_120_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.3 MiB)
26/01/04 17:04:45 INFO BlockManagerInfo: Added broadcast_120_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:04:45 INFO BlockManagerInfo: Removed broadcast_118_piece0 on spark-master:33535 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:04:45 INFO SparkContext: Created broadcast 120 from broadcast at DAGScheduler.scala:1585
26/01/04 17:04:45 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 82 (MapPartitionsRDD[412] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 17:04:45 INFO TaskSchedulerImpl: Adding task set 82.0 with 2 tasks resource profile 0
26/01/04 17:04:45 INFO BlockManagerInfo: Removed broadcast_118_piece0 on 172.18.0.13:44401 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:04:45 INFO TaskSetManager: Starting task 1.0 in stage 82.0 (TID 158) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 17:04:45 INFO TaskSetManager: Starting task 0.0 in stage 82.0 (TID 159) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 17:04:45 INFO BlockManagerInfo: Added broadcast_120_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:04:45 INFO BlockManagerInfo: Added broadcast_120_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:04:45 INFO TaskSetManager: Finished task 1.0 in stage 82.0 (TID 158) in 25 ms on 172.18.0.13 (executor 0) (1/2)
26/01/04 17:04:45 INFO TaskSetManager: Finished task 0.0 in stage 82.0 (TID 159) in 28 ms on 172.18.0.14 (executor 1) (2/2)
26/01/04 17:04:45 INFO TaskSchedulerImpl: Removed TaskSet 82.0, whose tasks have all completed, from pool 
26/01/04 17:04:45 INFO DAGScheduler: ResultStage 82 (start at NativeMethodAccessorImpl.java:0) finished in 0.039 s
26/01/04 17:04:45 INFO DAGScheduler: Job 81 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:04:45 INFO TaskSchedulerImpl: Killing all running tasks in stage 82: Stage finished
26/01/04 17:04:45 INFO DAGScheduler: Job 81 finished: start at NativeMethodAccessorImpl.java:0, took 0.041269 s
26/01/04 17:04:45 INFO MemoryStore: Block broadcast_121_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.3 MiB)
26/01/04 17:04:45 INFO BlockManagerInfo: Added broadcast_121_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:04:45 INFO SparkContext: Created broadcast 121 from start at NativeMethodAccessorImpl.java:0
26/01/04 17:04:45 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 39, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@1b6fe97a]. The input RDD has 1 partitions.
26/01/04 17:04:45 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:04:45 INFO DAGScheduler: Got job 82 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/01/04 17:04:45 INFO DAGScheduler: Final stage: ResultStage 83 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:04:45 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:04:45 INFO DAGScheduler: Missing parents: List()
26/01/04 17:04:45 INFO DAGScheduler: Submitting ResultStage 83 (MapPartitionsRDD[417] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:04:45 INFO MemoryStore: Block broadcast_122 stored as values in memory (estimated size 52.9 KiB, free 434.3 MiB)
26/01/04 17:04:45 INFO MemoryStore: Block broadcast_122_piece0 stored as bytes in memory (estimated size 20.1 KiB, free 434.3 MiB)
26/01/04 17:04:45 INFO BlockManagerInfo: Added broadcast_122_piece0 in memory on spark-master:33535 (size: 20.1 KiB, free: 434.4 MiB)
26/01/04 17:04:45 INFO SparkContext: Created broadcast 122 from broadcast at DAGScheduler.scala:1585
26/01/04 17:04:45 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 83 (MapPartitionsRDD[417] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/01/04 17:04:45 INFO TaskSchedulerImpl: Adding task set 83.0 with 1 tasks resource profile 0
26/01/04 17:04:45 INFO TaskSetManager: Starting task 0.0 in stage 83.0 (TID 160) (172.18.0.13, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:04:45 INFO BlockManagerInfo: Added broadcast_122_piece0 in memory on 172.18.0.13:44401 (size: 20.1 KiB, free: 434.4 MiB)
26/01/04 17:04:45 INFO BlockManagerInfo: Added broadcast_121_piece0 in memory on 172.18.0.13:44401 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:04:45 INFO TaskSetManager: Finished task 0.0 in stage 83.0 (TID 160) in 589 ms on 172.18.0.13 (executor 0) (1/1)
26/01/04 17:04:45 INFO TaskSchedulerImpl: Removed TaskSet 83.0, whose tasks have all completed, from pool 
26/01/04 17:04:45 INFO DAGScheduler: ResultStage 83 (start at NativeMethodAccessorImpl.java:0) finished in 0.595 s
26/01/04 17:04:45 INFO DAGScheduler: Job 82 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:04:45 INFO TaskSchedulerImpl: Killing all running tasks in stage 83: Stage finished
26/01/04 17:04:45 INFO DAGScheduler: Job 82 finished: start at NativeMethodAccessorImpl.java:0, took 0.596341 s
26/01/04 17:04:45 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 39, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@1b6fe97a] is committing.
26/01/04 17:04:45 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 39, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@1b6fe97a] committed.
26/01/04 17:04:45 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/39 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.39.3623fb71-1283-48d3-9860-91bce91cb8fc.tmp
26/01/04 17:04:45 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.39.3623fb71-1283-48d3-9860-91bce91cb8fc.tmp to file:/tmp/spark-checkpoint-enrichment/commits/39
26/01/04 17:04:45 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T17:04:44.865Z",
  "batchId" : 39,
  "numInputRows" : 30,
  "inputRowsPerSecond" : 2727.2727272727275,
  "processedRowsPerSecond" : 33.003300330033,
  "durationMs" : {
    "addBatch" : 714,
    "commitOffsets" : 72,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 37,
    "triggerExecution" : 909,
    "walCommit" : 84
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "0" : 459
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "0" : 489
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "0" : 489
      }
    },
    "numInputRows" : 30,
    "inputRowsPerSecond" : 2727.2727272727275,
    "processedRowsPerSecond" : 33.003300330033,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 31
  }
}
26/01/04 17:04:55 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/01/04 17:04:55 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/40 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.40.54b5d9a1-2486-44ba-855a-8d54ff75bf09.tmp
26/01/04 17:04:55 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.40.54b5d9a1-2486-44ba-855a-8d54ff75bf09.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/40
26/01/04 17:04:55 INFO MicroBatchExecution: Committed offsets for batch 40. Metadata OffsetSeqMetadata(0,1767546295870,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 17:04:55 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:04:55 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:04:56 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:04:56 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:04:56 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:04:56 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:04:56 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#33643 - airline_prefix.nullCount#33642) > 0)
26/01/04 17:04:56 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#33678 - min_flight_num.nullCount#33677) > 0)
26/01/04 17:04:56 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#33673 - max_flight_num.nullCount#33672) > 0)
26/01/04 17:04:56 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:04:56 INFO DAGScheduler: Got job 83 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 17:04:56 INFO DAGScheduler: Final stage: ResultStage 84 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:04:56 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:04:56 INFO DAGScheduler: Missing parents: List()
26/01/04 17:04:56 INFO DAGScheduler: Submitting ResultStage 84 (MapPartitionsRDD[422] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:04:56 INFO MemoryStore: Block broadcast_123 stored as values in memory (estimated size 38.5 KiB, free 434.2 MiB)
26/01/04 17:04:56 INFO MemoryStore: Block broadcast_123_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.2 MiB)
26/01/04 17:04:56 INFO BlockManagerInfo: Added broadcast_123_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:04:56 INFO SparkContext: Created broadcast 123 from broadcast at DAGScheduler.scala:1585
26/01/04 17:04:56 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 84 (MapPartitionsRDD[422] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 17:04:56 INFO TaskSchedulerImpl: Adding task set 84.0 with 2 tasks resource profile 0
26/01/04 17:04:56 INFO TaskSetManager: Starting task 0.0 in stage 84.0 (TID 161) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 17:04:56 INFO TaskSetManager: Starting task 1.0 in stage 84.0 (TID 162) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 17:04:56 INFO BlockManagerInfo: Added broadcast_123_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:04:56 INFO BlockManagerInfo: Added broadcast_123_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:04:56 INFO TaskSetManager: Finished task 1.0 in stage 84.0 (TID 162) in 24 ms on 172.18.0.13 (executor 0) (1/2)
26/01/04 17:04:56 INFO TaskSetManager: Finished task 0.0 in stage 84.0 (TID 161) in 28 ms on 172.18.0.14 (executor 1) (2/2)
26/01/04 17:04:56 INFO TaskSchedulerImpl: Removed TaskSet 84.0, whose tasks have all completed, from pool 
26/01/04 17:04:56 INFO DAGScheduler: ResultStage 84 (start at NativeMethodAccessorImpl.java:0) finished in 0.036 s
26/01/04 17:04:56 INFO DAGScheduler: Job 83 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:04:56 INFO TaskSchedulerImpl: Killing all running tasks in stage 84: Stage finished
26/01/04 17:04:56 INFO DAGScheduler: Job 83 finished: start at NativeMethodAccessorImpl.java:0, took 0.038681 s
26/01/04 17:04:56 INFO MemoryStore: Block broadcast_124_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.2 MiB)
26/01/04 17:04:56 INFO BlockManagerInfo: Added broadcast_124_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:04:56 INFO SparkContext: Created broadcast 124 from start at NativeMethodAccessorImpl.java:0
26/01/04 17:04:56 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 40, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@7d0d449a]. The input RDD has 1 partitions.
26/01/04 17:04:56 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:04:56 INFO DAGScheduler: Got job 84 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/01/04 17:04:56 INFO DAGScheduler: Final stage: ResultStage 85 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:04:56 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:04:56 INFO DAGScheduler: Missing parents: List()
26/01/04 17:04:56 INFO DAGScheduler: Submitting ResultStage 85 (MapPartitionsRDD[427] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:04:56 INFO MemoryStore: Block broadcast_125 stored as values in memory (estimated size 52.9 KiB, free 434.2 MiB)
26/01/04 17:04:56 INFO MemoryStore: Block broadcast_125_piece0 stored as bytes in memory (estimated size 20.2 KiB, free 434.1 MiB)
26/01/04 17:04:56 INFO BlockManagerInfo: Added broadcast_125_piece0 in memory on spark-master:33535 (size: 20.2 KiB, free: 434.3 MiB)
26/01/04 17:04:56 INFO SparkContext: Created broadcast 125 from broadcast at DAGScheduler.scala:1585
26/01/04 17:04:56 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 85 (MapPartitionsRDD[427] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/01/04 17:04:56 INFO TaskSchedulerImpl: Adding task set 85.0 with 1 tasks resource profile 0
26/01/04 17:04:56 INFO TaskSetManager: Starting task 0.0 in stage 85.0 (TID 163) (172.18.0.13, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:04:56 INFO BlockManagerInfo: Added broadcast_125_piece0 in memory on 172.18.0.13:44401 (size: 20.2 KiB, free: 434.3 MiB)
26/01/04 17:04:56 INFO BlockManagerInfo: Added broadcast_124_piece0 in memory on 172.18.0.13:44401 (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:04:56 INFO TaskSetManager: Finished task 0.0 in stage 85.0 (TID 163) in 584 ms on 172.18.0.13 (executor 0) (1/1)
26/01/04 17:04:56 INFO TaskSchedulerImpl: Removed TaskSet 85.0, whose tasks have all completed, from pool 
26/01/04 17:04:56 INFO DAGScheduler: ResultStage 85 (start at NativeMethodAccessorImpl.java:0) finished in 0.591 s
26/01/04 17:04:56 INFO DAGScheduler: Job 84 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:04:56 INFO TaskSchedulerImpl: Killing all running tasks in stage 85: Stage finished
26/01/04 17:04:56 INFO DAGScheduler: Job 84 finished: start at NativeMethodAccessorImpl.java:0, took 0.593020 s
26/01/04 17:04:56 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 40, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@7d0d449a] is committing.
26/01/04 17:04:56 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 40, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@7d0d449a] committed.
26/01/04 17:04:56 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/40 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.40.ea1c659b-eeca-4544-9e1e-310e32809036.tmp
26/01/04 17:04:56 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.40.ea1c659b-eeca-4544-9e1e-310e32809036.tmp to file:/tmp/spark-checkpoint-enrichment/commits/40
26/01/04 17:04:56 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T17:04:55.868Z",
  "batchId" : 40,
  "numInputRows" : 30,
  "inputRowsPerSecond" : 2727.2727272727275,
  "processedRowsPerSecond" : 33.333333333333336,
  "durationMs" : {
    "addBatch" : 710,
    "commitOffsets" : 70,
    "getBatch" : 0,
    "latestOffset" : 2,
    "queryPlanning" : 38,
    "triggerExecution" : 900,
    "walCommit" : 79
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "0" : 489
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "0" : 519
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "0" : 519
      }
    },
    "numInputRows" : 30,
    "inputRowsPerSecond" : 2727.2727272727275,
    "processedRowsPerSecond" : 33.333333333333336,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 31
  }
}
26/01/04 17:04:58 INFO BlockManagerInfo: Removed broadcast_121_piece0 on spark-master:33535 in memory (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:04:58 INFO BlockManagerInfo: Removed broadcast_121_piece0 on 172.18.0.13:44401 in memory (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:04:58 INFO BlockManagerInfo: Removed broadcast_125_piece0 on spark-master:33535 in memory (size: 20.2 KiB, free: 434.3 MiB)
26/01/04 17:04:58 INFO BlockManagerInfo: Removed broadcast_125_piece0 on 172.18.0.13:44401 in memory (size: 20.2 KiB, free: 434.3 MiB)
26/01/04 17:04:58 INFO BlockManagerInfo: Removed broadcast_122_piece0 on spark-master:33535 in memory (size: 20.1 KiB, free: 434.4 MiB)
26/01/04 17:04:58 INFO BlockManagerInfo: Removed broadcast_122_piece0 on 172.18.0.13:44401 in memory (size: 20.1 KiB, free: 434.4 MiB)
26/01/04 17:04:58 INFO BlockManagerInfo: Removed broadcast_123_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:04:58 INFO BlockManagerInfo: Removed broadcast_123_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:04:58 INFO BlockManagerInfo: Removed broadcast_123_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:04:58 INFO BlockManagerInfo: Removed broadcast_120_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:04:58 INFO BlockManagerInfo: Removed broadcast_120_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:04:58 INFO BlockManagerInfo: Removed broadcast_120_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:05:06 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/01/04 17:05:06 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/41 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.41.e3ef124e-259f-4b38-9f93-05dcfeb1b2cf.tmp
26/01/04 17:05:06 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.41.e3ef124e-259f-4b38-9f93-05dcfeb1b2cf.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/41
26/01/04 17:05:06 INFO MicroBatchExecution: Committed offsets for batch 41. Metadata OffsetSeqMetadata(0,1767546306890,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 17:05:07 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:05:07 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:05:07 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:05:07 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:05:07 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:05:07 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:05:07 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#34447 - airline_prefix.nullCount#34446) > 0)
26/01/04 17:05:07 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#34482 - min_flight_num.nullCount#34481) > 0)
26/01/04 17:05:07 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#34477 - max_flight_num.nullCount#34476) > 0)
26/01/04 17:05:07 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:05:07 INFO DAGScheduler: Got job 85 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 17:05:07 INFO DAGScheduler: Final stage: ResultStage 86 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:05:07 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:05:07 INFO DAGScheduler: Missing parents: List()
26/01/04 17:05:07 INFO DAGScheduler: Submitting ResultStage 86 (MapPartitionsRDD[432] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:05:07 INFO MemoryStore: Block broadcast_126 stored as values in memory (estimated size 38.5 KiB, free 434.4 MiB)
26/01/04 17:05:07 INFO MemoryStore: Block broadcast_126_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.3 MiB)
26/01/04 17:05:07 INFO BlockManagerInfo: Added broadcast_126_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:05:07 INFO SparkContext: Created broadcast 126 from broadcast at DAGScheduler.scala:1585
26/01/04 17:05:07 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 86 (MapPartitionsRDD[432] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 17:05:07 INFO TaskSchedulerImpl: Adding task set 86.0 with 2 tasks resource profile 0
26/01/04 17:05:07 INFO TaskSetManager: Starting task 0.0 in stage 86.0 (TID 164) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 17:05:07 INFO TaskSetManager: Starting task 1.0 in stage 86.0 (TID 165) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 17:05:07 INFO BlockManagerInfo: Added broadcast_126_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:05:07 INFO BlockManagerInfo: Added broadcast_126_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:05:07 INFO TaskSetManager: Finished task 1.0 in stage 86.0 (TID 165) in 20 ms on 172.18.0.13 (executor 0) (1/2)
26/01/04 17:05:07 INFO TaskSetManager: Finished task 0.0 in stage 86.0 (TID 164) in 21 ms on 172.18.0.14 (executor 1) (2/2)
26/01/04 17:05:07 INFO TaskSchedulerImpl: Removed TaskSet 86.0, whose tasks have all completed, from pool 
26/01/04 17:05:07 INFO DAGScheduler: ResultStage 86 (start at NativeMethodAccessorImpl.java:0) finished in 0.028 s
26/01/04 17:05:07 INFO DAGScheduler: Job 85 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:05:07 INFO TaskSchedulerImpl: Killing all running tasks in stage 86: Stage finished
26/01/04 17:05:07 INFO DAGScheduler: Job 85 finished: start at NativeMethodAccessorImpl.java:0, took 0.030789 s
26/01/04 17:05:07 INFO MemoryStore: Block broadcast_127_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.3 MiB)
26/01/04 17:05:07 INFO BlockManagerInfo: Added broadcast_127_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:05:07 INFO SparkContext: Created broadcast 127 from start at NativeMethodAccessorImpl.java:0
26/01/04 17:05:07 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 41, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@3c8497d7]. The input RDD has 1 partitions.
26/01/04 17:05:07 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:05:07 INFO DAGScheduler: Got job 86 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/01/04 17:05:07 INFO DAGScheduler: Final stage: ResultStage 87 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:05:07 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:05:07 INFO DAGScheduler: Missing parents: List()
26/01/04 17:05:07 INFO DAGScheduler: Submitting ResultStage 87 (MapPartitionsRDD[437] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:05:07 INFO MemoryStore: Block broadcast_128 stored as values in memory (estimated size 52.9 KiB, free 434.3 MiB)
26/01/04 17:05:07 INFO MemoryStore: Block broadcast_128_piece0 stored as bytes in memory (estimated size 20.2 KiB, free 434.3 MiB)
26/01/04 17:05:07 INFO BlockManagerInfo: Added broadcast_128_piece0 in memory on spark-master:33535 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:05:07 INFO SparkContext: Created broadcast 128 from broadcast at DAGScheduler.scala:1585
26/01/04 17:05:07 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 87 (MapPartitionsRDD[437] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/01/04 17:05:07 INFO TaskSchedulerImpl: Adding task set 87.0 with 1 tasks resource profile 0
26/01/04 17:05:07 INFO TaskSetManager: Starting task 0.0 in stage 87.0 (TID 166) (172.18.0.13, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:05:07 INFO BlockManagerInfo: Added broadcast_128_piece0 in memory on 172.18.0.13:44401 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:05:07 INFO BlockManagerInfo: Added broadcast_127_piece0 in memory on 172.18.0.13:44401 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:05:07 INFO TaskSetManager: Finished task 0.0 in stage 87.0 (TID 166) in 573 ms on 172.18.0.13 (executor 0) (1/1)
26/01/04 17:05:07 INFO TaskSchedulerImpl: Removed TaskSet 87.0, whose tasks have all completed, from pool 
26/01/04 17:05:07 INFO DAGScheduler: ResultStage 87 (start at NativeMethodAccessorImpl.java:0) finished in 0.578 s
26/01/04 17:05:07 INFO DAGScheduler: Job 86 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:05:07 INFO TaskSchedulerImpl: Killing all running tasks in stage 87: Stage finished
26/01/04 17:05:07 INFO DAGScheduler: Job 86 finished: start at NativeMethodAccessorImpl.java:0, took 0.580228 s
26/01/04 17:05:07 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 41, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@3c8497d7] is committing.
26/01/04 17:05:07 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 41, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@3c8497d7] committed.
26/01/04 17:05:07 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/41 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.41.4e4d1da3-ac5a-4529-8a8f-57c640c4ed68.tmp
26/01/04 17:05:07 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.41.4e4d1da3-ac5a-4529-8a8f-57c640c4ed68.tmp to file:/tmp/spark-checkpoint-enrichment/commits/41
26/01/04 17:05:07 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T17:05:06.889Z",
  "batchId" : 41,
  "numInputRows" : 30,
  "inputRowsPerSecond" : 2500.0,
  "processedRowsPerSecond" : 35.16998827667057,
  "durationMs" : {
    "addBatch" : 678,
    "commitOffsets" : 54,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 36,
    "triggerExecution" : 853,
    "walCommit" : 83
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "0" : 519
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "0" : 549
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "0" : 549
      }
    },
    "numInputRows" : 30,
    "inputRowsPerSecond" : 2500.0,
    "processedRowsPerSecond" : 35.16998827667057,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 31
  }
}
26/01/04 17:05:17 INFO BlockManagerInfo: Removed broadcast_128_piece0 on spark-master:33535 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:05:17 INFO BlockManagerInfo: Removed broadcast_128_piece0 on 172.18.0.13:44401 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:05:17 INFO BlockManagerInfo: Removed broadcast_124_piece0 on spark-master:33535 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:05:17 INFO BlockManagerInfo: Removed broadcast_124_piece0 on 172.18.0.13:44401 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:05:17 INFO BlockManagerInfo: Removed broadcast_126_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:05:17 INFO BlockManagerInfo: Removed broadcast_126_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:05:17 INFO BlockManagerInfo: Removed broadcast_126_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:05:17 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/01/04 17:05:17 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/42 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.42.6362c881-f287-446e-8f13-111489fad026.tmp
26/01/04 17:05:18 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.42.6362c881-f287-446e-8f13-111489fad026.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/42
26/01/04 17:05:18 INFO MicroBatchExecution: Committed offsets for batch 42. Metadata OffsetSeqMetadata(0,1767546317915,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 17:05:18 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:05:18 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:05:18 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:05:18 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:05:18 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:05:18 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:05:18 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#35251 - airline_prefix.nullCount#35250) > 0)
26/01/04 17:05:18 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#35286 - min_flight_num.nullCount#35285) > 0)
26/01/04 17:05:18 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#35281 - max_flight_num.nullCount#35280) > 0)
26/01/04 17:05:18 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:05:18 INFO DAGScheduler: Got job 87 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 17:05:18 INFO DAGScheduler: Final stage: ResultStage 88 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:05:18 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:05:18 INFO DAGScheduler: Missing parents: List()
26/01/04 17:05:18 INFO DAGScheduler: Submitting ResultStage 88 (MapPartitionsRDD[442] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:05:18 INFO MemoryStore: Block broadcast_129 stored as values in memory (estimated size 38.5 KiB, free 434.4 MiB)
26/01/04 17:05:18 INFO MemoryStore: Block broadcast_129_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.3 MiB)
26/01/04 17:05:18 INFO BlockManagerInfo: Removed broadcast_127_piece0 on spark-master:33535 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:05:18 INFO BlockManagerInfo: Added broadcast_129_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:05:18 INFO SparkContext: Created broadcast 129 from broadcast at DAGScheduler.scala:1585
26/01/04 17:05:18 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 88 (MapPartitionsRDD[442] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 17:05:18 INFO TaskSchedulerImpl: Adding task set 88.0 with 2 tasks resource profile 0
26/01/04 17:05:18 INFO BlockManagerInfo: Removed broadcast_127_piece0 on 172.18.0.13:44401 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:05:18 INFO TaskSetManager: Starting task 0.0 in stage 88.0 (TID 167) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 17:05:18 INFO TaskSetManager: Starting task 1.0 in stage 88.0 (TID 168) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 17:05:18 INFO BlockManagerInfo: Added broadcast_129_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:05:18 INFO BlockManagerInfo: Added broadcast_129_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:05:18 INFO TaskSetManager: Finished task 1.0 in stage 88.0 (TID 168) in 23 ms on 172.18.0.13 (executor 0) (1/2)
26/01/04 17:05:18 INFO TaskSetManager: Finished task 0.0 in stage 88.0 (TID 167) in 32 ms on 172.18.0.14 (executor 1) (2/2)
26/01/04 17:05:18 INFO TaskSchedulerImpl: Removed TaskSet 88.0, whose tasks have all completed, from pool 
26/01/04 17:05:18 INFO DAGScheduler: ResultStage 88 (start at NativeMethodAccessorImpl.java:0) finished in 0.045 s
26/01/04 17:05:18 INFO DAGScheduler: Job 87 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:05:18 INFO TaskSchedulerImpl: Killing all running tasks in stage 88: Stage finished
26/01/04 17:05:18 INFO DAGScheduler: Job 87 finished: start at NativeMethodAccessorImpl.java:0, took 0.049425 s
26/01/04 17:05:18 INFO MemoryStore: Block broadcast_130_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.3 MiB)
26/01/04 17:05:18 INFO BlockManagerInfo: Added broadcast_130_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:05:18 INFO SparkContext: Created broadcast 130 from start at NativeMethodAccessorImpl.java:0
26/01/04 17:05:18 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 42, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@3b2f468c]. The input RDD has 1 partitions.
26/01/04 17:05:18 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:05:18 INFO DAGScheduler: Got job 88 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/01/04 17:05:18 INFO DAGScheduler: Final stage: ResultStage 89 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:05:18 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:05:18 INFO DAGScheduler: Missing parents: List()
26/01/04 17:05:18 INFO DAGScheduler: Submitting ResultStage 89 (MapPartitionsRDD[447] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:05:18 INFO MemoryStore: Block broadcast_131 stored as values in memory (estimated size 52.9 KiB, free 434.3 MiB)
26/01/04 17:05:18 INFO MemoryStore: Block broadcast_131_piece0 stored as bytes in memory (estimated size 20.2 KiB, free 434.3 MiB)
26/01/04 17:05:18 INFO BlockManagerInfo: Added broadcast_131_piece0 in memory on spark-master:33535 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:05:18 INFO SparkContext: Created broadcast 131 from broadcast at DAGScheduler.scala:1585
26/01/04 17:05:18 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 89 (MapPartitionsRDD[447] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/01/04 17:05:18 INFO TaskSchedulerImpl: Adding task set 89.0 with 1 tasks resource profile 0
26/01/04 17:05:18 INFO TaskSetManager: Starting task 0.0 in stage 89.0 (TID 169) (172.18.0.13, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:05:18 INFO BlockManagerInfo: Added broadcast_131_piece0 in memory on 172.18.0.13:44401 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:05:18 INFO BlockManagerInfo: Added broadcast_130_piece0 in memory on 172.18.0.13:44401 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:05:18 INFO TaskSetManager: Finished task 0.0 in stage 89.0 (TID 169) in 573 ms on 172.18.0.13 (executor 0) (1/1)
26/01/04 17:05:18 INFO TaskSchedulerImpl: Removed TaskSet 89.0, whose tasks have all completed, from pool 
26/01/04 17:05:18 INFO DAGScheduler: ResultStage 89 (start at NativeMethodAccessorImpl.java:0) finished in 0.578 s
26/01/04 17:05:18 INFO DAGScheduler: Job 88 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:05:18 INFO TaskSchedulerImpl: Killing all running tasks in stage 89: Stage finished
26/01/04 17:05:18 INFO DAGScheduler: Job 88 finished: start at NativeMethodAccessorImpl.java:0, took 0.580380 s
26/01/04 17:05:18 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 42, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@3b2f468c] is committing.
26/01/04 17:05:18 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 42, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@3b2f468c] committed.
26/01/04 17:05:18 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/42 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.42.1228d6cd-ad71-4c5e-8323-1b4987f81a63.tmp
26/01/04 17:05:18 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.42.1228d6cd-ad71-4c5e-8323-1b4987f81a63.tmp to file:/tmp/spark-checkpoint-enrichment/commits/42
26/01/04 17:05:18 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T17:05:17.913Z",
  "batchId" : 42,
  "numInputRows" : 30,
  "inputRowsPerSecond" : 2500.0,
  "processedRowsPerSecond" : 33.482142857142854,
  "durationMs" : {
    "addBatch" : 702,
    "commitOffsets" : 65,
    "getBatch" : 0,
    "latestOffset" : 2,
    "queryPlanning" : 19,
    "triggerExecution" : 896,
    "walCommit" : 106
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "0" : 549
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "0" : 579
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "0" : 579
      }
    },
    "numInputRows" : 30,
    "inputRowsPerSecond" : 2500.0,
    "processedRowsPerSecond" : 33.482142857142854,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 31
  }
}
26/01/04 17:05:28 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/01/04 17:05:28 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/43 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.43.a2c27b72-0b7b-4d34-b8e0-d4b7a6a8c365.tmp
26/01/04 17:05:29 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.43.a2c27b72-0b7b-4d34-b8e0-d4b7a6a8c365.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/43
26/01/04 17:05:29 INFO MicroBatchExecution: Committed offsets for batch 43. Metadata OffsetSeqMetadata(0,1767546328915,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 17:05:29 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:05:29 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:05:29 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:05:29 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:05:29 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:05:29 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:05:29 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#36055 - airline_prefix.nullCount#36054) > 0)
26/01/04 17:05:29 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#36090 - min_flight_num.nullCount#36089) > 0)
26/01/04 17:05:29 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#36085 - max_flight_num.nullCount#36084) > 0)
26/01/04 17:05:29 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:05:29 INFO DAGScheduler: Got job 89 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 17:05:29 INFO DAGScheduler: Final stage: ResultStage 90 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:05:29 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:05:29 INFO DAGScheduler: Missing parents: List()
26/01/04 17:05:29 INFO DAGScheduler: Submitting ResultStage 90 (MapPartitionsRDD[452] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:05:29 INFO MemoryStore: Block broadcast_132 stored as values in memory (estimated size 38.5 KiB, free 434.2 MiB)
26/01/04 17:05:29 INFO MemoryStore: Block broadcast_132_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.2 MiB)
26/01/04 17:05:29 INFO BlockManagerInfo: Added broadcast_132_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:05:29 INFO SparkContext: Created broadcast 132 from broadcast at DAGScheduler.scala:1585
26/01/04 17:05:29 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 90 (MapPartitionsRDD[452] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 17:05:29 INFO TaskSchedulerImpl: Adding task set 90.0 with 2 tasks resource profile 0
26/01/04 17:05:29 INFO TaskSetManager: Starting task 0.0 in stage 90.0 (TID 170) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 17:05:29 INFO TaskSetManager: Starting task 1.0 in stage 90.0 (TID 171) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 17:05:29 INFO BlockManagerInfo: Added broadcast_132_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:05:29 INFO BlockManagerInfo: Added broadcast_132_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:05:29 INFO TaskSetManager: Finished task 1.0 in stage 90.0 (TID 171) in 20 ms on 172.18.0.13 (executor 0) (1/2)
26/01/04 17:05:29 INFO TaskSetManager: Finished task 0.0 in stage 90.0 (TID 170) in 25 ms on 172.18.0.14 (executor 1) (2/2)
26/01/04 17:05:29 INFO TaskSchedulerImpl: Removed TaskSet 90.0, whose tasks have all completed, from pool 
26/01/04 17:05:29 INFO DAGScheduler: ResultStage 90 (start at NativeMethodAccessorImpl.java:0) finished in 0.030 s
26/01/04 17:05:29 INFO DAGScheduler: Job 89 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:05:29 INFO TaskSchedulerImpl: Killing all running tasks in stage 90: Stage finished
26/01/04 17:05:29 INFO DAGScheduler: Job 89 finished: start at NativeMethodAccessorImpl.java:0, took 0.033111 s
26/01/04 17:05:29 INFO MemoryStore: Block broadcast_133_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.2 MiB)
26/01/04 17:05:29 INFO BlockManagerInfo: Added broadcast_133_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:05:29 INFO SparkContext: Created broadcast 133 from start at NativeMethodAccessorImpl.java:0
26/01/04 17:05:29 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 43, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@65fc282d]. The input RDD has 1 partitions.
26/01/04 17:05:29 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:05:29 INFO DAGScheduler: Got job 90 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/01/04 17:05:29 INFO DAGScheduler: Final stage: ResultStage 91 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:05:29 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:05:29 INFO DAGScheduler: Missing parents: List()
26/01/04 17:05:29 INFO DAGScheduler: Submitting ResultStage 91 (MapPartitionsRDD[457] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:05:29 INFO MemoryStore: Block broadcast_134 stored as values in memory (estimated size 52.9 KiB, free 434.2 MiB)
26/01/04 17:05:29 INFO MemoryStore: Block broadcast_134_piece0 stored as bytes in memory (estimated size 20.2 KiB, free 434.1 MiB)
26/01/04 17:05:29 INFO BlockManagerInfo: Added broadcast_134_piece0 in memory on spark-master:33535 (size: 20.2 KiB, free: 434.3 MiB)
26/01/04 17:05:29 INFO SparkContext: Created broadcast 134 from broadcast at DAGScheduler.scala:1585
26/01/04 17:05:29 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 91 (MapPartitionsRDD[457] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/01/04 17:05:29 INFO TaskSchedulerImpl: Adding task set 91.0 with 1 tasks resource profile 0
26/01/04 17:05:29 INFO TaskSetManager: Starting task 0.0 in stage 91.0 (TID 172) (172.18.0.13, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:05:29 INFO BlockManagerInfo: Added broadcast_134_piece0 in memory on 172.18.0.13:44401 (size: 20.2 KiB, free: 434.3 MiB)
26/01/04 17:05:29 INFO BlockManagerInfo: Added broadcast_133_piece0 in memory on 172.18.0.13:44401 (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:05:29 INFO TaskSetManager: Finished task 0.0 in stage 91.0 (TID 172) in 583 ms on 172.18.0.13 (executor 0) (1/1)
26/01/04 17:05:29 INFO TaskSchedulerImpl: Removed TaskSet 91.0, whose tasks have all completed, from pool 
26/01/04 17:05:29 INFO DAGScheduler: ResultStage 91 (start at NativeMethodAccessorImpl.java:0) finished in 0.590 s
26/01/04 17:05:29 INFO DAGScheduler: Job 90 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:05:29 INFO TaskSchedulerImpl: Killing all running tasks in stage 91: Stage finished
26/01/04 17:05:29 INFO DAGScheduler: Job 90 finished: start at NativeMethodAccessorImpl.java:0, took 0.592263 s
26/01/04 17:05:29 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 43, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@65fc282d] is committing.
26/01/04 17:05:29 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 43, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@65fc282d] committed.
26/01/04 17:05:29 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/43 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.43.5e25a6cf-c6c9-44c1-ab5b-b057207ef851.tmp
26/01/04 17:05:29 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.43.5e25a6cf-c6c9-44c1-ab5b-b057207ef851.tmp to file:/tmp/spark-checkpoint-enrichment/commits/43
26/01/04 17:05:29 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T17:05:28.914Z",
  "batchId" : 43,
  "numInputRows" : 30,
  "inputRowsPerSecond" : 2727.2727272727275,
  "processedRowsPerSecond" : 33.333333333333336,
  "durationMs" : {
    "addBatch" : 694,
    "commitOffsets" : 69,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 41,
    "triggerExecution" : 900,
    "walCommit" : 94
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "0" : 579
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "0" : 609
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "0" : 609
      }
    },
    "numInputRows" : 30,
    "inputRowsPerSecond" : 2727.2727272727275,
    "processedRowsPerSecond" : 33.333333333333336,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 31
  }
}
26/01/04 17:05:31 INFO BlockManagerInfo: Removed broadcast_130_piece0 on spark-master:33535 in memory (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:05:31 INFO BlockManagerInfo: Removed broadcast_130_piece0 on 172.18.0.13:44401 in memory (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:05:31 INFO BlockManagerInfo: Removed broadcast_132_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:05:31 INFO BlockManagerInfo: Removed broadcast_132_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:05:31 INFO BlockManagerInfo: Removed broadcast_132_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:05:31 INFO BlockManagerInfo: Removed broadcast_134_piece0 on spark-master:33535 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:05:31 INFO BlockManagerInfo: Removed broadcast_134_piece0 on 172.18.0.13:44401 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:05:31 INFO BlockManagerInfo: Removed broadcast_129_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:05:31 INFO BlockManagerInfo: Removed broadcast_129_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:05:31 INFO BlockManagerInfo: Removed broadcast_129_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:05:31 INFO BlockManagerInfo: Removed broadcast_131_piece0 on spark-master:33535 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:05:31 INFO BlockManagerInfo: Removed broadcast_131_piece0 on 172.18.0.13:44401 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:05:39 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/01/04 17:05:39 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/44 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.44.cb1e7700-211d-4d71-bba1-d8414fd681ff.tmp
26/01/04 17:05:40 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.44.cb1e7700-211d-4d71-bba1-d8414fd681ff.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/44
26/01/04 17:05:40 INFO MicroBatchExecution: Committed offsets for batch 44. Metadata OffsetSeqMetadata(0,1767546339924,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 17:05:40 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:05:40 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:05:40 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:05:40 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:05:40 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:05:40 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:05:40 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#36859 - airline_prefix.nullCount#36858) > 0)
26/01/04 17:05:40 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#36894 - min_flight_num.nullCount#36893) > 0)
26/01/04 17:05:40 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#36889 - max_flight_num.nullCount#36888) > 0)
26/01/04 17:05:40 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:05:40 INFO DAGScheduler: Got job 91 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 17:05:40 INFO DAGScheduler: Final stage: ResultStage 92 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:05:40 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:05:40 INFO DAGScheduler: Missing parents: List()
26/01/04 17:05:40 INFO DAGScheduler: Submitting ResultStage 92 (MapPartitionsRDD[462] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:05:40 INFO MemoryStore: Block broadcast_135 stored as values in memory (estimated size 38.5 KiB, free 434.4 MiB)
26/01/04 17:05:40 INFO MemoryStore: Block broadcast_135_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.3 MiB)
26/01/04 17:05:40 INFO BlockManagerInfo: Added broadcast_135_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:05:40 INFO SparkContext: Created broadcast 135 from broadcast at DAGScheduler.scala:1585
26/01/04 17:05:40 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 92 (MapPartitionsRDD[462] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 17:05:40 INFO TaskSchedulerImpl: Adding task set 92.0 with 2 tasks resource profile 0
26/01/04 17:05:40 INFO TaskSetManager: Starting task 1.0 in stage 92.0 (TID 173) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 17:05:40 INFO TaskSetManager: Starting task 0.0 in stage 92.0 (TID 174) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 17:05:40 INFO BlockManagerInfo: Added broadcast_135_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:05:40 INFO BlockManagerInfo: Added broadcast_135_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:05:40 INFO TaskSetManager: Finished task 1.0 in stage 92.0 (TID 173) in 69 ms on 172.18.0.13 (executor 0) (1/2)
26/01/04 17:05:40 INFO TaskSetManager: Finished task 0.0 in stage 92.0 (TID 174) in 78 ms on 172.18.0.14 (executor 1) (2/2)
26/01/04 17:05:40 INFO TaskSchedulerImpl: Removed TaskSet 92.0, whose tasks have all completed, from pool 
26/01/04 17:05:40 INFO DAGScheduler: ResultStage 92 (start at NativeMethodAccessorImpl.java:0) finished in 0.103 s
26/01/04 17:05:40 INFO DAGScheduler: Job 91 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:05:40 INFO TaskSchedulerImpl: Killing all running tasks in stage 92: Stage finished
26/01/04 17:05:40 INFO DAGScheduler: Job 91 finished: start at NativeMethodAccessorImpl.java:0, took 0.108694 s
26/01/04 17:05:40 INFO MemoryStore: Block broadcast_136_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.3 MiB)
26/01/04 17:05:40 INFO BlockManagerInfo: Added broadcast_136_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:05:40 INFO SparkContext: Created broadcast 136 from start at NativeMethodAccessorImpl.java:0
26/01/04 17:05:40 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 44, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@3c06923]. The input RDD has 1 partitions.
26/01/04 17:05:40 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:05:40 INFO DAGScheduler: Got job 92 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/01/04 17:05:40 INFO DAGScheduler: Final stage: ResultStage 93 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:05:40 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:05:40 INFO DAGScheduler: Missing parents: List()
26/01/04 17:05:40 INFO DAGScheduler: Submitting ResultStage 93 (MapPartitionsRDD[467] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:05:40 INFO MemoryStore: Block broadcast_137 stored as values in memory (estimated size 52.9 KiB, free 434.3 MiB)
26/01/04 17:05:40 INFO MemoryStore: Block broadcast_137_piece0 stored as bytes in memory (estimated size 20.2 KiB, free 434.3 MiB)
26/01/04 17:05:40 INFO BlockManagerInfo: Added broadcast_137_piece0 in memory on spark-master:33535 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:05:40 INFO SparkContext: Created broadcast 137 from broadcast at DAGScheduler.scala:1585
26/01/04 17:05:40 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 93 (MapPartitionsRDD[467] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/01/04 17:05:40 INFO TaskSchedulerImpl: Adding task set 93.0 with 1 tasks resource profile 0
26/01/04 17:05:40 INFO TaskSetManager: Starting task 0.0 in stage 93.0 (TID 175) (172.18.0.13, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:05:40 INFO BlockManagerInfo: Added broadcast_137_piece0 in memory on 172.18.0.13:44401 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:05:40 INFO BlockManagerInfo: Added broadcast_136_piece0 in memory on 172.18.0.13:44401 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:05:41 INFO TaskSetManager: Finished task 0.0 in stage 93.0 (TID 175) in 701 ms on 172.18.0.13 (executor 0) (1/1)
26/01/04 17:05:41 INFO TaskSchedulerImpl: Removed TaskSet 93.0, whose tasks have all completed, from pool 
26/01/04 17:05:41 INFO DAGScheduler: ResultStage 93 (start at NativeMethodAccessorImpl.java:0) finished in 0.720 s
26/01/04 17:05:41 INFO DAGScheduler: Job 92 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:05:41 INFO TaskSchedulerImpl: Killing all running tasks in stage 93: Stage finished
26/01/04 17:05:41 INFO DAGScheduler: Job 92 finished: start at NativeMethodAccessorImpl.java:0, took 0.727087 s
26/01/04 17:05:41 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 44, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@3c06923] is committing.
26/01/04 17:05:41 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 44, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@3c06923] committed.
26/01/04 17:05:41 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/44 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.44.0ad64ec3-de53-43d4-ba05-4b30ab428966.tmp
26/01/04 17:05:41 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.44.0ad64ec3-de53-43d4-ba05-4b30ab428966.tmp to file:/tmp/spark-checkpoint-enrichment/commits/44
26/01/04 17:05:41 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T17:05:39.921Z",
  "batchId" : 44,
  "numInputRows" : 30,
  "inputRowsPerSecond" : 2307.6923076923076,
  "processedRowsPerSecond" : 22.058823529411764,
  "durationMs" : {
    "addBatch" : 1039,
    "commitOffsets" : 140,
    "getBatch" : 0,
    "latestOffset" : 3,
    "queryPlanning" : 67,
    "triggerExecution" : 1360,
    "walCommit" : 109
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "0" : 609
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "0" : 639
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "0" : 639
      }
    },
    "numInputRows" : 30,
    "inputRowsPerSecond" : 2307.6923076923076,
    "processedRowsPerSecond" : 22.058823529411764,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 31
  }
}
26/01/04 17:05:50 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/45 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.45.87e8152c-eea5-4ed6-afba-813f8840caf1.tmp
26/01/04 17:05:51 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.45.87e8152c-eea5-4ed6-afba-813f8840caf1.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/45
26/01/04 17:05:51 INFO MicroBatchExecution: Committed offsets for batch 45. Metadata OffsetSeqMetadata(0,1767546350938,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 17:05:51 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:05:51 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:05:51 INFO BlockManagerInfo: Removed broadcast_135_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:05:51 INFO BlockManagerInfo: Removed broadcast_135_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:05:51 INFO BlockManagerInfo: Removed broadcast_135_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:05:51 INFO BlockManagerInfo: Removed broadcast_133_piece0 on spark-master:33535 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:05:51 INFO BlockManagerInfo: Removed broadcast_133_piece0 on 172.18.0.13:44401 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:05:51 INFO BlockManagerInfo: Removed broadcast_136_piece0 on spark-master:33535 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:05:51 INFO BlockManagerInfo: Removed broadcast_136_piece0 on 172.18.0.13:44401 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:05:51 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:05:51 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:05:51 INFO BlockManagerInfo: Removed broadcast_137_piece0 on spark-master:33535 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:05:51 INFO BlockManagerInfo: Removed broadcast_137_piece0 on 172.18.0.13:44401 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:05:51 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:05:51 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:05:51 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#37663 - airline_prefix.nullCount#37662) > 0)
26/01/04 17:05:51 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#37698 - min_flight_num.nullCount#37697) > 0)
26/01/04 17:05:51 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#37693 - max_flight_num.nullCount#37692) > 0)
26/01/04 17:05:51 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:05:51 INFO DAGScheduler: Got job 93 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 17:05:51 INFO DAGScheduler: Final stage: ResultStage 94 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:05:51 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:05:51 INFO DAGScheduler: Missing parents: List()
26/01/04 17:05:51 INFO DAGScheduler: Submitting ResultStage 94 (MapPartitionsRDD[472] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:05:51 INFO MemoryStore: Block broadcast_138 stored as values in memory (estimated size 38.5 KiB, free 434.4 MiB)
26/01/04 17:05:51 INFO MemoryStore: Block broadcast_138_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.3 MiB)
26/01/04 17:05:51 INFO BlockManagerInfo: Added broadcast_138_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:05:51 INFO SparkContext: Created broadcast 138 from broadcast at DAGScheduler.scala:1585
26/01/04 17:05:51 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 94 (MapPartitionsRDD[472] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 17:05:51 INFO TaskSchedulerImpl: Adding task set 94.0 with 2 tasks resource profile 0
26/01/04 17:05:51 INFO TaskSetManager: Starting task 1.0 in stage 94.0 (TID 176) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 17:05:51 INFO TaskSetManager: Starting task 0.0 in stage 94.0 (TID 177) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 17:05:51 INFO BlockManagerInfo: Added broadcast_138_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:05:51 INFO BlockManagerInfo: Added broadcast_138_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:05:51 INFO TaskSetManager: Finished task 1.0 in stage 94.0 (TID 176) in 63 ms on 172.18.0.13 (executor 0) (1/2)
26/01/04 17:05:51 INFO TaskSetManager: Finished task 0.0 in stage 94.0 (TID 177) in 97 ms on 172.18.0.14 (executor 1) (2/2)
26/01/04 17:05:51 INFO TaskSchedulerImpl: Removed TaskSet 94.0, whose tasks have all completed, from pool 
26/01/04 17:05:51 INFO DAGScheduler: ResultStage 94 (start at NativeMethodAccessorImpl.java:0) finished in 0.124 s
26/01/04 17:05:51 INFO DAGScheduler: Job 93 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:05:51 INFO TaskSchedulerImpl: Killing all running tasks in stage 94: Stage finished
26/01/04 17:05:51 INFO DAGScheduler: Job 93 finished: start at NativeMethodAccessorImpl.java:0, took 0.133423 s
26/01/04 17:05:51 INFO MemoryStore: Block broadcast_139_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.3 MiB)
26/01/04 17:05:51 INFO BlockManagerInfo: Added broadcast_139_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:05:51 INFO SparkContext: Created broadcast 139 from start at NativeMethodAccessorImpl.java:0
26/01/04 17:05:51 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 45, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@9624dec]. The input RDD has 1 partitions.
26/01/04 17:05:51 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:05:51 INFO DAGScheduler: Got job 94 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/01/04 17:05:51 INFO DAGScheduler: Final stage: ResultStage 95 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:05:51 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:05:51 INFO DAGScheduler: Missing parents: List()
26/01/04 17:05:51 INFO DAGScheduler: Submitting ResultStage 95 (MapPartitionsRDD[477] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:05:51 INFO MemoryStore: Block broadcast_140 stored as values in memory (estimated size 52.9 KiB, free 434.3 MiB)
26/01/04 17:05:51 INFO MemoryStore: Block broadcast_140_piece0 stored as bytes in memory (estimated size 20.2 KiB, free 434.3 MiB)
26/01/04 17:05:51 INFO BlockManagerInfo: Added broadcast_140_piece0 in memory on spark-master:33535 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:05:51 INFO SparkContext: Created broadcast 140 from broadcast at DAGScheduler.scala:1585
26/01/04 17:05:51 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 95 (MapPartitionsRDD[477] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/01/04 17:05:51 INFO TaskSchedulerImpl: Adding task set 95.0 with 1 tasks resource profile 0
26/01/04 17:05:51 INFO TaskSetManager: Starting task 0.0 in stage 95.0 (TID 178) (172.18.0.13, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:05:51 INFO BlockManagerInfo: Added broadcast_140_piece0 in memory on 172.18.0.13:44401 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:05:51 INFO BlockManagerInfo: Added broadcast_139_piece0 in memory on 172.18.0.13:44401 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:05:52 INFO TaskSetManager: Finished task 0.0 in stage 95.0 (TID 178) in 633 ms on 172.18.0.13 (executor 0) (1/1)
26/01/04 17:05:52 INFO TaskSchedulerImpl: Removed TaskSet 95.0, whose tasks have all completed, from pool 
26/01/04 17:05:52 INFO DAGScheduler: ResultStage 95 (start at NativeMethodAccessorImpl.java:0) finished in 0.643 s
26/01/04 17:05:52 INFO DAGScheduler: Job 94 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:05:52 INFO TaskSchedulerImpl: Killing all running tasks in stage 95: Stage finished
26/01/04 17:05:52 INFO DAGScheduler: Job 94 finished: start at NativeMethodAccessorImpl.java:0, took 0.648373 s
26/01/04 17:05:52 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 45, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@9624dec] is committing.
26/01/04 17:05:52 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 45, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@9624dec] committed.
26/01/04 17:05:52 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/45 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.45.84da15db-b2de-4c70-bef2-93580ef820bc.tmp
26/01/04 17:05:52 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.45.84da15db-b2de-4c70-bef2-93580ef820bc.tmp to file:/tmp/spark-checkpoint-enrichment/commits/45
26/01/04 17:05:52 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T17:05:50.936Z",
  "batchId" : 45,
  "numInputRows" : 30,
  "inputRowsPerSecond" : 2142.8571428571427,
  "processedRowsPerSecond" : 22.255192878338278,
  "durationMs" : {
    "addBatch" : 1017,
    "commitOffsets" : 91,
    "getBatch" : 0,
    "latestOffset" : 2,
    "queryPlanning" : 86,
    "triggerExecution" : 1348,
    "walCommit" : 149
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "0" : 639
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "0" : 669
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "0" : 669
      }
    },
    "numInputRows" : 30,
    "inputRowsPerSecond" : 2142.8571428571427,
    "processedRowsPerSecond" : 22.255192878338278,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 31
  }
}
26/01/04 17:06:01 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/46 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.46.735ce4b5-89f0-4c0c-b1cf-10a14c399648.tmp
26/01/04 17:06:02 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.46.735ce4b5-89f0-4c0c-b1cf-10a14c399648.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/46
26/01/04 17:06:02 INFO MicroBatchExecution: Committed offsets for batch 46. Metadata OffsetSeqMetadata(0,1767546361957,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 17:06:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:06:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:06:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:06:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:06:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:06:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:06:02 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#38467 - airline_prefix.nullCount#38466) > 0)
26/01/04 17:06:02 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#38502 - min_flight_num.nullCount#38501) > 0)
26/01/04 17:06:02 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#38497 - max_flight_num.nullCount#38496) > 0)
26/01/04 17:06:02 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:06:02 INFO DAGScheduler: Got job 95 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 17:06:02 INFO DAGScheduler: Final stage: ResultStage 96 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:06:02 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:06:02 INFO DAGScheduler: Missing parents: List()
26/01/04 17:06:02 INFO DAGScheduler: Submitting ResultStage 96 (MapPartitionsRDD[482] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:06:02 INFO MemoryStore: Block broadcast_141 stored as values in memory (estimated size 38.5 KiB, free 434.2 MiB)
26/01/04 17:06:02 INFO MemoryStore: Block broadcast_141_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.2 MiB)
26/01/04 17:06:02 INFO BlockManagerInfo: Added broadcast_141_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:06:02 INFO SparkContext: Created broadcast 141 from broadcast at DAGScheduler.scala:1585
26/01/04 17:06:02 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 96 (MapPartitionsRDD[482] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 17:06:02 INFO TaskSchedulerImpl: Adding task set 96.0 with 2 tasks resource profile 0
26/01/04 17:06:02 INFO TaskSetManager: Starting task 1.0 in stage 96.0 (TID 179) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 17:06:02 INFO TaskSetManager: Starting task 0.0 in stage 96.0 (TID 180) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 17:06:02 INFO BlockManagerInfo: Added broadcast_141_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:06:02 INFO BlockManagerInfo: Added broadcast_141_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:06:02 INFO TaskSetManager: Finished task 1.0 in stage 96.0 (TID 179) in 45 ms on 172.18.0.13 (executor 0) (1/2)
26/01/04 17:06:02 INFO TaskSetManager: Finished task 0.0 in stage 96.0 (TID 180) in 49 ms on 172.18.0.14 (executor 1) (2/2)
26/01/04 17:06:02 INFO TaskSchedulerImpl: Removed TaskSet 96.0, whose tasks have all completed, from pool 
26/01/04 17:06:02 INFO DAGScheduler: ResultStage 96 (start at NativeMethodAccessorImpl.java:0) finished in 0.063 s
26/01/04 17:06:02 INFO DAGScheduler: Job 95 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:06:02 INFO TaskSchedulerImpl: Killing all running tasks in stage 96: Stage finished
26/01/04 17:06:02 INFO DAGScheduler: Job 95 finished: start at NativeMethodAccessorImpl.java:0, took 0.067958 s
26/01/04 17:06:02 INFO MemoryStore: Block broadcast_142_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.2 MiB)
26/01/04 17:06:02 INFO BlockManagerInfo: Added broadcast_142_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:06:02 INFO SparkContext: Created broadcast 142 from start at NativeMethodAccessorImpl.java:0
26/01/04 17:06:02 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 46, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@1f63f48c]. The input RDD has 1 partitions.
26/01/04 17:06:02 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:06:02 INFO DAGScheduler: Got job 96 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/01/04 17:06:02 INFO DAGScheduler: Final stage: ResultStage 97 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:06:02 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:06:02 INFO DAGScheduler: Missing parents: List()
26/01/04 17:06:02 INFO DAGScheduler: Submitting ResultStage 97 (MapPartitionsRDD[487] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:06:02 INFO MemoryStore: Block broadcast_143 stored as values in memory (estimated size 52.9 KiB, free 434.2 MiB)
26/01/04 17:06:02 INFO MemoryStore: Block broadcast_143_piece0 stored as bytes in memory (estimated size 20.2 KiB, free 434.1 MiB)
26/01/04 17:06:02 INFO BlockManagerInfo: Added broadcast_143_piece0 in memory on spark-master:33535 (size: 20.2 KiB, free: 434.3 MiB)
26/01/04 17:06:02 INFO SparkContext: Created broadcast 143 from broadcast at DAGScheduler.scala:1585
26/01/04 17:06:02 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 97 (MapPartitionsRDD[487] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/01/04 17:06:02 INFO TaskSchedulerImpl: Adding task set 97.0 with 1 tasks resource profile 0
26/01/04 17:06:02 INFO TaskSetManager: Starting task 0.0 in stage 97.0 (TID 181) (172.18.0.13, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:06:02 INFO BlockManagerInfo: Added broadcast_143_piece0 in memory on 172.18.0.13:44401 (size: 20.2 KiB, free: 434.3 MiB)
26/01/04 17:06:02 INFO BlockManagerInfo: Added broadcast_142_piece0 in memory on 172.18.0.13:44401 (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:06:02 INFO TaskSetManager: Finished task 0.0 in stage 97.0 (TID 181) in 648 ms on 172.18.0.13 (executor 0) (1/1)
26/01/04 17:06:02 INFO TaskSchedulerImpl: Removed TaskSet 97.0, whose tasks have all completed, from pool 
26/01/04 17:06:02 INFO DAGScheduler: ResultStage 97 (start at NativeMethodAccessorImpl.java:0) finished in 0.664 s
26/01/04 17:06:02 INFO DAGScheduler: Job 96 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:06:02 INFO TaskSchedulerImpl: Killing all running tasks in stage 97: Stage finished
26/01/04 17:06:02 INFO DAGScheduler: Job 96 finished: start at NativeMethodAccessorImpl.java:0, took 0.669477 s
26/01/04 17:06:02 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 46, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@1f63f48c] is committing.
26/01/04 17:06:02 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 46, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@1f63f48c] committed.
26/01/04 17:06:02 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/46 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.46.10056a07-57ba-49a6-9dd9-5dc1c704fd39.tmp
26/01/04 17:06:03 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.46.10056a07-57ba-49a6-9dd9-5dc1c704fd39.tmp to file:/tmp/spark-checkpoint-enrichment/commits/46
26/01/04 17:06:03 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T17:06:01.955Z",
  "batchId" : 46,
  "numInputRows" : 30,
  "inputRowsPerSecond" : 2000.0,
  "processedRowsPerSecond" : 27.47252747252747,
  "durationMs" : {
    "addBatch" : 864,
    "commitOffsets" : 84,
    "getBatch" : 0,
    "latestOffset" : 2,
    "queryPlanning" : 46,
    "triggerExecution" : 1092,
    "walCommit" : 94
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "0" : 669
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "0" : 699
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "0" : 699
      }
    },
    "numInputRows" : 30,
    "inputRowsPerSecond" : 2000.0,
    "processedRowsPerSecond" : 27.47252747252747,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 31
  }
}
26/01/04 17:06:06 INFO BlockManagerInfo: Removed broadcast_143_piece0 on spark-master:33535 in memory (size: 20.2 KiB, free: 434.3 MiB)
26/01/04 17:06:06 INFO BlockManagerInfo: Removed broadcast_143_piece0 on 172.18.0.13:44401 in memory (size: 20.2 KiB, free: 434.3 MiB)
26/01/04 17:06:06 INFO BlockManagerInfo: Removed broadcast_138_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:06:06 INFO BlockManagerInfo: Removed broadcast_138_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:06:06 INFO BlockManagerInfo: Removed broadcast_138_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:06:06 INFO BlockManagerInfo: Removed broadcast_141_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:06:06 INFO BlockManagerInfo: Removed broadcast_141_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:06:06 INFO BlockManagerInfo: Removed broadcast_141_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:06:06 INFO BlockManagerInfo: Removed broadcast_140_piece0 on spark-master:33535 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:06:06 INFO BlockManagerInfo: Removed broadcast_140_piece0 on 172.18.0.13:44401 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:06:06 INFO BlockManagerInfo: Removed broadcast_139_piece0 on 172.18.0.13:44401 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:06:06 INFO BlockManagerInfo: Removed broadcast_139_piece0 on spark-master:33535 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:06:12 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/47 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.47.5b6e40d7-507c-43a9-b829-cc2885605298.tmp
26/01/04 17:06:13 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.47.5b6e40d7-507c-43a9-b829-cc2885605298.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/47
26/01/04 17:06:13 INFO MicroBatchExecution: Committed offsets for batch 47. Metadata OffsetSeqMetadata(0,1767546372967,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 17:06:13 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:06:13 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:06:13 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:06:13 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:06:13 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:06:13 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:06:13 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#39271 - airline_prefix.nullCount#39270) > 0)
26/01/04 17:06:13 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#39306 - min_flight_num.nullCount#39305) > 0)
26/01/04 17:06:13 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#39301 - max_flight_num.nullCount#39300) > 0)
26/01/04 17:06:13 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:06:13 INFO DAGScheduler: Got job 97 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 17:06:13 INFO DAGScheduler: Final stage: ResultStage 98 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:06:13 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:06:13 INFO DAGScheduler: Missing parents: List()
26/01/04 17:06:13 INFO DAGScheduler: Submitting ResultStage 98 (MapPartitionsRDD[492] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:06:13 INFO MemoryStore: Block broadcast_144 stored as values in memory (estimated size 38.5 KiB, free 434.4 MiB)
26/01/04 17:06:13 INFO MemoryStore: Block broadcast_144_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.3 MiB)
26/01/04 17:06:13 INFO BlockManagerInfo: Added broadcast_144_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:06:13 INFO SparkContext: Created broadcast 144 from broadcast at DAGScheduler.scala:1585
26/01/04 17:06:13 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 98 (MapPartitionsRDD[492] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 17:06:13 INFO TaskSchedulerImpl: Adding task set 98.0 with 2 tasks resource profile 0
26/01/04 17:06:13 INFO TaskSetManager: Starting task 0.0 in stage 98.0 (TID 182) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 17:06:13 INFO TaskSetManager: Starting task 1.0 in stage 98.0 (TID 183) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 17:06:13 INFO BlockManagerInfo: Added broadcast_144_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:06:13 INFO BlockManagerInfo: Added broadcast_144_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:06:13 INFO TaskSetManager: Finished task 1.0 in stage 98.0 (TID 183) in 26 ms on 172.18.0.13 (executor 0) (1/2)
26/01/04 17:06:13 INFO TaskSetManager: Finished task 0.0 in stage 98.0 (TID 182) in 36 ms on 172.18.0.14 (executor 1) (2/2)
26/01/04 17:06:13 INFO TaskSchedulerImpl: Removed TaskSet 98.0, whose tasks have all completed, from pool 
26/01/04 17:06:13 INFO DAGScheduler: ResultStage 98 (start at NativeMethodAccessorImpl.java:0) finished in 0.042 s
26/01/04 17:06:13 INFO DAGScheduler: Job 97 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:06:13 INFO TaskSchedulerImpl: Killing all running tasks in stage 98: Stage finished
26/01/04 17:06:13 INFO DAGScheduler: Job 97 finished: start at NativeMethodAccessorImpl.java:0, took 0.044909 s
26/01/04 17:06:13 INFO MemoryStore: Block broadcast_145_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.3 MiB)
26/01/04 17:06:13 INFO BlockManagerInfo: Added broadcast_145_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:06:13 INFO SparkContext: Created broadcast 145 from start at NativeMethodAccessorImpl.java:0
26/01/04 17:06:13 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 47, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@69d891d5]. The input RDD has 1 partitions.
26/01/04 17:06:13 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:06:13 INFO DAGScheduler: Got job 98 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/01/04 17:06:13 INFO DAGScheduler: Final stage: ResultStage 99 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:06:13 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:06:13 INFO DAGScheduler: Missing parents: List()
26/01/04 17:06:13 INFO DAGScheduler: Submitting ResultStage 99 (MapPartitionsRDD[497] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:06:13 INFO MemoryStore: Block broadcast_146 stored as values in memory (estimated size 52.9 KiB, free 434.3 MiB)
26/01/04 17:06:13 INFO MemoryStore: Block broadcast_146_piece0 stored as bytes in memory (estimated size 20.2 KiB, free 434.3 MiB)
26/01/04 17:06:13 INFO BlockManagerInfo: Added broadcast_146_piece0 in memory on spark-master:33535 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:06:13 INFO SparkContext: Created broadcast 146 from broadcast at DAGScheduler.scala:1585
26/01/04 17:06:13 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 99 (MapPartitionsRDD[497] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/01/04 17:06:13 INFO TaskSchedulerImpl: Adding task set 99.0 with 1 tasks resource profile 0
26/01/04 17:06:13 INFO TaskSetManager: Starting task 0.0 in stage 99.0 (TID 184) (172.18.0.13, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:06:13 INFO BlockManagerInfo: Added broadcast_146_piece0 in memory on 172.18.0.13:44401 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:06:13 INFO BlockManagerInfo: Added broadcast_145_piece0 in memory on 172.18.0.13:44401 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:06:13 INFO TaskSetManager: Finished task 0.0 in stage 99.0 (TID 184) in 592 ms on 172.18.0.13 (executor 0) (1/1)
26/01/04 17:06:13 INFO TaskSchedulerImpl: Removed TaskSet 99.0, whose tasks have all completed, from pool 
26/01/04 17:06:13 INFO DAGScheduler: ResultStage 99 (start at NativeMethodAccessorImpl.java:0) finished in 0.600 s
26/01/04 17:06:13 INFO DAGScheduler: Job 98 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:06:13 INFO TaskSchedulerImpl: Killing all running tasks in stage 99: Stage finished
26/01/04 17:06:13 INFO DAGScheduler: Job 98 finished: start at NativeMethodAccessorImpl.java:0, took 0.602662 s
26/01/04 17:06:13 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 47, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@69d891d5] is committing.
26/01/04 17:06:13 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 47, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@69d891d5] committed.
26/01/04 17:06:13 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/47 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.47.427950b5-f235-4c21-a1f4-b14f431b21a6.tmp
26/01/04 17:06:13 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.47.427950b5-f235-4c21-a1f4-b14f431b21a6.tmp to file:/tmp/spark-checkpoint-enrichment/commits/47
26/01/04 17:06:13 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T17:06:12.965Z",
  "batchId" : 47,
  "numInputRows" : 30,
  "inputRowsPerSecond" : 2500.0,
  "processedRowsPerSecond" : 32.93084522502744,
  "durationMs" : {
    "addBatch" : 723,
    "commitOffsets" : 66,
    "getBatch" : 0,
    "latestOffset" : 2,
    "queryPlanning" : 38,
    "triggerExecution" : 911,
    "walCommit" : 81
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "0" : 699
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "0" : 729
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "0" : 729
      }
    },
    "numInputRows" : 30,
    "inputRowsPerSecond" : 2500.0,
    "processedRowsPerSecond" : 32.93084522502744,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 31
  }
}
26/01/04 17:06:17 INFO NetworkClient: [AdminClient clientId=adminclient-2] Node -1 disconnected.
26/01/04 17:06:23 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/01/04 17:06:23 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/48 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.48.ac3f6eef-6068-45b9-9711-b73be49681e2.tmp
26/01/04 17:06:24 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.48.ac3f6eef-6068-45b9-9711-b73be49681e2.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/48
26/01/04 17:06:24 INFO MicroBatchExecution: Committed offsets for batch 48. Metadata OffsetSeqMetadata(0,1767546383980,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 17:06:24 INFO BlockManagerInfo: Removed broadcast_146_piece0 on spark-master:33535 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:06:24 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:06:24 INFO BlockManagerInfo: Removed broadcast_146_piece0 on 172.18.0.13:44401 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:06:24 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:06:24 INFO BlockManagerInfo: Removed broadcast_145_piece0 on spark-master:33535 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:06:24 INFO BlockManagerInfo: Removed broadcast_145_piece0 on 172.18.0.13:44401 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:06:24 INFO BlockManagerInfo: Removed broadcast_142_piece0 on spark-master:33535 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:06:24 INFO BlockManagerInfo: Removed broadcast_142_piece0 on 172.18.0.13:44401 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:06:24 INFO BlockManagerInfo: Removed broadcast_144_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:06:24 INFO BlockManagerInfo: Removed broadcast_144_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:06:24 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:06:24 INFO BlockManagerInfo: Removed broadcast_144_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:06:24 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:06:24 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:06:24 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:06:24 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#40075 - airline_prefix.nullCount#40074) > 0)
26/01/04 17:06:24 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#40110 - min_flight_num.nullCount#40109) > 0)
26/01/04 17:06:24 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#40105 - max_flight_num.nullCount#40104) > 0)
26/01/04 17:06:24 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:06:24 INFO DAGScheduler: Got job 99 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 17:06:24 INFO DAGScheduler: Final stage: ResultStage 100 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:06:24 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:06:24 INFO DAGScheduler: Missing parents: List()
26/01/04 17:06:24 INFO DAGScheduler: Submitting ResultStage 100 (MapPartitionsRDD[502] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:06:24 INFO MemoryStore: Block broadcast_147 stored as values in memory (estimated size 38.5 KiB, free 434.4 MiB)
26/01/04 17:06:24 INFO MemoryStore: Block broadcast_147_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.3 MiB)
26/01/04 17:06:24 INFO BlockManagerInfo: Added broadcast_147_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:06:24 INFO SparkContext: Created broadcast 147 from broadcast at DAGScheduler.scala:1585
26/01/04 17:06:24 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 100 (MapPartitionsRDD[502] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 17:06:24 INFO TaskSchedulerImpl: Adding task set 100.0 with 2 tasks resource profile 0
26/01/04 17:06:24 INFO TaskSetManager: Starting task 1.0 in stage 100.0 (TID 185) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 17:06:24 INFO TaskSetManager: Starting task 0.0 in stage 100.0 (TID 186) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 17:06:24 INFO BlockManagerInfo: Added broadcast_147_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:06:24 INFO BlockManagerInfo: Added broadcast_147_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:06:24 INFO TaskSetManager: Finished task 1.0 in stage 100.0 (TID 185) in 44 ms on 172.18.0.13 (executor 0) (1/2)
26/01/04 17:06:24 INFO TaskSetManager: Finished task 0.0 in stage 100.0 (TID 186) in 46 ms on 172.18.0.14 (executor 1) (2/2)
26/01/04 17:06:24 INFO TaskSchedulerImpl: Removed TaskSet 100.0, whose tasks have all completed, from pool 
26/01/04 17:06:24 INFO DAGScheduler: ResultStage 100 (start at NativeMethodAccessorImpl.java:0) finished in 0.066 s
26/01/04 17:06:24 INFO DAGScheduler: Job 99 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:06:24 INFO TaskSchedulerImpl: Killing all running tasks in stage 100: Stage finished
26/01/04 17:06:24 INFO DAGScheduler: Job 99 finished: start at NativeMethodAccessorImpl.java:0, took 0.072866 s
26/01/04 17:06:24 INFO MemoryStore: Block broadcast_148_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.3 MiB)
26/01/04 17:06:24 INFO BlockManagerInfo: Added broadcast_148_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:06:24 INFO SparkContext: Created broadcast 148 from start at NativeMethodAccessorImpl.java:0
26/01/04 17:06:24 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 48, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@6f2d23f9]. The input RDD has 1 partitions.
26/01/04 17:06:24 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:06:24 INFO DAGScheduler: Got job 100 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/01/04 17:06:24 INFO DAGScheduler: Final stage: ResultStage 101 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:06:24 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:06:24 INFO DAGScheduler: Missing parents: List()
26/01/04 17:06:24 INFO DAGScheduler: Submitting ResultStage 101 (MapPartitionsRDD[507] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:06:24 INFO MemoryStore: Block broadcast_149 stored as values in memory (estimated size 52.9 KiB, free 434.3 MiB)
26/01/04 17:06:24 INFO MemoryStore: Block broadcast_149_piece0 stored as bytes in memory (estimated size 20.2 KiB, free 434.3 MiB)
26/01/04 17:06:24 INFO BlockManagerInfo: Added broadcast_149_piece0 in memory on spark-master:33535 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:06:24 INFO SparkContext: Created broadcast 149 from broadcast at DAGScheduler.scala:1585
26/01/04 17:06:24 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 101 (MapPartitionsRDD[507] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/01/04 17:06:24 INFO TaskSchedulerImpl: Adding task set 101.0 with 1 tasks resource profile 0
26/01/04 17:06:24 INFO TaskSetManager: Starting task 0.0 in stage 101.0 (TID 187) (172.18.0.13, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:06:24 INFO BlockManagerInfo: Added broadcast_149_piece0 in memory on 172.18.0.13:44401 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:06:24 INFO BlockManagerInfo: Added broadcast_148_piece0 in memory on 172.18.0.13:44401 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:06:24 INFO TaskSetManager: Finished task 0.0 in stage 101.0 (TID 187) in 589 ms on 172.18.0.13 (executor 0) (1/1)
26/01/04 17:06:24 INFO TaskSchedulerImpl: Removed TaskSet 101.0, whose tasks have all completed, from pool 
26/01/04 17:06:24 INFO DAGScheduler: ResultStage 101 (start at NativeMethodAccessorImpl.java:0) finished in 0.595 s
26/01/04 17:06:24 INFO DAGScheduler: Job 100 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:06:24 INFO TaskSchedulerImpl: Killing all running tasks in stage 101: Stage finished
26/01/04 17:06:24 INFO DAGScheduler: Job 100 finished: start at NativeMethodAccessorImpl.java:0, took 0.597493 s
26/01/04 17:06:24 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 48, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@6f2d23f9] is committing.
26/01/04 17:06:24 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 48, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@6f2d23f9] committed.
26/01/04 17:06:24 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/48 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.48.c53dacb1-7157-489e-b6c6-ae7c5f0ed4fa.tmp
26/01/04 17:06:25 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.48.c53dacb1-7157-489e-b6c6-ae7c5f0ed4fa.tmp to file:/tmp/spark-checkpoint-enrichment/commits/48
26/01/04 17:06:25 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T17:06:23.979Z",
  "batchId" : 48,
  "numInputRows" : 30,
  "inputRowsPerSecond" : 2727.2727272727275,
  "processedRowsPerSecond" : 28.929604628736744,
  "durationMs" : {
    "addBatch" : 853,
    "commitOffsets" : 75,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 36,
    "triggerExecution" : 1037,
    "walCommit" : 71
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "0" : 729
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "0" : 759
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "0" : 759
      }
    },
    "numInputRows" : 30,
    "inputRowsPerSecond" : 2727.2727272727275,
    "processedRowsPerSecond" : 28.929604628736744,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 31
  }
}
26/01/04 17:06:35 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/49 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.49.6a0fac65-5d91-4638-82e1-30b6db6155b6.tmp
26/01/04 17:06:35 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.49.6a0fac65-5d91-4638-82e1-30b6db6155b6.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/49
26/01/04 17:06:35 INFO MicroBatchExecution: Committed offsets for batch 49. Metadata OffsetSeqMetadata(0,1767546394992,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 17:06:35 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:06:35 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:06:35 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:06:35 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:06:35 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:06:35 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:06:35 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#40879 - airline_prefix.nullCount#40878) > 0)
26/01/04 17:06:35 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#40914 - min_flight_num.nullCount#40913) > 0)
26/01/04 17:06:35 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#40909 - max_flight_num.nullCount#40908) > 0)
26/01/04 17:06:35 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:06:35 INFO DAGScheduler: Got job 101 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 17:06:35 INFO DAGScheduler: Final stage: ResultStage 102 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:06:35 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:06:35 INFO DAGScheduler: Missing parents: List()
26/01/04 17:06:35 INFO DAGScheduler: Submitting ResultStage 102 (MapPartitionsRDD[512] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:06:35 INFO MemoryStore: Block broadcast_150 stored as values in memory (estimated size 38.5 KiB, free 434.2 MiB)
26/01/04 17:06:35 INFO MemoryStore: Block broadcast_150_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.2 MiB)
26/01/04 17:06:35 INFO BlockManagerInfo: Added broadcast_150_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:06:35 INFO SparkContext: Created broadcast 150 from broadcast at DAGScheduler.scala:1585
26/01/04 17:06:35 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 102 (MapPartitionsRDD[512] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 17:06:35 INFO TaskSchedulerImpl: Adding task set 102.0 with 2 tasks resource profile 0
26/01/04 17:06:35 INFO TaskSetManager: Starting task 0.0 in stage 102.0 (TID 188) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 17:06:35 INFO TaskSetManager: Starting task 1.0 in stage 102.0 (TID 189) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 17:06:35 INFO BlockManagerInfo: Added broadcast_150_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:06:35 INFO BlockManagerInfo: Added broadcast_150_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:06:35 INFO TaskSetManager: Finished task 1.0 in stage 102.0 (TID 189) in 48 ms on 172.18.0.13 (executor 0) (1/2)
26/01/04 17:06:35 INFO TaskSetManager: Finished task 0.0 in stage 102.0 (TID 188) in 62 ms on 172.18.0.14 (executor 1) (2/2)
26/01/04 17:06:35 INFO TaskSchedulerImpl: Removed TaskSet 102.0, whose tasks have all completed, from pool 
26/01/04 17:06:35 INFO DAGScheduler: ResultStage 102 (start at NativeMethodAccessorImpl.java:0) finished in 0.076 s
26/01/04 17:06:35 INFO DAGScheduler: Job 101 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:06:35 INFO TaskSchedulerImpl: Killing all running tasks in stage 102: Stage finished
26/01/04 17:06:35 INFO DAGScheduler: Job 101 finished: start at NativeMethodAccessorImpl.java:0, took 0.082500 s
26/01/04 17:06:35 INFO MemoryStore: Block broadcast_151_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.2 MiB)
26/01/04 17:06:35 INFO BlockManagerInfo: Added broadcast_151_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:06:35 INFO SparkContext: Created broadcast 151 from start at NativeMethodAccessorImpl.java:0
26/01/04 17:06:35 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 49, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@362858b6]. The input RDD has 1 partitions.
26/01/04 17:06:35 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:06:35 INFO DAGScheduler: Got job 102 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/01/04 17:06:35 INFO DAGScheduler: Final stage: ResultStage 103 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:06:35 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:06:35 INFO DAGScheduler: Missing parents: List()
26/01/04 17:06:35 INFO DAGScheduler: Submitting ResultStage 103 (MapPartitionsRDD[517] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:06:35 INFO MemoryStore: Block broadcast_152 stored as values in memory (estimated size 52.9 KiB, free 434.2 MiB)
26/01/04 17:06:35 INFO MemoryStore: Block broadcast_152_piece0 stored as bytes in memory (estimated size 20.2 KiB, free 434.1 MiB)
26/01/04 17:06:35 INFO BlockManagerInfo: Added broadcast_152_piece0 in memory on spark-master:33535 (size: 20.2 KiB, free: 434.3 MiB)
26/01/04 17:06:35 INFO SparkContext: Created broadcast 152 from broadcast at DAGScheduler.scala:1585
26/01/04 17:06:35 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 103 (MapPartitionsRDD[517] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/01/04 17:06:35 INFO TaskSchedulerImpl: Adding task set 103.0 with 1 tasks resource profile 0
26/01/04 17:06:35 INFO TaskSetManager: Starting task 0.0 in stage 103.0 (TID 190) (172.18.0.13, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:06:35 INFO BlockManagerInfo: Added broadcast_152_piece0 in memory on 172.18.0.13:44401 (size: 20.2 KiB, free: 434.3 MiB)
26/01/04 17:06:35 INFO BlockManagerInfo: Added broadcast_151_piece0 in memory on 172.18.0.13:44401 (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:06:35 INFO TaskSetManager: Finished task 0.0 in stage 103.0 (TID 190) in 596 ms on 172.18.0.13 (executor 0) (1/1)
26/01/04 17:06:35 INFO TaskSchedulerImpl: Removed TaskSet 103.0, whose tasks have all completed, from pool 
26/01/04 17:06:35 INFO DAGScheduler: ResultStage 103 (start at NativeMethodAccessorImpl.java:0) finished in 0.608 s
26/01/04 17:06:35 INFO DAGScheduler: Job 102 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:06:35 INFO TaskSchedulerImpl: Killing all running tasks in stage 103: Stage finished
26/01/04 17:06:35 INFO DAGScheduler: Job 102 finished: start at NativeMethodAccessorImpl.java:0, took 0.610389 s
26/01/04 17:06:35 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 49, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@362858b6] is committing.
26/01/04 17:06:35 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 49, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@362858b6] committed.
26/01/04 17:06:35 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/49 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.49.73709dbd-930f-4e8a-9991-2d58308092d9.tmp
26/01/04 17:06:36 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.49.73709dbd-930f-4e8a-9991-2d58308092d9.tmp to file:/tmp/spark-checkpoint-enrichment/commits/49
26/01/04 17:06:36 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T17:06:34.990Z",
  "batchId" : 49,
  "numInputRows" : 30,
  "inputRowsPerSecond" : 2307.6923076923076,
  "processedRowsPerSecond" : 28.735632183908045,
  "durationMs" : {
    "addBatch" : 835,
    "commitOffsets" : 75,
    "getBatch" : 0,
    "latestOffset" : 2,
    "queryPlanning" : 36,
    "triggerExecution" : 1044,
    "walCommit" : 95
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "0" : 759
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "0" : 789
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "0" : 789
      }
    },
    "numInputRows" : 30,
    "inputRowsPerSecond" : 2307.6923076923076,
    "processedRowsPerSecond" : 28.735632183908045,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 31
  }
}
26/01/04 17:06:37 INFO BlockManagerInfo: Removed broadcast_147_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:06:37 INFO BlockManagerInfo: Removed broadcast_147_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:06:37 INFO BlockManagerInfo: Removed broadcast_147_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:06:37 INFO BlockManagerInfo: Removed broadcast_152_piece0 on spark-master:33535 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:06:37 INFO BlockManagerInfo: Removed broadcast_152_piece0 on 172.18.0.13:44401 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:06:37 INFO BlockManagerInfo: Removed broadcast_149_piece0 on spark-master:33535 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:06:37 INFO BlockManagerInfo: Removed broadcast_149_piece0 on 172.18.0.13:44401 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:06:37 INFO BlockManagerInfo: Removed broadcast_148_piece0 on spark-master:33535 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:06:37 INFO BlockManagerInfo: Removed broadcast_148_piece0 on 172.18.0.13:44401 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:06:37 INFO BlockManagerInfo: Removed broadcast_150_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:06:37 INFO BlockManagerInfo: Removed broadcast_150_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:06:37 INFO BlockManagerInfo: Removed broadcast_150_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:06:46 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/50 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.50.91aaa441-d363-48cc-85f2-b3641ad09e32.tmp
26/01/04 17:06:46 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.50.91aaa441-d363-48cc-85f2-b3641ad09e32.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/50
26/01/04 17:06:46 INFO MicroBatchExecution: Committed offsets for batch 50. Metadata OffsetSeqMetadata(0,1767546406007,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 17:06:46 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:06:46 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:06:46 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:06:46 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:06:46 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:06:46 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:06:46 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#41683 - airline_prefix.nullCount#41682) > 0)
26/01/04 17:06:46 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#41718 - min_flight_num.nullCount#41717) > 0)
26/01/04 17:06:46 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#41713 - max_flight_num.nullCount#41712) > 0)
26/01/04 17:06:46 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:06:46 INFO DAGScheduler: Got job 103 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 17:06:46 INFO DAGScheduler: Final stage: ResultStage 104 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:06:46 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:06:46 INFO DAGScheduler: Missing parents: List()
26/01/04 17:06:46 INFO DAGScheduler: Submitting ResultStage 104 (MapPartitionsRDD[522] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:06:46 INFO MemoryStore: Block broadcast_153 stored as values in memory (estimated size 38.5 KiB, free 434.4 MiB)
26/01/04 17:06:46 INFO MemoryStore: Block broadcast_153_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.3 MiB)
26/01/04 17:06:46 INFO BlockManagerInfo: Added broadcast_153_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:06:46 INFO SparkContext: Created broadcast 153 from broadcast at DAGScheduler.scala:1585
26/01/04 17:06:46 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 104 (MapPartitionsRDD[522] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 17:06:46 INFO TaskSchedulerImpl: Adding task set 104.0 with 2 tasks resource profile 0
26/01/04 17:06:46 INFO TaskSetManager: Starting task 0.0 in stage 104.0 (TID 191) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 17:06:46 INFO TaskSetManager: Starting task 1.0 in stage 104.0 (TID 192) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 17:06:46 INFO BlockManagerInfo: Added broadcast_153_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:06:46 INFO BlockManagerInfo: Added broadcast_153_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:06:46 INFO TaskSetManager: Finished task 1.0 in stage 104.0 (TID 192) in 47 ms on 172.18.0.13 (executor 0) (1/2)
26/01/04 17:06:46 INFO TaskSetManager: Finished task 0.0 in stage 104.0 (TID 191) in 74 ms on 172.18.0.14 (executor 1) (2/2)
26/01/04 17:06:46 INFO TaskSchedulerImpl: Removed TaskSet 104.0, whose tasks have all completed, from pool 
26/01/04 17:06:46 INFO DAGScheduler: ResultStage 104 (start at NativeMethodAccessorImpl.java:0) finished in 0.089 s
26/01/04 17:06:46 INFO DAGScheduler: Job 103 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:06:46 INFO TaskSchedulerImpl: Killing all running tasks in stage 104: Stage finished
26/01/04 17:06:46 INFO DAGScheduler: Job 103 finished: start at NativeMethodAccessorImpl.java:0, took 0.097956 s
26/01/04 17:06:46 INFO MemoryStore: Block broadcast_154_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.3 MiB)
26/01/04 17:06:46 INFO BlockManagerInfo: Added broadcast_154_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:06:46 INFO SparkContext: Created broadcast 154 from start at NativeMethodAccessorImpl.java:0
26/01/04 17:06:46 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 50, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@6fab4141]. The input RDD has 1 partitions.
26/01/04 17:06:46 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:06:46 INFO DAGScheduler: Got job 104 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/01/04 17:06:46 INFO DAGScheduler: Final stage: ResultStage 105 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:06:46 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:06:46 INFO DAGScheduler: Missing parents: List()
26/01/04 17:06:46 INFO DAGScheduler: Submitting ResultStage 105 (MapPartitionsRDD[527] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:06:46 INFO MemoryStore: Block broadcast_155 stored as values in memory (estimated size 52.9 KiB, free 434.3 MiB)
26/01/04 17:06:46 INFO MemoryStore: Block broadcast_155_piece0 stored as bytes in memory (estimated size 20.2 KiB, free 434.3 MiB)
26/01/04 17:06:46 INFO BlockManagerInfo: Added broadcast_155_piece0 in memory on spark-master:33535 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:06:46 INFO SparkContext: Created broadcast 155 from broadcast at DAGScheduler.scala:1585
26/01/04 17:06:46 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 105 (MapPartitionsRDD[527] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/01/04 17:06:46 INFO TaskSchedulerImpl: Adding task set 105.0 with 1 tasks resource profile 0
26/01/04 17:06:46 INFO TaskSetManager: Starting task 0.0 in stage 105.0 (TID 193) (172.18.0.13, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:06:46 INFO BlockManagerInfo: Added broadcast_155_piece0 in memory on 172.18.0.13:44401 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:06:46 INFO BlockManagerInfo: Added broadcast_154_piece0 in memory on 172.18.0.13:44401 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:06:47 INFO TaskSetManager: Finished task 0.0 in stage 105.0 (TID 193) in 638 ms on 172.18.0.13 (executor 0) (1/1)
26/01/04 17:06:47 INFO TaskSchedulerImpl: Removed TaskSet 105.0, whose tasks have all completed, from pool 
26/01/04 17:06:47 INFO DAGScheduler: ResultStage 105 (start at NativeMethodAccessorImpl.java:0) finished in 0.649 s
26/01/04 17:06:47 INFO DAGScheduler: Job 104 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:06:47 INFO TaskSchedulerImpl: Killing all running tasks in stage 105: Stage finished
26/01/04 17:06:47 INFO DAGScheduler: Job 104 finished: start at NativeMethodAccessorImpl.java:0, took 0.655978 s
26/01/04 17:06:47 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 50, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@6fab4141] is committing.
26/01/04 17:06:47 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 50, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@6fab4141] committed.
26/01/04 17:06:47 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/50 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.50.18831a5e-e357-4c24-9adc-531c69ea0476.tmp
26/01/04 17:06:47 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.50.18831a5e-e357-4c24-9adc-531c69ea0476.tmp to file:/tmp/spark-checkpoint-enrichment/commits/50
26/01/04 17:06:47 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T17:06:46.004Z",
  "batchId" : 50,
  "numInputRows" : 30,
  "inputRowsPerSecond" : 2142.8571428571427,
  "processedRowsPerSecond" : 24.67105263157895,
  "durationMs" : {
    "addBatch" : 910,
    "commitOffsets" : 118,
    "getBatch" : 0,
    "latestOffset" : 3,
    "queryPlanning" : 49,
    "triggerExecution" : 1216,
    "walCommit" : 135
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "0" : 789
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "0" : 819
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "0" : 819
      }
    },
    "numInputRows" : 30,
    "inputRowsPerSecond" : 2142.8571428571427,
    "processedRowsPerSecond" : 24.67105263157895,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 31
  }
}
26/01/04 17:06:57 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/51 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.51.f25269e1-65c7-4b84-8e75-802d55fdc568.tmp
26/01/04 17:06:57 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.51.f25269e1-65c7-4b84-8e75-802d55fdc568.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/51
26/01/04 17:06:57 INFO MicroBatchExecution: Committed offsets for batch 51. Metadata OffsetSeqMetadata(0,1767546417021,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 17:06:57 INFO BlockManagerInfo: Removed broadcast_154_piece0 on spark-master:33535 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:06:57 INFO BlockManagerInfo: Removed broadcast_154_piece0 on 172.18.0.13:44401 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:06:57 INFO BlockManagerInfo: Removed broadcast_151_piece0 on spark-master:33535 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:06:57 INFO BlockManagerInfo: Removed broadcast_151_piece0 on 172.18.0.13:44401 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:06:57 INFO BlockManagerInfo: Removed broadcast_153_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:06:57 INFO BlockManagerInfo: Removed broadcast_153_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:06:57 INFO BlockManagerInfo: Removed broadcast_153_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:06:57 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:06:57 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:06:57 INFO BlockManagerInfo: Removed broadcast_155_piece0 on 172.18.0.13:44401 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:06:57 INFO BlockManagerInfo: Removed broadcast_155_piece0 on spark-master:33535 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:06:57 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:06:57 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:06:57 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:06:57 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:06:57 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#42487 - airline_prefix.nullCount#42486) > 0)
26/01/04 17:06:57 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#42522 - min_flight_num.nullCount#42521) > 0)
26/01/04 17:06:57 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#42517 - max_flight_num.nullCount#42516) > 0)
26/01/04 17:06:57 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:06:57 INFO DAGScheduler: Got job 105 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 17:06:57 INFO DAGScheduler: Final stage: ResultStage 106 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:06:57 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:06:57 INFO DAGScheduler: Missing parents: List()
26/01/04 17:06:57 INFO DAGScheduler: Submitting ResultStage 106 (MapPartitionsRDD[532] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:06:57 INFO MemoryStore: Block broadcast_156 stored as values in memory (estimated size 38.5 KiB, free 434.4 MiB)
26/01/04 17:06:57 INFO MemoryStore: Block broadcast_156_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.3 MiB)
26/01/04 17:06:57 INFO BlockManagerInfo: Added broadcast_156_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:06:57 INFO SparkContext: Created broadcast 156 from broadcast at DAGScheduler.scala:1585
26/01/04 17:06:57 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 106 (MapPartitionsRDD[532] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 17:06:57 INFO TaskSchedulerImpl: Adding task set 106.0 with 2 tasks resource profile 0
26/01/04 17:06:57 INFO TaskSetManager: Starting task 0.0 in stage 106.0 (TID 194) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 17:06:57 INFO TaskSetManager: Starting task 1.0 in stage 106.0 (TID 195) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 17:06:57 INFO BlockManagerInfo: Added broadcast_156_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:06:57 INFO BlockManagerInfo: Added broadcast_156_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:06:57 INFO TaskSetManager: Finished task 1.0 in stage 106.0 (TID 195) in 102 ms on 172.18.0.13 (executor 0) (1/2)
26/01/04 17:06:57 INFO TaskSetManager: Finished task 0.0 in stage 106.0 (TID 194) in 106 ms on 172.18.0.14 (executor 1) (2/2)
26/01/04 17:06:57 INFO TaskSchedulerImpl: Removed TaskSet 106.0, whose tasks have all completed, from pool 
26/01/04 17:06:57 INFO DAGScheduler: ResultStage 106 (start at NativeMethodAccessorImpl.java:0) finished in 0.149 s
26/01/04 17:06:57 INFO DAGScheduler: Job 105 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:06:57 INFO TaskSchedulerImpl: Killing all running tasks in stage 106: Stage finished
26/01/04 17:06:57 INFO DAGScheduler: Job 105 finished: start at NativeMethodAccessorImpl.java:0, took 0.157109 s
26/01/04 17:06:57 INFO MemoryStore: Block broadcast_157_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.3 MiB)
26/01/04 17:06:57 INFO BlockManagerInfo: Added broadcast_157_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:06:57 INFO SparkContext: Created broadcast 157 from start at NativeMethodAccessorImpl.java:0
26/01/04 17:06:57 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 51, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@66062856]. The input RDD has 1 partitions.
26/01/04 17:06:57 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:06:57 INFO DAGScheduler: Got job 106 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/01/04 17:06:57 INFO DAGScheduler: Final stage: ResultStage 107 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:06:57 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:06:57 INFO DAGScheduler: Missing parents: List()
26/01/04 17:06:57 INFO DAGScheduler: Submitting ResultStage 107 (MapPartitionsRDD[537] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:06:57 INFO MemoryStore: Block broadcast_158 stored as values in memory (estimated size 52.9 KiB, free 434.3 MiB)
26/01/04 17:06:57 INFO MemoryStore: Block broadcast_158_piece0 stored as bytes in memory (estimated size 20.2 KiB, free 434.3 MiB)
26/01/04 17:06:57 INFO BlockManagerInfo: Added broadcast_158_piece0 in memory on spark-master:33535 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:06:57 INFO SparkContext: Created broadcast 158 from broadcast at DAGScheduler.scala:1585
26/01/04 17:06:57 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 107 (MapPartitionsRDD[537] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/01/04 17:06:57 INFO TaskSchedulerImpl: Adding task set 107.0 with 1 tasks resource profile 0
26/01/04 17:06:57 INFO TaskSetManager: Starting task 0.0 in stage 107.0 (TID 196) (172.18.0.13, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:06:57 INFO BlockManagerInfo: Added broadcast_158_piece0 in memory on 172.18.0.13:44401 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:06:57 INFO BlockManagerInfo: Added broadcast_157_piece0 in memory on 172.18.0.13:44401 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:06:58 INFO TaskSetManager: Finished task 0.0 in stage 107.0 (TID 196) in 696 ms on 172.18.0.13 (executor 0) (1/1)
26/01/04 17:06:58 INFO TaskSchedulerImpl: Removed TaskSet 107.0, whose tasks have all completed, from pool 
26/01/04 17:06:58 INFO DAGScheduler: ResultStage 107 (start at NativeMethodAccessorImpl.java:0) finished in 0.721 s
26/01/04 17:06:58 INFO DAGScheduler: Job 106 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:06:58 INFO TaskSchedulerImpl: Killing all running tasks in stage 107: Stage finished
26/01/04 17:06:58 INFO DAGScheduler: Job 106 finished: start at NativeMethodAccessorImpl.java:0, took 0.732766 s
26/01/04 17:06:58 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 51, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@66062856] is committing.
26/01/04 17:06:58 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 51, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@66062856] committed.
26/01/04 17:06:58 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/51 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.51.bd34eacf-c56b-4dd7-b0b9-838988df681c.tmp
26/01/04 17:06:58 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.51.bd34eacf-c56b-4dd7-b0b9-838988df681c.tmp to file:/tmp/spark-checkpoint-enrichment/commits/51
26/01/04 17:06:58 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T17:06:57.018Z",
  "batchId" : 51,
  "numInputRows" : 30,
  "inputRowsPerSecond" : 2142.8571428571427,
  "processedRowsPerSecond" : 19.64636542239686,
  "durationMs" : {
    "addBatch" : 1074,
    "commitOffsets" : 136,
    "getBatch" : 0,
    "latestOffset" : 3,
    "queryPlanning" : 126,
    "triggerExecution" : 1527,
    "walCommit" : 184
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "0" : 819
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "0" : 849
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "0" : 849
      }
    },
    "numInputRows" : 30,
    "inputRowsPerSecond" : 2142.8571428571427,
    "processedRowsPerSecond" : 19.64636542239686,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 31
  }
}
26/01/04 17:07:08 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/52 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.52.cf0504ab-16fd-45c6-aebb-e64cf8955ad3.tmp
26/01/04 17:07:08 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.52.cf0504ab-16fd-45c6-aebb-e64cf8955ad3.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/52
26/01/04 17:07:08 INFO MicroBatchExecution: Committed offsets for batch 52. Metadata OffsetSeqMetadata(0,1767546428041,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 17:07:08 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:07:08 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:07:08 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:07:08 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:07:08 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:07:08 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:07:08 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#43291 - airline_prefix.nullCount#43290) > 0)
26/01/04 17:07:08 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#43326 - min_flight_num.nullCount#43325) > 0)
26/01/04 17:07:08 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#43321 - max_flight_num.nullCount#43320) > 0)
26/01/04 17:07:08 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:07:08 INFO DAGScheduler: Got job 107 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 17:07:08 INFO DAGScheduler: Final stage: ResultStage 108 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:07:08 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:07:08 INFO DAGScheduler: Missing parents: List()
26/01/04 17:07:08 INFO DAGScheduler: Submitting ResultStage 108 (MapPartitionsRDD[542] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:07:08 INFO MemoryStore: Block broadcast_159 stored as values in memory (estimated size 38.5 KiB, free 434.2 MiB)
26/01/04 17:07:08 INFO MemoryStore: Block broadcast_159_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.2 MiB)
26/01/04 17:07:08 INFO BlockManagerInfo: Added broadcast_159_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:07:08 INFO SparkContext: Created broadcast 159 from broadcast at DAGScheduler.scala:1585
26/01/04 17:07:08 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 108 (MapPartitionsRDD[542] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 17:07:08 INFO TaskSchedulerImpl: Adding task set 108.0 with 2 tasks resource profile 0
26/01/04 17:07:08 INFO TaskSetManager: Starting task 0.0 in stage 108.0 (TID 197) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 17:07:08 INFO TaskSetManager: Starting task 1.0 in stage 108.0 (TID 198) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 17:07:08 INFO BlockManagerInfo: Added broadcast_159_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:07:08 INFO BlockManagerInfo: Added broadcast_159_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:07:08 INFO TaskSetManager: Finished task 1.0 in stage 108.0 (TID 198) in 31 ms on 172.18.0.13 (executor 0) (1/2)
26/01/04 17:07:08 INFO TaskSetManager: Finished task 0.0 in stage 108.0 (TID 197) in 35 ms on 172.18.0.14 (executor 1) (2/2)
26/01/04 17:07:08 INFO TaskSchedulerImpl: Removed TaskSet 108.0, whose tasks have all completed, from pool 
26/01/04 17:07:08 INFO DAGScheduler: ResultStage 108 (start at NativeMethodAccessorImpl.java:0) finished in 0.043 s
26/01/04 17:07:08 INFO DAGScheduler: Job 107 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:07:08 INFO TaskSchedulerImpl: Killing all running tasks in stage 108: Stage finished
26/01/04 17:07:08 INFO DAGScheduler: Job 107 finished: start at NativeMethodAccessorImpl.java:0, took 0.045815 s
26/01/04 17:07:08 INFO MemoryStore: Block broadcast_160_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.2 MiB)
26/01/04 17:07:08 INFO BlockManagerInfo: Added broadcast_160_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:07:08 INFO SparkContext: Created broadcast 160 from start at NativeMethodAccessorImpl.java:0
26/01/04 17:07:08 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 52, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@43e8c2ca]. The input RDD has 1 partitions.
26/01/04 17:07:08 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:07:08 INFO DAGScheduler: Got job 108 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/01/04 17:07:08 INFO DAGScheduler: Final stage: ResultStage 109 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:07:08 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:07:08 INFO DAGScheduler: Missing parents: List()
26/01/04 17:07:08 INFO DAGScheduler: Submitting ResultStage 109 (MapPartitionsRDD[547] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:07:08 INFO MemoryStore: Block broadcast_161 stored as values in memory (estimated size 52.9 KiB, free 434.2 MiB)
26/01/04 17:07:08 INFO MemoryStore: Block broadcast_161_piece0 stored as bytes in memory (estimated size 20.2 KiB, free 434.1 MiB)
26/01/04 17:07:08 INFO BlockManagerInfo: Added broadcast_161_piece0 in memory on spark-master:33535 (size: 20.2 KiB, free: 434.3 MiB)
26/01/04 17:07:08 INFO SparkContext: Created broadcast 161 from broadcast at DAGScheduler.scala:1585
26/01/04 17:07:08 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 109 (MapPartitionsRDD[547] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/01/04 17:07:08 INFO TaskSchedulerImpl: Adding task set 109.0 with 1 tasks resource profile 0
26/01/04 17:07:08 INFO TaskSetManager: Starting task 0.0 in stage 109.0 (TID 199) (172.18.0.13, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:07:08 INFO BlockManagerInfo: Added broadcast_161_piece0 in memory on 172.18.0.13:44401 (size: 20.2 KiB, free: 434.3 MiB)
26/01/04 17:07:08 INFO BlockManagerInfo: Added broadcast_160_piece0 in memory on 172.18.0.13:44401 (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:07:09 INFO TaskSetManager: Finished task 0.0 in stage 109.0 (TID 199) in 592 ms on 172.18.0.13 (executor 0) (1/1)
26/01/04 17:07:09 INFO TaskSchedulerImpl: Removed TaskSet 109.0, whose tasks have all completed, from pool 
26/01/04 17:07:09 INFO DAGScheduler: ResultStage 109 (start at NativeMethodAccessorImpl.java:0) finished in 0.600 s
26/01/04 17:07:09 INFO DAGScheduler: Job 108 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:07:09 INFO TaskSchedulerImpl: Killing all running tasks in stage 109: Stage finished
26/01/04 17:07:09 INFO DAGScheduler: Job 108 finished: start at NativeMethodAccessorImpl.java:0, took 0.602567 s
26/01/04 17:07:09 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 52, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@43e8c2ca] is committing.
26/01/04 17:07:09 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 52, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@43e8c2ca] committed.
26/01/04 17:07:09 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/52 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.52.f90ebef1-43a6-4a29-8659-b3bdd42368c7.tmp
26/01/04 17:07:09 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.52.f90ebef1-43a6-4a29-8659-b3bdd42368c7.tmp to file:/tmp/spark-checkpoint-enrichment/commits/52
26/01/04 17:07:09 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T17:07:08.039Z",
  "batchId" : 52,
  "numInputRows" : 30,
  "inputRowsPerSecond" : 2307.6923076923076,
  "processedRowsPerSecond" : 25.40220152413209,
  "durationMs" : {
    "addBatch" : 897,
    "commitOffsets" : 69,
    "getBatch" : 0,
    "latestOffset" : 2,
    "queryPlanning" : 66,
    "triggerExecution" : 1181,
    "walCommit" : 146
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "0" : 849
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "0" : 879
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "0" : 879
      }
    },
    "numInputRows" : 30,
    "inputRowsPerSecond" : 2307.6923076923076,
    "processedRowsPerSecond" : 25.40220152413209,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 31
  }
}
26/01/04 17:07:13 INFO BlockManagerInfo: Removed broadcast_161_piece0 on spark-master:33535 in memory (size: 20.2 KiB, free: 434.3 MiB)
26/01/04 17:07:13 INFO BlockManagerInfo: Removed broadcast_161_piece0 on 172.18.0.13:44401 in memory (size: 20.2 KiB, free: 434.3 MiB)
26/01/04 17:07:13 INFO BlockManagerInfo: Removed broadcast_156_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:07:13 INFO BlockManagerInfo: Removed broadcast_156_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:07:13 INFO BlockManagerInfo: Removed broadcast_156_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:07:13 INFO BlockManagerInfo: Removed broadcast_157_piece0 on spark-master:33535 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:07:13 INFO BlockManagerInfo: Removed broadcast_157_piece0 on 172.18.0.13:44401 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:07:13 INFO BlockManagerInfo: Removed broadcast_158_piece0 on spark-master:33535 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:07:13 INFO BlockManagerInfo: Removed broadcast_158_piece0 on 172.18.0.13:44401 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:07:13 INFO BlockManagerInfo: Removed broadcast_159_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:07:13 INFO BlockManagerInfo: Removed broadcast_159_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:07:13 INFO BlockManagerInfo: Removed broadcast_159_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:07:19 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/53 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.53.26fa1730-ad75-4f50-8ab3-18fa09dcbfcb.tmp
26/01/04 17:07:19 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.53.26fa1730-ad75-4f50-8ab3-18fa09dcbfcb.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/53
26/01/04 17:07:19 INFO MicroBatchExecution: Committed offsets for batch 53. Metadata OffsetSeqMetadata(0,1767546439058,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 17:07:19 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:07:19 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:07:19 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:07:19 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:07:19 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:07:19 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:07:19 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#44095 - airline_prefix.nullCount#44094) > 0)
26/01/04 17:07:19 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#44130 - min_flight_num.nullCount#44129) > 0)
26/01/04 17:07:19 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#44125 - max_flight_num.nullCount#44124) > 0)
26/01/04 17:07:19 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:07:19 INFO DAGScheduler: Got job 109 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 17:07:19 INFO DAGScheduler: Final stage: ResultStage 110 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:07:19 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:07:19 INFO DAGScheduler: Missing parents: List()
26/01/04 17:07:19 INFO DAGScheduler: Submitting ResultStage 110 (MapPartitionsRDD[552] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:07:19 INFO MemoryStore: Block broadcast_162 stored as values in memory (estimated size 38.5 KiB, free 434.4 MiB)
26/01/04 17:07:19 INFO MemoryStore: Block broadcast_162_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.3 MiB)
26/01/04 17:07:19 INFO BlockManagerInfo: Added broadcast_162_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:07:19 INFO SparkContext: Created broadcast 162 from broadcast at DAGScheduler.scala:1585
26/01/04 17:07:19 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 110 (MapPartitionsRDD[552] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 17:07:19 INFO TaskSchedulerImpl: Adding task set 110.0 with 2 tasks resource profile 0
26/01/04 17:07:19 INFO TaskSetManager: Starting task 0.0 in stage 110.0 (TID 200) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 17:07:19 INFO TaskSetManager: Starting task 1.0 in stage 110.0 (TID 201) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 17:07:19 INFO BlockManagerInfo: Added broadcast_162_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:07:19 INFO BlockManagerInfo: Added broadcast_162_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:07:19 INFO TaskSetManager: Finished task 1.0 in stage 110.0 (TID 201) in 32 ms on 172.18.0.13 (executor 0) (1/2)
26/01/04 17:07:19 INFO TaskSetManager: Finished task 0.0 in stage 110.0 (TID 200) in 38 ms on 172.18.0.14 (executor 1) (2/2)
26/01/04 17:07:19 INFO TaskSchedulerImpl: Removed TaskSet 110.0, whose tasks have all completed, from pool 
26/01/04 17:07:19 INFO DAGScheduler: ResultStage 110 (start at NativeMethodAccessorImpl.java:0) finished in 0.050 s
26/01/04 17:07:19 INFO DAGScheduler: Job 109 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:07:19 INFO TaskSchedulerImpl: Killing all running tasks in stage 110: Stage finished
26/01/04 17:07:19 INFO DAGScheduler: Job 109 finished: start at NativeMethodAccessorImpl.java:0, took 0.054515 s
26/01/04 17:07:19 INFO MemoryStore: Block broadcast_163_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.3 MiB)
26/01/04 17:07:19 INFO BlockManagerInfo: Added broadcast_163_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:07:19 INFO SparkContext: Created broadcast 163 from start at NativeMethodAccessorImpl.java:0
26/01/04 17:07:19 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 53, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@3168f88c]. The input RDD has 1 partitions.
26/01/04 17:07:19 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:07:19 INFO DAGScheduler: Got job 110 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/01/04 17:07:19 INFO DAGScheduler: Final stage: ResultStage 111 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:07:19 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:07:19 INFO DAGScheduler: Missing parents: List()
26/01/04 17:07:19 INFO DAGScheduler: Submitting ResultStage 111 (MapPartitionsRDD[557] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:07:19 INFO MemoryStore: Block broadcast_164 stored as values in memory (estimated size 52.9 KiB, free 434.3 MiB)
26/01/04 17:07:19 INFO MemoryStore: Block broadcast_164_piece0 stored as bytes in memory (estimated size 20.2 KiB, free 434.3 MiB)
26/01/04 17:07:19 INFO BlockManagerInfo: Added broadcast_164_piece0 in memory on spark-master:33535 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:07:19 INFO SparkContext: Created broadcast 164 from broadcast at DAGScheduler.scala:1585
26/01/04 17:07:19 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 111 (MapPartitionsRDD[557] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/01/04 17:07:19 INFO TaskSchedulerImpl: Adding task set 111.0 with 1 tasks resource profile 0
26/01/04 17:07:19 INFO TaskSetManager: Starting task 0.0 in stage 111.0 (TID 202) (172.18.0.13, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:07:19 INFO BlockManagerInfo: Added broadcast_164_piece0 in memory on 172.18.0.13:44401 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:07:19 INFO BlockManagerInfo: Added broadcast_163_piece0 in memory on 172.18.0.13:44401 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:07:20 INFO TaskSetManager: Finished task 0.0 in stage 111.0 (TID 202) in 597 ms on 172.18.0.13 (executor 0) (1/1)
26/01/04 17:07:20 INFO TaskSchedulerImpl: Removed TaskSet 111.0, whose tasks have all completed, from pool 
26/01/04 17:07:20 INFO DAGScheduler: ResultStage 111 (start at NativeMethodAccessorImpl.java:0) finished in 0.603 s
26/01/04 17:07:20 INFO DAGScheduler: Job 110 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:07:20 INFO TaskSchedulerImpl: Killing all running tasks in stage 111: Stage finished
26/01/04 17:07:20 INFO DAGScheduler: Job 110 finished: start at NativeMethodAccessorImpl.java:0, took 0.605179 s
26/01/04 17:07:20 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 53, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@3168f88c] is committing.
26/01/04 17:07:20 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 53, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@3168f88c] committed.
26/01/04 17:07:20 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/53 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.53.d8b2fdce-37a9-4af9-a97b-f9adb7c3d15d.tmp
26/01/04 17:07:20 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.53.d8b2fdce-37a9-4af9-a97b-f9adb7c3d15d.tmp to file:/tmp/spark-checkpoint-enrichment/commits/53
26/01/04 17:07:20 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T17:07:19.055Z",
  "batchId" : 53,
  "numInputRows" : 30,
  "inputRowsPerSecond" : 2142.8571428571427,
  "processedRowsPerSecond" : 28.222013170272813,
  "durationMs" : {
    "addBatch" : 840,
    "commitOffsets" : 78,
    "getBatch" : 1,
    "latestOffset" : 3,
    "queryPlanning" : 56,
    "triggerExecution" : 1063,
    "walCommit" : 84
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "0" : 879
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "0" : 909
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "0" : 909
      }
    },
    "numInputRows" : 30,
    "inputRowsPerSecond" : 2142.8571428571427,
    "processedRowsPerSecond" : 28.222013170272813,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 31
  }
}
26/01/04 17:07:30 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/54 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.54.b552ab60-09aa-4a2b-b324-797d8182d9b4.tmp
26/01/04 17:07:30 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.54.b552ab60-09aa-4a2b-b324-797d8182d9b4.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/54
26/01/04 17:07:30 INFO MicroBatchExecution: Committed offsets for batch 54. Metadata OffsetSeqMetadata(0,1767546450083,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 17:07:30 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:07:30 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:07:30 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:07:30 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:07:30 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:07:30 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:07:30 INFO BlockManagerInfo: Removed broadcast_162_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:07:30 INFO BlockManagerInfo: Removed broadcast_162_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:07:30 INFO BlockManagerInfo: Removed broadcast_162_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:07:30 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#44899 - airline_prefix.nullCount#44898) > 0)
26/01/04 17:07:30 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#44934 - min_flight_num.nullCount#44933) > 0)
26/01/04 17:07:30 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#44929 - max_flight_num.nullCount#44928) > 0)
26/01/04 17:07:30 INFO BlockManagerInfo: Removed broadcast_160_piece0 on spark-master:33535 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:07:30 INFO BlockManagerInfo: Removed broadcast_160_piece0 on 172.18.0.13:44401 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:07:30 INFO BlockManagerInfo: Removed broadcast_163_piece0 on spark-master:33535 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:07:30 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:07:30 INFO BlockManagerInfo: Removed broadcast_163_piece0 on 172.18.0.13:44401 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:07:30 INFO DAGScheduler: Got job 111 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 17:07:30 INFO DAGScheduler: Final stage: ResultStage 112 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:07:30 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:07:30 INFO DAGScheduler: Missing parents: List()
26/01/04 17:07:30 INFO DAGScheduler: Submitting ResultStage 112 (MapPartitionsRDD[562] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:07:30 INFO MemoryStore: Block broadcast_165 stored as values in memory (estimated size 38.5 KiB, free 434.3 MiB)
26/01/04 17:07:30 INFO MemoryStore: Block broadcast_165_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.3 MiB)
26/01/04 17:07:30 INFO BlockManagerInfo: Added broadcast_165_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:07:30 INFO SparkContext: Created broadcast 165 from broadcast at DAGScheduler.scala:1585
26/01/04 17:07:30 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 112 (MapPartitionsRDD[562] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 17:07:30 INFO TaskSchedulerImpl: Adding task set 112.0 with 2 tasks resource profile 0
26/01/04 17:07:30 INFO BlockManagerInfo: Removed broadcast_164_piece0 on spark-master:33535 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:07:30 INFO TaskSetManager: Starting task 0.0 in stage 112.0 (TID 203) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 17:07:30 INFO BlockManagerInfo: Removed broadcast_164_piece0 on 172.18.0.13:44401 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:07:30 INFO TaskSetManager: Starting task 1.0 in stage 112.0 (TID 204) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 17:07:30 INFO BlockManagerInfo: Added broadcast_165_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:07:30 INFO BlockManagerInfo: Added broadcast_165_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:07:30 INFO TaskSetManager: Finished task 0.0 in stage 112.0 (TID 203) in 90 ms on 172.18.0.14 (executor 1) (1/2)
26/01/04 17:07:30 INFO TaskSetManager: Finished task 1.0 in stage 112.0 (TID 204) in 89 ms on 172.18.0.13 (executor 0) (2/2)
26/01/04 17:07:30 INFO TaskSchedulerImpl: Removed TaskSet 112.0, whose tasks have all completed, from pool 
26/01/04 17:07:30 INFO DAGScheduler: ResultStage 112 (start at NativeMethodAccessorImpl.java:0) finished in 0.112 s
26/01/04 17:07:30 INFO DAGScheduler: Job 111 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:07:30 INFO TaskSchedulerImpl: Killing all running tasks in stage 112: Stage finished
26/01/04 17:07:30 INFO DAGScheduler: Job 111 finished: start at NativeMethodAccessorImpl.java:0, took 0.118193 s
26/01/04 17:07:30 INFO MemoryStore: Block broadcast_166_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.3 MiB)
26/01/04 17:07:30 INFO BlockManagerInfo: Added broadcast_166_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:07:30 INFO SparkContext: Created broadcast 166 from start at NativeMethodAccessorImpl.java:0
26/01/04 17:07:30 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 54, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@71e84b0b]. The input RDD has 1 partitions.
26/01/04 17:07:30 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:07:30 INFO DAGScheduler: Got job 112 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/01/04 17:07:30 INFO DAGScheduler: Final stage: ResultStage 113 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:07:30 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:07:30 INFO DAGScheduler: Missing parents: List()
26/01/04 17:07:30 INFO DAGScheduler: Submitting ResultStage 113 (MapPartitionsRDD[567] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:07:30 INFO MemoryStore: Block broadcast_167 stored as values in memory (estimated size 52.9 KiB, free 434.3 MiB)
26/01/04 17:07:30 INFO MemoryStore: Block broadcast_167_piece0 stored as bytes in memory (estimated size 20.2 KiB, free 434.3 MiB)
26/01/04 17:07:30 INFO BlockManagerInfo: Added broadcast_167_piece0 in memory on spark-master:33535 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:07:30 INFO SparkContext: Created broadcast 167 from broadcast at DAGScheduler.scala:1585
26/01/04 17:07:30 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 113 (MapPartitionsRDD[567] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/01/04 17:07:30 INFO TaskSchedulerImpl: Adding task set 113.0 with 1 tasks resource profile 0
26/01/04 17:07:30 INFO TaskSetManager: Starting task 0.0 in stage 113.0 (TID 205) (172.18.0.13, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:07:30 INFO BlockManagerInfo: Added broadcast_167_piece0 in memory on 172.18.0.13:44401 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:07:30 INFO BlockManagerInfo: Added broadcast_166_piece0 in memory on 172.18.0.13:44401 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:07:31 INFO TaskSetManager: Finished task 0.0 in stage 113.0 (TID 205) in 679 ms on 172.18.0.13 (executor 0) (1/1)
26/01/04 17:07:31 INFO TaskSchedulerImpl: Removed TaskSet 113.0, whose tasks have all completed, from pool 
26/01/04 17:07:31 INFO DAGScheduler: ResultStage 113 (start at NativeMethodAccessorImpl.java:0) finished in 0.686 s
26/01/04 17:07:31 INFO DAGScheduler: Job 112 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:07:31 INFO TaskSchedulerImpl: Killing all running tasks in stage 113: Stage finished
26/01/04 17:07:31 INFO DAGScheduler: Job 112 finished: start at NativeMethodAccessorImpl.java:0, took 0.689617 s
26/01/04 17:07:31 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 54, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@71e84b0b] is committing.
26/01/04 17:07:31 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 54, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@71e84b0b] committed.
26/01/04 17:07:31 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/54 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.54.63ffe491-8a99-467b-8bd7-346451e48575.tmp
26/01/04 17:07:31 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.54.63ffe491-8a99-467b-8bd7-346451e48575.tmp to file:/tmp/spark-checkpoint-enrichment/commits/54
26/01/04 17:07:31 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T17:07:30.081Z",
  "batchId" : 54,
  "numInputRows" : 30,
  "inputRowsPerSecond" : 2307.6923076923076,
  "processedRowsPerSecond" : 26.269702276707534,
  "durationMs" : {
    "addBatch" : 921,
    "commitOffsets" : 121,
    "getBatch" : 0,
    "latestOffset" : 2,
    "queryPlanning" : 22,
    "triggerExecution" : 1142,
    "walCommit" : 75
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "0" : 909
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "0" : 939
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "0" : 939
      }
    },
    "numInputRows" : 30,
    "inputRowsPerSecond" : 2307.6923076923076,
    "processedRowsPerSecond" : 26.269702276707534,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 31
  }
}
26/01/04 17:07:41 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/55 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.55.4bde3fd5-615b-4813-8b19-1cf6c38762e7.tmp
26/01/04 17:07:41 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.55.4bde3fd5-615b-4813-8b19-1cf6c38762e7.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/55
26/01/04 17:07:41 INFO MicroBatchExecution: Committed offsets for batch 55. Metadata OffsetSeqMetadata(0,1767546461098,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 17:07:41 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:07:41 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:07:41 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:07:41 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:07:41 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:07:41 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:07:41 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#45703 - airline_prefix.nullCount#45702) > 0)
26/01/04 17:07:41 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#45738 - min_flight_num.nullCount#45737) > 0)
26/01/04 17:07:41 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#45733 - max_flight_num.nullCount#45732) > 0)
26/01/04 17:07:41 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:07:41 INFO DAGScheduler: Got job 113 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 17:07:41 INFO DAGScheduler: Final stage: ResultStage 114 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:07:41 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:07:41 INFO DAGScheduler: Missing parents: List()
26/01/04 17:07:41 INFO DAGScheduler: Submitting ResultStage 114 (MapPartitionsRDD[572] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:07:41 INFO MemoryStore: Block broadcast_168 stored as values in memory (estimated size 38.5 KiB, free 434.2 MiB)
26/01/04 17:07:41 INFO MemoryStore: Block broadcast_168_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.2 MiB)
26/01/04 17:07:41 INFO BlockManagerInfo: Added broadcast_168_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:07:41 INFO SparkContext: Created broadcast 168 from broadcast at DAGScheduler.scala:1585
26/01/04 17:07:41 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 114 (MapPartitionsRDD[572] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 17:07:41 INFO TaskSchedulerImpl: Adding task set 114.0 with 2 tasks resource profile 0
26/01/04 17:07:41 INFO TaskSetManager: Starting task 0.0 in stage 114.0 (TID 206) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 17:07:41 INFO TaskSetManager: Starting task 1.0 in stage 114.0 (TID 207) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 17:07:41 INFO BlockManagerInfo: Added broadcast_168_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:07:41 INFO BlockManagerInfo: Added broadcast_168_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:07:41 INFO TaskSetManager: Finished task 1.0 in stage 114.0 (TID 207) in 35 ms on 172.18.0.13 (executor 0) (1/2)
26/01/04 17:07:41 INFO TaskSetManager: Finished task 0.0 in stage 114.0 (TID 206) in 51 ms on 172.18.0.14 (executor 1) (2/2)
26/01/04 17:07:41 INFO TaskSchedulerImpl: Removed TaskSet 114.0, whose tasks have all completed, from pool 
26/01/04 17:07:41 INFO DAGScheduler: ResultStage 114 (start at NativeMethodAccessorImpl.java:0) finished in 0.059 s
26/01/04 17:07:41 INFO DAGScheduler: Job 113 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:07:41 INFO TaskSchedulerImpl: Killing all running tasks in stage 114: Stage finished
26/01/04 17:07:41 INFO DAGScheduler: Job 113 finished: start at NativeMethodAccessorImpl.java:0, took 0.063454 s
26/01/04 17:07:41 INFO MemoryStore: Block broadcast_169_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.2 MiB)
26/01/04 17:07:41 INFO BlockManagerInfo: Added broadcast_169_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:07:41 INFO SparkContext: Created broadcast 169 from start at NativeMethodAccessorImpl.java:0
26/01/04 17:07:41 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 55, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@3d3379e4]. The input RDD has 1 partitions.
26/01/04 17:07:41 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:07:41 INFO DAGScheduler: Got job 114 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/01/04 17:07:41 INFO DAGScheduler: Final stage: ResultStage 115 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:07:41 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:07:41 INFO DAGScheduler: Missing parents: List()
26/01/04 17:07:41 INFO DAGScheduler: Submitting ResultStage 115 (MapPartitionsRDD[577] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:07:41 INFO MemoryStore: Block broadcast_170 stored as values in memory (estimated size 52.9 KiB, free 434.2 MiB)
26/01/04 17:07:41 INFO MemoryStore: Block broadcast_170_piece0 stored as bytes in memory (estimated size 20.2 KiB, free 434.1 MiB)
26/01/04 17:07:41 INFO BlockManagerInfo: Added broadcast_170_piece0 in memory on spark-master:33535 (size: 20.2 KiB, free: 434.3 MiB)
26/01/04 17:07:41 INFO SparkContext: Created broadcast 170 from broadcast at DAGScheduler.scala:1585
26/01/04 17:07:41 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 115 (MapPartitionsRDD[577] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/01/04 17:07:41 INFO TaskSchedulerImpl: Adding task set 115.0 with 1 tasks resource profile 0
26/01/04 17:07:41 INFO TaskSetManager: Starting task 0.0 in stage 115.0 (TID 208) (172.18.0.13, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:07:41 INFO BlockManagerInfo: Added broadcast_170_piece0 in memory on 172.18.0.13:44401 (size: 20.2 KiB, free: 434.3 MiB)
26/01/04 17:07:41 INFO BlockManagerInfo: Added broadcast_169_piece0 in memory on 172.18.0.13:44401 (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:07:41 INFO TaskSetManager: Finished task 0.0 in stage 115.0 (TID 208) in 575 ms on 172.18.0.13 (executor 0) (1/1)
26/01/04 17:07:41 INFO TaskSchedulerImpl: Removed TaskSet 115.0, whose tasks have all completed, from pool 
26/01/04 17:07:41 INFO DAGScheduler: ResultStage 115 (start at NativeMethodAccessorImpl.java:0) finished in 0.583 s
26/01/04 17:07:41 INFO DAGScheduler: Job 114 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:07:41 INFO TaskSchedulerImpl: Killing all running tasks in stage 115: Stage finished
26/01/04 17:07:41 INFO DAGScheduler: Job 114 finished: start at NativeMethodAccessorImpl.java:0, took 0.586341 s
26/01/04 17:07:41 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 55, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@3d3379e4] is committing.
26/01/04 17:07:41 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 55, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@3d3379e4] committed.
26/01/04 17:07:41 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/55 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.55.c0f575c3-cf13-48e4-b329-1c6c461a38a6.tmp
26/01/04 17:07:42 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.55.c0f575c3-cf13-48e4-b329-1c6c461a38a6.tmp to file:/tmp/spark-checkpoint-enrichment/commits/55
26/01/04 17:07:42 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T17:07:41.097Z",
  "batchId" : 55,
  "numInputRows" : 30,
  "inputRowsPerSecond" : 2500.0,
  "processedRowsPerSecond" : 32.32758620689655,
  "durationMs" : {
    "addBatch" : 740,
    "commitOffsets" : 90,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 31,
    "triggerExecution" : 928,
    "walCommit" : 65
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "0" : 939
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "0" : 969
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "0" : 969
      }
    },
    "numInputRows" : 30,
    "inputRowsPerSecond" : 2500.0,
    "processedRowsPerSecond" : 32.32758620689655,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 31
  }
}
26/01/04 17:07:43 INFO BlockManagerInfo: Removed broadcast_167_piece0 on spark-master:33535 in memory (size: 20.2 KiB, free: 434.3 MiB)
26/01/04 17:07:43 INFO BlockManagerInfo: Removed broadcast_167_piece0 on 172.18.0.13:44401 in memory (size: 20.2 KiB, free: 434.3 MiB)
26/01/04 17:07:43 INFO BlockManagerInfo: Removed broadcast_166_piece0 on spark-master:33535 in memory (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:07:43 INFO BlockManagerInfo: Removed broadcast_166_piece0 on 172.18.0.13:44401 in memory (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:07:43 INFO BlockManagerInfo: Removed broadcast_165_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:07:43 INFO BlockManagerInfo: Removed broadcast_165_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:07:43 INFO BlockManagerInfo: Removed broadcast_165_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:07:43 INFO BlockManagerInfo: Removed broadcast_168_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:07:43 INFO BlockManagerInfo: Removed broadcast_168_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:07:43 INFO BlockManagerInfo: Removed broadcast_168_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:07:43 INFO BlockManagerInfo: Removed broadcast_170_piece0 on spark-master:33535 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:07:43 INFO BlockManagerInfo: Removed broadcast_170_piece0 on 172.18.0.13:44401 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:07:52 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/01/04 17:07:52 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/56 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.56.99312ed1-1e80-4f37-a20b-a22a4cd338ce.tmp
26/01/04 17:07:52 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.56.99312ed1-1e80-4f37-a20b-a22a4cd338ce.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/56
26/01/04 17:07:52 INFO MicroBatchExecution: Committed offsets for batch 56. Metadata OffsetSeqMetadata(0,1767546472118,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 17:07:52 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:07:52 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:07:52 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:07:52 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:07:52 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:07:52 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:07:52 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#46507 - airline_prefix.nullCount#46506) > 0)
26/01/04 17:07:52 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#46542 - min_flight_num.nullCount#46541) > 0)
26/01/04 17:07:52 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#46537 - max_flight_num.nullCount#46536) > 0)
26/01/04 17:07:52 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:07:52 INFO DAGScheduler: Got job 115 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 17:07:52 INFO DAGScheduler: Final stage: ResultStage 116 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:07:52 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:07:52 INFO DAGScheduler: Missing parents: List()
26/01/04 17:07:52 INFO DAGScheduler: Submitting ResultStage 116 (MapPartitionsRDD[582] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:07:52 INFO MemoryStore: Block broadcast_171 stored as values in memory (estimated size 38.5 KiB, free 434.4 MiB)
26/01/04 17:07:52 INFO MemoryStore: Block broadcast_171_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.3 MiB)
26/01/04 17:07:52 INFO BlockManagerInfo: Added broadcast_171_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:07:52 INFO SparkContext: Created broadcast 171 from broadcast at DAGScheduler.scala:1585
26/01/04 17:07:52 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 116 (MapPartitionsRDD[582] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 17:07:52 INFO TaskSchedulerImpl: Adding task set 116.0 with 2 tasks resource profile 0
26/01/04 17:07:52 INFO TaskSetManager: Starting task 0.0 in stage 116.0 (TID 209) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 17:07:52 INFO TaskSetManager: Starting task 1.0 in stage 116.0 (TID 210) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 17:07:52 INFO BlockManagerInfo: Added broadcast_171_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:07:52 INFO BlockManagerInfo: Added broadcast_171_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:07:52 INFO TaskSetManager: Finished task 1.0 in stage 116.0 (TID 210) in 38 ms on 172.18.0.13 (executor 0) (1/2)
26/01/04 17:07:52 INFO TaskSetManager: Finished task 0.0 in stage 116.0 (TID 209) in 53 ms on 172.18.0.14 (executor 1) (2/2)
26/01/04 17:07:52 INFO TaskSchedulerImpl: Removed TaskSet 116.0, whose tasks have all completed, from pool 
26/01/04 17:07:52 INFO DAGScheduler: ResultStage 116 (start at NativeMethodAccessorImpl.java:0) finished in 0.064 s
26/01/04 17:07:52 INFO DAGScheduler: Job 115 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:07:52 INFO TaskSchedulerImpl: Killing all running tasks in stage 116: Stage finished
26/01/04 17:07:52 INFO DAGScheduler: Job 115 finished: start at NativeMethodAccessorImpl.java:0, took 0.067884 s
26/01/04 17:07:52 INFO MemoryStore: Block broadcast_172_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.3 MiB)
26/01/04 17:07:52 INFO BlockManagerInfo: Added broadcast_172_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:07:52 INFO SparkContext: Created broadcast 172 from start at NativeMethodAccessorImpl.java:0
26/01/04 17:07:52 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 56, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@55a2c278]. The input RDD has 1 partitions.
26/01/04 17:07:52 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:07:52 INFO DAGScheduler: Got job 116 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/01/04 17:07:52 INFO DAGScheduler: Final stage: ResultStage 117 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:07:52 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:07:52 INFO DAGScheduler: Missing parents: List()
26/01/04 17:07:52 INFO DAGScheduler: Submitting ResultStage 117 (MapPartitionsRDD[587] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:07:52 INFO MemoryStore: Block broadcast_173 stored as values in memory (estimated size 52.9 KiB, free 434.3 MiB)
26/01/04 17:07:52 INFO MemoryStore: Block broadcast_173_piece0 stored as bytes in memory (estimated size 20.2 KiB, free 434.3 MiB)
26/01/04 17:07:52 INFO BlockManagerInfo: Added broadcast_173_piece0 in memory on spark-master:33535 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:07:52 INFO SparkContext: Created broadcast 173 from broadcast at DAGScheduler.scala:1585
26/01/04 17:07:52 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 117 (MapPartitionsRDD[587] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/01/04 17:07:52 INFO TaskSchedulerImpl: Adding task set 117.0 with 1 tasks resource profile 0
26/01/04 17:07:52 INFO TaskSetManager: Starting task 0.0 in stage 117.0 (TID 211) (172.18.0.13, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:07:52 INFO BlockManagerInfo: Added broadcast_173_piece0 in memory on 172.18.0.13:44401 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:07:52 INFO BlockManagerInfo: Added broadcast_172_piece0 in memory on 172.18.0.13:44401 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:07:52 INFO TaskSetManager: Finished task 0.0 in stage 117.0 (TID 211) in 593 ms on 172.18.0.13 (executor 0) (1/1)
26/01/04 17:07:52 INFO TaskSchedulerImpl: Removed TaskSet 117.0, whose tasks have all completed, from pool 
26/01/04 17:07:52 INFO DAGScheduler: ResultStage 117 (start at NativeMethodAccessorImpl.java:0) finished in 0.599 s
26/01/04 17:07:52 INFO DAGScheduler: Job 116 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:07:52 INFO TaskSchedulerImpl: Killing all running tasks in stage 117: Stage finished
26/01/04 17:07:52 INFO DAGScheduler: Job 116 finished: start at NativeMethodAccessorImpl.java:0, took 0.600412 s
26/01/04 17:07:52 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 56, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@55a2c278] is committing.
26/01/04 17:07:52 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 56, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@55a2c278] committed.
26/01/04 17:07:53 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/56 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.56.322e84a4-6cf9-42b5-a52a-bb92ba7cfae1.tmp
26/01/04 17:07:53 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.56.322e84a4-6cf9-42b5-a52a-bb92ba7cfae1.tmp to file:/tmp/spark-checkpoint-enrichment/commits/56
26/01/04 17:07:53 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T17:07:52.115Z",
  "batchId" : 56,
  "numInputRows" : 30,
  "inputRowsPerSecond" : 2500.0,
  "processedRowsPerSecond" : 31.41361256544503,
  "durationMs" : {
    "addBatch" : 748,
    "commitOffsets" : 87,
    "getBatch" : 0,
    "latestOffset" : 3,
    "queryPlanning" : 31,
    "triggerExecution" : 955,
    "walCommit" : 86
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "0" : 969
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "0" : 999
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "0" : 999
      }
    },
    "numInputRows" : 30,
    "inputRowsPerSecond" : 2500.0,
    "processedRowsPerSecond" : 31.41361256544503,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 31
  }
}
26/01/04 17:08:02 INFO BlockManagerInfo: Removed broadcast_173_piece0 on spark-master:33535 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:08:02 INFO BlockManagerInfo: Removed broadcast_173_piece0 on 172.18.0.13:44401 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:08:02 INFO BlockManagerInfo: Removed broadcast_171_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:08:02 INFO BlockManagerInfo: Removed broadcast_171_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:08:02 INFO BlockManagerInfo: Removed broadcast_171_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:08:02 INFO BlockManagerInfo: Removed broadcast_169_piece0 on spark-master:33535 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:08:02 INFO BlockManagerInfo: Removed broadcast_169_piece0 on 172.18.0.13:44401 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:08:03 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/01/04 17:08:03 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/57 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.57.70fc921a-d4b8-4400-a7d2-f1a472996cb7.tmp
26/01/04 17:08:03 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.57.70fc921a-d4b8-4400-a7d2-f1a472996cb7.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/57
26/01/04 17:08:03 INFO MicroBatchExecution: Committed offsets for batch 57. Metadata OffsetSeqMetadata(0,1767546483126,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 17:08:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:08:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:08:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:08:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:08:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:08:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:08:03 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#47311 - airline_prefix.nullCount#47310) > 0)
26/01/04 17:08:03 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#47346 - min_flight_num.nullCount#47345) > 0)
26/01/04 17:08:03 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#47341 - max_flight_num.nullCount#47340) > 0)
26/01/04 17:08:03 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:08:03 INFO DAGScheduler: Got job 117 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 17:08:03 INFO DAGScheduler: Final stage: ResultStage 118 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:08:03 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:08:03 INFO DAGScheduler: Missing parents: List()
26/01/04 17:08:03 INFO DAGScheduler: Submitting ResultStage 118 (MapPartitionsRDD[592] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:08:03 INFO MemoryStore: Block broadcast_174 stored as values in memory (estimated size 38.5 KiB, free 434.4 MiB)
26/01/04 17:08:03 INFO MemoryStore: Block broadcast_174_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.3 MiB)
26/01/04 17:08:03 INFO BlockManagerInfo: Added broadcast_174_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:08:03 INFO BlockManagerInfo: Removed broadcast_172_piece0 on spark-master:33535 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:08:03 INFO SparkContext: Created broadcast 174 from broadcast at DAGScheduler.scala:1585
26/01/04 17:08:03 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 118 (MapPartitionsRDD[592] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 17:08:03 INFO TaskSchedulerImpl: Adding task set 118.0 with 2 tasks resource profile 0
26/01/04 17:08:03 INFO TaskSetManager: Starting task 0.0 in stage 118.0 (TID 212) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 17:08:03 INFO TaskSetManager: Starting task 1.0 in stage 118.0 (TID 213) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 17:08:03 INFO BlockManagerInfo: Removed broadcast_172_piece0 on 172.18.0.13:44401 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:08:03 INFO BlockManagerInfo: Added broadcast_174_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:08:03 INFO BlockManagerInfo: Added broadcast_174_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:08:03 INFO TaskSetManager: Finished task 1.0 in stage 118.0 (TID 213) in 41 ms on 172.18.0.13 (executor 0) (1/2)
26/01/04 17:08:03 INFO TaskSetManager: Finished task 0.0 in stage 118.0 (TID 212) in 55 ms on 172.18.0.14 (executor 1) (2/2)
26/01/04 17:08:03 INFO TaskSchedulerImpl: Removed TaskSet 118.0, whose tasks have all completed, from pool 
26/01/04 17:08:03 INFO DAGScheduler: ResultStage 118 (start at NativeMethodAccessorImpl.java:0) finished in 0.081 s
26/01/04 17:08:03 INFO DAGScheduler: Job 117 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:08:03 INFO TaskSchedulerImpl: Killing all running tasks in stage 118: Stage finished
26/01/04 17:08:03 INFO DAGScheduler: Job 117 finished: start at NativeMethodAccessorImpl.java:0, took 0.084260 s
26/01/04 17:08:03 INFO MemoryStore: Block broadcast_175_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.3 MiB)
26/01/04 17:08:03 INFO BlockManagerInfo: Added broadcast_175_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:08:03 INFO SparkContext: Created broadcast 175 from start at NativeMethodAccessorImpl.java:0
26/01/04 17:08:03 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 57, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@4c5f24bf]. The input RDD has 1 partitions.
26/01/04 17:08:03 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:08:03 INFO DAGScheduler: Got job 118 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/01/04 17:08:03 INFO DAGScheduler: Final stage: ResultStage 119 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:08:03 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:08:03 INFO DAGScheduler: Missing parents: List()
26/01/04 17:08:03 INFO DAGScheduler: Submitting ResultStage 119 (MapPartitionsRDD[597] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:08:03 INFO MemoryStore: Block broadcast_176 stored as values in memory (estimated size 52.9 KiB, free 434.3 MiB)
26/01/04 17:08:03 INFO MemoryStore: Block broadcast_176_piece0 stored as bytes in memory (estimated size 20.2 KiB, free 434.3 MiB)
26/01/04 17:08:03 INFO BlockManagerInfo: Added broadcast_176_piece0 in memory on spark-master:33535 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:08:03 INFO SparkContext: Created broadcast 176 from broadcast at DAGScheduler.scala:1585
26/01/04 17:08:03 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 119 (MapPartitionsRDD[597] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/01/04 17:08:03 INFO TaskSchedulerImpl: Adding task set 119.0 with 1 tasks resource profile 0
26/01/04 17:08:03 INFO TaskSetManager: Starting task 0.0 in stage 119.0 (TID 214) (172.18.0.13, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:08:03 INFO BlockManagerInfo: Added broadcast_176_piece0 in memory on 172.18.0.13:44401 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:08:03 INFO BlockManagerInfo: Added broadcast_175_piece0 in memory on 172.18.0.13:44401 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:08:03 INFO TaskSetManager: Finished task 0.0 in stage 119.0 (TID 214) in 571 ms on 172.18.0.13 (executor 0) (1/1)
26/01/04 17:08:03 INFO TaskSchedulerImpl: Removed TaskSet 119.0, whose tasks have all completed, from pool 
26/01/04 17:08:03 INFO DAGScheduler: ResultStage 119 (start at NativeMethodAccessorImpl.java:0) finished in 0.578 s
26/01/04 17:08:03 INFO DAGScheduler: Job 118 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:08:03 INFO TaskSchedulerImpl: Killing all running tasks in stage 119: Stage finished
26/01/04 17:08:03 INFO DAGScheduler: Job 118 finished: start at NativeMethodAccessorImpl.java:0, took 0.580753 s
26/01/04 17:08:03 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 57, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@4c5f24bf] is committing.
26/01/04 17:08:03 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 57, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@4c5f24bf] committed.
26/01/04 17:08:03 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/57 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.57.ccc32164-4722-4ba4-a92b-52a4a0ba0286.tmp
26/01/04 17:08:04 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.57.ccc32164-4722-4ba4-a92b-52a4a0ba0286.tmp to file:/tmp/spark-checkpoint-enrichment/commits/57
26/01/04 17:08:04 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T17:08:03.124Z",
  "batchId" : 57,
  "numInputRows" : 30,
  "inputRowsPerSecond" : 2307.6923076923076,
  "processedRowsPerSecond" : 32.608695652173914,
  "durationMs" : {
    "addBatch" : 743,
    "commitOffsets" : 71,
    "getBatch" : 0,
    "latestOffset" : 2,
    "queryPlanning" : 20,
    "triggerExecution" : 920,
    "walCommit" : 82
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "0" : 999
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "0" : 1029
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "0" : 1029
      }
    },
    "numInputRows" : 30,
    "inputRowsPerSecond" : 2307.6923076923076,
    "processedRowsPerSecond" : 32.608695652173914,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 31
  }
}
26/01/04 17:08:14 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/01/04 17:08:14 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/58 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.58.1ae66cd5-4264-4287-a0db-564e9e014d72.tmp
26/01/04 17:08:14 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.58.1ae66cd5-4264-4287-a0db-564e9e014d72.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/58
26/01/04 17:08:14 INFO MicroBatchExecution: Committed offsets for batch 58. Metadata OffsetSeqMetadata(0,1767546494142,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 17:08:14 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:08:14 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:08:14 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:08:14 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:08:14 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:08:14 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:08:14 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#48115 - airline_prefix.nullCount#48114) > 0)
26/01/04 17:08:14 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#48150 - min_flight_num.nullCount#48149) > 0)
26/01/04 17:08:14 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#48145 - max_flight_num.nullCount#48144) > 0)
26/01/04 17:08:14 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:08:14 INFO DAGScheduler: Got job 119 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 17:08:14 INFO DAGScheduler: Final stage: ResultStage 120 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:08:14 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:08:14 INFO DAGScheduler: Missing parents: List()
26/01/04 17:08:14 INFO DAGScheduler: Submitting ResultStage 120 (MapPartitionsRDD[602] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:08:14 INFO MemoryStore: Block broadcast_177 stored as values in memory (estimated size 38.5 KiB, free 434.2 MiB)
26/01/04 17:08:14 INFO MemoryStore: Block broadcast_177_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.2 MiB)
26/01/04 17:08:14 INFO BlockManagerInfo: Added broadcast_177_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:08:14 INFO SparkContext: Created broadcast 177 from broadcast at DAGScheduler.scala:1585
26/01/04 17:08:14 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 120 (MapPartitionsRDD[602] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 17:08:14 INFO TaskSchedulerImpl: Adding task set 120.0 with 2 tasks resource profile 0
26/01/04 17:08:14 INFO TaskSetManager: Starting task 0.0 in stage 120.0 (TID 215) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 17:08:14 INFO TaskSetManager: Starting task 1.0 in stage 120.0 (TID 216) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 17:08:14 INFO BlockManagerInfo: Added broadcast_177_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:08:14 INFO BlockManagerInfo: Added broadcast_177_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:08:14 INFO TaskSetManager: Finished task 1.0 in stage 120.0 (TID 216) in 32 ms on 172.18.0.13 (executor 0) (1/2)
26/01/04 17:08:14 INFO TaskSetManager: Finished task 0.0 in stage 120.0 (TID 215) in 35 ms on 172.18.0.14 (executor 1) (2/2)
26/01/04 17:08:14 INFO TaskSchedulerImpl: Removed TaskSet 120.0, whose tasks have all completed, from pool 
26/01/04 17:08:14 INFO DAGScheduler: ResultStage 120 (start at NativeMethodAccessorImpl.java:0) finished in 0.044 s
26/01/04 17:08:14 INFO DAGScheduler: Job 119 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:08:14 INFO TaskSchedulerImpl: Killing all running tasks in stage 120: Stage finished
26/01/04 17:08:14 INFO DAGScheduler: Job 119 finished: start at NativeMethodAccessorImpl.java:0, took 0.049227 s
26/01/04 17:08:14 INFO MemoryStore: Block broadcast_178_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.2 MiB)
26/01/04 17:08:14 INFO BlockManagerInfo: Added broadcast_178_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:08:14 INFO SparkContext: Created broadcast 178 from start at NativeMethodAccessorImpl.java:0
26/01/04 17:08:14 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 58, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@5a84cd25]. The input RDD has 1 partitions.
26/01/04 17:08:14 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:08:14 INFO DAGScheduler: Got job 120 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/01/04 17:08:14 INFO DAGScheduler: Final stage: ResultStage 121 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:08:14 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:08:14 INFO DAGScheduler: Missing parents: List()
26/01/04 17:08:14 INFO DAGScheduler: Submitting ResultStage 121 (MapPartitionsRDD[607] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:08:14 INFO MemoryStore: Block broadcast_179 stored as values in memory (estimated size 52.9 KiB, free 434.2 MiB)
26/01/04 17:08:14 INFO MemoryStore: Block broadcast_179_piece0 stored as bytes in memory (estimated size 20.2 KiB, free 434.1 MiB)
26/01/04 17:08:14 INFO BlockManagerInfo: Added broadcast_179_piece0 in memory on spark-master:33535 (size: 20.2 KiB, free: 434.3 MiB)
26/01/04 17:08:14 INFO SparkContext: Created broadcast 179 from broadcast at DAGScheduler.scala:1585
26/01/04 17:08:14 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 121 (MapPartitionsRDD[607] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/01/04 17:08:14 INFO TaskSchedulerImpl: Adding task set 121.0 with 1 tasks resource profile 0
26/01/04 17:08:14 INFO TaskSetManager: Starting task 0.0 in stage 121.0 (TID 217) (172.18.0.13, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:08:14 INFO BlockManagerInfo: Added broadcast_179_piece0 in memory on 172.18.0.13:44401 (size: 20.2 KiB, free: 434.3 MiB)
26/01/04 17:08:14 INFO BlockManagerInfo: Added broadcast_178_piece0 in memory on 172.18.0.13:44401 (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:08:15 INFO TaskSetManager: Finished task 0.0 in stage 121.0 (TID 217) in 583 ms on 172.18.0.13 (executor 0) (1/1)
26/01/04 17:08:15 INFO TaskSchedulerImpl: Removed TaskSet 121.0, whose tasks have all completed, from pool 
26/01/04 17:08:15 INFO DAGScheduler: ResultStage 121 (start at NativeMethodAccessorImpl.java:0) finished in 0.590 s
26/01/04 17:08:15 INFO DAGScheduler: Job 120 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:08:15 INFO TaskSchedulerImpl: Killing all running tasks in stage 121: Stage finished
26/01/04 17:08:15 INFO DAGScheduler: Job 120 finished: start at NativeMethodAccessorImpl.java:0, took 0.593064 s
26/01/04 17:08:15 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 58, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@5a84cd25] is committing.
26/01/04 17:08:15 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 58, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@5a84cd25] committed.
26/01/04 17:08:15 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/58 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.58.66f13493-76b6-414a-9bdf-7e769d0c1c0e.tmp
26/01/04 17:08:15 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.58.66f13493-76b6-414a-9bdf-7e769d0c1c0e.tmp to file:/tmp/spark-checkpoint-enrichment/commits/58
26/01/04 17:08:15 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T17:08:14.140Z",
  "batchId" : 58,
  "numInputRows" : 30,
  "inputRowsPerSecond" : 2500.0,
  "processedRowsPerSecond" : 31.54574132492114,
  "durationMs" : {
    "addBatch" : 740,
    "commitOffsets" : 68,
    "getBatch" : 0,
    "latestOffset" : 2,
    "queryPlanning" : 43,
    "triggerExecution" : 951,
    "walCommit" : 98
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "0" : 1029
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "0" : 1059
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "0" : 1059
      }
    },
    "numInputRows" : 30,
    "inputRowsPerSecond" : 2500.0,
    "processedRowsPerSecond" : 31.54574132492114,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 31
  }
}
26/01/04 17:08:17 INFO BlockManagerInfo: Removed broadcast_177_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:08:17 INFO BlockManagerInfo: Removed broadcast_177_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:08:17 INFO BlockManagerInfo: Removed broadcast_177_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:08:17 INFO BlockManagerInfo: Removed broadcast_175_piece0 on spark-master:33535 in memory (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:08:17 INFO BlockManagerInfo: Removed broadcast_175_piece0 on 172.18.0.13:44401 in memory (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:08:17 INFO BlockManagerInfo: Removed broadcast_176_piece0 on spark-master:33535 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:08:17 INFO BlockManagerInfo: Removed broadcast_176_piece0 on 172.18.0.13:44401 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:08:17 INFO BlockManagerInfo: Removed broadcast_174_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:08:17 INFO BlockManagerInfo: Removed broadcast_174_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:08:17 INFO BlockManagerInfo: Removed broadcast_174_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:08:17 INFO BlockManagerInfo: Removed broadcast_179_piece0 on spark-master:33535 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:08:17 INFO BlockManagerInfo: Removed broadcast_179_piece0 on 172.18.0.13:44401 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:08:25 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/01/04 17:08:25 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/59 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.59.51ac3eca-fb10-409b-bc67-63168aff053b.tmp
26/01/04 17:08:25 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.59.51ac3eca-fb10-409b-bc67-63168aff053b.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/59
26/01/04 17:08:25 INFO MicroBatchExecution: Committed offsets for batch 59. Metadata OffsetSeqMetadata(0,1767546505171,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 17:08:25 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:08:25 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:08:25 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:08:25 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:08:25 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:08:25 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:08:25 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#48919 - airline_prefix.nullCount#48918) > 0)
26/01/04 17:08:25 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#48954 - min_flight_num.nullCount#48953) > 0)
26/01/04 17:08:25 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#48949 - max_flight_num.nullCount#48948) > 0)
26/01/04 17:08:25 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:08:25 INFO DAGScheduler: Got job 121 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 17:08:25 INFO DAGScheduler: Final stage: ResultStage 122 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:08:25 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:08:25 INFO DAGScheduler: Missing parents: List()
26/01/04 17:08:25 INFO DAGScheduler: Submitting ResultStage 122 (MapPartitionsRDD[612] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:08:25 INFO MemoryStore: Block broadcast_180 stored as values in memory (estimated size 38.5 KiB, free 434.4 MiB)
26/01/04 17:08:25 INFO MemoryStore: Block broadcast_180_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.3 MiB)
26/01/04 17:08:25 INFO BlockManagerInfo: Added broadcast_180_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:08:25 INFO SparkContext: Created broadcast 180 from broadcast at DAGScheduler.scala:1585
26/01/04 17:08:25 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 122 (MapPartitionsRDD[612] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 17:08:25 INFO TaskSchedulerImpl: Adding task set 122.0 with 2 tasks resource profile 0
26/01/04 17:08:25 INFO TaskSetManager: Starting task 1.0 in stage 122.0 (TID 218) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 17:08:25 INFO TaskSetManager: Starting task 0.0 in stage 122.0 (TID 219) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 17:08:25 INFO BlockManagerInfo: Added broadcast_180_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:08:25 INFO BlockManagerInfo: Added broadcast_180_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:08:25 INFO TaskSetManager: Finished task 1.0 in stage 122.0 (TID 218) in 43 ms on 172.18.0.13 (executor 0) (1/2)
26/01/04 17:08:25 INFO TaskSetManager: Finished task 0.0 in stage 122.0 (TID 219) in 55 ms on 172.18.0.14 (executor 1) (2/2)
26/01/04 17:08:25 INFO DAGScheduler: ResultStage 122 (start at NativeMethodAccessorImpl.java:0) finished in 0.066 s
26/01/04 17:08:25 INFO DAGScheduler: Job 121 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:08:25 INFO TaskSchedulerImpl: Removed TaskSet 122.0, whose tasks have all completed, from pool 
26/01/04 17:08:25 INFO TaskSchedulerImpl: Killing all running tasks in stage 122: Stage finished
26/01/04 17:08:25 INFO DAGScheduler: Job 121 finished: start at NativeMethodAccessorImpl.java:0, took 0.070309 s
26/01/04 17:08:25 INFO MemoryStore: Block broadcast_181_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.3 MiB)
26/01/04 17:08:25 INFO BlockManagerInfo: Added broadcast_181_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:08:25 INFO SparkContext: Created broadcast 181 from start at NativeMethodAccessorImpl.java:0
26/01/04 17:08:25 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 59, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@b300933]. The input RDD has 1 partitions.
26/01/04 17:08:25 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:08:25 INFO DAGScheduler: Got job 122 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/01/04 17:08:25 INFO DAGScheduler: Final stage: ResultStage 123 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:08:25 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:08:25 INFO DAGScheduler: Missing parents: List()
26/01/04 17:08:25 INFO DAGScheduler: Submitting ResultStage 123 (MapPartitionsRDD[617] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:08:25 INFO MemoryStore: Block broadcast_182 stored as values in memory (estimated size 52.9 KiB, free 434.3 MiB)
26/01/04 17:08:25 INFO MemoryStore: Block broadcast_182_piece0 stored as bytes in memory (estimated size 20.2 KiB, free 434.3 MiB)
26/01/04 17:08:25 INFO BlockManagerInfo: Added broadcast_182_piece0 in memory on spark-master:33535 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:08:25 INFO SparkContext: Created broadcast 182 from broadcast at DAGScheduler.scala:1585
26/01/04 17:08:25 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 123 (MapPartitionsRDD[617] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/01/04 17:08:25 INFO TaskSchedulerImpl: Adding task set 123.0 with 1 tasks resource profile 0
26/01/04 17:08:25 INFO TaskSetManager: Starting task 0.0 in stage 123.0 (TID 220) (172.18.0.13, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:08:25 INFO BlockManagerInfo: Added broadcast_182_piece0 in memory on 172.18.0.13:44401 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:08:25 INFO BlockManagerInfo: Added broadcast_181_piece0 in memory on 172.18.0.13:44401 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:08:26 INFO TaskSetManager: Finished task 0.0 in stage 123.0 (TID 220) in 626 ms on 172.18.0.13 (executor 0) (1/1)
26/01/04 17:08:26 INFO TaskSchedulerImpl: Removed TaskSet 123.0, whose tasks have all completed, from pool 
26/01/04 17:08:26 INFO DAGScheduler: ResultStage 123 (start at NativeMethodAccessorImpl.java:0) finished in 0.640 s
26/01/04 17:08:26 INFO DAGScheduler: Job 122 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:08:26 INFO TaskSchedulerImpl: Killing all running tasks in stage 123: Stage finished
26/01/04 17:08:26 INFO DAGScheduler: Job 122 finished: start at NativeMethodAccessorImpl.java:0, took 0.644099 s
26/01/04 17:08:26 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 59, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@b300933] is committing.
26/01/04 17:08:26 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 59, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@b300933] committed.
26/01/04 17:08:26 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/59 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.59.eebd9b69-9236-4386-b97c-d4c70c2eb321.tmp
26/01/04 17:08:26 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.59.eebd9b69-9236-4386-b97c-d4c70c2eb321.tmp to file:/tmp/spark-checkpoint-enrichment/commits/59
26/01/04 17:08:26 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T17:08:25.169Z",
  "batchId" : 59,
  "numInputRows" : 30,
  "inputRowsPerSecond" : 2500.0,
  "processedRowsPerSecond" : 29.296875,
  "durationMs" : {
    "addBatch" : 802,
    "commitOffsets" : 121,
    "getBatch" : 0,
    "latestOffset" : 2,
    "queryPlanning" : 23,
    "triggerExecution" : 1024,
    "walCommit" : 76
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "0" : 1059
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "0" : 1089
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "0" : 1089
      }
    },
    "numInputRows" : 30,
    "inputRowsPerSecond" : 2500.0,
    "processedRowsPerSecond" : 29.296875,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 31
  }
}
26/01/04 17:08:35 INFO BlockManagerInfo: Removed broadcast_180_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:08:35 INFO BlockManagerInfo: Removed broadcast_180_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:08:35 INFO BlockManagerInfo: Removed broadcast_180_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:08:35 INFO BlockManagerInfo: Removed broadcast_182_piece0 on spark-master:33535 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:08:35 INFO BlockManagerInfo: Removed broadcast_182_piece0 on 172.18.0.13:44401 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:08:35 INFO BlockManagerInfo: Removed broadcast_178_piece0 on spark-master:33535 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:08:35 INFO BlockManagerInfo: Removed broadcast_178_piece0 on 172.18.0.13:44401 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:08:36 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/60 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.60.f4a374cf-2ab2-445e-92f3-b0969a1d4508.tmp
26/01/04 17:08:36 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.60.f4a374cf-2ab2-445e-92f3-b0969a1d4508.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/60
26/01/04 17:08:36 INFO MicroBatchExecution: Committed offsets for batch 60. Metadata OffsetSeqMetadata(0,1767546516192,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 17:08:36 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:08:36 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:08:36 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:08:36 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:08:36 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:08:36 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:08:36 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#49723 - airline_prefix.nullCount#49722) > 0)
26/01/04 17:08:36 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#49758 - min_flight_num.nullCount#49757) > 0)
26/01/04 17:08:36 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#49753 - max_flight_num.nullCount#49752) > 0)
26/01/04 17:08:36 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:08:36 INFO DAGScheduler: Got job 123 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 17:08:36 INFO DAGScheduler: Final stage: ResultStage 124 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:08:36 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:08:36 INFO DAGScheduler: Missing parents: List()
26/01/04 17:08:36 INFO DAGScheduler: Submitting ResultStage 124 (MapPartitionsRDD[622] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:08:36 INFO MemoryStore: Block broadcast_183 stored as values in memory (estimated size 38.5 KiB, free 434.4 MiB)
26/01/04 17:08:36 INFO MemoryStore: Block broadcast_183_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.3 MiB)
26/01/04 17:08:36 INFO BlockManagerInfo: Added broadcast_183_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:08:36 INFO SparkContext: Created broadcast 183 from broadcast at DAGScheduler.scala:1585
26/01/04 17:08:36 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 124 (MapPartitionsRDD[622] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 17:08:36 INFO TaskSchedulerImpl: Adding task set 124.0 with 2 tasks resource profile 0
26/01/04 17:08:36 INFO BlockManagerInfo: Removed broadcast_181_piece0 on spark-master:33535 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:08:36 INFO BlockManagerInfo: Removed broadcast_181_piece0 on 172.18.0.13:44401 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:08:36 INFO TaskSetManager: Starting task 0.0 in stage 124.0 (TID 221) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 17:08:36 INFO TaskSetManager: Starting task 1.0 in stage 124.0 (TID 222) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 17:08:36 INFO BlockManagerInfo: Added broadcast_183_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:08:36 INFO BlockManagerInfo: Added broadcast_183_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:08:36 INFO TaskSetManager: Finished task 1.0 in stage 124.0 (TID 222) in 39 ms on 172.18.0.13 (executor 0) (1/2)
26/01/04 17:08:36 INFO TaskSetManager: Finished task 0.0 in stage 124.0 (TID 221) in 68 ms on 172.18.0.14 (executor 1) (2/2)
26/01/04 17:08:36 INFO TaskSchedulerImpl: Removed TaskSet 124.0, whose tasks have all completed, from pool 
26/01/04 17:08:36 INFO DAGScheduler: ResultStage 124 (start at NativeMethodAccessorImpl.java:0) finished in 0.082 s
26/01/04 17:08:36 INFO DAGScheduler: Job 123 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:08:36 INFO TaskSchedulerImpl: Killing all running tasks in stage 124: Stage finished
26/01/04 17:08:36 INFO DAGScheduler: Job 123 finished: start at NativeMethodAccessorImpl.java:0, took 0.085177 s
26/01/04 17:08:36 INFO MemoryStore: Block broadcast_184_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.3 MiB)
26/01/04 17:08:36 INFO BlockManagerInfo: Added broadcast_184_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:08:36 INFO SparkContext: Created broadcast 184 from start at NativeMethodAccessorImpl.java:0
26/01/04 17:08:36 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 60, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@1d04b9bb]. The input RDD has 1 partitions.
26/01/04 17:08:36 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:08:36 INFO DAGScheduler: Got job 124 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/01/04 17:08:36 INFO DAGScheduler: Final stage: ResultStage 125 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:08:36 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:08:36 INFO DAGScheduler: Missing parents: List()
26/01/04 17:08:36 INFO DAGScheduler: Submitting ResultStage 125 (MapPartitionsRDD[627] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:08:36 INFO MemoryStore: Block broadcast_185 stored as values in memory (estimated size 52.9 KiB, free 434.3 MiB)
26/01/04 17:08:36 INFO MemoryStore: Block broadcast_185_piece0 stored as bytes in memory (estimated size 20.2 KiB, free 434.3 MiB)
26/01/04 17:08:36 INFO BlockManagerInfo: Added broadcast_185_piece0 in memory on spark-master:33535 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:08:36 INFO SparkContext: Created broadcast 185 from broadcast at DAGScheduler.scala:1585
26/01/04 17:08:36 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 125 (MapPartitionsRDD[627] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/01/04 17:08:36 INFO TaskSchedulerImpl: Adding task set 125.0 with 1 tasks resource profile 0
26/01/04 17:08:36 INFO TaskSetManager: Starting task 0.0 in stage 125.0 (TID 223) (172.18.0.13, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:08:36 INFO BlockManagerInfo: Added broadcast_185_piece0 in memory on 172.18.0.13:44401 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:08:36 INFO BlockManagerInfo: Added broadcast_184_piece0 in memory on 172.18.0.13:44401 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:08:37 INFO TaskSetManager: Finished task 0.0 in stage 125.0 (TID 223) in 572 ms on 172.18.0.13 (executor 0) (1/1)
26/01/04 17:08:37 INFO TaskSchedulerImpl: Removed TaskSet 125.0, whose tasks have all completed, from pool 
26/01/04 17:08:37 INFO DAGScheduler: ResultStage 125 (start at NativeMethodAccessorImpl.java:0) finished in 0.579 s
26/01/04 17:08:37 INFO DAGScheduler: Job 124 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:08:37 INFO TaskSchedulerImpl: Killing all running tasks in stage 125: Stage finished
26/01/04 17:08:37 INFO DAGScheduler: Job 124 finished: start at NativeMethodAccessorImpl.java:0, took 0.584265 s
26/01/04 17:08:37 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 60, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@1d04b9bb] is committing.
26/01/04 17:08:37 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 60, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@1d04b9bb] committed.
26/01/04 17:08:37 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/60 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.60.0af83cb4-a95b-4ed8-b96e-6aaf5e821643.tmp
26/01/04 17:08:37 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.60.0af83cb4-a95b-4ed8-b96e-6aaf5e821643.tmp to file:/tmp/spark-checkpoint-enrichment/commits/60
26/01/04 17:08:37 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T17:08:36.190Z",
  "batchId" : 60,
  "numInputRows" : 30,
  "inputRowsPerSecond" : 2500.0,
  "processedRowsPerSecond" : 32.93084522502744,
  "durationMs" : {
    "addBatch" : 739,
    "commitOffsets" : 65,
    "getBatch" : 0,
    "latestOffset" : 2,
    "queryPlanning" : 25,
    "triggerExecution" : 911,
    "walCommit" : 78
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "0" : 1089
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "0" : 1119
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "0" : 1119
      }
    },
    "numInputRows" : 30,
    "inputRowsPerSecond" : 2500.0,
    "processedRowsPerSecond" : 32.93084522502744,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 31
  }
}
26/01/04 17:08:47 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/01/04 17:08:47 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/61 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.61.15d5206f-db9c-4d3b-8f10-7cf8bb867788.tmp
26/01/04 17:08:47 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.61.15d5206f-db9c-4d3b-8f10-7cf8bb867788.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/61
26/01/04 17:08:47 INFO MicroBatchExecution: Committed offsets for batch 61. Metadata OffsetSeqMetadata(0,1767546527207,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 17:08:47 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:08:47 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:08:47 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:08:47 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:08:47 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:08:47 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:08:47 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#50527 - airline_prefix.nullCount#50526) > 0)
26/01/04 17:08:47 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#50562 - min_flight_num.nullCount#50561) > 0)
26/01/04 17:08:47 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#50557 - max_flight_num.nullCount#50556) > 0)
26/01/04 17:08:47 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:08:47 INFO DAGScheduler: Got job 125 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 17:08:47 INFO DAGScheduler: Final stage: ResultStage 126 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:08:47 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:08:47 INFO DAGScheduler: Missing parents: List()
26/01/04 17:08:47 INFO DAGScheduler: Submitting ResultStage 126 (MapPartitionsRDD[632] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:08:47 INFO MemoryStore: Block broadcast_186 stored as values in memory (estimated size 38.5 KiB, free 434.2 MiB)
26/01/04 17:08:47 INFO MemoryStore: Block broadcast_186_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.2 MiB)
26/01/04 17:08:47 INFO BlockManagerInfo: Added broadcast_186_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:08:47 INFO SparkContext: Created broadcast 186 from broadcast at DAGScheduler.scala:1585
26/01/04 17:08:47 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 126 (MapPartitionsRDD[632] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 17:08:47 INFO TaskSchedulerImpl: Adding task set 126.0 with 2 tasks resource profile 0
26/01/04 17:08:47 INFO TaskSetManager: Starting task 0.0 in stage 126.0 (TID 224) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 17:08:47 INFO TaskSetManager: Starting task 1.0 in stage 126.0 (TID 225) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 17:08:47 INFO BlockManagerInfo: Added broadcast_186_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:08:47 INFO BlockManagerInfo: Added broadcast_186_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:08:47 INFO TaskSetManager: Finished task 1.0 in stage 126.0 (TID 225) in 31 ms on 172.18.0.13 (executor 0) (1/2)
26/01/04 17:08:47 INFO TaskSetManager: Finished task 0.0 in stage 126.0 (TID 224) in 50 ms on 172.18.0.14 (executor 1) (2/2)
26/01/04 17:08:47 INFO TaskSchedulerImpl: Removed TaskSet 126.0, whose tasks have all completed, from pool 
26/01/04 17:08:47 INFO DAGScheduler: ResultStage 126 (start at NativeMethodAccessorImpl.java:0) finished in 0.060 s
26/01/04 17:08:47 INFO DAGScheduler: Job 125 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:08:47 INFO TaskSchedulerImpl: Killing all running tasks in stage 126: Stage finished
26/01/04 17:08:47 INFO DAGScheduler: Job 125 finished: start at NativeMethodAccessorImpl.java:0, took 0.068555 s
26/01/04 17:08:47 INFO MemoryStore: Block broadcast_187_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.2 MiB)
26/01/04 17:08:47 INFO BlockManagerInfo: Added broadcast_187_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:08:47 INFO SparkContext: Created broadcast 187 from start at NativeMethodAccessorImpl.java:0
26/01/04 17:08:47 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 61, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@1712cfbb]. The input RDD has 1 partitions.
26/01/04 17:08:47 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:08:47 INFO DAGScheduler: Got job 126 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/01/04 17:08:47 INFO DAGScheduler: Final stage: ResultStage 127 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:08:47 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:08:47 INFO DAGScheduler: Missing parents: List()
26/01/04 17:08:47 INFO DAGScheduler: Submitting ResultStage 127 (MapPartitionsRDD[637] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:08:47 INFO MemoryStore: Block broadcast_188 stored as values in memory (estimated size 52.9 KiB, free 434.2 MiB)
26/01/04 17:08:47 INFO MemoryStore: Block broadcast_188_piece0 stored as bytes in memory (estimated size 20.2 KiB, free 434.1 MiB)
26/01/04 17:08:47 INFO BlockManagerInfo: Added broadcast_188_piece0 in memory on spark-master:33535 (size: 20.2 KiB, free: 434.3 MiB)
26/01/04 17:08:47 INFO SparkContext: Created broadcast 188 from broadcast at DAGScheduler.scala:1585
26/01/04 17:08:47 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 127 (MapPartitionsRDD[637] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/01/04 17:08:47 INFO TaskSchedulerImpl: Adding task set 127.0 with 1 tasks resource profile 0
26/01/04 17:08:47 INFO TaskSetManager: Starting task 0.0 in stage 127.0 (TID 226) (172.18.0.13, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:08:47 INFO BlockManagerInfo: Added broadcast_188_piece0 in memory on 172.18.0.13:44401 (size: 20.2 KiB, free: 434.3 MiB)
26/01/04 17:08:47 INFO BlockManagerInfo: Added broadcast_187_piece0 in memory on 172.18.0.13:44401 (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:08:48 INFO TaskSetManager: Finished task 0.0 in stage 127.0 (TID 226) in 560 ms on 172.18.0.13 (executor 0) (1/1)
26/01/04 17:08:48 INFO TaskSchedulerImpl: Removed TaskSet 127.0, whose tasks have all completed, from pool 
26/01/04 17:08:48 INFO DAGScheduler: ResultStage 127 (start at NativeMethodAccessorImpl.java:0) finished in 0.567 s
26/01/04 17:08:48 INFO DAGScheduler: Job 126 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:08:48 INFO TaskSchedulerImpl: Killing all running tasks in stage 127: Stage finished
26/01/04 17:08:48 INFO DAGScheduler: Job 126 finished: start at NativeMethodAccessorImpl.java:0, took 0.570991 s
26/01/04 17:08:48 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 61, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@1712cfbb] is committing.
26/01/04 17:08:48 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 61, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@1712cfbb] committed.
26/01/04 17:08:48 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/61 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.61.a4982ce6-5e4b-4456-aa0d-e032937b8c01.tmp
26/01/04 17:08:48 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.61.a4982ce6-5e4b-4456-aa0d-e032937b8c01.tmp to file:/tmp/spark-checkpoint-enrichment/commits/61
26/01/04 17:08:48 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T17:08:47.204Z",
  "batchId" : 61,
  "numInputRows" : 30,
  "inputRowsPerSecond" : 2500.0,
  "processedRowsPerSecond" : 31.21748178980229,
  "durationMs" : {
    "addBatch" : 719,
    "commitOffsets" : 89,
    "getBatch" : 0,
    "latestOffset" : 3,
    "queryPlanning" : 24,
    "triggerExecution" : 961,
    "walCommit" : 125
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "0" : 1119
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "0" : 1149
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "0" : 1149
      }
    },
    "numInputRows" : 30,
    "inputRowsPerSecond" : 2500.0,
    "processedRowsPerSecond" : 31.21748178980229,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 31
  }
}
26/01/04 17:08:50 INFO BlockManagerInfo: Removed broadcast_185_piece0 on spark-master:33535 in memory (size: 20.2 KiB, free: 434.3 MiB)
26/01/04 17:08:50 INFO BlockManagerInfo: Removed broadcast_185_piece0 on 172.18.0.13:44401 in memory (size: 20.2 KiB, free: 434.3 MiB)
26/01/04 17:08:50 INFO BlockManagerInfo: Removed broadcast_183_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:08:50 INFO BlockManagerInfo: Removed broadcast_183_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:08:50 INFO BlockManagerInfo: Removed broadcast_183_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:08:50 INFO BlockManagerInfo: Removed broadcast_184_piece0 on spark-master:33535 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:08:50 INFO BlockManagerInfo: Removed broadcast_184_piece0 on 172.18.0.13:44401 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:08:50 INFO BlockManagerInfo: Removed broadcast_186_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:08:50 INFO BlockManagerInfo: Removed broadcast_186_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:08:50 INFO BlockManagerInfo: Removed broadcast_186_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:08:50 INFO BlockManagerInfo: Removed broadcast_188_piece0 on spark-master:33535 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:08:50 INFO BlockManagerInfo: Removed broadcast_188_piece0 on 172.18.0.13:44401 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:08:58 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/01/04 17:08:58 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/62 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.62.4d942573-09d1-4bad-b27d-3bcc89d4e625.tmp
26/01/04 17:08:58 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.62.4d942573-09d1-4bad-b27d-3bcc89d4e625.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/62
26/01/04 17:08:58 INFO MicroBatchExecution: Committed offsets for batch 62. Metadata OffsetSeqMetadata(0,1767546538218,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 17:08:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:08:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:08:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:08:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:08:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:08:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:08:58 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#51331 - airline_prefix.nullCount#51330) > 0)
26/01/04 17:08:58 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#51366 - min_flight_num.nullCount#51365) > 0)
26/01/04 17:08:58 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#51361 - max_flight_num.nullCount#51360) > 0)
26/01/04 17:08:58 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:08:58 INFO DAGScheduler: Got job 127 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 17:08:58 INFO DAGScheduler: Final stage: ResultStage 128 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:08:58 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:08:58 INFO DAGScheduler: Missing parents: List()
26/01/04 17:08:58 INFO DAGScheduler: Submitting ResultStage 128 (MapPartitionsRDD[642] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:08:58 INFO MemoryStore: Block broadcast_189 stored as values in memory (estimated size 38.5 KiB, free 434.4 MiB)
26/01/04 17:08:58 INFO MemoryStore: Block broadcast_189_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.3 MiB)
26/01/04 17:08:58 INFO BlockManagerInfo: Added broadcast_189_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:08:58 INFO SparkContext: Created broadcast 189 from broadcast at DAGScheduler.scala:1585
26/01/04 17:08:58 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 128 (MapPartitionsRDD[642] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 17:08:58 INFO TaskSchedulerImpl: Adding task set 128.0 with 2 tasks resource profile 0
26/01/04 17:08:58 INFO TaskSetManager: Starting task 0.0 in stage 128.0 (TID 227) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 17:08:58 INFO TaskSetManager: Starting task 1.0 in stage 128.0 (TID 228) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 17:08:58 INFO BlockManagerInfo: Added broadcast_189_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:08:58 INFO BlockManagerInfo: Added broadcast_189_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:08:58 INFO TaskSetManager: Finished task 1.0 in stage 128.0 (TID 228) in 25 ms on 172.18.0.13 (executor 0) (1/2)
26/01/04 17:08:58 INFO TaskSetManager: Finished task 0.0 in stage 128.0 (TID 227) in 34 ms on 172.18.0.14 (executor 1) (2/2)
26/01/04 17:08:58 INFO TaskSchedulerImpl: Removed TaskSet 128.0, whose tasks have all completed, from pool 
26/01/04 17:08:58 INFO DAGScheduler: ResultStage 128 (start at NativeMethodAccessorImpl.java:0) finished in 0.041 s
26/01/04 17:08:58 INFO DAGScheduler: Job 127 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:08:58 INFO TaskSchedulerImpl: Killing all running tasks in stage 128: Stage finished
26/01/04 17:08:58 INFO DAGScheduler: Job 127 finished: start at NativeMethodAccessorImpl.java:0, took 0.044591 s
26/01/04 17:08:58 INFO MemoryStore: Block broadcast_190_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.3 MiB)
26/01/04 17:08:58 INFO BlockManagerInfo: Added broadcast_190_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:08:58 INFO SparkContext: Created broadcast 190 from start at NativeMethodAccessorImpl.java:0
26/01/04 17:08:58 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 62, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@4ddf6b6b]. The input RDD has 1 partitions.
26/01/04 17:08:58 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:08:58 INFO DAGScheduler: Got job 128 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/01/04 17:08:58 INFO DAGScheduler: Final stage: ResultStage 129 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:08:58 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:08:58 INFO DAGScheduler: Missing parents: List()
26/01/04 17:08:58 INFO DAGScheduler: Submitting ResultStage 129 (MapPartitionsRDD[647] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:08:58 INFO MemoryStore: Block broadcast_191 stored as values in memory (estimated size 52.9 KiB, free 434.3 MiB)
26/01/04 17:08:58 INFO MemoryStore: Block broadcast_191_piece0 stored as bytes in memory (estimated size 20.2 KiB, free 434.3 MiB)
26/01/04 17:08:58 INFO BlockManagerInfo: Added broadcast_191_piece0 in memory on spark-master:33535 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:08:58 INFO SparkContext: Created broadcast 191 from broadcast at DAGScheduler.scala:1585
26/01/04 17:08:58 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 129 (MapPartitionsRDD[647] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/01/04 17:08:58 INFO TaskSchedulerImpl: Adding task set 129.0 with 1 tasks resource profile 0
26/01/04 17:08:58 INFO TaskSetManager: Starting task 0.0 in stage 129.0 (TID 229) (172.18.0.13, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:08:58 INFO BlockManagerInfo: Added broadcast_191_piece0 in memory on 172.18.0.13:44401 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:08:58 INFO BlockManagerInfo: Added broadcast_190_piece0 in memory on 172.18.0.13:44401 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:08:58 INFO TaskSetManager: Finished task 0.0 in stage 129.0 (TID 229) in 559 ms on 172.18.0.13 (executor 0) (1/1)
26/01/04 17:08:58 INFO TaskSchedulerImpl: Removed TaskSet 129.0, whose tasks have all completed, from pool 
26/01/04 17:08:58 INFO DAGScheduler: ResultStage 129 (start at NativeMethodAccessorImpl.java:0) finished in 0.566 s
26/01/04 17:08:58 INFO DAGScheduler: Job 128 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:08:58 INFO TaskSchedulerImpl: Killing all running tasks in stage 129: Stage finished
26/01/04 17:08:58 INFO DAGScheduler: Job 128 finished: start at NativeMethodAccessorImpl.java:0, took 0.568042 s
26/01/04 17:08:58 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 62, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@4ddf6b6b] is committing.
26/01/04 17:08:58 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 62, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@4ddf6b6b] committed.
26/01/04 17:08:59 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/62 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.62.f6ef6d92-60fd-4030-89d0-238f6e70731e.tmp
26/01/04 17:08:59 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.62.f6ef6d92-60fd-4030-89d0-238f6e70731e.tmp to file:/tmp/spark-checkpoint-enrichment/commits/62
26/01/04 17:08:59 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T17:08:58.216Z",
  "batchId" : 62,
  "numInputRows" : 30,
  "inputRowsPerSecond" : 2500.0,
  "processedRowsPerSecond" : 35.54502369668246,
  "durationMs" : {
    "addBatch" : 682,
    "commitOffsets" : 67,
    "getBatch" : 0,
    "latestOffset" : 2,
    "queryPlanning" : 18,
    "triggerExecution" : 844,
    "walCommit" : 73
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "0" : 1149
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "0" : 1179
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "0" : 1179
      }
    },
    "numInputRows" : 30,
    "inputRowsPerSecond" : 2500.0,
    "processedRowsPerSecond" : 35.54502369668246,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 31
  }
}
26/01/04 17:09:08 INFO BlockManagerInfo: Removed broadcast_187_piece0 on spark-master:33535 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:09:08 INFO BlockManagerInfo: Removed broadcast_187_piece0 on 172.18.0.13:44401 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:09:08 INFO BlockManagerInfo: Removed broadcast_189_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:09:08 INFO BlockManagerInfo: Removed broadcast_189_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:09:08 INFO BlockManagerInfo: Removed broadcast_189_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:09:08 INFO BlockManagerInfo: Removed broadcast_191_piece0 on spark-master:33535 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:09:08 INFO BlockManagerInfo: Removed broadcast_191_piece0 on 172.18.0.13:44401 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:09:09 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/01/04 17:09:09 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/63 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.63.ea129315-5f09-4bac-9c92-36888a987a6e.tmp
26/01/04 17:09:09 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.63.ea129315-5f09-4bac-9c92-36888a987a6e.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/63
26/01/04 17:09:09 INFO MicroBatchExecution: Committed offsets for batch 63. Metadata OffsetSeqMetadata(0,1767546549234,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 17:09:09 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:09:09 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:09:09 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:09:09 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:09:09 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:09:09 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:09:09 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#52135 - airline_prefix.nullCount#52134) > 0)
26/01/04 17:09:09 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#52170 - min_flight_num.nullCount#52169) > 0)
26/01/04 17:09:09 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#52165 - max_flight_num.nullCount#52164) > 0)
26/01/04 17:09:09 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:09:09 INFO DAGScheduler: Got job 129 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 17:09:09 INFO DAGScheduler: Final stage: ResultStage 130 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:09:09 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:09:09 INFO DAGScheduler: Missing parents: List()
26/01/04 17:09:09 INFO DAGScheduler: Submitting ResultStage 130 (MapPartitionsRDD[652] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:09:09 INFO MemoryStore: Block broadcast_192 stored as values in memory (estimated size 38.5 KiB, free 434.4 MiB)
26/01/04 17:09:09 INFO BlockManagerInfo: Removed broadcast_190_piece0 on spark-master:33535 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:09:09 INFO MemoryStore: Block broadcast_192_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.3 MiB)
26/01/04 17:09:09 INFO BlockManagerInfo: Added broadcast_192_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:09:09 INFO SparkContext: Created broadcast 192 from broadcast at DAGScheduler.scala:1585
26/01/04 17:09:09 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 130 (MapPartitionsRDD[652] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 17:09:09 INFO TaskSchedulerImpl: Adding task set 130.0 with 2 tasks resource profile 0
26/01/04 17:09:09 INFO BlockManagerInfo: Removed broadcast_190_piece0 on 172.18.0.13:44401 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:09:09 INFO TaskSetManager: Starting task 1.0 in stage 130.0 (TID 230) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 17:09:09 INFO TaskSetManager: Starting task 0.0 in stage 130.0 (TID 231) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 17:09:09 INFO BlockManagerInfo: Added broadcast_192_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:09:09 INFO TaskSetManager: Finished task 0.0 in stage 130.0 (TID 231) in 49 ms on 172.18.0.14 (executor 1) (1/2)
26/01/04 17:09:09 INFO BlockManagerInfo: Added broadcast_192_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:09:09 INFO TaskSetManager: Finished task 1.0 in stage 130.0 (TID 230) in 138 ms on 172.18.0.13 (executor 0) (2/2)
26/01/04 17:09:09 INFO TaskSchedulerImpl: Removed TaskSet 130.0, whose tasks have all completed, from pool 
26/01/04 17:09:09 INFO DAGScheduler: ResultStage 130 (start at NativeMethodAccessorImpl.java:0) finished in 0.152 s
26/01/04 17:09:09 INFO DAGScheduler: Job 129 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:09:09 INFO TaskSchedulerImpl: Killing all running tasks in stage 130: Stage finished
26/01/04 17:09:09 INFO DAGScheduler: Job 129 finished: start at NativeMethodAccessorImpl.java:0, took 0.156084 s
26/01/04 17:09:09 INFO MemoryStore: Block broadcast_193_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.3 MiB)
26/01/04 17:09:09 INFO BlockManagerInfo: Added broadcast_193_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:09:09 INFO SparkContext: Created broadcast 193 from start at NativeMethodAccessorImpl.java:0
26/01/04 17:09:09 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 63, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@1a5487e]. The input RDD has 1 partitions.
26/01/04 17:09:09 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:09:09 INFO DAGScheduler: Got job 130 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/01/04 17:09:09 INFO DAGScheduler: Final stage: ResultStage 131 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:09:09 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:09:09 INFO DAGScheduler: Missing parents: List()
26/01/04 17:09:09 INFO DAGScheduler: Submitting ResultStage 131 (MapPartitionsRDD[657] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:09:09 INFO MemoryStore: Block broadcast_194 stored as values in memory (estimated size 52.9 KiB, free 434.3 MiB)
26/01/04 17:09:09 INFO MemoryStore: Block broadcast_194_piece0 stored as bytes in memory (estimated size 20.2 KiB, free 434.3 MiB)
26/01/04 17:09:09 INFO BlockManagerInfo: Added broadcast_194_piece0 in memory on spark-master:33535 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:09:09 INFO SparkContext: Created broadcast 194 from broadcast at DAGScheduler.scala:1585
26/01/04 17:09:09 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 131 (MapPartitionsRDD[657] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/01/04 17:09:09 INFO TaskSchedulerImpl: Adding task set 131.0 with 1 tasks resource profile 0
26/01/04 17:09:09 INFO TaskSetManager: Starting task 0.0 in stage 131.0 (TID 232) (172.18.0.13, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:09:09 INFO BlockManagerInfo: Added broadcast_194_piece0 in memory on 172.18.0.13:44401 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:09:09 INFO BlockManagerInfo: Added broadcast_193_piece0 in memory on 172.18.0.13:44401 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:09:10 INFO TaskSetManager: Finished task 0.0 in stage 131.0 (TID 232) in 586 ms on 172.18.0.13 (executor 0) (1/1)
26/01/04 17:09:10 INFO TaskSchedulerImpl: Removed TaskSet 131.0, whose tasks have all completed, from pool 
26/01/04 17:09:10 INFO DAGScheduler: ResultStage 131 (start at NativeMethodAccessorImpl.java:0) finished in 0.594 s
26/01/04 17:09:10 INFO DAGScheduler: Job 130 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:09:10 INFO TaskSchedulerImpl: Killing all running tasks in stage 131: Stage finished
26/01/04 17:09:10 INFO DAGScheduler: Job 130 finished: start at NativeMethodAccessorImpl.java:0, took 0.598077 s
26/01/04 17:09:10 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 63, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@1a5487e] is committing.
26/01/04 17:09:10 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 63, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@1a5487e] committed.
26/01/04 17:09:10 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/63 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.63.3a1ed8ef-a687-4522-97b7-8a9b715b3d06.tmp
26/01/04 17:09:10 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.63.3a1ed8ef-a687-4522-97b7-8a9b715b3d06.tmp to file:/tmp/spark-checkpoint-enrichment/commits/63
26/01/04 17:09:10 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T17:09:09.232Z",
  "batchId" : 63,
  "numInputRows" : 30,
  "inputRowsPerSecond" : 2727.2727272727275,
  "processedRowsPerSecond" : 29.126213592233007,
  "durationMs" : {
    "addBatch" : 844,
    "commitOffsets" : 80,
    "getBatch" : 0,
    "latestOffset" : 2,
    "queryPlanning" : 36,
    "triggerExecution" : 1030,
    "walCommit" : 66
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "0" : 1179
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "0" : 1209
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "0" : 1209
      }
    },
    "numInputRows" : 30,
    "inputRowsPerSecond" : 2727.2727272727275,
    "processedRowsPerSecond" : 29.126213592233007,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 31
  }
}
26/01/04 17:09:20 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/64 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.64.0e2b96e1-cb67-4b7b-a8e7-e2ed4cd5eacc.tmp
26/01/04 17:09:20 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.64.0e2b96e1-cb67-4b7b-a8e7-e2ed4cd5eacc.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/64
26/01/04 17:09:20 INFO MicroBatchExecution: Committed offsets for batch 64. Metadata OffsetSeqMetadata(0,1767546560263,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 17:09:20 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:09:20 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:09:20 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:09:20 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:09:20 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:09:20 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:09:20 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#52939 - airline_prefix.nullCount#52938) > 0)
26/01/04 17:09:20 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#52974 - min_flight_num.nullCount#52973) > 0)
26/01/04 17:09:20 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#52969 - max_flight_num.nullCount#52968) > 0)
26/01/04 17:09:20 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:09:20 INFO DAGScheduler: Got job 131 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 17:09:20 INFO DAGScheduler: Final stage: ResultStage 132 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:09:20 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:09:20 INFO DAGScheduler: Missing parents: List()
26/01/04 17:09:20 INFO DAGScheduler: Submitting ResultStage 132 (MapPartitionsRDD[662] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:09:20 INFO MemoryStore: Block broadcast_195 stored as values in memory (estimated size 38.5 KiB, free 434.2 MiB)
26/01/04 17:09:20 INFO MemoryStore: Block broadcast_195_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.2 MiB)
26/01/04 17:09:20 INFO BlockManagerInfo: Added broadcast_195_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:09:20 INFO SparkContext: Created broadcast 195 from broadcast at DAGScheduler.scala:1585
26/01/04 17:09:20 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 132 (MapPartitionsRDD[662] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 17:09:20 INFO TaskSchedulerImpl: Adding task set 132.0 with 2 tasks resource profile 0
26/01/04 17:09:20 INFO TaskSetManager: Starting task 0.0 in stage 132.0 (TID 233) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 17:09:20 INFO TaskSetManager: Starting task 1.0 in stage 132.0 (TID 234) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 17:09:20 INFO BlockManagerInfo: Added broadcast_195_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:09:20 INFO BlockManagerInfo: Added broadcast_195_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:09:20 INFO TaskSetManager: Finished task 0.0 in stage 132.0 (TID 233) in 22 ms on 172.18.0.14 (executor 1) (1/2)
26/01/04 17:09:20 INFO TaskSetManager: Finished task 1.0 in stage 132.0 (TID 234) in 29 ms on 172.18.0.13 (executor 0) (2/2)
26/01/04 17:09:20 INFO TaskSchedulerImpl: Removed TaskSet 132.0, whose tasks have all completed, from pool 
26/01/04 17:09:20 INFO DAGScheduler: ResultStage 132 (start at NativeMethodAccessorImpl.java:0) finished in 0.036 s
26/01/04 17:09:20 INFO DAGScheduler: Job 131 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:09:20 INFO TaskSchedulerImpl: Killing all running tasks in stage 132: Stage finished
26/01/04 17:09:20 INFO DAGScheduler: Job 131 finished: start at NativeMethodAccessorImpl.java:0, took 0.038122 s
26/01/04 17:09:20 INFO MemoryStore: Block broadcast_196_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.2 MiB)
26/01/04 17:09:20 INFO BlockManagerInfo: Added broadcast_196_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:09:20 INFO SparkContext: Created broadcast 196 from start at NativeMethodAccessorImpl.java:0
26/01/04 17:09:20 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 64, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@7a3d5ed0]. The input RDD has 1 partitions.
26/01/04 17:09:20 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:09:20 INFO DAGScheduler: Got job 132 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/01/04 17:09:20 INFO DAGScheduler: Final stage: ResultStage 133 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:09:20 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:09:20 INFO DAGScheduler: Missing parents: List()
26/01/04 17:09:20 INFO DAGScheduler: Submitting ResultStage 133 (MapPartitionsRDD[667] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:09:20 INFO MemoryStore: Block broadcast_197 stored as values in memory (estimated size 52.9 KiB, free 434.2 MiB)
26/01/04 17:09:20 INFO MemoryStore: Block broadcast_197_piece0 stored as bytes in memory (estimated size 20.2 KiB, free 434.1 MiB)
26/01/04 17:09:20 INFO BlockManagerInfo: Added broadcast_197_piece0 in memory on spark-master:33535 (size: 20.2 KiB, free: 434.3 MiB)
26/01/04 17:09:20 INFO SparkContext: Created broadcast 197 from broadcast at DAGScheduler.scala:1585
26/01/04 17:09:20 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 133 (MapPartitionsRDD[667] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/01/04 17:09:20 INFO TaskSchedulerImpl: Adding task set 133.0 with 1 tasks resource profile 0
26/01/04 17:09:20 INFO TaskSetManager: Starting task 0.0 in stage 133.0 (TID 235) (172.18.0.13, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:09:20 INFO BlockManagerInfo: Added broadcast_197_piece0 in memory on 172.18.0.13:44401 (size: 20.2 KiB, free: 434.3 MiB)
26/01/04 17:09:20 INFO BlockManagerInfo: Added broadcast_196_piece0 in memory on 172.18.0.13:44401 (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:09:21 INFO TaskSetManager: Finished task 0.0 in stage 133.0 (TID 235) in 558 ms on 172.18.0.13 (executor 0) (1/1)
26/01/04 17:09:21 INFO TaskSchedulerImpl: Removed TaskSet 133.0, whose tasks have all completed, from pool 
26/01/04 17:09:21 INFO DAGScheduler: ResultStage 133 (start at NativeMethodAccessorImpl.java:0) finished in 0.563 s
26/01/04 17:09:21 INFO DAGScheduler: Job 132 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:09:21 INFO TaskSchedulerImpl: Killing all running tasks in stage 133: Stage finished
26/01/04 17:09:21 INFO DAGScheduler: Job 132 finished: start at NativeMethodAccessorImpl.java:0, took 0.564188 s
26/01/04 17:09:21 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 64, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@7a3d5ed0] is committing.
26/01/04 17:09:21 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 64, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@7a3d5ed0] committed.
26/01/04 17:09:21 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/64 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.64.3d4e1d98-ba59-4d72-8c01-66b953a458d6.tmp
26/01/04 17:09:21 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.64.3d4e1d98-ba59-4d72-8c01-66b953a458d6.tmp to file:/tmp/spark-checkpoint-enrichment/commits/64
26/01/04 17:09:21 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T17:09:20.262Z",
  "batchId" : 64,
  "numInputRows" : 30,
  "inputRowsPerSecond" : 2500.0,
  "processedRowsPerSecond" : 35.41912632821724,
  "durationMs" : {
    "addBatch" : 672,
    "commitOffsets" : 84,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 20,
    "triggerExecution" : 847,
    "walCommit" : 69
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "0" : 1209
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "0" : 1239
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "0" : 1239
      }
    },
    "numInputRows" : 30,
    "inputRowsPerSecond" : 2500.0,
    "processedRowsPerSecond" : 35.41912632821724,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 31
  }
}
26/01/04 17:09:23 INFO BlockManagerInfo: Removed broadcast_194_piece0 on spark-master:33535 in memory (size: 20.2 KiB, free: 434.3 MiB)
26/01/04 17:09:23 INFO BlockManagerInfo: Removed broadcast_194_piece0 on 172.18.0.13:44401 in memory (size: 20.2 KiB, free: 434.3 MiB)
26/01/04 17:09:23 INFO BlockManagerInfo: Removed broadcast_193_piece0 on spark-master:33535 in memory (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:09:23 INFO BlockManagerInfo: Removed broadcast_193_piece0 on 172.18.0.13:44401 in memory (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:09:23 INFO BlockManagerInfo: Removed broadcast_195_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:09:23 INFO BlockManagerInfo: Removed broadcast_195_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:09:23 INFO BlockManagerInfo: Removed broadcast_195_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:09:23 INFO BlockManagerInfo: Removed broadcast_192_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:09:23 INFO BlockManagerInfo: Removed broadcast_192_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:09:23 INFO BlockManagerInfo: Removed broadcast_192_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:09:23 INFO BlockManagerInfo: Removed broadcast_197_piece0 on spark-master:33535 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:09:23 INFO BlockManagerInfo: Removed broadcast_197_piece0 on 172.18.0.13:44401 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:09:31 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/01/04 17:09:31 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/65 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.65.424c45e5-6027-486c-b764-3828732501a8.tmp
26/01/04 17:09:31 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.65.424c45e5-6027-486c-b764-3828732501a8.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/65
26/01/04 17:09:31 INFO MicroBatchExecution: Committed offsets for batch 65. Metadata OffsetSeqMetadata(0,1767546571268,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 17:09:31 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:09:31 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:09:31 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:09:31 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:09:31 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:09:31 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:09:31 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#53743 - airline_prefix.nullCount#53742) > 0)
26/01/04 17:09:31 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#53778 - min_flight_num.nullCount#53777) > 0)
26/01/04 17:09:31 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#53773 - max_flight_num.nullCount#53772) > 0)
26/01/04 17:09:31 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:09:31 INFO DAGScheduler: Got job 133 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 17:09:31 INFO DAGScheduler: Final stage: ResultStage 134 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:09:31 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:09:31 INFO DAGScheduler: Missing parents: List()
26/01/04 17:09:31 INFO DAGScheduler: Submitting ResultStage 134 (MapPartitionsRDD[672] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:09:31 INFO MemoryStore: Block broadcast_198 stored as values in memory (estimated size 38.5 KiB, free 434.4 MiB)
26/01/04 17:09:31 INFO MemoryStore: Block broadcast_198_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.3 MiB)
26/01/04 17:09:31 INFO BlockManagerInfo: Added broadcast_198_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:09:31 INFO SparkContext: Created broadcast 198 from broadcast at DAGScheduler.scala:1585
26/01/04 17:09:31 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 134 (MapPartitionsRDD[672] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 17:09:31 INFO TaskSchedulerImpl: Adding task set 134.0 with 2 tasks resource profile 0
26/01/04 17:09:31 INFO TaskSetManager: Starting task 1.0 in stage 134.0 (TID 236) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 17:09:31 INFO TaskSetManager: Starting task 0.0 in stage 134.0 (TID 237) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 17:09:31 INFO BlockManagerInfo: Added broadcast_198_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:09:31 INFO BlockManagerInfo: Added broadcast_198_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:09:31 INFO TaskSetManager: Finished task 1.0 in stage 134.0 (TID 236) in 21 ms on 172.18.0.13 (executor 0) (1/2)
26/01/04 17:09:31 INFO TaskSetManager: Finished task 0.0 in stage 134.0 (TID 237) in 28 ms on 172.18.0.14 (executor 1) (2/2)
26/01/04 17:09:31 INFO TaskSchedulerImpl: Removed TaskSet 134.0, whose tasks have all completed, from pool 
26/01/04 17:09:31 INFO DAGScheduler: ResultStage 134 (start at NativeMethodAccessorImpl.java:0) finished in 0.034 s
26/01/04 17:09:31 INFO DAGScheduler: Job 133 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:09:31 INFO TaskSchedulerImpl: Killing all running tasks in stage 134: Stage finished
26/01/04 17:09:31 INFO DAGScheduler: Job 133 finished: start at NativeMethodAccessorImpl.java:0, took 0.036042 s
26/01/04 17:09:31 INFO MemoryStore: Block broadcast_199_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.3 MiB)
26/01/04 17:09:31 INFO BlockManagerInfo: Added broadcast_199_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:09:31 INFO SparkContext: Created broadcast 199 from start at NativeMethodAccessorImpl.java:0
26/01/04 17:09:31 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 65, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@20e22ed9]. The input RDD has 1 partitions.
26/01/04 17:09:31 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:09:31 INFO DAGScheduler: Got job 134 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/01/04 17:09:31 INFO DAGScheduler: Final stage: ResultStage 135 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:09:31 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:09:31 INFO DAGScheduler: Missing parents: List()
26/01/04 17:09:31 INFO DAGScheduler: Submitting ResultStage 135 (MapPartitionsRDD[677] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:09:31 INFO MemoryStore: Block broadcast_200 stored as values in memory (estimated size 52.9 KiB, free 434.3 MiB)
26/01/04 17:09:31 INFO MemoryStore: Block broadcast_200_piece0 stored as bytes in memory (estimated size 20.2 KiB, free 434.3 MiB)
26/01/04 17:09:31 INFO BlockManagerInfo: Added broadcast_200_piece0 in memory on spark-master:33535 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:09:31 INFO SparkContext: Created broadcast 200 from broadcast at DAGScheduler.scala:1585
26/01/04 17:09:31 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 135 (MapPartitionsRDD[677] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/01/04 17:09:31 INFO TaskSchedulerImpl: Adding task set 135.0 with 1 tasks resource profile 0
26/01/04 17:09:31 INFO TaskSetManager: Starting task 0.0 in stage 135.0 (TID 238) (172.18.0.13, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:09:31 INFO BlockManagerInfo: Added broadcast_200_piece0 in memory on 172.18.0.13:44401 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:09:31 INFO BlockManagerInfo: Added broadcast_199_piece0 in memory on 172.18.0.13:44401 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:09:32 INFO TaskSetManager: Finished task 0.0 in stage 135.0 (TID 238) in 552 ms on 172.18.0.13 (executor 0) (1/1)
26/01/04 17:09:32 INFO TaskSchedulerImpl: Removed TaskSet 135.0, whose tasks have all completed, from pool 
26/01/04 17:09:32 INFO DAGScheduler: ResultStage 135 (start at NativeMethodAccessorImpl.java:0) finished in 0.557 s
26/01/04 17:09:32 INFO DAGScheduler: Job 134 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:09:32 INFO TaskSchedulerImpl: Killing all running tasks in stage 135: Stage finished
26/01/04 17:09:32 INFO DAGScheduler: Job 134 finished: start at NativeMethodAccessorImpl.java:0, took 0.559262 s
26/01/04 17:09:32 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 65, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@20e22ed9] is committing.
26/01/04 17:09:32 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 65, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@20e22ed9] committed.
26/01/04 17:09:32 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/65 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.65.afd32300-8560-446e-a72e-3d7834e071b2.tmp
26/01/04 17:09:32 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.65.afd32300-8560-446e-a72e-3d7834e071b2.tmp to file:/tmp/spark-checkpoint-enrichment/commits/65
26/01/04 17:09:32 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T17:09:31.267Z",
  "batchId" : 65,
  "numInputRows" : 30,
  "inputRowsPerSecond" : 2500.0,
  "processedRowsPerSecond" : 34.96503496503497,
  "durationMs" : {
    "addBatch" : 674,
    "commitOffsets" : 76,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 28,
    "triggerExecution" : 858,
    "walCommit" : 79
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "0" : 1239
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "0" : 1269
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "0" : 1269
      }
    },
    "numInputRows" : 30,
    "inputRowsPerSecond" : 2500.0,
    "processedRowsPerSecond" : 34.96503496503497,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 31
  }
}
26/01/04 17:09:41 INFO BlockManagerInfo: Removed broadcast_198_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:09:41 INFO BlockManagerInfo: Removed broadcast_198_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:09:41 INFO BlockManagerInfo: Removed broadcast_198_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:09:41 INFO BlockManagerInfo: Removed broadcast_196_piece0 on spark-master:33535 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:09:41 INFO BlockManagerInfo: Removed broadcast_196_piece0 on 172.18.0.13:44401 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:09:41 INFO BlockManagerInfo: Removed broadcast_200_piece0 on spark-master:33535 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:09:41 INFO BlockManagerInfo: Removed broadcast_200_piece0 on 172.18.0.13:44401 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:09:42 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/01/04 17:09:42 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/66 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.66.fda09924-d768-4a5d-b8b2-f9375ab9ef86.tmp
26/01/04 17:09:42 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.66.fda09924-d768-4a5d-b8b2-f9375ab9ef86.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/66
26/01/04 17:09:42 INFO MicroBatchExecution: Committed offsets for batch 66. Metadata OffsetSeqMetadata(0,1767546582275,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 17:09:42 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:09:42 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:09:42 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:09:42 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:09:42 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:09:42 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:09:42 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#54547 - airline_prefix.nullCount#54546) > 0)
26/01/04 17:09:42 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#54582 - min_flight_num.nullCount#54581) > 0)
26/01/04 17:09:42 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#54577 - max_flight_num.nullCount#54576) > 0)
26/01/04 17:09:42 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:09:42 INFO DAGScheduler: Got job 135 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 17:09:42 INFO DAGScheduler: Final stage: ResultStage 136 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:09:42 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:09:42 INFO DAGScheduler: Missing parents: List()
26/01/04 17:09:42 INFO DAGScheduler: Submitting ResultStage 136 (MapPartitionsRDD[682] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:09:42 INFO MemoryStore: Block broadcast_201 stored as values in memory (estimated size 38.5 KiB, free 434.4 MiB)
26/01/04 17:09:42 INFO MemoryStore: Block broadcast_201_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.3 MiB)
26/01/04 17:09:42 INFO BlockManagerInfo: Added broadcast_201_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:09:42 INFO BlockManagerInfo: Removed broadcast_199_piece0 on spark-master:33535 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:09:42 INFO SparkContext: Created broadcast 201 from broadcast at DAGScheduler.scala:1585
26/01/04 17:09:42 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 136 (MapPartitionsRDD[682] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 17:09:42 INFO TaskSchedulerImpl: Adding task set 136.0 with 2 tasks resource profile 0
26/01/04 17:09:42 INFO BlockManagerInfo: Removed broadcast_199_piece0 on 172.18.0.13:44401 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:09:42 INFO TaskSetManager: Starting task 0.0 in stage 136.0 (TID 239) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 17:09:42 INFO TaskSetManager: Starting task 1.0 in stage 136.0 (TID 240) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 17:09:42 INFO BlockManagerInfo: Added broadcast_201_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:09:42 INFO BlockManagerInfo: Added broadcast_201_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:09:42 INFO TaskSetManager: Finished task 1.0 in stage 136.0 (TID 240) in 21 ms on 172.18.0.13 (executor 0) (1/2)
26/01/04 17:09:42 INFO TaskSetManager: Finished task 0.0 in stage 136.0 (TID 239) in 32 ms on 172.18.0.14 (executor 1) (2/2)
26/01/04 17:09:42 INFO TaskSchedulerImpl: Removed TaskSet 136.0, whose tasks have all completed, from pool 
26/01/04 17:09:42 INFO DAGScheduler: ResultStage 136 (start at NativeMethodAccessorImpl.java:0) finished in 0.040 s
26/01/04 17:09:42 INFO DAGScheduler: Job 135 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:09:42 INFO TaskSchedulerImpl: Killing all running tasks in stage 136: Stage finished
26/01/04 17:09:42 INFO DAGScheduler: Job 135 finished: start at NativeMethodAccessorImpl.java:0, took 0.042089 s
26/01/04 17:09:42 INFO MemoryStore: Block broadcast_202_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.3 MiB)
26/01/04 17:09:42 INFO BlockManagerInfo: Added broadcast_202_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:09:42 INFO SparkContext: Created broadcast 202 from start at NativeMethodAccessorImpl.java:0
26/01/04 17:09:42 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 66, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@5c1bb664]. The input RDD has 1 partitions.
26/01/04 17:09:42 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:09:42 INFO DAGScheduler: Got job 136 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/01/04 17:09:42 INFO DAGScheduler: Final stage: ResultStage 137 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:09:42 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:09:42 INFO DAGScheduler: Missing parents: List()
26/01/04 17:09:42 INFO DAGScheduler: Submitting ResultStage 137 (MapPartitionsRDD[687] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:09:42 INFO MemoryStore: Block broadcast_203 stored as values in memory (estimated size 52.9 KiB, free 434.3 MiB)
26/01/04 17:09:42 INFO MemoryStore: Block broadcast_203_piece0 stored as bytes in memory (estimated size 20.2 KiB, free 434.3 MiB)
26/01/04 17:09:42 INFO BlockManagerInfo: Added broadcast_203_piece0 in memory on spark-master:33535 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:09:42 INFO SparkContext: Created broadcast 203 from broadcast at DAGScheduler.scala:1585
26/01/04 17:09:42 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 137 (MapPartitionsRDD[687] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/01/04 17:09:42 INFO TaskSchedulerImpl: Adding task set 137.0 with 1 tasks resource profile 0
26/01/04 17:09:42 INFO TaskSetManager: Starting task 0.0 in stage 137.0 (TID 241) (172.18.0.13, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:09:42 INFO BlockManagerInfo: Added broadcast_203_piece0 in memory on 172.18.0.13:44401 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:09:42 INFO BlockManagerInfo: Added broadcast_202_piece0 in memory on 172.18.0.13:44401 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:09:43 INFO TaskSetManager: Finished task 0.0 in stage 137.0 (TID 241) in 546 ms on 172.18.0.13 (executor 0) (1/1)
26/01/04 17:09:43 INFO TaskSchedulerImpl: Removed TaskSet 137.0, whose tasks have all completed, from pool 
26/01/04 17:09:43 INFO DAGScheduler: ResultStage 137 (start at NativeMethodAccessorImpl.java:0) finished in 0.551 s
26/01/04 17:09:43 INFO DAGScheduler: Job 136 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:09:43 INFO TaskSchedulerImpl: Killing all running tasks in stage 137: Stage finished
26/01/04 17:09:43 INFO DAGScheduler: Job 136 finished: start at NativeMethodAccessorImpl.java:0, took 0.552451 s
26/01/04 17:09:43 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 66, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@5c1bb664] is committing.
26/01/04 17:09:43 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 66, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@5c1bb664] committed.
26/01/04 17:09:43 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/66 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.66.3252a54d-7e11-48af-a04c-30bf5ad3d612.tmp
26/01/04 17:09:43 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.66.3252a54d-7e11-48af-a04c-30bf5ad3d612.tmp to file:/tmp/spark-checkpoint-enrichment/commits/66
26/01/04 17:09:43 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T17:09:42.274Z",
  "batchId" : 66,
  "numInputRows" : 30,
  "inputRowsPerSecond" : 2500.0,
  "processedRowsPerSecond" : 36.99136868064118,
  "durationMs" : {
    "addBatch" : 656,
    "commitOffsets" : 67,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 17,
    "triggerExecution" : 811,
    "walCommit" : 69
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "0" : 1269
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "0" : 1299
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "0" : 1299
      }
    },
    "numInputRows" : 30,
    "inputRowsPerSecond" : 2500.0,
    "processedRowsPerSecond" : 36.99136868064118,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 31
  }
}
26/01/04 17:09:53 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/01/04 17:09:53 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/67 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.67.5956df09-9030-479a-8d04-df830f17687d.tmp
26/01/04 17:09:53 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.67.5956df09-9030-479a-8d04-df830f17687d.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/67
26/01/04 17:09:53 INFO MicroBatchExecution: Committed offsets for batch 67. Metadata OffsetSeqMetadata(0,1767546593289,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 17:09:53 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:09:53 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:09:53 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:09:53 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:09:53 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:09:53 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:09:53 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#55351 - airline_prefix.nullCount#55350) > 0)
26/01/04 17:09:53 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#55386 - min_flight_num.nullCount#55385) > 0)
26/01/04 17:09:53 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#55381 - max_flight_num.nullCount#55380) > 0)
26/01/04 17:09:53 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:09:53 INFO DAGScheduler: Got job 137 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 17:09:53 INFO DAGScheduler: Final stage: ResultStage 138 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:09:53 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:09:53 INFO DAGScheduler: Missing parents: List()
26/01/04 17:09:53 INFO DAGScheduler: Submitting ResultStage 138 (MapPartitionsRDD[692] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:09:53 INFO MemoryStore: Block broadcast_204 stored as values in memory (estimated size 38.5 KiB, free 434.2 MiB)
26/01/04 17:09:53 INFO MemoryStore: Block broadcast_204_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.2 MiB)
26/01/04 17:09:53 INFO BlockManagerInfo: Added broadcast_204_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:09:53 INFO SparkContext: Created broadcast 204 from broadcast at DAGScheduler.scala:1585
26/01/04 17:09:53 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 138 (MapPartitionsRDD[692] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 17:09:53 INFO TaskSchedulerImpl: Adding task set 138.0 with 2 tasks resource profile 0
26/01/04 17:09:53 INFO TaskSetManager: Starting task 1.0 in stage 138.0 (TID 242) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 17:09:53 INFO TaskSetManager: Starting task 0.0 in stage 138.0 (TID 243) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 17:09:53 INFO BlockManagerInfo: Added broadcast_204_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:09:53 INFO BlockManagerInfo: Added broadcast_204_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:09:53 INFO TaskSetManager: Finished task 1.0 in stage 138.0 (TID 242) in 19 ms on 172.18.0.13 (executor 0) (1/2)
26/01/04 17:09:53 INFO TaskSetManager: Finished task 0.0 in stage 138.0 (TID 243) in 35 ms on 172.18.0.14 (executor 1) (2/2)
26/01/04 17:09:53 INFO TaskSchedulerImpl: Removed TaskSet 138.0, whose tasks have all completed, from pool 
26/01/04 17:09:53 INFO DAGScheduler: ResultStage 138 (start at NativeMethodAccessorImpl.java:0) finished in 0.042 s
26/01/04 17:09:53 INFO DAGScheduler: Job 137 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:09:53 INFO TaskSchedulerImpl: Killing all running tasks in stage 138: Stage finished
26/01/04 17:09:53 INFO DAGScheduler: Job 137 finished: start at NativeMethodAccessorImpl.java:0, took 0.043173 s
26/01/04 17:09:53 INFO MemoryStore: Block broadcast_205_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.2 MiB)
26/01/04 17:09:53 INFO BlockManagerInfo: Added broadcast_205_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:09:53 INFO SparkContext: Created broadcast 205 from start at NativeMethodAccessorImpl.java:0
26/01/04 17:09:53 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 67, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@8481a68]. The input RDD has 1 partitions.
26/01/04 17:09:53 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:09:53 INFO DAGScheduler: Got job 138 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/01/04 17:09:53 INFO DAGScheduler: Final stage: ResultStage 139 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:09:53 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:09:53 INFO DAGScheduler: Missing parents: List()
26/01/04 17:09:53 INFO DAGScheduler: Submitting ResultStage 139 (MapPartitionsRDD[697] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:09:53 INFO MemoryStore: Block broadcast_206 stored as values in memory (estimated size 52.9 KiB, free 434.2 MiB)
26/01/04 17:09:53 INFO MemoryStore: Block broadcast_206_piece0 stored as bytes in memory (estimated size 20.2 KiB, free 434.1 MiB)
26/01/04 17:09:53 INFO BlockManagerInfo: Added broadcast_206_piece0 in memory on spark-master:33535 (size: 20.2 KiB, free: 434.3 MiB)
26/01/04 17:09:53 INFO SparkContext: Created broadcast 206 from broadcast at DAGScheduler.scala:1585
26/01/04 17:09:53 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 139 (MapPartitionsRDD[697] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/01/04 17:09:53 INFO TaskSchedulerImpl: Adding task set 139.0 with 1 tasks resource profile 0
26/01/04 17:09:53 INFO TaskSetManager: Starting task 0.0 in stage 139.0 (TID 244) (172.18.0.13, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:09:53 INFO BlockManagerInfo: Added broadcast_206_piece0 in memory on 172.18.0.13:44401 (size: 20.2 KiB, free: 434.3 MiB)
26/01/04 17:09:53 INFO BlockManagerInfo: Added broadcast_205_piece0 in memory on 172.18.0.13:44401 (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:09:54 INFO TaskSetManager: Finished task 0.0 in stage 139.0 (TID 244) in 564 ms on 172.18.0.13 (executor 0) (1/1)
26/01/04 17:09:54 INFO TaskSchedulerImpl: Removed TaskSet 139.0, whose tasks have all completed, from pool 
26/01/04 17:09:54 INFO DAGScheduler: ResultStage 139 (start at NativeMethodAccessorImpl.java:0) finished in 0.570 s
26/01/04 17:09:54 INFO DAGScheduler: Job 138 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:09:54 INFO TaskSchedulerImpl: Killing all running tasks in stage 139: Stage finished
26/01/04 17:09:54 INFO DAGScheduler: Job 138 finished: start at NativeMethodAccessorImpl.java:0, took 0.571873 s
26/01/04 17:09:54 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 67, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@8481a68] is committing.
26/01/04 17:09:54 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 67, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@8481a68] committed.
26/01/04 17:09:54 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/67 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.67.992afc0d-7f3c-4be0-865b-d82f1fe54828.tmp
26/01/04 17:09:54 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.67.992afc0d-7f3c-4be0-865b-d82f1fe54828.tmp to file:/tmp/spark-checkpoint-enrichment/commits/67
26/01/04 17:09:54 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T17:09:53.288Z",
  "batchId" : 67,
  "numInputRows" : 30,
  "inputRowsPerSecond" : 2727.2727272727275,
  "processedRowsPerSecond" : 34.80278422273782,
  "durationMs" : {
    "addBatch" : 686,
    "commitOffsets" : 70,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 23,
    "triggerExecution" : 862,
    "walCommit" : 82
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "0" : 1299
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "0" : 1329
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "0" : 1329
      }
    },
    "numInputRows" : 30,
    "inputRowsPerSecond" : 2727.2727272727275,
    "processedRowsPerSecond" : 34.80278422273782,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 31
  }
}
26/01/04 17:09:55 INFO BlockManagerInfo: Removed broadcast_203_piece0 on spark-master:33535 in memory (size: 20.2 KiB, free: 434.3 MiB)
26/01/04 17:09:55 INFO BlockManagerInfo: Removed broadcast_203_piece0 on 172.18.0.13:44401 in memory (size: 20.2 KiB, free: 434.3 MiB)
26/01/04 17:09:55 INFO BlockManagerInfo: Removed broadcast_206_piece0 on spark-master:33535 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:09:55 INFO BlockManagerInfo: Removed broadcast_206_piece0 on 172.18.0.13:44401 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:09:55 INFO BlockManagerInfo: Removed broadcast_204_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:09:55 INFO BlockManagerInfo: Removed broadcast_204_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:09:55 INFO BlockManagerInfo: Removed broadcast_204_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:09:55 INFO BlockManagerInfo: Removed broadcast_201_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:09:55 INFO BlockManagerInfo: Removed broadcast_201_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:09:55 INFO BlockManagerInfo: Removed broadcast_201_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:09:55 INFO BlockManagerInfo: Removed broadcast_202_piece0 on spark-master:33535 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:09:55 INFO BlockManagerInfo: Removed broadcast_202_piece0 on 172.18.0.13:44401 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:10:04 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/01/04 17:10:04 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/68 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.68.6d60eb8f-c164-4137-bac4-954c145a10d1.tmp
26/01/04 17:10:04 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.68.6d60eb8f-c164-4137-bac4-954c145a10d1.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/68
26/01/04 17:10:04 INFO MicroBatchExecution: Committed offsets for batch 68. Metadata OffsetSeqMetadata(0,1767546604304,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 17:10:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:10:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:10:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:10:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:10:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:10:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:10:04 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#56155 - airline_prefix.nullCount#56154) > 0)
26/01/04 17:10:04 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#56190 - min_flight_num.nullCount#56189) > 0)
26/01/04 17:10:04 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#56185 - max_flight_num.nullCount#56184) > 0)
26/01/04 17:10:04 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:10:04 INFO DAGScheduler: Got job 139 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 17:10:04 INFO DAGScheduler: Final stage: ResultStage 140 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:10:04 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:10:04 INFO DAGScheduler: Missing parents: List()
26/01/04 17:10:04 INFO DAGScheduler: Submitting ResultStage 140 (MapPartitionsRDD[702] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:10:04 INFO MemoryStore: Block broadcast_207 stored as values in memory (estimated size 38.5 KiB, free 434.4 MiB)
26/01/04 17:10:04 INFO MemoryStore: Block broadcast_207_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.3 MiB)
26/01/04 17:10:04 INFO BlockManagerInfo: Added broadcast_207_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:10:04 INFO SparkContext: Created broadcast 207 from broadcast at DAGScheduler.scala:1585
26/01/04 17:10:04 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 140 (MapPartitionsRDD[702] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 17:10:04 INFO TaskSchedulerImpl: Adding task set 140.0 with 2 tasks resource profile 0
26/01/04 17:10:04 INFO TaskSetManager: Starting task 0.0 in stage 140.0 (TID 245) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 17:10:04 INFO TaskSetManager: Starting task 1.0 in stage 140.0 (TID 246) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 17:10:04 INFO BlockManagerInfo: Added broadcast_207_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:10:04 INFO BlockManagerInfo: Added broadcast_207_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:10:04 INFO TaskSetManager: Finished task 1.0 in stage 140.0 (TID 246) in 56 ms on 172.18.0.13 (executor 0) (1/2)
26/01/04 17:10:04 INFO TaskSetManager: Finished task 0.0 in stage 140.0 (TID 245) in 118 ms on 172.18.0.14 (executor 1) (2/2)
26/01/04 17:10:04 INFO TaskSchedulerImpl: Removed TaskSet 140.0, whose tasks have all completed, from pool 
26/01/04 17:10:04 INFO DAGScheduler: ResultStage 140 (start at NativeMethodAccessorImpl.java:0) finished in 0.156 s
26/01/04 17:10:04 INFO DAGScheduler: Job 139 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:10:04 INFO TaskSchedulerImpl: Killing all running tasks in stage 140: Stage finished
26/01/04 17:10:04 INFO DAGScheduler: Job 139 finished: start at NativeMethodAccessorImpl.java:0, took 0.164482 s
26/01/04 17:10:04 INFO MemoryStore: Block broadcast_208_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.3 MiB)
26/01/04 17:10:04 INFO BlockManagerInfo: Added broadcast_208_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:10:04 INFO SparkContext: Created broadcast 208 from start at NativeMethodAccessorImpl.java:0
26/01/04 17:10:04 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 68, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@624fce26]. The input RDD has 1 partitions.
26/01/04 17:10:04 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:10:04 INFO DAGScheduler: Got job 140 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/01/04 17:10:04 INFO DAGScheduler: Final stage: ResultStage 141 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:10:04 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:10:04 INFO DAGScheduler: Missing parents: List()
26/01/04 17:10:04 INFO DAGScheduler: Submitting ResultStage 141 (MapPartitionsRDD[707] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:10:04 INFO MemoryStore: Block broadcast_209 stored as values in memory (estimated size 52.9 KiB, free 434.3 MiB)
26/01/04 17:10:04 INFO MemoryStore: Block broadcast_209_piece0 stored as bytes in memory (estimated size 20.2 KiB, free 434.3 MiB)
26/01/04 17:10:04 INFO BlockManagerInfo: Added broadcast_209_piece0 in memory on spark-master:33535 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:10:04 INFO SparkContext: Created broadcast 209 from broadcast at DAGScheduler.scala:1585
26/01/04 17:10:04 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 141 (MapPartitionsRDD[707] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/01/04 17:10:04 INFO TaskSchedulerImpl: Adding task set 141.0 with 1 tasks resource profile 0
26/01/04 17:10:04 INFO TaskSetManager: Starting task 0.0 in stage 141.0 (TID 247) (172.18.0.13, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:10:05 INFO BlockManagerInfo: Added broadcast_209_piece0 in memory on 172.18.0.13:44401 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:10:05 INFO BlockManagerInfo: Added broadcast_208_piece0 in memory on 172.18.0.13:44401 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:10:05 INFO TaskSetManager: Finished task 0.0 in stage 141.0 (TID 247) in 718 ms on 172.18.0.13 (executor 0) (1/1)
26/01/04 17:10:05 INFO TaskSchedulerImpl: Removed TaskSet 141.0, whose tasks have all completed, from pool 
26/01/04 17:10:05 INFO DAGScheduler: ResultStage 141 (start at NativeMethodAccessorImpl.java:0) finished in 0.736 s
26/01/04 17:10:05 INFO DAGScheduler: Job 140 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:10:05 INFO TaskSchedulerImpl: Killing all running tasks in stage 141: Stage finished
26/01/04 17:10:05 INFO DAGScheduler: Job 140 finished: start at NativeMethodAccessorImpl.java:0, took 0.748139 s
26/01/04 17:10:05 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 68, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@624fce26] is committing.
26/01/04 17:10:05 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 68, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@624fce26] committed.
26/01/04 17:10:05 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/68 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.68.07db9e5f-9d6a-4454-be37-f25973f750ba.tmp
26/01/04 17:10:05 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.68.07db9e5f-9d6a-4454-be37-f25973f750ba.tmp to file:/tmp/spark-checkpoint-enrichment/commits/68
26/01/04 17:10:05 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T17:10:04.293Z",
  "batchId" : 68,
  "numInputRows" : 14,
  "inputRowsPerSecond" : 933.3333333333334,
  "processedRowsPerSecond" : 9.085009733939001,
  "durationMs" : {
    "addBatch" : 1157,
    "commitOffsets" : 151,
    "getBatch" : 0,
    "latestOffset" : 11,
    "queryPlanning" : 57,
    "triggerExecution" : 1541,
    "walCommit" : 163
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "0" : 1329
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "0" : 1343
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "0" : 1343
      }
    },
    "numInputRows" : 14,
    "inputRowsPerSecond" : 933.3333333333334,
    "processedRowsPerSecond" : 9.085009733939001,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 14
  }
}
26/01/04 17:10:05 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/69 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.69.b915402e-e3cc-414e-8bc1-4cebb421791c.tmp
26/01/04 17:10:06 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.69.b915402e-e3cc-414e-8bc1-4cebb421791c.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/69
26/01/04 17:10:06 INFO MicroBatchExecution: Committed offsets for batch 69. Metadata OffsetSeqMetadata(0,1767546605843,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 17:10:06 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:10:06 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:10:06 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:10:06 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:10:06 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:10:06 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:10:06 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#56959 - airline_prefix.nullCount#56958) > 0)
26/01/04 17:10:06 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#56994 - min_flight_num.nullCount#56993) > 0)
26/01/04 17:10:06 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#56989 - max_flight_num.nullCount#56988) > 0)
26/01/04 17:10:06 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:10:06 INFO DAGScheduler: Got job 141 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 17:10:06 INFO DAGScheduler: Final stage: ResultStage 142 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:10:06 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:10:06 INFO DAGScheduler: Missing parents: List()
26/01/04 17:10:06 INFO DAGScheduler: Submitting ResultStage 142 (MapPartitionsRDD[712] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:10:06 INFO MemoryStore: Block broadcast_210 stored as values in memory (estimated size 38.5 KiB, free 434.2 MiB)
26/01/04 17:10:06 INFO MemoryStore: Block broadcast_210_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.2 MiB)
26/01/04 17:10:06 INFO BlockManagerInfo: Added broadcast_210_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:10:06 INFO SparkContext: Created broadcast 210 from broadcast at DAGScheduler.scala:1585
26/01/04 17:10:06 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 142 (MapPartitionsRDD[712] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 17:10:06 INFO TaskSchedulerImpl: Adding task set 142.0 with 2 tasks resource profile 0
26/01/04 17:10:06 INFO TaskSetManager: Starting task 1.0 in stage 142.0 (TID 248) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 17:10:06 INFO TaskSetManager: Starting task 0.0 in stage 142.0 (TID 249) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 17:10:06 INFO BlockManagerInfo: Added broadcast_210_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:10:06 INFO BlockManagerInfo: Added broadcast_210_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:10:06 INFO TaskSetManager: Finished task 1.0 in stage 142.0 (TID 248) in 75 ms on 172.18.0.13 (executor 0) (1/2)
26/01/04 17:10:06 INFO TaskSetManager: Finished task 0.0 in stage 142.0 (TID 249) in 143 ms on 172.18.0.14 (executor 1) (2/2)
26/01/04 17:10:06 INFO TaskSchedulerImpl: Removed TaskSet 142.0, whose tasks have all completed, from pool 
26/01/04 17:10:06 INFO DAGScheduler: ResultStage 142 (start at NativeMethodAccessorImpl.java:0) finished in 0.160 s
26/01/04 17:10:06 INFO DAGScheduler: Job 141 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:10:06 INFO TaskSchedulerImpl: Killing all running tasks in stage 142: Stage finished
26/01/04 17:10:06 INFO DAGScheduler: Job 141 finished: start at NativeMethodAccessorImpl.java:0, took 0.169453 s
26/01/04 17:10:06 INFO MemoryStore: Block broadcast_211_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.2 MiB)
26/01/04 17:10:06 INFO BlockManagerInfo: Added broadcast_211_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:10:06 INFO SparkContext: Created broadcast 211 from start at NativeMethodAccessorImpl.java:0
26/01/04 17:10:06 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 69, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@6b2ab2ac]. The input RDD has 1 partitions.
26/01/04 17:10:06 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:10:06 INFO DAGScheduler: Got job 142 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/01/04 17:10:06 INFO DAGScheduler: Final stage: ResultStage 143 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:10:06 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:10:06 INFO DAGScheduler: Missing parents: List()
26/01/04 17:10:06 INFO DAGScheduler: Submitting ResultStage 143 (MapPartitionsRDD[717] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:10:06 INFO MemoryStore: Block broadcast_212 stored as values in memory (estimated size 52.9 KiB, free 434.2 MiB)
26/01/04 17:10:06 INFO MemoryStore: Block broadcast_212_piece0 stored as bytes in memory (estimated size 20.1 KiB, free 434.1 MiB)
26/01/04 17:10:06 INFO BlockManagerInfo: Added broadcast_212_piece0 in memory on spark-master:33535 (size: 20.1 KiB, free: 434.3 MiB)
26/01/04 17:10:06 INFO SparkContext: Created broadcast 212 from broadcast at DAGScheduler.scala:1585
26/01/04 17:10:06 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 143 (MapPartitionsRDD[717] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/01/04 17:10:06 INFO TaskSchedulerImpl: Adding task set 143.0 with 1 tasks resource profile 0
26/01/04 17:10:06 INFO TaskSetManager: Starting task 0.0 in stage 143.0 (TID 250) (172.18.0.13, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:10:06 INFO BlockManagerInfo: Added broadcast_212_piece0 in memory on 172.18.0.13:44401 (size: 20.1 KiB, free: 434.3 MiB)
26/01/04 17:10:06 INFO BlockManagerInfo: Added broadcast_211_piece0 in memory on 172.18.0.13:44401 (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:10:06 INFO TaskSetManager: Finished task 0.0 in stage 143.0 (TID 250) in 142 ms on 172.18.0.13 (executor 0) (1/1)
26/01/04 17:10:06 INFO TaskSchedulerImpl: Removed TaskSet 143.0, whose tasks have all completed, from pool 
26/01/04 17:10:06 INFO DAGScheduler: ResultStage 143 (start at NativeMethodAccessorImpl.java:0) finished in 0.156 s
26/01/04 17:10:06 INFO DAGScheduler: Job 142 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:10:06 INFO TaskSchedulerImpl: Killing all running tasks in stage 143: Stage finished
26/01/04 17:10:06 INFO DAGScheduler: Job 142 finished: start at NativeMethodAccessorImpl.java:0, took 0.163122 s
26/01/04 17:10:06 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 69, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@6b2ab2ac] is committing.
26/01/04 17:10:06 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 69, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@6b2ab2ac] committed.
26/01/04 17:10:06 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/69 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.69.1e8788f4-aa8d-4cc7-9b93-2481d63e9d68.tmp
26/01/04 17:10:06 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.69.1e8788f4-aa8d-4cc7-9b93-2481d63e9d68.tmp to file:/tmp/spark-checkpoint-enrichment/commits/69
26/01/04 17:10:06 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T17:10:05.837Z",
  "batchId" : 69,
  "numInputRows" : 16,
  "inputRowsPerSecond" : 10.362694300518134,
  "processedRowsPerSecond" : 16.701461377870565,
  "durationMs" : {
    "addBatch" : 534,
    "commitOffsets" : 182,
    "getBatch" : 1,
    "latestOffset" : 6,
    "queryPlanning" : 78,
    "triggerExecution" : 958,
    "walCommit" : 157
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "0" : 1343
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "0" : 1359
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "0" : 1359
      }
    },
    "numInputRows" : 16,
    "inputRowsPerSecond" : 10.362694300518134,
    "processedRowsPerSecond" : 16.701461377870565,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 17
  }
}
26/01/04 17:10:09 INFO BlockManagerInfo: Removed broadcast_210_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:10:09 INFO BlockManagerInfo: Removed broadcast_210_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:10:09 INFO BlockManagerInfo: Removed broadcast_210_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:10:09 INFO BlockManagerInfo: Removed broadcast_208_piece0 on spark-master:33535 in memory (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:10:09 INFO BlockManagerInfo: Removed broadcast_208_piece0 on 172.18.0.13:44401 in memory (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:10:09 INFO BlockManagerInfo: Removed broadcast_205_piece0 on 172.18.0.13:44401 in memory (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:10:10 INFO BlockManagerInfo: Removed broadcast_205_piece0 on spark-master:33535 in memory (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:10:10 INFO BlockManagerInfo: Removed broadcast_212_piece0 on spark-master:33535 in memory (size: 20.1 KiB, free: 434.4 MiB)
26/01/04 17:10:10 INFO BlockManagerInfo: Removed broadcast_212_piece0 on 172.18.0.13:44401 in memory (size: 20.1 KiB, free: 434.4 MiB)
26/01/04 17:10:10 INFO BlockManagerInfo: Removed broadcast_207_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:10:10 INFO BlockManagerInfo: Removed broadcast_207_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:10:10 INFO BlockManagerInfo: Removed broadcast_207_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:10:10 INFO BlockManagerInfo: Removed broadcast_209_piece0 on spark-master:33535 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:10:10 INFO BlockManagerInfo: Removed broadcast_209_piece0 on 172.18.0.13:44401 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:10:15 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/70 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.70.c2e9604f-39cd-437a-b0b7-5f9118e657d1.tmp
26/01/04 17:10:15 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.70.c2e9604f-39cd-437a-b0b7-5f9118e657d1.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/70
26/01/04 17:10:15 INFO MicroBatchExecution: Committed offsets for batch 70. Metadata OffsetSeqMetadata(0,1767546615345,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 17:10:15 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:10:15 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:10:15 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:10:15 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:10:15 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:10:15 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:10:15 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#57763 - airline_prefix.nullCount#57762) > 0)
26/01/04 17:10:15 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#57798 - min_flight_num.nullCount#57797) > 0)
26/01/04 17:10:15 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#57793 - max_flight_num.nullCount#57792) > 0)
26/01/04 17:10:15 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:10:15 INFO DAGScheduler: Got job 143 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 17:10:15 INFO DAGScheduler: Final stage: ResultStage 144 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:10:15 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:10:15 INFO DAGScheduler: Missing parents: List()
26/01/04 17:10:15 INFO DAGScheduler: Submitting ResultStage 144 (MapPartitionsRDD[722] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:10:15 INFO MemoryStore: Block broadcast_213 stored as values in memory (estimated size 38.5 KiB, free 434.4 MiB)
26/01/04 17:10:15 INFO MemoryStore: Block broadcast_213_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.3 MiB)
26/01/04 17:10:15 INFO BlockManagerInfo: Added broadcast_213_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:10:15 INFO SparkContext: Created broadcast 213 from broadcast at DAGScheduler.scala:1585
26/01/04 17:10:15 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 144 (MapPartitionsRDD[722] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 17:10:15 INFO TaskSchedulerImpl: Adding task set 144.0 with 2 tasks resource profile 0
26/01/04 17:10:15 INFO TaskSetManager: Starting task 0.0 in stage 144.0 (TID 251) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 17:10:15 INFO TaskSetManager: Starting task 1.0 in stage 144.0 (TID 252) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 17:10:15 INFO BlockManagerInfo: Removed broadcast_211_piece0 on spark-master:33535 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:10:15 INFO BlockManagerInfo: Removed broadcast_211_piece0 on 172.18.0.13:44401 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:10:15 INFO BlockManagerInfo: Added broadcast_213_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:10:15 INFO BlockManagerInfo: Added broadcast_213_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:10:15 INFO TaskSetManager: Finished task 1.0 in stage 144.0 (TID 252) in 103 ms on 172.18.0.13 (executor 0) (1/2)
26/01/04 17:10:16 INFO TaskSetManager: Finished task 0.0 in stage 144.0 (TID 251) in 171 ms on 172.18.0.14 (executor 1) (2/2)
26/01/04 17:10:16 INFO TaskSchedulerImpl: Removed TaskSet 144.0, whose tasks have all completed, from pool 
26/01/04 17:10:16 INFO DAGScheduler: ResultStage 144 (start at NativeMethodAccessorImpl.java:0) finished in 0.209 s
26/01/04 17:10:16 INFO DAGScheduler: Job 143 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:10:16 INFO TaskSchedulerImpl: Killing all running tasks in stage 144: Stage finished
26/01/04 17:10:16 INFO DAGScheduler: Job 143 finished: start at NativeMethodAccessorImpl.java:0, took 0.220627 s
26/01/04 17:10:16 INFO MemoryStore: Block broadcast_214_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.3 MiB)
26/01/04 17:10:16 INFO BlockManagerInfo: Added broadcast_214_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:10:16 INFO SparkContext: Created broadcast 214 from start at NativeMethodAccessorImpl.java:0
26/01/04 17:10:16 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 70, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@1977e979]. The input RDD has 1 partitions.
26/01/04 17:10:16 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:10:16 INFO DAGScheduler: Got job 144 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/01/04 17:10:16 INFO DAGScheduler: Final stage: ResultStage 145 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:10:16 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:10:16 INFO DAGScheduler: Missing parents: List()
26/01/04 17:10:16 INFO DAGScheduler: Submitting ResultStage 145 (MapPartitionsRDD[727] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:10:16 INFO MemoryStore: Block broadcast_215 stored as values in memory (estimated size 52.9 KiB, free 434.3 MiB)
26/01/04 17:10:16 INFO MemoryStore: Block broadcast_215_piece0 stored as bytes in memory (estimated size 20.2 KiB, free 434.3 MiB)
26/01/04 17:10:16 INFO BlockManagerInfo: Added broadcast_215_piece0 in memory on spark-master:33535 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:10:16 INFO SparkContext: Created broadcast 215 from broadcast at DAGScheduler.scala:1585
26/01/04 17:10:16 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 145 (MapPartitionsRDD[727] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/01/04 17:10:16 INFO TaskSchedulerImpl: Adding task set 145.0 with 1 tasks resource profile 0
26/01/04 17:10:16 INFO TaskSetManager: Starting task 0.0 in stage 145.0 (TID 253) (172.18.0.13, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:10:16 INFO BlockManagerInfo: Added broadcast_215_piece0 in memory on 172.18.0.13:44401 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:10:16 INFO BlockManagerInfo: Added broadcast_214_piece0 in memory on 172.18.0.13:44401 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:10:16 INFO TaskSetManager: Finished task 0.0 in stage 145.0 (TID 253) in 720 ms on 172.18.0.13 (executor 0) (1/1)
26/01/04 17:10:16 INFO DAGScheduler: ResultStage 145 (start at NativeMethodAccessorImpl.java:0) finished in 0.742 s
26/01/04 17:10:16 INFO DAGScheduler: Job 144 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:10:16 INFO TaskSchedulerImpl: Removed TaskSet 145.0, whose tasks have all completed, from pool 
26/01/04 17:10:16 INFO TaskSchedulerImpl: Killing all running tasks in stage 145: Stage finished
26/01/04 17:10:16 INFO DAGScheduler: Job 144 finished: start at NativeMethodAccessorImpl.java:0, took 0.748453 s
26/01/04 17:10:16 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 70, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@1977e979] is committing.
26/01/04 17:10:16 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 70, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@1977e979] committed.
26/01/04 17:10:16 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/70 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.70.04bcb782-b247-4508-8a70-a0ae5352e9e2.tmp
26/01/04 17:10:17 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.70.04bcb782-b247-4508-8a70-a0ae5352e9e2.tmp to file:/tmp/spark-checkpoint-enrichment/commits/70
26/01/04 17:10:17 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T17:10:15.341Z",
  "batchId" : 70,
  "numInputRows" : 30,
  "inputRowsPerSecond" : 1764.705882352941,
  "processedRowsPerSecond" : 17.636684303350968,
  "durationMs" : {
    "addBatch" : 1145,
    "commitOffsets" : 229,
    "getBatch" : 0,
    "latestOffset" : 4,
    "queryPlanning" : 84,
    "triggerExecution" : 1701,
    "walCommit" : 237
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "0" : 1359
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "0" : 1389
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "0" : 1389
      }
    },
    "numInputRows" : 30,
    "inputRowsPerSecond" : 1764.705882352941,
    "processedRowsPerSecond" : 17.636684303350968,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 31
  }
}
26/01/04 17:10:26 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/71 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.71.d8cf9689-b8c8-4b71-b3f3-53d1391959c7.tmp
26/01/04 17:10:26 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.71.d8cf9689-b8c8-4b71-b3f3-53d1391959c7.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/71
26/01/04 17:10:26 INFO MicroBatchExecution: Committed offsets for batch 71. Metadata OffsetSeqMetadata(0,1767546626364,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 17:10:26 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:10:26 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:10:26 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:10:26 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:10:26 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:10:26 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:10:26 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#58567 - airline_prefix.nullCount#58566) > 0)
26/01/04 17:10:26 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#58602 - min_flight_num.nullCount#58601) > 0)
26/01/04 17:10:26 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#58597 - max_flight_num.nullCount#58596) > 0)
26/01/04 17:10:26 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:10:26 INFO DAGScheduler: Got job 145 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 17:10:26 INFO DAGScheduler: Final stage: ResultStage 146 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:10:26 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:10:26 INFO DAGScheduler: Missing parents: List()
26/01/04 17:10:26 INFO DAGScheduler: Submitting ResultStage 146 (MapPartitionsRDD[732] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:10:26 INFO MemoryStore: Block broadcast_216 stored as values in memory (estimated size 38.5 KiB, free 434.2 MiB)
26/01/04 17:10:26 INFO MemoryStore: Block broadcast_216_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.2 MiB)
26/01/04 17:10:26 INFO BlockManagerInfo: Added broadcast_216_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:10:26 INFO SparkContext: Created broadcast 216 from broadcast at DAGScheduler.scala:1585
26/01/04 17:10:26 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 146 (MapPartitionsRDD[732] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 17:10:26 INFO TaskSchedulerImpl: Adding task set 146.0 with 2 tasks resource profile 0
26/01/04 17:10:26 INFO TaskSetManager: Starting task 1.0 in stage 146.0 (TID 254) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 17:10:26 INFO TaskSetManager: Starting task 0.0 in stage 146.0 (TID 255) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 17:10:26 INFO BlockManagerInfo: Added broadcast_216_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:10:26 INFO BlockManagerInfo: Added broadcast_216_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:10:26 INFO TaskSetManager: Finished task 1.0 in stage 146.0 (TID 254) in 82 ms on 172.18.0.13 (executor 0) (1/2)
26/01/04 17:10:26 INFO TaskSetManager: Finished task 0.0 in stage 146.0 (TID 255) in 105 ms on 172.18.0.14 (executor 1) (2/2)
26/01/04 17:10:26 INFO TaskSchedulerImpl: Removed TaskSet 146.0, whose tasks have all completed, from pool 
26/01/04 17:10:26 INFO DAGScheduler: ResultStage 146 (start at NativeMethodAccessorImpl.java:0) finished in 0.122 s
26/01/04 17:10:26 INFO DAGScheduler: Job 145 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:10:26 INFO TaskSchedulerImpl: Killing all running tasks in stage 146: Stage finished
26/01/04 17:10:26 INFO DAGScheduler: Job 145 finished: start at NativeMethodAccessorImpl.java:0, took 0.132037 s
26/01/04 17:10:26 INFO MemoryStore: Block broadcast_217_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.2 MiB)
26/01/04 17:10:26 INFO BlockManagerInfo: Added broadcast_217_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:10:26 INFO SparkContext: Created broadcast 217 from start at NativeMethodAccessorImpl.java:0
26/01/04 17:10:26 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 71, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@1127e98a]. The input RDD has 1 partitions.
26/01/04 17:10:26 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:10:26 INFO DAGScheduler: Got job 146 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/01/04 17:10:26 INFO DAGScheduler: Final stage: ResultStage 147 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:10:26 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:10:26 INFO DAGScheduler: Missing parents: List()
26/01/04 17:10:26 INFO DAGScheduler: Submitting ResultStage 147 (MapPartitionsRDD[737] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:10:26 INFO MemoryStore: Block broadcast_218 stored as values in memory (estimated size 52.9 KiB, free 434.2 MiB)
26/01/04 17:10:26 INFO MemoryStore: Block broadcast_218_piece0 stored as bytes in memory (estimated size 20.2 KiB, free 434.1 MiB)
26/01/04 17:10:26 INFO BlockManagerInfo: Added broadcast_218_piece0 in memory on spark-master:33535 (size: 20.2 KiB, free: 434.3 MiB)
26/01/04 17:10:26 INFO SparkContext: Created broadcast 218 from broadcast at DAGScheduler.scala:1585
26/01/04 17:10:26 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 147 (MapPartitionsRDD[737] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/01/04 17:10:26 INFO TaskSchedulerImpl: Adding task set 147.0 with 1 tasks resource profile 0
26/01/04 17:10:26 INFO TaskSetManager: Starting task 0.0 in stage 147.0 (TID 256) (172.18.0.13, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:10:26 INFO BlockManagerInfo: Added broadcast_218_piece0 in memory on 172.18.0.13:44401 (size: 20.2 KiB, free: 434.3 MiB)
26/01/04 17:10:27 INFO BlockManagerInfo: Added broadcast_217_piece0 in memory on 172.18.0.13:44401 (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:10:27 INFO TaskSetManager: Finished task 0.0 in stage 147.0 (TID 256) in 679 ms on 172.18.0.13 (executor 0) (1/1)
26/01/04 17:10:27 INFO TaskSchedulerImpl: Removed TaskSet 147.0, whose tasks have all completed, from pool 
26/01/04 17:10:27 INFO DAGScheduler: ResultStage 147 (start at NativeMethodAccessorImpl.java:0) finished in 0.691 s
26/01/04 17:10:27 INFO DAGScheduler: Job 146 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:10:27 INFO TaskSchedulerImpl: Killing all running tasks in stage 147: Stage finished
26/01/04 17:10:27 INFO DAGScheduler: Job 146 finished: start at NativeMethodAccessorImpl.java:0, took 0.696735 s
26/01/04 17:10:27 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 71, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@1127e98a] is committing.
26/01/04 17:10:27 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 71, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@1127e98a] committed.
26/01/04 17:10:27 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/71 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.71.544dcfdc-46cf-4cb7-9ce9-eb75c3311f0e.tmp
26/01/04 17:10:27 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.71.544dcfdc-46cf-4cb7-9ce9-eb75c3311f0e.tmp to file:/tmp/spark-checkpoint-enrichment/commits/71
26/01/04 17:10:27 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T17:10:26.360Z",
  "batchId" : 71,
  "numInputRows" : 30,
  "inputRowsPerSecond" : 2000.0,
  "processedRowsPerSecond" : 20.833333333333336,
  "durationMs" : {
    "addBatch" : 1056,
    "commitOffsets" : 156,
    "getBatch" : 1,
    "latestOffset" : 4,
    "queryPlanning" : 77,
    "triggerExecution" : 1440,
    "walCommit" : 145
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "0" : 1389
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "0" : 1419
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "0" : 1419
      }
    },
    "numInputRows" : 30,
    "inputRowsPerSecond" : 2000.0,
    "processedRowsPerSecond" : 20.833333333333336,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 31
  }
}
26/01/04 17:10:35 INFO BlockManagerInfo: Removed broadcast_215_piece0 on spark-master:33535 in memory (size: 20.2 KiB, free: 434.3 MiB)
26/01/04 17:10:35 INFO BlockManagerInfo: Removed broadcast_215_piece0 on 172.18.0.13:44401 in memory (size: 20.2 KiB, free: 434.3 MiB)
26/01/04 17:10:35 INFO BlockManagerInfo: Removed broadcast_218_piece0 on spark-master:33535 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:10:35 INFO BlockManagerInfo: Removed broadcast_218_piece0 on 172.18.0.13:44401 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:10:35 INFO BlockManagerInfo: Removed broadcast_214_piece0 on spark-master:33535 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:10:35 INFO BlockManagerInfo: Removed broadcast_214_piece0 on 172.18.0.13:44401 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:10:35 INFO BlockManagerInfo: Removed broadcast_216_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:10:35 INFO BlockManagerInfo: Removed broadcast_216_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:10:35 INFO BlockManagerInfo: Removed broadcast_216_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:10:35 INFO BlockManagerInfo: Removed broadcast_213_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:10:35 INFO BlockManagerInfo: Removed broadcast_213_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:10:35 INFO BlockManagerInfo: Removed broadcast_213_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:10:37 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/72 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.72.35e50627-0641-4bf6-b47f-68a9d8efe2f3.tmp
26/01/04 17:10:37 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.72.35e50627-0641-4bf6-b47f-68a9d8efe2f3.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/72
26/01/04 17:10:37 INFO MicroBatchExecution: Committed offsets for batch 72. Metadata OffsetSeqMetadata(0,1767546637531,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 17:10:37 INFO BlockManagerInfo: Removed broadcast_217_piece0 on spark-master:33535 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:10:37 INFO BlockManagerInfo: Removed broadcast_217_piece0 on 172.18.0.13:44401 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:10:37 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:10:37 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:10:38 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:10:38 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:10:38 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:10:38 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:10:38 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#59371 - airline_prefix.nullCount#59370) > 0)
26/01/04 17:10:38 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#59406 - min_flight_num.nullCount#59405) > 0)
26/01/04 17:10:38 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#59401 - max_flight_num.nullCount#59400) > 0)
26/01/04 17:10:38 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:10:38 INFO DAGScheduler: Got job 147 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 17:10:38 INFO DAGScheduler: Final stage: ResultStage 148 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:10:38 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:10:38 INFO DAGScheduler: Missing parents: List()
26/01/04 17:10:38 INFO DAGScheduler: Submitting ResultStage 148 (MapPartitionsRDD[742] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:10:38 INFO MemoryStore: Block broadcast_219 stored as values in memory (estimated size 38.5 KiB, free 434.4 MiB)
26/01/04 17:10:38 INFO MemoryStore: Block broadcast_219_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.3 MiB)
26/01/04 17:10:38 INFO BlockManagerInfo: Added broadcast_219_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:10:38 INFO SparkContext: Created broadcast 219 from broadcast at DAGScheduler.scala:1585
26/01/04 17:10:38 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 148 (MapPartitionsRDD[742] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 17:10:38 INFO TaskSchedulerImpl: Adding task set 148.0 with 2 tasks resource profile 0
26/01/04 17:10:38 INFO TaskSetManager: Starting task 1.0 in stage 148.0 (TID 257) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 17:10:38 INFO TaskSetManager: Starting task 0.0 in stage 148.0 (TID 258) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 17:10:38 INFO BlockManagerInfo: Added broadcast_219_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:10:38 INFO TaskSetManager: Finished task 1.0 in stage 148.0 (TID 257) in 59 ms on 172.18.0.13 (executor 0) (1/2)
26/01/04 17:10:38 INFO BlockManagerInfo: Added broadcast_219_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:10:38 INFO TaskSetManager: Finished task 0.0 in stage 148.0 (TID 258) in 128 ms on 172.18.0.14 (executor 1) (2/2)
26/01/04 17:10:38 INFO TaskSchedulerImpl: Removed TaskSet 148.0, whose tasks have all completed, from pool 
26/01/04 17:10:38 INFO DAGScheduler: ResultStage 148 (start at NativeMethodAccessorImpl.java:0) finished in 0.143 s
26/01/04 17:10:38 INFO DAGScheduler: Job 147 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:10:38 INFO TaskSchedulerImpl: Killing all running tasks in stage 148: Stage finished
26/01/04 17:10:38 INFO DAGScheduler: Job 147 finished: start at NativeMethodAccessorImpl.java:0, took 0.153399 s
26/01/04 17:10:38 INFO MemoryStore: Block broadcast_220_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.3 MiB)
26/01/04 17:10:38 INFO BlockManagerInfo: Added broadcast_220_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:10:38 INFO SparkContext: Created broadcast 220 from start at NativeMethodAccessorImpl.java:0
26/01/04 17:10:38 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 72, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@34f15c4b]. The input RDD has 1 partitions.
26/01/04 17:10:38 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:10:38 INFO DAGScheduler: Got job 148 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/01/04 17:10:38 INFO DAGScheduler: Final stage: ResultStage 149 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:10:38 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:10:38 INFO DAGScheduler: Missing parents: List()
26/01/04 17:10:38 INFO DAGScheduler: Submitting ResultStage 149 (MapPartitionsRDD[747] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:10:38 INFO MemoryStore: Block broadcast_221 stored as values in memory (estimated size 52.9 KiB, free 434.3 MiB)
26/01/04 17:10:38 INFO MemoryStore: Block broadcast_221_piece0 stored as bytes in memory (estimated size 20.2 KiB, free 434.3 MiB)
26/01/04 17:10:38 INFO BlockManagerInfo: Added broadcast_221_piece0 in memory on spark-master:33535 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:10:38 INFO SparkContext: Created broadcast 221 from broadcast at DAGScheduler.scala:1585
26/01/04 17:10:38 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 149 (MapPartitionsRDD[747] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/01/04 17:10:38 INFO TaskSchedulerImpl: Adding task set 149.0 with 1 tasks resource profile 0
26/01/04 17:10:38 INFO TaskSetManager: Starting task 0.0 in stage 149.0 (TID 259) (172.18.0.13, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:10:38 INFO BlockManagerInfo: Added broadcast_221_piece0 in memory on 172.18.0.13:44401 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:10:38 INFO BlockManagerInfo: Added broadcast_220_piece0 in memory on 172.18.0.13:44401 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:10:39 INFO TaskSetManager: Finished task 0.0 in stage 149.0 (TID 259) in 818 ms on 172.18.0.13 (executor 0) (1/1)
26/01/04 17:10:39 INFO TaskSchedulerImpl: Removed TaskSet 149.0, whose tasks have all completed, from pool 
26/01/04 17:10:39 INFO DAGScheduler: ResultStage 149 (start at NativeMethodAccessorImpl.java:0) finished in 0.835 s
26/01/04 17:10:39 INFO DAGScheduler: Job 148 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:10:39 INFO TaskSchedulerImpl: Killing all running tasks in stage 149: Stage finished
26/01/04 17:10:39 INFO DAGScheduler: Job 148 finished: start at NativeMethodAccessorImpl.java:0, took 0.841158 s
26/01/04 17:10:39 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 72, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@34f15c4b] is committing.
26/01/04 17:10:39 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 72, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@34f15c4b] committed.
26/01/04 17:10:39 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/72 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.72.e703debb-4bc0-41d9-b625-52103982e32a.tmp
26/01/04 17:10:39 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.72.e703debb-4bc0-41d9-b625-52103982e32a.tmp to file:/tmp/spark-checkpoint-enrichment/commits/72
26/01/04 17:10:39 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T17:10:37.449Z",
  "batchId" : 72,
  "numInputRows" : 30,
  "inputRowsPerSecond" : 769.2307692307693,
  "processedRowsPerSecond" : 14.563106796116504,
  "durationMs" : {
    "addBatch" : 1310,
    "commitOffsets" : 247,
    "getBatch" : 1,
    "latestOffset" : 82,
    "queryPlanning" : 157,
    "triggerExecution" : 2060,
    "walCommit" : 262
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "0" : 1419
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "0" : 1449
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "0" : 1449
      }
    },
    "numInputRows" : 30,
    "inputRowsPerSecond" : 769.2307692307693,
    "processedRowsPerSecond" : 14.563106796116504,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 31
  }
}
26/01/04 17:10:48 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/73 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.73.fe7e9782-db66-4c8f-8e28-6ce7fcfba3a1.tmp
26/01/04 17:10:48 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.73.fe7e9782-db66-4c8f-8e28-6ce7fcfba3a1.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/73
26/01/04 17:10:48 INFO MicroBatchExecution: Committed offsets for batch 73. Metadata OffsetSeqMetadata(0,1767546648454,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 17:10:48 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:10:48 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:10:48 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:10:48 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:10:48 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:10:48 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:10:48 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#60175 - airline_prefix.nullCount#60174) > 0)
26/01/04 17:10:48 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#60210 - min_flight_num.nullCount#60209) > 0)
26/01/04 17:10:48 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#60205 - max_flight_num.nullCount#60204) > 0)
26/01/04 17:10:48 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:10:48 INFO DAGScheduler: Got job 149 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 17:10:48 INFO DAGScheduler: Final stage: ResultStage 150 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:10:48 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:10:48 INFO DAGScheduler: Missing parents: List()
26/01/04 17:10:48 INFO DAGScheduler: Submitting ResultStage 150 (MapPartitionsRDD[752] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:10:48 INFO MemoryStore: Block broadcast_222 stored as values in memory (estimated size 38.5 KiB, free 434.2 MiB)
26/01/04 17:10:48 INFO MemoryStore: Block broadcast_222_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.2 MiB)
26/01/04 17:10:48 INFO BlockManagerInfo: Added broadcast_222_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:10:48 INFO SparkContext: Created broadcast 222 from broadcast at DAGScheduler.scala:1585
26/01/04 17:10:48 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 150 (MapPartitionsRDD[752] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 17:10:48 INFO TaskSchedulerImpl: Adding task set 150.0 with 2 tasks resource profile 0
26/01/04 17:10:48 INFO TaskSetManager: Starting task 1.0 in stage 150.0 (TID 260) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 17:10:48 INFO TaskSetManager: Starting task 0.0 in stage 150.0 (TID 261) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 17:10:48 INFO BlockManagerInfo: Added broadcast_222_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:10:48 INFO TaskSetManager: Finished task 1.0 in stage 150.0 (TID 260) in 35 ms on 172.18.0.13 (executor 0) (1/2)
26/01/04 17:10:48 INFO BlockManagerInfo: Added broadcast_222_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:10:48 INFO TaskSetManager: Finished task 0.0 in stage 150.0 (TID 261) in 76 ms on 172.18.0.14 (executor 1) (2/2)
26/01/04 17:10:48 INFO TaskSchedulerImpl: Removed TaskSet 150.0, whose tasks have all completed, from pool 
26/01/04 17:10:48 INFO DAGScheduler: ResultStage 150 (start at NativeMethodAccessorImpl.java:0) finished in 0.107 s
26/01/04 17:10:48 INFO DAGScheduler: Job 149 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:10:48 INFO TaskSchedulerImpl: Killing all running tasks in stage 150: Stage finished
26/01/04 17:10:48 INFO DAGScheduler: Job 149 finished: start at NativeMethodAccessorImpl.java:0, took 0.109368 s
26/01/04 17:10:48 INFO MemoryStore: Block broadcast_223_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.2 MiB)
26/01/04 17:10:48 INFO BlockManagerInfo: Added broadcast_223_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:10:48 INFO SparkContext: Created broadcast 223 from start at NativeMethodAccessorImpl.java:0
26/01/04 17:10:48 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 73, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@54f14b29]. The input RDD has 1 partitions.
26/01/04 17:10:48 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:10:48 INFO DAGScheduler: Got job 150 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/01/04 17:10:48 INFO DAGScheduler: Final stage: ResultStage 151 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:10:48 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:10:48 INFO DAGScheduler: Missing parents: List()
26/01/04 17:10:48 INFO DAGScheduler: Submitting ResultStage 151 (MapPartitionsRDD[757] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:10:48 INFO MemoryStore: Block broadcast_224 stored as values in memory (estimated size 52.9 KiB, free 434.2 MiB)
26/01/04 17:10:48 INFO MemoryStore: Block broadcast_224_piece0 stored as bytes in memory (estimated size 20.2 KiB, free 434.1 MiB)
26/01/04 17:10:48 INFO BlockManagerInfo: Added broadcast_224_piece0 in memory on spark-master:33535 (size: 20.2 KiB, free: 434.3 MiB)
26/01/04 17:10:48 INFO SparkContext: Created broadcast 224 from broadcast at DAGScheduler.scala:1585
26/01/04 17:10:48 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 151 (MapPartitionsRDD[757] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/01/04 17:10:48 INFO TaskSchedulerImpl: Adding task set 151.0 with 1 tasks resource profile 0
26/01/04 17:10:48 INFO TaskSetManager: Starting task 0.0 in stage 151.0 (TID 262) (172.18.0.13, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:10:48 INFO BlockManagerInfo: Added broadcast_224_piece0 in memory on 172.18.0.13:44401 (size: 20.2 KiB, free: 434.3 MiB)
26/01/04 17:10:48 INFO BlockManagerInfo: Added broadcast_223_piece0 in memory on 172.18.0.13:44401 (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:10:49 INFO TaskSetManager: Finished task 0.0 in stage 151.0 (TID 262) in 637 ms on 172.18.0.13 (executor 0) (1/1)
26/01/04 17:10:49 INFO TaskSchedulerImpl: Removed TaskSet 151.0, whose tasks have all completed, from pool 
26/01/04 17:10:49 INFO DAGScheduler: ResultStage 151 (start at NativeMethodAccessorImpl.java:0) finished in 0.647 s
26/01/04 17:10:49 INFO DAGScheduler: Job 150 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:10:49 INFO TaskSchedulerImpl: Killing all running tasks in stage 151: Stage finished
26/01/04 17:10:49 INFO DAGScheduler: Job 150 finished: start at NativeMethodAccessorImpl.java:0, took 0.651788 s
26/01/04 17:10:49 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 73, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@54f14b29] is committing.
26/01/04 17:10:49 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 73, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@54f14b29] committed.
26/01/04 17:10:49 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/73 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.73.a536d6fc-4f4c-488e-b5da-db26bb0a4287.tmp
26/01/04 17:10:49 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.73.a536d6fc-4f4c-488e-b5da-db26bb0a4287.tmp to file:/tmp/spark-checkpoint-enrichment/commits/73
26/01/04 17:10:49 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T17:10:48.452Z",
  "batchId" : 73,
  "numInputRows" : 30,
  "inputRowsPerSecond" : 2500.0,
  "processedRowsPerSecond" : 27.598896044158234,
  "durationMs" : {
    "addBatch" : 844,
    "commitOffsets" : 133,
    "getBatch" : 0,
    "latestOffset" : 2,
    "queryPlanning" : 23,
    "triggerExecution" : 1087,
    "walCommit" : 85
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "0" : 1449
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "0" : 1479
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "0" : 1479
      }
    },
    "numInputRows" : 30,
    "inputRowsPerSecond" : 2500.0,
    "processedRowsPerSecond" : 27.598896044158234,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 31
  }
}
26/01/04 17:10:51 INFO BlockManagerInfo: Removed broadcast_224_piece0 on spark-master:33535 in memory (size: 20.2 KiB, free: 434.3 MiB)
26/01/04 17:10:51 INFO BlockManagerInfo: Removed broadcast_224_piece0 on 172.18.0.13:44401 in memory (size: 20.2 KiB, free: 434.3 MiB)
26/01/04 17:10:51 INFO BlockManagerInfo: Removed broadcast_220_piece0 on spark-master:33535 in memory (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:10:51 INFO BlockManagerInfo: Removed broadcast_220_piece0 on 172.18.0.13:44401 in memory (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:10:51 INFO BlockManagerInfo: Removed broadcast_222_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:10:51 INFO BlockManagerInfo: Removed broadcast_222_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:10:51 INFO BlockManagerInfo: Removed broadcast_222_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:10:51 INFO BlockManagerInfo: Removed broadcast_219_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:10:51 INFO BlockManagerInfo: Removed broadcast_219_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:10:51 INFO BlockManagerInfo: Removed broadcast_219_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:10:51 INFO BlockManagerInfo: Removed broadcast_221_piece0 on spark-master:33535 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:10:51 INFO BlockManagerInfo: Removed broadcast_221_piece0 on 172.18.0.13:44401 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:10:59 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/74 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.74.7e709685-b3d9-4dd7-8989-1b4f5f299350.tmp
26/01/04 17:10:59 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.74.7e709685-b3d9-4dd7-8989-1b4f5f299350.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/74
26/01/04 17:10:59 INFO MicroBatchExecution: Committed offsets for batch 74. Metadata OffsetSeqMetadata(0,1767546659466,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 17:10:59 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:10:59 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:10:59 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:10:59 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:10:59 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:10:59 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:10:59 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#60979 - airline_prefix.nullCount#60978) > 0)
26/01/04 17:10:59 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#61014 - min_flight_num.nullCount#61013) > 0)
26/01/04 17:10:59 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#61009 - max_flight_num.nullCount#61008) > 0)
26/01/04 17:10:59 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:10:59 INFO DAGScheduler: Got job 151 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 17:10:59 INFO DAGScheduler: Final stage: ResultStage 152 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:10:59 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:10:59 INFO DAGScheduler: Missing parents: List()
26/01/04 17:10:59 INFO DAGScheduler: Submitting ResultStage 152 (MapPartitionsRDD[762] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:10:59 INFO MemoryStore: Block broadcast_225 stored as values in memory (estimated size 38.5 KiB, free 434.4 MiB)
26/01/04 17:10:59 INFO MemoryStore: Block broadcast_225_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.3 MiB)
26/01/04 17:10:59 INFO BlockManagerInfo: Added broadcast_225_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:10:59 INFO BlockManagerInfo: Removed broadcast_223_piece0 on spark-master:33535 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:10:59 INFO SparkContext: Created broadcast 225 from broadcast at DAGScheduler.scala:1585
26/01/04 17:10:59 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 152 (MapPartitionsRDD[762] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 17:10:59 INFO TaskSchedulerImpl: Adding task set 152.0 with 2 tasks resource profile 0
26/01/04 17:10:59 INFO BlockManagerInfo: Removed broadcast_223_piece0 on 172.18.0.13:44401 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:10:59 INFO TaskSetManager: Starting task 1.0 in stage 152.0 (TID 263) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 17:10:59 INFO TaskSetManager: Starting task 0.0 in stage 152.0 (TID 264) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 17:10:59 INFO BlockManagerInfo: Added broadcast_225_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:10:59 INFO BlockManagerInfo: Added broadcast_225_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:10:59 INFO TaskSetManager: Finished task 1.0 in stage 152.0 (TID 263) in 27 ms on 172.18.0.13 (executor 0) (1/2)
26/01/04 17:10:59 INFO TaskSetManager: Finished task 0.0 in stage 152.0 (TID 264) in 49 ms on 172.18.0.14 (executor 1) (2/2)
26/01/04 17:10:59 INFO TaskSchedulerImpl: Removed TaskSet 152.0, whose tasks have all completed, from pool 
26/01/04 17:10:59 INFO DAGScheduler: ResultStage 152 (start at NativeMethodAccessorImpl.java:0) finished in 0.061 s
26/01/04 17:10:59 INFO DAGScheduler: Job 151 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:10:59 INFO TaskSchedulerImpl: Killing all running tasks in stage 152: Stage finished
26/01/04 17:10:59 INFO DAGScheduler: Job 151 finished: start at NativeMethodAccessorImpl.java:0, took 0.062904 s
26/01/04 17:10:59 INFO MemoryStore: Block broadcast_226_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.3 MiB)
26/01/04 17:10:59 INFO BlockManagerInfo: Added broadcast_226_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:10:59 INFO SparkContext: Created broadcast 226 from start at NativeMethodAccessorImpl.java:0
26/01/04 17:10:59 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 74, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@675b720e]. The input RDD has 1 partitions.
26/01/04 17:10:59 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:10:59 INFO DAGScheduler: Got job 152 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/01/04 17:10:59 INFO DAGScheduler: Final stage: ResultStage 153 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:10:59 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:10:59 INFO DAGScheduler: Missing parents: List()
26/01/04 17:10:59 INFO DAGScheduler: Submitting ResultStage 153 (MapPartitionsRDD[767] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:10:59 INFO MemoryStore: Block broadcast_227 stored as values in memory (estimated size 52.9 KiB, free 434.3 MiB)
26/01/04 17:10:59 INFO MemoryStore: Block broadcast_227_piece0 stored as bytes in memory (estimated size 20.2 KiB, free 434.3 MiB)
26/01/04 17:10:59 INFO BlockManagerInfo: Added broadcast_227_piece0 in memory on spark-master:33535 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:10:59 INFO SparkContext: Created broadcast 227 from broadcast at DAGScheduler.scala:1585
26/01/04 17:10:59 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 153 (MapPartitionsRDD[767] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/01/04 17:10:59 INFO TaskSchedulerImpl: Adding task set 153.0 with 1 tasks resource profile 0
26/01/04 17:10:59 INFO TaskSetManager: Starting task 0.0 in stage 153.0 (TID 265) (172.18.0.13, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:10:59 INFO BlockManagerInfo: Added broadcast_227_piece0 in memory on 172.18.0.13:44401 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:10:59 INFO BlockManagerInfo: Added broadcast_226_piece0 in memory on 172.18.0.13:44401 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:11:00 INFO TaskSetManager: Finished task 0.0 in stage 153.0 (TID 265) in 552 ms on 172.18.0.13 (executor 0) (1/1)
26/01/04 17:11:00 INFO TaskSchedulerImpl: Removed TaskSet 153.0, whose tasks have all completed, from pool 
26/01/04 17:11:00 INFO DAGScheduler: ResultStage 153 (start at NativeMethodAccessorImpl.java:0) finished in 0.558 s
26/01/04 17:11:00 INFO DAGScheduler: Job 152 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:11:00 INFO TaskSchedulerImpl: Killing all running tasks in stage 153: Stage finished
26/01/04 17:11:00 INFO DAGScheduler: Job 152 finished: start at NativeMethodAccessorImpl.java:0, took 0.559746 s
26/01/04 17:11:00 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 74, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@675b720e] is committing.
26/01/04 17:11:00 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 74, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@675b720e] committed.
26/01/04 17:11:00 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/74 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.74.040802e5-e3b2-46af-b5e1-61f0469a8719.tmp
26/01/04 17:11:00 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.74.040802e5-e3b2-46af-b5e1-61f0469a8719.tmp to file:/tmp/spark-checkpoint-enrichment/commits/74
26/01/04 17:11:00 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T17:10:59.464Z",
  "batchId" : 74,
  "numInputRows" : 30,
  "inputRowsPerSecond" : 2307.6923076923076,
  "processedRowsPerSecond" : 33.59462486002239,
  "durationMs" : {
    "addBatch" : 689,
    "commitOffsets" : 77,
    "getBatch" : 0,
    "latestOffset" : 2,
    "queryPlanning" : 24,
    "triggerExecution" : 893,
    "walCommit" : 100
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "0" : 1479
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "0" : 1509
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "0" : 1509
      }
    },
    "numInputRows" : 30,
    "inputRowsPerSecond" : 2307.6923076923076,
    "processedRowsPerSecond" : 33.59462486002239,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 31
  }
}
26/01/04 17:11:10 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/01/04 17:11:10 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/75 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.75.1ca54610-e94e-4860-bfc8-55c9ed128541.tmp
26/01/04 17:11:10 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.75.1ca54610-e94e-4860-bfc8-55c9ed128541.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/75
26/01/04 17:11:10 INFO MicroBatchExecution: Committed offsets for batch 75. Metadata OffsetSeqMetadata(0,1767546670489,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 17:11:10 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:11:10 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:11:10 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:11:10 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:11:10 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:11:10 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:11:10 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#61783 - airline_prefix.nullCount#61782) > 0)
26/01/04 17:11:10 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#61818 - min_flight_num.nullCount#61817) > 0)
26/01/04 17:11:10 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#61813 - max_flight_num.nullCount#61812) > 0)
26/01/04 17:11:10 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:11:10 INFO DAGScheduler: Got job 153 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 17:11:10 INFO DAGScheduler: Final stage: ResultStage 154 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:11:10 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:11:10 INFO DAGScheduler: Missing parents: List()
26/01/04 17:11:10 INFO DAGScheduler: Submitting ResultStage 154 (MapPartitionsRDD[772] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:11:10 INFO MemoryStore: Block broadcast_228 stored as values in memory (estimated size 38.5 KiB, free 434.2 MiB)
26/01/04 17:11:10 INFO MemoryStore: Block broadcast_228_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.2 MiB)
26/01/04 17:11:10 INFO BlockManagerInfo: Added broadcast_228_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:11:10 INFO SparkContext: Created broadcast 228 from broadcast at DAGScheduler.scala:1585
26/01/04 17:11:10 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 154 (MapPartitionsRDD[772] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 17:11:10 INFO TaskSchedulerImpl: Adding task set 154.0 with 2 tasks resource profile 0
26/01/04 17:11:10 INFO TaskSetManager: Starting task 0.0 in stage 154.0 (TID 266) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 17:11:10 INFO TaskSetManager: Starting task 1.0 in stage 154.0 (TID 267) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 17:11:10 INFO BlockManagerInfo: Added broadcast_228_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:11:10 INFO BlockManagerInfo: Added broadcast_228_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:11:10 INFO TaskSetManager: Finished task 1.0 in stage 154.0 (TID 267) in 23 ms on 172.18.0.13 (executor 0) (1/2)
26/01/04 17:11:10 INFO TaskSetManager: Finished task 0.0 in stage 154.0 (TID 266) in 44 ms on 172.18.0.14 (executor 1) (2/2)
26/01/04 17:11:10 INFO TaskSchedulerImpl: Removed TaskSet 154.0, whose tasks have all completed, from pool 
26/01/04 17:11:10 INFO DAGScheduler: ResultStage 154 (start at NativeMethodAccessorImpl.java:0) finished in 0.051 s
26/01/04 17:11:10 INFO DAGScheduler: Job 153 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:11:10 INFO TaskSchedulerImpl: Killing all running tasks in stage 154: Stage finished
26/01/04 17:11:10 INFO DAGScheduler: Job 153 finished: start at NativeMethodAccessorImpl.java:0, took 0.055260 s
26/01/04 17:11:10 INFO MemoryStore: Block broadcast_229_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.2 MiB)
26/01/04 17:11:10 INFO BlockManagerInfo: Added broadcast_229_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:11:10 INFO SparkContext: Created broadcast 229 from start at NativeMethodAccessorImpl.java:0
26/01/04 17:11:10 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 75, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@2cc69415]. The input RDD has 1 partitions.
26/01/04 17:11:10 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:11:10 INFO DAGScheduler: Got job 154 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/01/04 17:11:10 INFO DAGScheduler: Final stage: ResultStage 155 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:11:10 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:11:10 INFO DAGScheduler: Missing parents: List()
26/01/04 17:11:10 INFO DAGScheduler: Submitting ResultStage 155 (MapPartitionsRDD[777] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:11:10 INFO MemoryStore: Block broadcast_230 stored as values in memory (estimated size 52.9 KiB, free 434.2 MiB)
26/01/04 17:11:10 INFO MemoryStore: Block broadcast_230_piece0 stored as bytes in memory (estimated size 20.2 KiB, free 434.1 MiB)
26/01/04 17:11:10 INFO BlockManagerInfo: Added broadcast_230_piece0 in memory on spark-master:33535 (size: 20.2 KiB, free: 434.3 MiB)
26/01/04 17:11:10 INFO SparkContext: Created broadcast 230 from broadcast at DAGScheduler.scala:1585
26/01/04 17:11:10 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 155 (MapPartitionsRDD[777] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/01/04 17:11:10 INFO TaskSchedulerImpl: Adding task set 155.0 with 1 tasks resource profile 0
26/01/04 17:11:10 INFO TaskSetManager: Starting task 0.0 in stage 155.0 (TID 268) (172.18.0.13, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:11:10 INFO BlockManagerInfo: Added broadcast_230_piece0 in memory on 172.18.0.13:44401 (size: 20.2 KiB, free: 434.3 MiB)
26/01/04 17:11:10 INFO BlockManagerInfo: Added broadcast_229_piece0 in memory on 172.18.0.13:44401 (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:11:11 INFO TaskSetManager: Finished task 0.0 in stage 155.0 (TID 268) in 572 ms on 172.18.0.13 (executor 0) (1/1)
26/01/04 17:11:11 INFO TaskSchedulerImpl: Removed TaskSet 155.0, whose tasks have all completed, from pool 
26/01/04 17:11:11 INFO DAGScheduler: ResultStage 155 (start at NativeMethodAccessorImpl.java:0) finished in 0.580 s
26/01/04 17:11:11 INFO DAGScheduler: Job 154 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:11:11 INFO TaskSchedulerImpl: Killing all running tasks in stage 155: Stage finished
26/01/04 17:11:11 INFO DAGScheduler: Job 154 finished: start at NativeMethodAccessorImpl.java:0, took 0.581814 s
26/01/04 17:11:11 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 75, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@2cc69415] is committing.
26/01/04 17:11:11 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 75, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@2cc69415] committed.
26/01/04 17:11:11 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/75 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.75.788a93e6-08aa-4ae7-a281-5d3776602e1d.tmp
26/01/04 17:11:11 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.75.788a93e6-08aa-4ae7-a281-5d3776602e1d.tmp to file:/tmp/spark-checkpoint-enrichment/commits/75
26/01/04 17:11:11 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T17:11:10.487Z",
  "batchId" : 75,
  "numInputRows" : 30,
  "inputRowsPerSecond" : 2500.0,
  "processedRowsPerSecond" : 34.0522133938706,
  "durationMs" : {
    "addBatch" : 716,
    "commitOffsets" : 66,
    "getBatch" : 0,
    "latestOffset" : 2,
    "queryPlanning" : 35,
    "triggerExecution" : 881,
    "walCommit" : 61
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "0" : 1509
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "0" : 1539
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "0" : 1539
      }
    },
    "numInputRows" : 30,
    "inputRowsPerSecond" : 2500.0,
    "processedRowsPerSecond" : 34.0522133938706,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 31
  }
}
26/01/04 17:11:13 INFO BlockManagerInfo: Removed broadcast_228_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:11:13 INFO BlockManagerInfo: Removed broadcast_228_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:11:13 INFO BlockManagerInfo: Removed broadcast_228_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:11:13 INFO BlockManagerInfo: Removed broadcast_225_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:11:13 INFO BlockManagerInfo: Removed broadcast_225_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:11:13 INFO BlockManagerInfo: Removed broadcast_225_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:11:13 INFO BlockManagerInfo: Removed broadcast_226_piece0 on spark-master:33535 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:11:13 INFO BlockManagerInfo: Removed broadcast_226_piece0 on 172.18.0.13:44401 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:11:13 INFO BlockManagerInfo: Removed broadcast_227_piece0 on spark-master:33535 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:11:13 INFO BlockManagerInfo: Removed broadcast_227_piece0 on 172.18.0.13:44401 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:11:13 INFO BlockManagerInfo: Removed broadcast_230_piece0 on spark-master:33535 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:11:13 INFO BlockManagerInfo: Removed broadcast_230_piece0 on 172.18.0.13:44401 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:11:21 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/01/04 17:11:21 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/76 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.76.d5e1cc1d-e347-451f-999f-66fcfce9dfb5.tmp
26/01/04 17:11:21 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.76.d5e1cc1d-e347-451f-999f-66fcfce9dfb5.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/76
26/01/04 17:11:21 INFO MicroBatchExecution: Committed offsets for batch 76. Metadata OffsetSeqMetadata(0,1767546681499,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 17:11:21 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:11:21 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:11:21 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:11:21 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:11:21 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:11:21 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:11:21 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#62587 - airline_prefix.nullCount#62586) > 0)
26/01/04 17:11:21 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#62622 - min_flight_num.nullCount#62621) > 0)
26/01/04 17:11:21 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#62617 - max_flight_num.nullCount#62616) > 0)
26/01/04 17:11:21 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:11:21 INFO DAGScheduler: Got job 155 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 17:11:21 INFO DAGScheduler: Final stage: ResultStage 156 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:11:21 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:11:21 INFO DAGScheduler: Missing parents: List()
26/01/04 17:11:21 INFO DAGScheduler: Submitting ResultStage 156 (MapPartitionsRDD[782] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:11:21 INFO MemoryStore: Block broadcast_231 stored as values in memory (estimated size 38.5 KiB, free 434.4 MiB)
26/01/04 17:11:21 INFO MemoryStore: Block broadcast_231_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.3 MiB)
26/01/04 17:11:21 INFO BlockManagerInfo: Added broadcast_231_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:11:21 INFO SparkContext: Created broadcast 231 from broadcast at DAGScheduler.scala:1585
26/01/04 17:11:21 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 156 (MapPartitionsRDD[782] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 17:11:21 INFO TaskSchedulerImpl: Adding task set 156.0 with 2 tasks resource profile 0
26/01/04 17:11:21 INFO TaskSetManager: Starting task 0.0 in stage 156.0 (TID 269) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 17:11:21 INFO TaskSetManager: Starting task 1.0 in stage 156.0 (TID 270) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 17:11:21 INFO BlockManagerInfo: Added broadcast_231_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:11:21 INFO BlockManagerInfo: Added broadcast_231_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:11:21 INFO TaskSetManager: Finished task 1.0 in stage 156.0 (TID 270) in 38 ms on 172.18.0.13 (executor 0) (1/2)
26/01/04 17:11:21 INFO TaskSetManager: Finished task 0.0 in stage 156.0 (TID 269) in 75 ms on 172.18.0.14 (executor 1) (2/2)
26/01/04 17:11:21 INFO TaskSchedulerImpl: Removed TaskSet 156.0, whose tasks have all completed, from pool 
26/01/04 17:11:21 INFO DAGScheduler: ResultStage 156 (start at NativeMethodAccessorImpl.java:0) finished in 0.086 s
26/01/04 17:11:21 INFO DAGScheduler: Job 155 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:11:21 INFO TaskSchedulerImpl: Killing all running tasks in stage 156: Stage finished
26/01/04 17:11:21 INFO DAGScheduler: Job 155 finished: start at NativeMethodAccessorImpl.java:0, took 0.090557 s
26/01/04 17:11:21 INFO MemoryStore: Block broadcast_232_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.3 MiB)
26/01/04 17:11:21 INFO BlockManagerInfo: Added broadcast_232_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:11:21 INFO SparkContext: Created broadcast 232 from start at NativeMethodAccessorImpl.java:0
26/01/04 17:11:21 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 76, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@3c0a6086]. The input RDD has 1 partitions.
26/01/04 17:11:21 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:11:21 INFO DAGScheduler: Got job 156 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/01/04 17:11:21 INFO DAGScheduler: Final stage: ResultStage 157 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:11:21 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:11:21 INFO DAGScheduler: Missing parents: List()
26/01/04 17:11:21 INFO DAGScheduler: Submitting ResultStage 157 (MapPartitionsRDD[787] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:11:21 INFO MemoryStore: Block broadcast_233 stored as values in memory (estimated size 52.9 KiB, free 434.3 MiB)
26/01/04 17:11:21 INFO MemoryStore: Block broadcast_233_piece0 stored as bytes in memory (estimated size 20.2 KiB, free 434.3 MiB)
26/01/04 17:11:21 INFO BlockManagerInfo: Added broadcast_233_piece0 in memory on spark-master:33535 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:11:21 INFO SparkContext: Created broadcast 233 from broadcast at DAGScheduler.scala:1585
26/01/04 17:11:21 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 157 (MapPartitionsRDD[787] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/01/04 17:11:21 INFO TaskSchedulerImpl: Adding task set 157.0 with 1 tasks resource profile 0
26/01/04 17:11:21 INFO TaskSetManager: Starting task 0.0 in stage 157.0 (TID 271) (172.18.0.13, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:11:21 INFO BlockManagerInfo: Added broadcast_233_piece0 in memory on 172.18.0.13:44401 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:11:21 INFO BlockManagerInfo: Added broadcast_232_piece0 in memory on 172.18.0.13:44401 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:11:22 INFO TaskSetManager: Finished task 0.0 in stage 157.0 (TID 271) in 611 ms on 172.18.0.13 (executor 0) (1/1)
26/01/04 17:11:22 INFO TaskSchedulerImpl: Removed TaskSet 157.0, whose tasks have all completed, from pool 
26/01/04 17:11:22 INFO DAGScheduler: ResultStage 157 (start at NativeMethodAccessorImpl.java:0) finished in 0.620 s
26/01/04 17:11:22 INFO DAGScheduler: Job 156 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:11:22 INFO TaskSchedulerImpl: Killing all running tasks in stage 157: Stage finished
26/01/04 17:11:22 INFO DAGScheduler: Job 156 finished: start at NativeMethodAccessorImpl.java:0, took 0.622076 s
26/01/04 17:11:22 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 76, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@3c0a6086] is committing.
26/01/04 17:11:22 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 76, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@3c0a6086] committed.
26/01/04 17:11:22 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/76 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.76.33c96988-3138-4af7-aa9c-0558834f91d0.tmp
26/01/04 17:11:22 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.76.33c96988-3138-4af7-aa9c-0558834f91d0.tmp to file:/tmp/spark-checkpoint-enrichment/commits/76
26/01/04 17:11:22 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T17:11:21.497Z",
  "batchId" : 76,
  "numInputRows" : 30,
  "inputRowsPerSecond" : 2307.6923076923076,
  "processedRowsPerSecond" : 29.67359050445104,
  "durationMs" : {
    "addBatch" : 845,
    "commitOffsets" : 57,
    "getBatch" : 0,
    "latestOffset" : 2,
    "queryPlanning" : 36,
    "triggerExecution" : 1011,
    "walCommit" : 70
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "0" : 1539
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "0" : 1569
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "0" : 1569
      }
    },
    "numInputRows" : 30,
    "inputRowsPerSecond" : 2307.6923076923076,
    "processedRowsPerSecond" : 29.67359050445104,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 31
  }
}
26/01/04 17:11:32 INFO BlockManagerInfo: Removed broadcast_231_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:11:32 INFO BlockManagerInfo: Removed broadcast_231_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:11:32 INFO BlockManagerInfo: Removed broadcast_231_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:11:32 INFO BlockManagerInfo: Removed broadcast_229_piece0 on spark-master:33535 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:11:32 INFO BlockManagerInfo: Removed broadcast_229_piece0 on 172.18.0.13:44401 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:11:32 INFO BlockManagerInfo: Removed broadcast_233_piece0 on spark-master:33535 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:11:32 INFO BlockManagerInfo: Removed broadcast_233_piece0 on 172.18.0.13:44401 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:11:32 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/77 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.77.c773de85-8c02-4f62-96fa-9f60f9f293c3.tmp
26/01/04 17:11:32 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.77.c773de85-8c02-4f62-96fa-9f60f9f293c3.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/77
26/01/04 17:11:32 INFO MicroBatchExecution: Committed offsets for batch 77. Metadata OffsetSeqMetadata(0,1767546692511,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 17:11:32 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:11:32 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:11:32 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:11:32 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:11:32 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:11:32 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:11:32 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#63391 - airline_prefix.nullCount#63390) > 0)
26/01/04 17:11:32 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#63426 - min_flight_num.nullCount#63425) > 0)
26/01/04 17:11:32 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#63421 - max_flight_num.nullCount#63420) > 0)
26/01/04 17:11:32 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:11:32 INFO DAGScheduler: Got job 157 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 17:11:32 INFO DAGScheduler: Final stage: ResultStage 158 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:11:32 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:11:32 INFO DAGScheduler: Missing parents: List()
26/01/04 17:11:32 INFO DAGScheduler: Submitting ResultStage 158 (MapPartitionsRDD[792] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:11:32 INFO MemoryStore: Block broadcast_234 stored as values in memory (estimated size 38.5 KiB, free 434.4 MiB)
26/01/04 17:11:32 INFO MemoryStore: Block broadcast_234_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.3 MiB)
26/01/04 17:11:32 INFO BlockManagerInfo: Removed broadcast_232_piece0 on spark-master:33535 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:11:32 INFO BlockManagerInfo: Added broadcast_234_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:11:32 INFO SparkContext: Created broadcast 234 from broadcast at DAGScheduler.scala:1585
26/01/04 17:11:32 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 158 (MapPartitionsRDD[792] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 17:11:32 INFO TaskSchedulerImpl: Adding task set 158.0 with 2 tasks resource profile 0
26/01/04 17:11:32 INFO BlockManagerInfo: Removed broadcast_232_piece0 on 172.18.0.13:44401 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:11:32 INFO TaskSetManager: Starting task 1.0 in stage 158.0 (TID 272) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 17:11:32 INFO TaskSetManager: Starting task 0.0 in stage 158.0 (TID 273) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 17:11:32 INFO BlockManagerInfo: Added broadcast_234_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:11:32 INFO BlockManagerInfo: Added broadcast_234_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:11:32 INFO TaskSetManager: Finished task 1.0 in stage 158.0 (TID 272) in 21 ms on 172.18.0.13 (executor 0) (1/2)
26/01/04 17:11:32 INFO TaskSetManager: Finished task 0.0 in stage 158.0 (TID 273) in 48 ms on 172.18.0.14 (executor 1) (2/2)
26/01/04 17:11:32 INFO TaskSchedulerImpl: Removed TaskSet 158.0, whose tasks have all completed, from pool 
26/01/04 17:11:32 INFO DAGScheduler: ResultStage 158 (start at NativeMethodAccessorImpl.java:0) finished in 0.058 s
26/01/04 17:11:32 INFO DAGScheduler: Job 157 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:11:32 INFO TaskSchedulerImpl: Killing all running tasks in stage 158: Stage finished
26/01/04 17:11:32 INFO DAGScheduler: Job 157 finished: start at NativeMethodAccessorImpl.java:0, took 0.059858 s
26/01/04 17:11:32 INFO MemoryStore: Block broadcast_235_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.3 MiB)
26/01/04 17:11:32 INFO BlockManagerInfo: Added broadcast_235_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:11:32 INFO SparkContext: Created broadcast 235 from start at NativeMethodAccessorImpl.java:0
26/01/04 17:11:32 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 77, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@382f628c]. The input RDD has 1 partitions.
26/01/04 17:11:32 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:11:32 INFO DAGScheduler: Got job 158 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/01/04 17:11:32 INFO DAGScheduler: Final stage: ResultStage 159 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:11:32 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:11:32 INFO DAGScheduler: Missing parents: List()
26/01/04 17:11:32 INFO DAGScheduler: Submitting ResultStage 159 (MapPartitionsRDD[797] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:11:32 INFO MemoryStore: Block broadcast_236 stored as values in memory (estimated size 52.9 KiB, free 434.3 MiB)
26/01/04 17:11:32 INFO MemoryStore: Block broadcast_236_piece0 stored as bytes in memory (estimated size 20.2 KiB, free 434.3 MiB)
26/01/04 17:11:32 INFO BlockManagerInfo: Added broadcast_236_piece0 in memory on spark-master:33535 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:11:32 INFO SparkContext: Created broadcast 236 from broadcast at DAGScheduler.scala:1585
26/01/04 17:11:32 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 159 (MapPartitionsRDD[797] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/01/04 17:11:32 INFO TaskSchedulerImpl: Adding task set 159.0 with 1 tasks resource profile 0
26/01/04 17:11:32 INFO TaskSetManager: Starting task 0.0 in stage 159.0 (TID 274) (172.18.0.13, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:11:32 INFO BlockManagerInfo: Added broadcast_236_piece0 in memory on 172.18.0.13:44401 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:11:32 INFO BlockManagerInfo: Added broadcast_235_piece0 in memory on 172.18.0.13:44401 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:11:33 INFO TaskSetManager: Finished task 0.0 in stage 159.0 (TID 274) in 545 ms on 172.18.0.13 (executor 0) (1/1)
26/01/04 17:11:33 INFO TaskSchedulerImpl: Removed TaskSet 159.0, whose tasks have all completed, from pool 
26/01/04 17:11:33 INFO DAGScheduler: ResultStage 159 (start at NativeMethodAccessorImpl.java:0) finished in 0.549 s
26/01/04 17:11:33 INFO DAGScheduler: Job 158 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:11:33 INFO TaskSchedulerImpl: Killing all running tasks in stage 159: Stage finished
26/01/04 17:11:33 INFO DAGScheduler: Job 158 finished: start at NativeMethodAccessorImpl.java:0, took 0.550409 s
26/01/04 17:11:33 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 77, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@382f628c] is committing.
26/01/04 17:11:33 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 77, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@382f628c] committed.
26/01/04 17:11:33 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/77 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.77.cbeed2fa-dc63-4af0-8dad-5661a889a5c9.tmp
26/01/04 17:11:33 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.77.cbeed2fa-dc63-4af0-8dad-5661a889a5c9.tmp to file:/tmp/spark-checkpoint-enrichment/commits/77
26/01/04 17:11:33 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T17:11:32.510Z",
  "batchId" : 77,
  "numInputRows" : 30,
  "inputRowsPerSecond" : 1875.0,
  "processedRowsPerSecond" : 36.630036630036635,
  "durationMs" : {
    "addBatch" : 679,
    "commitOffsets" : 62,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 16,
    "triggerExecution" : 819,
    "walCommit" : 59
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "0" : 1569
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "0" : 1599
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "0" : 1599
      }
    },
    "numInputRows" : 30,
    "inputRowsPerSecond" : 1875.0,
    "processedRowsPerSecond" : 36.630036630036635,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 31
  }
}
26/01/04 17:11:43 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/01/04 17:11:43 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/78 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.78.8085c189-4a46-4abc-8d4b-d1077d39503c.tmp
26/01/04 17:11:43 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.78.8085c189-4a46-4abc-8d4b-d1077d39503c.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/78
26/01/04 17:11:43 INFO MicroBatchExecution: Committed offsets for batch 78. Metadata OffsetSeqMetadata(0,1767546703535,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 17:11:43 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:11:43 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:11:43 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:11:43 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:11:43 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:11:43 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:11:43 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#64195 - airline_prefix.nullCount#64194) > 0)
26/01/04 17:11:43 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#64230 - min_flight_num.nullCount#64229) > 0)
26/01/04 17:11:43 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#64225 - max_flight_num.nullCount#64224) > 0)
26/01/04 17:11:43 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:11:43 INFO DAGScheduler: Got job 159 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 17:11:43 INFO DAGScheduler: Final stage: ResultStage 160 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:11:43 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:11:43 INFO DAGScheduler: Missing parents: List()
26/01/04 17:11:43 INFO DAGScheduler: Submitting ResultStage 160 (MapPartitionsRDD[802] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:11:43 INFO MemoryStore: Block broadcast_237 stored as values in memory (estimated size 38.5 KiB, free 434.2 MiB)
26/01/04 17:11:43 INFO MemoryStore: Block broadcast_237_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.2 MiB)
26/01/04 17:11:43 INFO BlockManagerInfo: Added broadcast_237_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:11:43 INFO SparkContext: Created broadcast 237 from broadcast at DAGScheduler.scala:1585
26/01/04 17:11:43 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 160 (MapPartitionsRDD[802] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 17:11:43 INFO TaskSchedulerImpl: Adding task set 160.0 with 2 tasks resource profile 0
26/01/04 17:11:43 INFO TaskSetManager: Starting task 1.0 in stage 160.0 (TID 275) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 17:11:43 INFO TaskSetManager: Starting task 0.0 in stage 160.0 (TID 276) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 17:11:43 INFO BlockManagerInfo: Added broadcast_237_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:11:43 INFO BlockManagerInfo: Added broadcast_237_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:11:43 INFO TaskSetManager: Finished task 1.0 in stage 160.0 (TID 275) in 21 ms on 172.18.0.13 (executor 0) (1/2)
26/01/04 17:11:43 INFO TaskSetManager: Finished task 0.0 in stage 160.0 (TID 276) in 30 ms on 172.18.0.14 (executor 1) (2/2)
26/01/04 17:11:43 INFO TaskSchedulerImpl: Removed TaskSet 160.0, whose tasks have all completed, from pool 
26/01/04 17:11:43 INFO DAGScheduler: ResultStage 160 (start at NativeMethodAccessorImpl.java:0) finished in 0.038 s
26/01/04 17:11:43 INFO DAGScheduler: Job 159 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:11:43 INFO TaskSchedulerImpl: Killing all running tasks in stage 160: Stage finished
26/01/04 17:11:43 INFO DAGScheduler: Job 159 finished: start at NativeMethodAccessorImpl.java:0, took 0.039549 s
26/01/04 17:11:43 INFO MemoryStore: Block broadcast_238_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.2 MiB)
26/01/04 17:11:43 INFO BlockManagerInfo: Added broadcast_238_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:11:43 INFO SparkContext: Created broadcast 238 from start at NativeMethodAccessorImpl.java:0
26/01/04 17:11:43 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 78, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@4cf9f3dc]. The input RDD has 1 partitions.
26/01/04 17:11:43 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:11:43 INFO DAGScheduler: Got job 160 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/01/04 17:11:43 INFO DAGScheduler: Final stage: ResultStage 161 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:11:43 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:11:43 INFO DAGScheduler: Missing parents: List()
26/01/04 17:11:43 INFO DAGScheduler: Submitting ResultStage 161 (MapPartitionsRDD[807] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:11:43 INFO MemoryStore: Block broadcast_239 stored as values in memory (estimated size 52.9 KiB, free 434.2 MiB)
26/01/04 17:11:43 INFO MemoryStore: Block broadcast_239_piece0 stored as bytes in memory (estimated size 20.2 KiB, free 434.1 MiB)
26/01/04 17:11:43 INFO BlockManagerInfo: Added broadcast_239_piece0 in memory on spark-master:33535 (size: 20.2 KiB, free: 434.3 MiB)
26/01/04 17:11:43 INFO SparkContext: Created broadcast 239 from broadcast at DAGScheduler.scala:1585
26/01/04 17:11:43 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 161 (MapPartitionsRDD[807] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/01/04 17:11:43 INFO TaskSchedulerImpl: Adding task set 161.0 with 1 tasks resource profile 0
26/01/04 17:11:43 INFO TaskSetManager: Starting task 0.0 in stage 161.0 (TID 277) (172.18.0.13, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:11:43 INFO BlockManagerInfo: Added broadcast_239_piece0 in memory on 172.18.0.13:44401 (size: 20.2 KiB, free: 434.3 MiB)
26/01/04 17:11:43 INFO BlockManagerInfo: Added broadcast_238_piece0 in memory on 172.18.0.13:44401 (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:11:44 INFO TaskSetManager: Finished task 0.0 in stage 161.0 (TID 277) in 564 ms on 172.18.0.13 (executor 0) (1/1)
26/01/04 17:11:44 INFO TaskSchedulerImpl: Removed TaskSet 161.0, whose tasks have all completed, from pool 
26/01/04 17:11:44 INFO DAGScheduler: ResultStage 161 (start at NativeMethodAccessorImpl.java:0) finished in 0.570 s
26/01/04 17:11:44 INFO DAGScheduler: Job 160 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:11:44 INFO TaskSchedulerImpl: Killing all running tasks in stage 161: Stage finished
26/01/04 17:11:44 INFO DAGScheduler: Job 160 finished: start at NativeMethodAccessorImpl.java:0, took 0.572033 s
26/01/04 17:11:44 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 78, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@4cf9f3dc] is committing.
26/01/04 17:11:44 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 78, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@4cf9f3dc] committed.
26/01/04 17:11:44 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/78 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.78.468a8c8f-867c-4ee1-8ced-bde4d131edcd.tmp
26/01/04 17:11:44 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.78.468a8c8f-867c-4ee1-8ced-bde4d131edcd.tmp to file:/tmp/spark-checkpoint-enrichment/commits/78
26/01/04 17:11:44 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T17:11:43.534Z",
  "batchId" : 78,
  "numInputRows" : 30,
  "inputRowsPerSecond" : 2500.0,
  "processedRowsPerSecond" : 35.08771929824562,
  "durationMs" : {
    "addBatch" : 677,
    "commitOffsets" : 72,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 23,
    "triggerExecution" : 855,
    "walCommit" : 81
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "0" : 1599
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "0" : 1629
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "0" : 1629
      }
    },
    "numInputRows" : 30,
    "inputRowsPerSecond" : 2500.0,
    "processedRowsPerSecond" : 35.08771929824562,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 31
  }
}
26/01/04 17:11:45 INFO BlockManagerInfo: Removed broadcast_236_piece0 on spark-master:33535 in memory (size: 20.2 KiB, free: 434.3 MiB)
26/01/04 17:11:45 INFO BlockManagerInfo: Removed broadcast_236_piece0 on 172.18.0.13:44401 in memory (size: 20.2 KiB, free: 434.3 MiB)
26/01/04 17:11:45 INFO BlockManagerInfo: Removed broadcast_235_piece0 on spark-master:33535 in memory (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:11:45 INFO BlockManagerInfo: Removed broadcast_235_piece0 on 172.18.0.13:44401 in memory (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:11:45 INFO BlockManagerInfo: Removed broadcast_237_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:11:45 INFO BlockManagerInfo: Removed broadcast_237_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:11:45 INFO BlockManagerInfo: Removed broadcast_237_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:11:45 INFO BlockManagerInfo: Removed broadcast_234_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:11:45 INFO BlockManagerInfo: Removed broadcast_234_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:11:45 INFO BlockManagerInfo: Removed broadcast_234_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:11:45 INFO BlockManagerInfo: Removed broadcast_239_piece0 on spark-master:33535 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:11:45 INFO BlockManagerInfo: Removed broadcast_239_piece0 on 172.18.0.13:44401 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:11:54 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/01/04 17:11:54 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/79 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.79.4bd7b631-395f-4212-96d6-68006ff0d822.tmp
26/01/04 17:11:54 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.79.4bd7b631-395f-4212-96d6-68006ff0d822.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/79
26/01/04 17:11:54 INFO MicroBatchExecution: Committed offsets for batch 79. Metadata OffsetSeqMetadata(0,1767546714544,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 17:11:54 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:11:54 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:11:54 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:11:54 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:11:54 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:11:54 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:11:54 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#64999 - airline_prefix.nullCount#64998) > 0)
26/01/04 17:11:54 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#65034 - min_flight_num.nullCount#65033) > 0)
26/01/04 17:11:54 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#65029 - max_flight_num.nullCount#65028) > 0)
26/01/04 17:11:54 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:11:54 INFO DAGScheduler: Got job 161 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 17:11:54 INFO DAGScheduler: Final stage: ResultStage 162 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:11:54 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:11:54 INFO DAGScheduler: Missing parents: List()
26/01/04 17:11:54 INFO DAGScheduler: Submitting ResultStage 162 (MapPartitionsRDD[812] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:11:54 INFO MemoryStore: Block broadcast_240 stored as values in memory (estimated size 38.5 KiB, free 434.4 MiB)
26/01/04 17:11:54 INFO MemoryStore: Block broadcast_240_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.3 MiB)
26/01/04 17:11:54 INFO BlockManagerInfo: Added broadcast_240_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:11:54 INFO SparkContext: Created broadcast 240 from broadcast at DAGScheduler.scala:1585
26/01/04 17:11:54 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 162 (MapPartitionsRDD[812] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 17:11:54 INFO TaskSchedulerImpl: Adding task set 162.0 with 2 tasks resource profile 0
26/01/04 17:11:54 INFO TaskSetManager: Starting task 1.0 in stage 162.0 (TID 278) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 17:11:54 INFO TaskSetManager: Starting task 0.0 in stage 162.0 (TID 279) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 17:11:54 INFO BlockManagerInfo: Added broadcast_240_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:11:54 INFO BlockManagerInfo: Added broadcast_240_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:11:54 INFO TaskSetManager: Finished task 1.0 in stage 162.0 (TID 278) in 27 ms on 172.18.0.13 (executor 0) (1/2)
26/01/04 17:11:54 INFO TaskSetManager: Finished task 0.0 in stage 162.0 (TID 279) in 37 ms on 172.18.0.14 (executor 1) (2/2)
26/01/04 17:11:54 INFO TaskSchedulerImpl: Removed TaskSet 162.0, whose tasks have all completed, from pool 
26/01/04 17:11:54 INFO DAGScheduler: ResultStage 162 (start at NativeMethodAccessorImpl.java:0) finished in 0.042 s
26/01/04 17:11:54 INFO DAGScheduler: Job 161 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:11:54 INFO TaskSchedulerImpl: Killing all running tasks in stage 162: Stage finished
26/01/04 17:11:54 INFO DAGScheduler: Job 161 finished: start at NativeMethodAccessorImpl.java:0, took 0.044667 s
26/01/04 17:11:54 INFO MemoryStore: Block broadcast_241_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.3 MiB)
26/01/04 17:11:54 INFO BlockManagerInfo: Added broadcast_241_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:11:54 INFO SparkContext: Created broadcast 241 from start at NativeMethodAccessorImpl.java:0
26/01/04 17:11:54 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 79, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@282b6694]. The input RDD has 1 partitions.
26/01/04 17:11:54 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:11:54 INFO DAGScheduler: Got job 162 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/01/04 17:11:54 INFO DAGScheduler: Final stage: ResultStage 163 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:11:54 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:11:54 INFO DAGScheduler: Missing parents: List()
26/01/04 17:11:54 INFO DAGScheduler: Submitting ResultStage 163 (MapPartitionsRDD[817] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:11:54 INFO MemoryStore: Block broadcast_242 stored as values in memory (estimated size 52.9 KiB, free 434.3 MiB)
26/01/04 17:11:54 INFO MemoryStore: Block broadcast_242_piece0 stored as bytes in memory (estimated size 20.2 KiB, free 434.3 MiB)
26/01/04 17:11:54 INFO BlockManagerInfo: Added broadcast_242_piece0 in memory on spark-master:33535 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:11:54 INFO SparkContext: Created broadcast 242 from broadcast at DAGScheduler.scala:1585
26/01/04 17:11:54 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 163 (MapPartitionsRDD[817] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/01/04 17:11:54 INFO TaskSchedulerImpl: Adding task set 163.0 with 1 tasks resource profile 0
26/01/04 17:11:54 INFO TaskSetManager: Starting task 0.0 in stage 163.0 (TID 280) (172.18.0.13, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:11:54 INFO BlockManagerInfo: Added broadcast_242_piece0 in memory on 172.18.0.13:44401 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:11:54 INFO BlockManagerInfo: Added broadcast_241_piece0 in memory on 172.18.0.13:44401 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:11:55 INFO TaskSetManager: Finished task 0.0 in stage 163.0 (TID 280) in 550 ms on 172.18.0.13 (executor 0) (1/1)
26/01/04 17:11:55 INFO TaskSchedulerImpl: Removed TaskSet 163.0, whose tasks have all completed, from pool 
26/01/04 17:11:55 INFO DAGScheduler: ResultStage 163 (start at NativeMethodAccessorImpl.java:0) finished in 0.556 s
26/01/04 17:11:55 INFO DAGScheduler: Job 162 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:11:55 INFO TaskSchedulerImpl: Killing all running tasks in stage 163: Stage finished
26/01/04 17:11:55 INFO DAGScheduler: Job 162 finished: start at NativeMethodAccessorImpl.java:0, took 0.557543 s
26/01/04 17:11:55 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 79, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@282b6694] is committing.
26/01/04 17:11:55 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 79, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@282b6694] committed.
26/01/04 17:11:55 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/79 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.79.0de8d745-5954-4ed1-a20a-7b1afddf040d.tmp
26/01/04 17:11:55 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.79.0de8d745-5954-4ed1-a20a-7b1afddf040d.tmp to file:/tmp/spark-checkpoint-enrichment/commits/79
26/01/04 17:11:55 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T17:11:54.543Z",
  "batchId" : 79,
  "numInputRows" : 30,
  "inputRowsPerSecond" : 2307.6923076923076,
  "processedRowsPerSecond" : 37.08281829419035,
  "durationMs" : {
    "addBatch" : 664,
    "commitOffsets" : 66,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 18,
    "triggerExecution" : 809,
    "walCommit" : 59
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "0" : 1629
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "0" : 1659
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "0" : 1659
      }
    },
    "numInputRows" : 30,
    "inputRowsPerSecond" : 2307.6923076923076,
    "processedRowsPerSecond" : 37.08281829419035,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 31
  }
}
26/01/04 17:12:04 INFO BlockManagerInfo: Removed broadcast_242_piece0 on spark-master:33535 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:12:04 INFO BlockManagerInfo: Removed broadcast_242_piece0 on 172.18.0.13:44401 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:12:04 INFO BlockManagerInfo: Removed broadcast_240_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:12:04 INFO BlockManagerInfo: Removed broadcast_240_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:12:04 INFO BlockManagerInfo: Removed broadcast_240_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:12:04 INFO BlockManagerInfo: Removed broadcast_238_piece0 on spark-master:33535 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:12:04 INFO BlockManagerInfo: Removed broadcast_238_piece0 on 172.18.0.13:44401 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:12:05 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/01/04 17:12:05 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/80 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.80.addf65c0-a99a-41ac-9a94-4e09518c7b81.tmp
26/01/04 17:12:05 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.80.addf65c0-a99a-41ac-9a94-4e09518c7b81.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/80
26/01/04 17:12:05 INFO MicroBatchExecution: Committed offsets for batch 80. Metadata OffsetSeqMetadata(0,1767546725556,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 17:12:05 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:12:05 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:12:05 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:12:05 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:12:05 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:12:05 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:12:05 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#65803 - airline_prefix.nullCount#65802) > 0)
26/01/04 17:12:05 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#65838 - min_flight_num.nullCount#65837) > 0)
26/01/04 17:12:05 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#65833 - max_flight_num.nullCount#65832) > 0)
26/01/04 17:12:05 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:12:05 INFO DAGScheduler: Got job 163 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 17:12:05 INFO DAGScheduler: Final stage: ResultStage 164 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:12:05 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:12:05 INFO DAGScheduler: Missing parents: List()
26/01/04 17:12:05 INFO DAGScheduler: Submitting ResultStage 164 (MapPartitionsRDD[822] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:12:05 INFO MemoryStore: Block broadcast_243 stored as values in memory (estimated size 38.5 KiB, free 434.4 MiB)
26/01/04 17:12:05 INFO MemoryStore: Block broadcast_243_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.3 MiB)
26/01/04 17:12:05 INFO BlockManagerInfo: Added broadcast_243_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:12:05 INFO SparkContext: Created broadcast 243 from broadcast at DAGScheduler.scala:1585
26/01/04 17:12:05 INFO BlockManagerInfo: Removed broadcast_241_piece0 on spark-master:33535 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:12:05 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 164 (MapPartitionsRDD[822] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 17:12:05 INFO TaskSchedulerImpl: Adding task set 164.0 with 2 tasks resource profile 0
26/01/04 17:12:05 INFO BlockManagerInfo: Removed broadcast_241_piece0 on 172.18.0.13:44401 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:12:05 INFO TaskSetManager: Starting task 1.0 in stage 164.0 (TID 281) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 17:12:05 INFO TaskSetManager: Starting task 0.0 in stage 164.0 (TID 282) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 17:12:05 INFO BlockManagerInfo: Added broadcast_243_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:12:05 INFO BlockManagerInfo: Added broadcast_243_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:12:05 INFO TaskSetManager: Finished task 1.0 in stage 164.0 (TID 281) in 40 ms on 172.18.0.13 (executor 0) (1/2)
26/01/04 17:12:05 INFO TaskSetManager: Finished task 0.0 in stage 164.0 (TID 282) in 50 ms on 172.18.0.14 (executor 1) (2/2)
26/01/04 17:12:05 INFO TaskSchedulerImpl: Removed TaskSet 164.0, whose tasks have all completed, from pool 
26/01/04 17:12:05 INFO DAGScheduler: ResultStage 164 (start at NativeMethodAccessorImpl.java:0) finished in 0.066 s
26/01/04 17:12:05 INFO DAGScheduler: Job 163 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:12:05 INFO TaskSchedulerImpl: Killing all running tasks in stage 164: Stage finished
26/01/04 17:12:05 INFO DAGScheduler: Job 163 finished: start at NativeMethodAccessorImpl.java:0, took 0.069787 s
26/01/04 17:12:05 INFO MemoryStore: Block broadcast_244_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.3 MiB)
26/01/04 17:12:05 INFO BlockManagerInfo: Added broadcast_244_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:12:05 INFO SparkContext: Created broadcast 244 from start at NativeMethodAccessorImpl.java:0
26/01/04 17:12:05 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 80, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@74c676da]. The input RDD has 1 partitions.
26/01/04 17:12:05 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:12:05 INFO DAGScheduler: Got job 164 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/01/04 17:12:05 INFO DAGScheduler: Final stage: ResultStage 165 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:12:05 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:12:05 INFO DAGScheduler: Missing parents: List()
26/01/04 17:12:05 INFO DAGScheduler: Submitting ResultStage 165 (MapPartitionsRDD[827] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:12:05 INFO MemoryStore: Block broadcast_245 stored as values in memory (estimated size 52.9 KiB, free 434.3 MiB)
26/01/04 17:12:05 INFO MemoryStore: Block broadcast_245_piece0 stored as bytes in memory (estimated size 20.2 KiB, free 434.3 MiB)
26/01/04 17:12:05 INFO BlockManagerInfo: Added broadcast_245_piece0 in memory on spark-master:33535 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:12:05 INFO SparkContext: Created broadcast 245 from broadcast at DAGScheduler.scala:1585
26/01/04 17:12:05 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 165 (MapPartitionsRDD[827] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/01/04 17:12:05 INFO TaskSchedulerImpl: Adding task set 165.0 with 1 tasks resource profile 0
26/01/04 17:12:05 INFO TaskSetManager: Starting task 0.0 in stage 165.0 (TID 283) (172.18.0.13, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:12:05 INFO BlockManagerInfo: Added broadcast_245_piece0 in memory on 172.18.0.13:44401 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:12:05 INFO BlockManagerInfo: Added broadcast_244_piece0 in memory on 172.18.0.13:44401 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:12:06 INFO TaskSetManager: Finished task 0.0 in stage 165.0 (TID 283) in 621 ms on 172.18.0.13 (executor 0) (1/1)
26/01/04 17:12:06 INFO TaskSchedulerImpl: Removed TaskSet 165.0, whose tasks have all completed, from pool 
26/01/04 17:12:06 INFO DAGScheduler: ResultStage 165 (start at NativeMethodAccessorImpl.java:0) finished in 0.633 s
26/01/04 17:12:06 INFO DAGScheduler: Job 164 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:12:06 INFO TaskSchedulerImpl: Killing all running tasks in stage 165: Stage finished
26/01/04 17:12:06 INFO DAGScheduler: Job 164 finished: start at NativeMethodAccessorImpl.java:0, took 0.636153 s
26/01/04 17:12:06 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 80, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@74c676da] is committing.
26/01/04 17:12:06 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 80, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@74c676da] committed.
26/01/04 17:12:06 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/80 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.80.1208f527-730f-428c-a560-4c058e853d4b.tmp
26/01/04 17:12:06 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.80.1208f527-730f-428c-a560-4c058e853d4b.tmp to file:/tmp/spark-checkpoint-enrichment/commits/80
26/01/04 17:12:06 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T17:12:05.555Z",
  "batchId" : 80,
  "numInputRows" : 30,
  "inputRowsPerSecond" : 2142.8571428571427,
  "processedRowsPerSecond" : 29.211295034079846,
  "durationMs" : {
    "addBatch" : 822,
    "commitOffsets" : 79,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 36,
    "triggerExecution" : 1027,
    "walCommit" : 88
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "0" : 1659
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "0" : 1689
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "0" : 1689
      }
    },
    "numInputRows" : 30,
    "inputRowsPerSecond" : 2142.8571428571427,
    "processedRowsPerSecond" : 29.211295034079846,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 31
  }
}
26/01/04 17:12:16 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/81 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.81.d0f386f8-3bf1-471e-a11b-62e73d7976a3.tmp
26/01/04 17:12:16 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.81.d0f386f8-3bf1-471e-a11b-62e73d7976a3.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/81
26/01/04 17:12:16 INFO MicroBatchExecution: Committed offsets for batch 81. Metadata OffsetSeqMetadata(0,1767546736572,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 17:12:16 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:12:16 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:12:16 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:12:16 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:12:16 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:12:16 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:12:16 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#66607 - airline_prefix.nullCount#66606) > 0)
26/01/04 17:12:16 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#66642 - min_flight_num.nullCount#66641) > 0)
26/01/04 17:12:16 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#66637 - max_flight_num.nullCount#66636) > 0)
26/01/04 17:12:16 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:12:16 INFO DAGScheduler: Got job 165 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 17:12:16 INFO DAGScheduler: Final stage: ResultStage 166 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:12:16 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:12:16 INFO DAGScheduler: Missing parents: List()
26/01/04 17:12:16 INFO DAGScheduler: Submitting ResultStage 166 (MapPartitionsRDD[832] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:12:16 INFO MemoryStore: Block broadcast_246 stored as values in memory (estimated size 38.5 KiB, free 434.2 MiB)
26/01/04 17:12:16 INFO MemoryStore: Block broadcast_246_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.2 MiB)
26/01/04 17:12:16 INFO BlockManagerInfo: Added broadcast_246_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:12:16 INFO SparkContext: Created broadcast 246 from broadcast at DAGScheduler.scala:1585
26/01/04 17:12:16 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 166 (MapPartitionsRDD[832] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 17:12:16 INFO TaskSchedulerImpl: Adding task set 166.0 with 2 tasks resource profile 0
26/01/04 17:12:16 INFO TaskSetManager: Starting task 1.0 in stage 166.0 (TID 284) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 17:12:16 INFO TaskSetManager: Starting task 0.0 in stage 166.0 (TID 285) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 17:12:16 INFO BlockManagerInfo: Added broadcast_246_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:12:16 INFO BlockManagerInfo: Added broadcast_246_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:12:16 INFO TaskSetManager: Finished task 1.0 in stage 166.0 (TID 284) in 15 ms on 172.18.0.13 (executor 0) (1/2)
26/01/04 17:12:16 INFO TaskSetManager: Finished task 0.0 in stage 166.0 (TID 285) in 41 ms on 172.18.0.14 (executor 1) (2/2)
26/01/04 17:12:16 INFO TaskSchedulerImpl: Removed TaskSet 166.0, whose tasks have all completed, from pool 
26/01/04 17:12:16 INFO DAGScheduler: ResultStage 166 (start at NativeMethodAccessorImpl.java:0) finished in 0.050 s
26/01/04 17:12:16 INFO DAGScheduler: Job 165 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:12:16 INFO TaskSchedulerImpl: Killing all running tasks in stage 166: Stage finished
26/01/04 17:12:16 INFO DAGScheduler: Job 165 finished: start at NativeMethodAccessorImpl.java:0, took 0.051764 s
26/01/04 17:12:16 INFO MemoryStore: Block broadcast_247_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.2 MiB)
26/01/04 17:12:16 INFO BlockManagerInfo: Added broadcast_247_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:12:16 INFO SparkContext: Created broadcast 247 from start at NativeMethodAccessorImpl.java:0
26/01/04 17:12:16 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 81, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@b877e05]. The input RDD has 1 partitions.
26/01/04 17:12:16 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:12:16 INFO DAGScheduler: Got job 166 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/01/04 17:12:16 INFO DAGScheduler: Final stage: ResultStage 167 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:12:16 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:12:16 INFO DAGScheduler: Missing parents: List()
26/01/04 17:12:16 INFO DAGScheduler: Submitting ResultStage 167 (MapPartitionsRDD[837] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:12:16 INFO MemoryStore: Block broadcast_248 stored as values in memory (estimated size 52.9 KiB, free 434.2 MiB)
26/01/04 17:12:16 INFO MemoryStore: Block broadcast_248_piece0 stored as bytes in memory (estimated size 20.2 KiB, free 434.1 MiB)
26/01/04 17:12:16 INFO BlockManagerInfo: Added broadcast_248_piece0 in memory on spark-master:33535 (size: 20.2 KiB, free: 434.3 MiB)
26/01/04 17:12:16 INFO SparkContext: Created broadcast 248 from broadcast at DAGScheduler.scala:1585
26/01/04 17:12:16 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 167 (MapPartitionsRDD[837] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/01/04 17:12:16 INFO TaskSchedulerImpl: Adding task set 167.0 with 1 tasks resource profile 0
26/01/04 17:12:16 INFO TaskSetManager: Starting task 0.0 in stage 167.0 (TID 286) (172.18.0.13, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:12:16 INFO BlockManagerInfo: Added broadcast_248_piece0 in memory on 172.18.0.13:44401 (size: 20.2 KiB, free: 434.3 MiB)
26/01/04 17:12:16 INFO BlockManagerInfo: Added broadcast_247_piece0 in memory on 172.18.0.13:44401 (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:12:17 INFO TaskSetManager: Finished task 0.0 in stage 167.0 (TID 286) in 555 ms on 172.18.0.13 (executor 0) (1/1)
26/01/04 17:12:17 INFO TaskSchedulerImpl: Removed TaskSet 167.0, whose tasks have all completed, from pool 
26/01/04 17:12:17 INFO DAGScheduler: ResultStage 167 (start at NativeMethodAccessorImpl.java:0) finished in 0.562 s
26/01/04 17:12:17 INFO DAGScheduler: Job 166 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:12:17 INFO TaskSchedulerImpl: Killing all running tasks in stage 167: Stage finished
26/01/04 17:12:17 INFO DAGScheduler: Job 166 finished: start at NativeMethodAccessorImpl.java:0, took 0.564082 s
26/01/04 17:12:17 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 81, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@b877e05] is committing.
26/01/04 17:12:17 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 81, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@b877e05] committed.
26/01/04 17:12:17 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/81 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.81.f4970d7f-d8a3-4794-a74d-00562992e485.tmp
26/01/04 17:12:17 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.81.f4970d7f-d8a3-4794-a74d-00562992e485.tmp to file:/tmp/spark-checkpoint-enrichment/commits/81
26/01/04 17:12:17 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T17:12:16.569Z",
  "batchId" : 81,
  "numInputRows" : 30,
  "inputRowsPerSecond" : 2307.6923076923076,
  "processedRowsPerSecond" : 35.046728971962615,
  "durationMs" : {
    "addBatch" : 671,
    "commitOffsets" : 92,
    "getBatch" : 0,
    "latestOffset" : 3,
    "queryPlanning" : 18,
    "triggerExecution" : 856,
    "walCommit" : 71
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "0" : 1689
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "0" : 1719
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "0" : 1719
      }
    },
    "numInputRows" : 30,
    "inputRowsPerSecond" : 2307.6923076923076,
    "processedRowsPerSecond" : 35.046728971962615,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 31
  }
}
26/01/04 17:12:18 INFO BlockManagerInfo: Removed broadcast_243_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:12:18 INFO BlockManagerInfo: Removed broadcast_243_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:12:18 INFO BlockManagerInfo: Removed broadcast_243_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:12:18 INFO BlockManagerInfo: Removed broadcast_246_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:12:18 INFO BlockManagerInfo: Removed broadcast_246_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:12:18 INFO BlockManagerInfo: Removed broadcast_246_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:12:18 INFO BlockManagerInfo: Removed broadcast_244_piece0 on spark-master:33535 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:12:18 INFO BlockManagerInfo: Removed broadcast_244_piece0 on 172.18.0.13:44401 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:12:18 INFO BlockManagerInfo: Removed broadcast_248_piece0 on spark-master:33535 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:12:18 INFO BlockManagerInfo: Removed broadcast_248_piece0 on 172.18.0.13:44401 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:12:18 INFO BlockManagerInfo: Removed broadcast_245_piece0 on spark-master:33535 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:12:18 INFO BlockManagerInfo: Removed broadcast_245_piece0 on 172.18.0.13:44401 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:12:27 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/01/04 17:12:27 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/82 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.82.b56fc371-72be-4abd-bd92-ef7fc69d4bac.tmp
26/01/04 17:12:27 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.82.b56fc371-72be-4abd-bd92-ef7fc69d4bac.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/82
26/01/04 17:12:27 INFO MicroBatchExecution: Committed offsets for batch 82. Metadata OffsetSeqMetadata(0,1767546747589,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 17:12:27 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:12:27 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:12:27 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:12:27 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:12:27 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:12:27 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:12:27 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#67411 - airline_prefix.nullCount#67410) > 0)
26/01/04 17:12:27 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#67446 - min_flight_num.nullCount#67445) > 0)
26/01/04 17:12:27 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#67441 - max_flight_num.nullCount#67440) > 0)
26/01/04 17:12:27 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:12:27 INFO DAGScheduler: Got job 167 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 17:12:27 INFO DAGScheduler: Final stage: ResultStage 168 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:12:27 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:12:27 INFO DAGScheduler: Missing parents: List()
26/01/04 17:12:27 INFO DAGScheduler: Submitting ResultStage 168 (MapPartitionsRDD[842] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:12:27 INFO MemoryStore: Block broadcast_249 stored as values in memory (estimated size 38.5 KiB, free 434.4 MiB)
26/01/04 17:12:27 INFO MemoryStore: Block broadcast_249_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.3 MiB)
26/01/04 17:12:27 INFO BlockManagerInfo: Added broadcast_249_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:12:27 INFO SparkContext: Created broadcast 249 from broadcast at DAGScheduler.scala:1585
26/01/04 17:12:27 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 168 (MapPartitionsRDD[842] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 17:12:27 INFO TaskSchedulerImpl: Adding task set 168.0 with 2 tasks resource profile 0
26/01/04 17:12:27 INFO TaskSetManager: Starting task 0.0 in stage 168.0 (TID 287) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 17:12:27 INFO TaskSetManager: Starting task 1.0 in stage 168.0 (TID 288) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 17:12:27 INFO BlockManagerInfo: Added broadcast_249_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:12:27 INFO BlockManagerInfo: Added broadcast_249_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:12:27 INFO TaskSetManager: Finished task 1.0 in stage 168.0 (TID 288) in 17 ms on 172.18.0.13 (executor 0) (1/2)
26/01/04 17:12:27 INFO TaskSetManager: Finished task 0.0 in stage 168.0 (TID 287) in 31 ms on 172.18.0.14 (executor 1) (2/2)
26/01/04 17:12:27 INFO TaskSchedulerImpl: Removed TaskSet 168.0, whose tasks have all completed, from pool 
26/01/04 17:12:27 INFO DAGScheduler: ResultStage 168 (start at NativeMethodAccessorImpl.java:0) finished in 0.036 s
26/01/04 17:12:27 INFO DAGScheduler: Job 167 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:12:27 INFO TaskSchedulerImpl: Killing all running tasks in stage 168: Stage finished
26/01/04 17:12:27 INFO DAGScheduler: Job 167 finished: start at NativeMethodAccessorImpl.java:0, took 0.037920 s
26/01/04 17:12:27 INFO MemoryStore: Block broadcast_250_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.3 MiB)
26/01/04 17:12:27 INFO BlockManagerInfo: Added broadcast_250_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:12:27 INFO SparkContext: Created broadcast 250 from start at NativeMethodAccessorImpl.java:0
26/01/04 17:12:27 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 82, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@716bf66a]. The input RDD has 1 partitions.
26/01/04 17:12:27 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:12:27 INFO DAGScheduler: Got job 168 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/01/04 17:12:27 INFO DAGScheduler: Final stage: ResultStage 169 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:12:27 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:12:27 INFO DAGScheduler: Missing parents: List()
26/01/04 17:12:27 INFO DAGScheduler: Submitting ResultStage 169 (MapPartitionsRDD[847] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:12:27 INFO MemoryStore: Block broadcast_251 stored as values in memory (estimated size 52.9 KiB, free 434.3 MiB)
26/01/04 17:12:27 INFO MemoryStore: Block broadcast_251_piece0 stored as bytes in memory (estimated size 20.2 KiB, free 434.3 MiB)
26/01/04 17:12:27 INFO BlockManagerInfo: Added broadcast_251_piece0 in memory on spark-master:33535 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:12:27 INFO SparkContext: Created broadcast 251 from broadcast at DAGScheduler.scala:1585
26/01/04 17:12:27 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 169 (MapPartitionsRDD[847] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/01/04 17:12:27 INFO TaskSchedulerImpl: Adding task set 169.0 with 1 tasks resource profile 0
26/01/04 17:12:27 INFO TaskSetManager: Starting task 0.0 in stage 169.0 (TID 289) (172.18.0.13, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:12:27 INFO BlockManagerInfo: Added broadcast_251_piece0 in memory on 172.18.0.13:44401 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:12:27 INFO BlockManagerInfo: Added broadcast_250_piece0 in memory on 172.18.0.13:44401 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:12:28 INFO TaskSetManager: Finished task 0.0 in stage 169.0 (TID 289) in 582 ms on 172.18.0.13 (executor 0) (1/1)
26/01/04 17:12:28 INFO TaskSchedulerImpl: Removed TaskSet 169.0, whose tasks have all completed, from pool 
26/01/04 17:12:28 INFO DAGScheduler: ResultStage 169 (start at NativeMethodAccessorImpl.java:0) finished in 0.588 s
26/01/04 17:12:28 INFO DAGScheduler: Job 168 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:12:28 INFO TaskSchedulerImpl: Killing all running tasks in stage 169: Stage finished
26/01/04 17:12:28 INFO DAGScheduler: Job 168 finished: start at NativeMethodAccessorImpl.java:0, took 0.590733 s
26/01/04 17:12:28 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 82, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@716bf66a] is committing.
26/01/04 17:12:28 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 82, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@716bf66a] committed.
26/01/04 17:12:28 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/82 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.82.4936db01-1f32-489f-b6d1-a61c32846ae9.tmp
26/01/04 17:12:28 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.82.4936db01-1f32-489f-b6d1-a61c32846ae9.tmp to file:/tmp/spark-checkpoint-enrichment/commits/82
26/01/04 17:12:28 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T17:12:27.588Z",
  "batchId" : 82,
  "numInputRows" : 30,
  "inputRowsPerSecond" : 2500.0,
  "processedRowsPerSecond" : 33.482142857142854,
  "durationMs" : {
    "addBatch" : 692,
    "commitOffsets" : 104,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 23,
    "triggerExecution" : 896,
    "walCommit" : 76
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "0" : 1719
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "0" : 1749
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "0" : 1749
      }
    },
    "numInputRows" : 30,
    "inputRowsPerSecond" : 2500.0,
    "processedRowsPerSecond" : 33.482142857142854,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 31
  }
}
26/01/04 17:12:37 INFO BlockManagerInfo: Removed broadcast_249_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:12:37 INFO BlockManagerInfo: Removed broadcast_249_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:12:37 INFO BlockManagerInfo: Removed broadcast_249_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:12:37 INFO BlockManagerInfo: Removed broadcast_247_piece0 on spark-master:33535 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:12:37 INFO BlockManagerInfo: Removed broadcast_247_piece0 on 172.18.0.13:44401 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:12:37 INFO BlockManagerInfo: Removed broadcast_251_piece0 on spark-master:33535 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:12:37 INFO BlockManagerInfo: Removed broadcast_251_piece0 on 172.18.0.13:44401 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:12:38 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/01/04 17:12:38 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/83 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.83.b5a0b65b-df86-49c8-bccf-9bd922f9a9e5.tmp
26/01/04 17:12:38 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.83.b5a0b65b-df86-49c8-bccf-9bd922f9a9e5.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/83
26/01/04 17:12:38 INFO MicroBatchExecution: Committed offsets for batch 83. Metadata OffsetSeqMetadata(0,1767546758605,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 17:12:38 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:12:38 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:12:38 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:12:38 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:12:38 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:12:38 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:12:38 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#68215 - airline_prefix.nullCount#68214) > 0)
26/01/04 17:12:38 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#68250 - min_flight_num.nullCount#68249) > 0)
26/01/04 17:12:38 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#68245 - max_flight_num.nullCount#68244) > 0)
26/01/04 17:12:38 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:12:38 INFO DAGScheduler: Got job 169 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 17:12:38 INFO DAGScheduler: Final stage: ResultStage 170 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:12:38 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:12:38 INFO DAGScheduler: Missing parents: List()
26/01/04 17:12:38 INFO DAGScheduler: Submitting ResultStage 170 (MapPartitionsRDD[852] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:12:38 INFO MemoryStore: Block broadcast_252 stored as values in memory (estimated size 38.5 KiB, free 434.4 MiB)
26/01/04 17:12:38 INFO MemoryStore: Block broadcast_252_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.3 MiB)
26/01/04 17:12:38 INFO BlockManagerInfo: Added broadcast_252_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:12:38 INFO BlockManagerInfo: Removed broadcast_250_piece0 on spark-master:33535 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:12:38 INFO SparkContext: Created broadcast 252 from broadcast at DAGScheduler.scala:1585
26/01/04 17:12:38 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 170 (MapPartitionsRDD[852] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 17:12:38 INFO TaskSchedulerImpl: Adding task set 170.0 with 2 tasks resource profile 0
26/01/04 17:12:38 INFO TaskSetManager: Starting task 0.0 in stage 170.0 (TID 290) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 17:12:38 INFO BlockManagerInfo: Removed broadcast_250_piece0 on 172.18.0.13:44401 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:12:38 INFO TaskSetManager: Starting task 1.0 in stage 170.0 (TID 291) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 17:12:38 INFO BlockManagerInfo: Added broadcast_252_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:12:38 INFO BlockManagerInfo: Added broadcast_252_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:12:38 INFO TaskSetManager: Finished task 1.0 in stage 170.0 (TID 291) in 22 ms on 172.18.0.13 (executor 0) (1/2)
26/01/04 17:12:38 INFO TaskSetManager: Finished task 0.0 in stage 170.0 (TID 290) in 38 ms on 172.18.0.14 (executor 1) (2/2)
26/01/04 17:12:38 INFO TaskSchedulerImpl: Removed TaskSet 170.0, whose tasks have all completed, from pool 
26/01/04 17:12:38 INFO DAGScheduler: ResultStage 170 (start at NativeMethodAccessorImpl.java:0) finished in 0.050 s
26/01/04 17:12:38 INFO DAGScheduler: Job 169 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:12:38 INFO TaskSchedulerImpl: Killing all running tasks in stage 170: Stage finished
26/01/04 17:12:38 INFO DAGScheduler: Job 169 finished: start at NativeMethodAccessorImpl.java:0, took 0.051549 s
26/01/04 17:12:38 INFO BlockManagerInfo: Removed broadcast_252_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:12:38 INFO BlockManagerInfo: Removed broadcast_252_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:12:38 INFO MemoryStore: Block broadcast_253_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.4 MiB)
26/01/04 17:12:38 INFO BlockManagerInfo: Removed broadcast_252_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:12:38 INFO BlockManagerInfo: Added broadcast_253_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:12:38 INFO SparkContext: Created broadcast 253 from start at NativeMethodAccessorImpl.java:0
26/01/04 17:12:38 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 83, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@28cec38b]. The input RDD has 1 partitions.
26/01/04 17:12:38 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:12:38 INFO DAGScheduler: Got job 170 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/01/04 17:12:38 INFO DAGScheduler: Final stage: ResultStage 171 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:12:38 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:12:38 INFO DAGScheduler: Missing parents: List()
26/01/04 17:12:38 INFO DAGScheduler: Submitting ResultStage 171 (MapPartitionsRDD[857] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:12:38 INFO MemoryStore: Block broadcast_254 stored as values in memory (estimated size 52.9 KiB, free 434.3 MiB)
26/01/04 17:12:38 INFO MemoryStore: Block broadcast_254_piece0 stored as bytes in memory (estimated size 20.2 KiB, free 434.3 MiB)
26/01/04 17:12:38 INFO BlockManagerInfo: Added broadcast_254_piece0 in memory on spark-master:33535 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:12:38 INFO SparkContext: Created broadcast 254 from broadcast at DAGScheduler.scala:1585
26/01/04 17:12:38 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 171 (MapPartitionsRDD[857] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/01/04 17:12:38 INFO TaskSchedulerImpl: Adding task set 171.0 with 1 tasks resource profile 0
26/01/04 17:12:38 INFO TaskSetManager: Starting task 0.0 in stage 171.0 (TID 292) (172.18.0.13, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:12:38 INFO BlockManagerInfo: Added broadcast_254_piece0 in memory on 172.18.0.13:44401 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:12:38 INFO BlockManagerInfo: Added broadcast_253_piece0 in memory on 172.18.0.13:44401 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:12:39 INFO TaskSetManager: Finished task 0.0 in stage 171.0 (TID 292) in 569 ms on 172.18.0.13 (executor 0) (1/1)
26/01/04 17:12:39 INFO TaskSchedulerImpl: Removed TaskSet 171.0, whose tasks have all completed, from pool 
26/01/04 17:12:39 INFO DAGScheduler: ResultStage 171 (start at NativeMethodAccessorImpl.java:0) finished in 0.579 s
26/01/04 17:12:39 INFO DAGScheduler: Job 170 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:12:39 INFO TaskSchedulerImpl: Killing all running tasks in stage 171: Stage finished
26/01/04 17:12:39 INFO DAGScheduler: Job 170 finished: start at NativeMethodAccessorImpl.java:0, took 0.581651 s
26/01/04 17:12:39 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 83, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@28cec38b] is committing.
26/01/04 17:12:39 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 83, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@28cec38b] committed.
26/01/04 17:12:39 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/83 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.83.ffba5bab-18b3-4a7c-9221-cdf11bc8b324.tmp
26/01/04 17:12:39 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.83.ffba5bab-18b3-4a7c-9221-cdf11bc8b324.tmp to file:/tmp/spark-checkpoint-enrichment/commits/83
26/01/04 17:12:39 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T17:12:38.604Z",
  "batchId" : 83,
  "numInputRows" : 30,
  "inputRowsPerSecond" : 2500.0,
  "processedRowsPerSecond" : 33.333333333333336,
  "durationMs" : {
    "addBatch" : 719,
    "commitOffsets" : 78,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 32,
    "triggerExecution" : 900,
    "walCommit" : 69
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "0" : 1749
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "0" : 1779
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "0" : 1779
      }
    },
    "numInputRows" : 30,
    "inputRowsPerSecond" : 2500.0,
    "processedRowsPerSecond" : 33.333333333333336,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 31
  }
}
26/01/04 17:12:49 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/01/04 17:12:49 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/84 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.84.190df91b-d7b2-4ea7-8650-b0d9d9b2c7d5.tmp
26/01/04 17:12:49 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.84.190df91b-d7b2-4ea7-8650-b0d9d9b2c7d5.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/84
26/01/04 17:12:49 INFO MicroBatchExecution: Committed offsets for batch 84. Metadata OffsetSeqMetadata(0,1767546769613,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 17:12:49 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:12:49 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:12:49 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:12:49 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:12:49 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:12:49 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:12:49 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#69019 - airline_prefix.nullCount#69018) > 0)
26/01/04 17:12:49 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#69054 - min_flight_num.nullCount#69053) > 0)
26/01/04 17:12:49 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#69049 - max_flight_num.nullCount#69048) > 0)
26/01/04 17:12:49 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:12:49 INFO DAGScheduler: Got job 171 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 17:12:49 INFO DAGScheduler: Final stage: ResultStage 172 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:12:49 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:12:49 INFO DAGScheduler: Missing parents: List()
26/01/04 17:12:49 INFO DAGScheduler: Submitting ResultStage 172 (MapPartitionsRDD[862] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:12:49 INFO MemoryStore: Block broadcast_255 stored as values in memory (estimated size 38.5 KiB, free 434.3 MiB)
26/01/04 17:12:49 INFO MemoryStore: Block broadcast_255_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.3 MiB)
26/01/04 17:12:49 INFO BlockManagerInfo: Added broadcast_255_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:12:49 INFO SparkContext: Created broadcast 255 from broadcast at DAGScheduler.scala:1585
26/01/04 17:12:49 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 172 (MapPartitionsRDD[862] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 17:12:49 INFO TaskSchedulerImpl: Adding task set 172.0 with 2 tasks resource profile 0
26/01/04 17:12:49 INFO TaskSetManager: Starting task 1.0 in stage 172.0 (TID 293) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 17:12:49 INFO TaskSetManager: Starting task 0.0 in stage 172.0 (TID 294) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 17:12:49 INFO BlockManagerInfo: Added broadcast_255_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:12:49 INFO BlockManagerInfo: Added broadcast_255_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:12:49 INFO TaskSetManager: Finished task 1.0 in stage 172.0 (TID 293) in 45 ms on 172.18.0.13 (executor 0) (1/2)
26/01/04 17:12:49 INFO TaskSetManager: Finished task 0.0 in stage 172.0 (TID 294) in 83 ms on 172.18.0.14 (executor 1) (2/2)
26/01/04 17:12:49 INFO TaskSchedulerImpl: Removed TaskSet 172.0, whose tasks have all completed, from pool 
26/01/04 17:12:49 INFO DAGScheduler: ResultStage 172 (start at NativeMethodAccessorImpl.java:0) finished in 0.098 s
26/01/04 17:12:49 INFO DAGScheduler: Job 171 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:12:49 INFO TaskSchedulerImpl: Killing all running tasks in stage 172: Stage finished
26/01/04 17:12:49 INFO DAGScheduler: Job 171 finished: start at NativeMethodAccessorImpl.java:0, took 0.101734 s
26/01/04 17:12:49 INFO MemoryStore: Block broadcast_256_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.3 MiB)
26/01/04 17:12:49 INFO BlockManagerInfo: Added broadcast_256_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:12:49 INFO SparkContext: Created broadcast 256 from start at NativeMethodAccessorImpl.java:0
26/01/04 17:12:49 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 84, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@61b9ce2a]. The input RDD has 1 partitions.
26/01/04 17:12:49 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:12:49 INFO DAGScheduler: Got job 172 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/01/04 17:12:49 INFO DAGScheduler: Final stage: ResultStage 173 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:12:49 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:12:49 INFO DAGScheduler: Missing parents: List()
26/01/04 17:12:49 INFO DAGScheduler: Submitting ResultStage 173 (MapPartitionsRDD[867] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:12:49 INFO MemoryStore: Block broadcast_257 stored as values in memory (estimated size 52.9 KiB, free 434.2 MiB)
26/01/04 17:12:49 INFO MemoryStore: Block broadcast_257_piece0 stored as bytes in memory (estimated size 20.2 KiB, free 434.2 MiB)
26/01/04 17:12:49 INFO BlockManagerInfo: Added broadcast_257_piece0 in memory on spark-master:33535 (size: 20.2 KiB, free: 434.3 MiB)
26/01/04 17:12:49 INFO SparkContext: Created broadcast 257 from broadcast at DAGScheduler.scala:1585
26/01/04 17:12:49 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 173 (MapPartitionsRDD[867] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/01/04 17:12:49 INFO TaskSchedulerImpl: Adding task set 173.0 with 1 tasks resource profile 0
26/01/04 17:12:49 INFO TaskSetManager: Starting task 0.0 in stage 173.0 (TID 295) (172.18.0.13, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:12:49 INFO BlockManagerInfo: Added broadcast_257_piece0 in memory on 172.18.0.13:44401 (size: 20.2 KiB, free: 434.3 MiB)
26/01/04 17:12:49 INFO BlockManagerInfo: Added broadcast_256_piece0 in memory on 172.18.0.13:44401 (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:12:50 INFO TaskSetManager: Finished task 0.0 in stage 173.0 (TID 295) in 599 ms on 172.18.0.13 (executor 0) (1/1)
26/01/04 17:12:50 INFO TaskSchedulerImpl: Removed TaskSet 173.0, whose tasks have all completed, from pool 
26/01/04 17:12:50 INFO DAGScheduler: ResultStage 173 (start at NativeMethodAccessorImpl.java:0) finished in 0.617 s
26/01/04 17:12:50 INFO DAGScheduler: Job 172 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:12:50 INFO TaskSchedulerImpl: Killing all running tasks in stage 173: Stage finished
26/01/04 17:12:50 INFO DAGScheduler: Job 172 finished: start at NativeMethodAccessorImpl.java:0, took 0.618691 s
26/01/04 17:12:50 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 84, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@61b9ce2a] is committing.
26/01/04 17:12:50 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 84, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@61b9ce2a] committed.
26/01/04 17:12:50 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/84 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.84.c5a47dd7-479d-40b0-9f8c-996cac1fbdba.tmp
26/01/04 17:12:50 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.84.c5a47dd7-479d-40b0-9f8c-996cac1fbdba.tmp to file:/tmp/spark-checkpoint-enrichment/commits/84
26/01/04 17:12:50 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T17:12:49.612Z",
  "batchId" : 84,
  "numInputRows" : 30,
  "inputRowsPerSecond" : 2142.8571428571427,
  "processedRowsPerSecond" : 29.585798816568047,
  "durationMs" : {
    "addBatch" : 837,
    "commitOffsets" : 67,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 35,
    "triggerExecution" : 1014,
    "walCommit" : 73
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "0" : 1779
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "0" : 1809
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "0" : 1809
      }
    },
    "numInputRows" : 30,
    "inputRowsPerSecond" : 2142.8571428571427,
    "processedRowsPerSecond" : 29.585798816568047,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 31
  }
}
26/01/04 17:12:54 INFO BlockManagerInfo: Removed broadcast_253_piece0 on spark-master:33535 in memory (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:12:54 INFO BlockManagerInfo: Removed broadcast_253_piece0 on 172.18.0.13:44401 in memory (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:12:54 INFO BlockManagerInfo: Removed broadcast_257_piece0 on spark-master:33535 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:12:54 INFO BlockManagerInfo: Removed broadcast_257_piece0 on 172.18.0.13:44401 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:12:54 INFO BlockManagerInfo: Removed broadcast_255_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:12:54 INFO BlockManagerInfo: Removed broadcast_255_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:12:54 INFO BlockManagerInfo: Removed broadcast_255_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:12:54 INFO BlockManagerInfo: Removed broadcast_254_piece0 on spark-master:33535 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:12:54 INFO BlockManagerInfo: Removed broadcast_254_piece0 on 172.18.0.13:44401 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:13:00 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/85 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.85.5226836c-9c66-4408-9d1f-f3c931355905.tmp
26/01/04 17:13:00 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.85.5226836c-9c66-4408-9d1f-f3c931355905.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/85
26/01/04 17:13:00 INFO MicroBatchExecution: Committed offsets for batch 85. Metadata OffsetSeqMetadata(0,1767546780625,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 17:13:00 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:13:00 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:13:00 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:13:00 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:13:00 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:13:00 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:13:00 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#69823 - airline_prefix.nullCount#69822) > 0)
26/01/04 17:13:00 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#69858 - min_flight_num.nullCount#69857) > 0)
26/01/04 17:13:00 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#69853 - max_flight_num.nullCount#69852) > 0)
26/01/04 17:13:00 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:13:00 INFO DAGScheduler: Got job 173 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 17:13:00 INFO DAGScheduler: Final stage: ResultStage 174 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:13:00 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:13:00 INFO DAGScheduler: Missing parents: List()
26/01/04 17:13:00 INFO DAGScheduler: Submitting ResultStage 174 (MapPartitionsRDD[872] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:13:00 INFO MemoryStore: Block broadcast_258 stored as values in memory (estimated size 38.5 KiB, free 434.4 MiB)
26/01/04 17:13:00 INFO MemoryStore: Block broadcast_258_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.3 MiB)
26/01/04 17:13:00 INFO BlockManagerInfo: Removed broadcast_256_piece0 on spark-master:33535 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:13:00 INFO BlockManagerInfo: Added broadcast_258_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:13:00 INFO SparkContext: Created broadcast 258 from broadcast at DAGScheduler.scala:1585
26/01/04 17:13:00 INFO BlockManagerInfo: Removed broadcast_256_piece0 on 172.18.0.13:44401 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:13:00 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 174 (MapPartitionsRDD[872] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 17:13:00 INFO TaskSchedulerImpl: Adding task set 174.0 with 2 tasks resource profile 0
26/01/04 17:13:00 INFO TaskSetManager: Starting task 0.0 in stage 174.0 (TID 296) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 17:13:00 INFO TaskSetManager: Starting task 1.0 in stage 174.0 (TID 297) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 17:13:00 INFO BlockManagerInfo: Added broadcast_258_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:13:00 INFO BlockManagerInfo: Added broadcast_258_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:13:00 INFO TaskSetManager: Finished task 1.0 in stage 174.0 (TID 297) in 26 ms on 172.18.0.13 (executor 0) (1/2)
26/01/04 17:13:00 INFO TaskSetManager: Finished task 0.0 in stage 174.0 (TID 296) in 98 ms on 172.18.0.14 (executor 1) (2/2)
26/01/04 17:13:00 INFO TaskSchedulerImpl: Removed TaskSet 174.0, whose tasks have all completed, from pool 
26/01/04 17:13:00 INFO DAGScheduler: ResultStage 174 (start at NativeMethodAccessorImpl.java:0) finished in 0.111 s
26/01/04 17:13:00 INFO DAGScheduler: Job 173 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:13:00 INFO TaskSchedulerImpl: Killing all running tasks in stage 174: Stage finished
26/01/04 17:13:00 INFO DAGScheduler: Job 173 finished: start at NativeMethodAccessorImpl.java:0, took 0.112959 s
26/01/04 17:13:00 INFO MemoryStore: Block broadcast_259_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.3 MiB)
26/01/04 17:13:00 INFO BlockManagerInfo: Added broadcast_259_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:13:00 INFO SparkContext: Created broadcast 259 from start at NativeMethodAccessorImpl.java:0
26/01/04 17:13:00 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 85, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@493e5df1]. The input RDD has 1 partitions.
26/01/04 17:13:00 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:13:00 INFO DAGScheduler: Got job 174 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/01/04 17:13:00 INFO DAGScheduler: Final stage: ResultStage 175 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:13:00 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:13:00 INFO DAGScheduler: Missing parents: List()
26/01/04 17:13:00 INFO DAGScheduler: Submitting ResultStage 175 (MapPartitionsRDD[877] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:13:00 INFO MemoryStore: Block broadcast_260 stored as values in memory (estimated size 52.9 KiB, free 434.3 MiB)
26/01/04 17:13:00 INFO MemoryStore: Block broadcast_260_piece0 stored as bytes in memory (estimated size 20.2 KiB, free 434.3 MiB)
26/01/04 17:13:00 INFO BlockManagerInfo: Added broadcast_260_piece0 in memory on spark-master:33535 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:13:00 INFO SparkContext: Created broadcast 260 from broadcast at DAGScheduler.scala:1585
26/01/04 17:13:00 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 175 (MapPartitionsRDD[877] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/01/04 17:13:00 INFO TaskSchedulerImpl: Adding task set 175.0 with 1 tasks resource profile 0
26/01/04 17:13:00 INFO TaskSetManager: Starting task 0.0 in stage 175.0 (TID 298) (172.18.0.13, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:13:00 INFO BlockManagerInfo: Added broadcast_260_piece0 in memory on 172.18.0.13:44401 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:13:00 INFO BlockManagerInfo: Added broadcast_259_piece0 in memory on 172.18.0.13:44401 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:13:01 INFO TaskSetManager: Finished task 0.0 in stage 175.0 (TID 298) in 553 ms on 172.18.0.13 (executor 0) (1/1)
26/01/04 17:13:01 INFO TaskSchedulerImpl: Removed TaskSet 175.0, whose tasks have all completed, from pool 
26/01/04 17:13:01 INFO DAGScheduler: ResultStage 175 (start at NativeMethodAccessorImpl.java:0) finished in 0.562 s
26/01/04 17:13:01 INFO DAGScheduler: Job 174 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:13:01 INFO TaskSchedulerImpl: Killing all running tasks in stage 175: Stage finished
26/01/04 17:13:01 INFO DAGScheduler: Job 174 finished: start at NativeMethodAccessorImpl.java:0, took 0.562934 s
26/01/04 17:13:01 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 85, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@493e5df1] is committing.
26/01/04 17:13:01 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 85, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@493e5df1] committed.
26/01/04 17:13:01 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/85 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.85.bd93b151-c68e-4ae7-b9f3-f8645d14ebd7.tmp
26/01/04 17:13:01 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.85.bd93b151-c68e-4ae7-b9f3-f8645d14ebd7.tmp to file:/tmp/spark-checkpoint-enrichment/commits/85
26/01/04 17:13:01 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T17:13:00.623Z",
  "batchId" : 85,
  "numInputRows" : 30,
  "inputRowsPerSecond" : 2500.0,
  "processedRowsPerSecond" : 32.29278794402583,
  "durationMs" : {
    "addBatch" : 746,
    "commitOffsets" : 75,
    "getBatch" : 0,
    "latestOffset" : 2,
    "queryPlanning" : 26,
    "triggerExecution" : 929,
    "walCommit" : 79
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "0" : 1809
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "0" : 1839
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "0" : 1839
      }
    },
    "numInputRows" : 30,
    "inputRowsPerSecond" : 2500.0,
    "processedRowsPerSecond" : 32.29278794402583,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 31
  }
}
26/01/04 17:13:11 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/01/04 17:13:11 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/86 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.86.2f507998-63f0-4e71-bd22-cabc4139bd27.tmp
26/01/04 17:13:11 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.86.2f507998-63f0-4e71-bd22-cabc4139bd27.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/86
26/01/04 17:13:11 INFO MicroBatchExecution: Committed offsets for batch 86. Metadata OffsetSeqMetadata(0,1767546791643,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 17:13:11 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:13:11 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:13:11 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:13:11 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:13:11 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:13:11 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:13:11 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#70627 - airline_prefix.nullCount#70626) > 0)
26/01/04 17:13:11 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#70662 - min_flight_num.nullCount#70661) > 0)
26/01/04 17:13:11 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#70657 - max_flight_num.nullCount#70656) > 0)
26/01/04 17:13:11 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:13:11 INFO DAGScheduler: Got job 175 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 17:13:11 INFO DAGScheduler: Final stage: ResultStage 176 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:13:11 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:13:11 INFO DAGScheduler: Missing parents: List()
26/01/04 17:13:11 INFO DAGScheduler: Submitting ResultStage 176 (MapPartitionsRDD[882] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:13:11 INFO MemoryStore: Block broadcast_261 stored as values in memory (estimated size 38.5 KiB, free 434.2 MiB)
26/01/04 17:13:11 INFO MemoryStore: Block broadcast_261_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.2 MiB)
26/01/04 17:13:11 INFO BlockManagerInfo: Added broadcast_261_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:13:11 INFO SparkContext: Created broadcast 261 from broadcast at DAGScheduler.scala:1585
26/01/04 17:13:11 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 176 (MapPartitionsRDD[882] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 17:13:11 INFO TaskSchedulerImpl: Adding task set 176.0 with 2 tasks resource profile 0
26/01/04 17:13:11 INFO TaskSetManager: Starting task 1.0 in stage 176.0 (TID 299) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 17:13:11 INFO TaskSetManager: Starting task 0.0 in stage 176.0 (TID 300) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 17:13:11 INFO BlockManagerInfo: Added broadcast_261_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:13:11 INFO TaskSetManager: Finished task 1.0 in stage 176.0 (TID 299) in 16 ms on 172.18.0.13 (executor 0) (1/2)
26/01/04 17:13:11 INFO BlockManagerInfo: Added broadcast_261_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:13:11 INFO TaskSetManager: Finished task 0.0 in stage 176.0 (TID 300) in 37 ms on 172.18.0.14 (executor 1) (2/2)
26/01/04 17:13:11 INFO TaskSchedulerImpl: Removed TaskSet 176.0, whose tasks have all completed, from pool 
26/01/04 17:13:11 INFO DAGScheduler: ResultStage 176 (start at NativeMethodAccessorImpl.java:0) finished in 0.043 s
26/01/04 17:13:11 INFO DAGScheduler: Job 175 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:13:11 INFO TaskSchedulerImpl: Killing all running tasks in stage 176: Stage finished
26/01/04 17:13:11 INFO DAGScheduler: Job 175 finished: start at NativeMethodAccessorImpl.java:0, took 0.045081 s
26/01/04 17:13:11 INFO MemoryStore: Block broadcast_262_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.2 MiB)
26/01/04 17:13:11 INFO BlockManagerInfo: Added broadcast_262_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:13:11 INFO SparkContext: Created broadcast 262 from start at NativeMethodAccessorImpl.java:0
26/01/04 17:13:11 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 86, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@1c1ff693]. The input RDD has 1 partitions.
26/01/04 17:13:11 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:13:11 INFO DAGScheduler: Got job 176 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/01/04 17:13:11 INFO DAGScheduler: Final stage: ResultStage 177 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:13:11 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:13:11 INFO DAGScheduler: Missing parents: List()
26/01/04 17:13:11 INFO DAGScheduler: Submitting ResultStage 177 (MapPartitionsRDD[887] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:13:11 INFO MemoryStore: Block broadcast_263 stored as values in memory (estimated size 52.9 KiB, free 434.2 MiB)
26/01/04 17:13:11 INFO MemoryStore: Block broadcast_263_piece0 stored as bytes in memory (estimated size 20.2 KiB, free 434.1 MiB)
26/01/04 17:13:11 INFO BlockManagerInfo: Added broadcast_263_piece0 in memory on spark-master:33535 (size: 20.2 KiB, free: 434.3 MiB)
26/01/04 17:13:11 INFO SparkContext: Created broadcast 263 from broadcast at DAGScheduler.scala:1585
26/01/04 17:13:11 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 177 (MapPartitionsRDD[887] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/01/04 17:13:11 INFO TaskSchedulerImpl: Adding task set 177.0 with 1 tasks resource profile 0
26/01/04 17:13:11 INFO TaskSetManager: Starting task 0.0 in stage 177.0 (TID 301) (172.18.0.13, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:13:11 INFO BlockManagerInfo: Added broadcast_263_piece0 in memory on 172.18.0.13:44401 (size: 20.2 KiB, free: 434.3 MiB)
26/01/04 17:13:11 INFO BlockManagerInfo: Added broadcast_262_piece0 in memory on 172.18.0.13:44401 (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:13:12 INFO TaskSetManager: Finished task 0.0 in stage 177.0 (TID 301) in 552 ms on 172.18.0.13 (executor 0) (1/1)
26/01/04 17:13:12 INFO TaskSchedulerImpl: Removed TaskSet 177.0, whose tasks have all completed, from pool 
26/01/04 17:13:12 INFO DAGScheduler: ResultStage 177 (start at NativeMethodAccessorImpl.java:0) finished in 0.559 s
26/01/04 17:13:12 INFO DAGScheduler: Job 176 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:13:12 INFO TaskSchedulerImpl: Killing all running tasks in stage 177: Stage finished
26/01/04 17:13:12 INFO DAGScheduler: Job 176 finished: start at NativeMethodAccessorImpl.java:0, took 0.560849 s
26/01/04 17:13:12 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 86, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@1c1ff693] is committing.
26/01/04 17:13:12 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 86, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@1c1ff693] committed.
26/01/04 17:13:12 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/86 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.86.4e39d03f-ab45-4db0-8032-650e4c8f899e.tmp
26/01/04 17:13:12 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.86.4e39d03f-ab45-4db0-8032-650e4c8f899e.tmp to file:/tmp/spark-checkpoint-enrichment/commits/86
26/01/04 17:13:12 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T17:13:11.642Z",
  "batchId" : 86,
  "numInputRows" : 30,
  "inputRowsPerSecond" : 2500.0,
  "processedRowsPerSecond" : 35.046728971962615,
  "durationMs" : {
    "addBatch" : 674,
    "commitOffsets" : 65,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 33,
    "triggerExecution" : 856,
    "walCommit" : 83
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "0" : 1839
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "0" : 1869
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "0" : 1869
      }
    },
    "numInputRows" : 30,
    "inputRowsPerSecond" : 2500.0,
    "processedRowsPerSecond" : 35.046728971962615,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 31
  }
}
26/01/04 17:13:13 INFO BlockManagerInfo: Removed broadcast_260_piece0 on spark-master:33535 in memory (size: 20.2 KiB, free: 434.3 MiB)
26/01/04 17:13:13 INFO BlockManagerInfo: Removed broadcast_260_piece0 on 172.18.0.13:44401 in memory (size: 20.2 KiB, free: 434.3 MiB)
26/01/04 17:13:13 INFO BlockManagerInfo: Removed broadcast_258_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:13:13 INFO BlockManagerInfo: Removed broadcast_258_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:13:13 INFO BlockManagerInfo: Removed broadcast_258_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:13:13 INFO BlockManagerInfo: Removed broadcast_259_piece0 on spark-master:33535 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:13:13 INFO BlockManagerInfo: Removed broadcast_259_piece0 on 172.18.0.13:44401 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:13:13 INFO BlockManagerInfo: Removed broadcast_263_piece0 on spark-master:33535 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:13:13 INFO BlockManagerInfo: Removed broadcast_263_piece0 on 172.18.0.13:44401 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:13:13 INFO BlockManagerInfo: Removed broadcast_261_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:13:13 INFO BlockManagerInfo: Removed broadcast_261_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:13:13 INFO BlockManagerInfo: Removed broadcast_261_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:13:22 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/01/04 17:13:22 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/87 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.87.d7f46b47-9531-4852-bc68-4b6dde55cb6a.tmp
26/01/04 17:13:22 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.87.d7f46b47-9531-4852-bc68-4b6dde55cb6a.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/87
26/01/04 17:13:22 INFO MicroBatchExecution: Committed offsets for batch 87. Metadata OffsetSeqMetadata(0,1767546802671,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 17:13:22 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:13:22 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:13:22 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:13:22 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:13:22 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:13:22 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:13:22 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#71431 - airline_prefix.nullCount#71430) > 0)
26/01/04 17:13:22 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#71466 - min_flight_num.nullCount#71465) > 0)
26/01/04 17:13:22 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#71461 - max_flight_num.nullCount#71460) > 0)
26/01/04 17:13:22 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:13:22 INFO DAGScheduler: Got job 177 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 17:13:22 INFO DAGScheduler: Final stage: ResultStage 178 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:13:22 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:13:22 INFO DAGScheduler: Missing parents: List()
26/01/04 17:13:22 INFO DAGScheduler: Submitting ResultStage 178 (MapPartitionsRDD[892] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:13:22 INFO MemoryStore: Block broadcast_264 stored as values in memory (estimated size 38.5 KiB, free 434.4 MiB)
26/01/04 17:13:22 INFO BlockManagerInfo: Removed broadcast_262_piece0 on spark-master:33535 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:13:22 INFO MemoryStore: Block broadcast_264_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.3 MiB)
26/01/04 17:13:22 INFO BlockManagerInfo: Added broadcast_264_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:13:22 INFO SparkContext: Created broadcast 264 from broadcast at DAGScheduler.scala:1585
26/01/04 17:13:22 INFO BlockManagerInfo: Removed broadcast_262_piece0 on 172.18.0.13:44401 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:13:22 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 178 (MapPartitionsRDD[892] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 17:13:22 INFO TaskSchedulerImpl: Adding task set 178.0 with 2 tasks resource profile 0
26/01/04 17:13:22 INFO TaskSetManager: Starting task 1.0 in stage 178.0 (TID 302) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 17:13:22 INFO TaskSetManager: Starting task 0.0 in stage 178.0 (TID 303) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 17:13:22 INFO BlockManagerInfo: Added broadcast_264_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:13:22 INFO BlockManagerInfo: Added broadcast_264_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:13:22 INFO TaskSetManager: Finished task 1.0 in stage 178.0 (TID 302) in 23 ms on 172.18.0.13 (executor 0) (1/2)
26/01/04 17:13:22 INFO TaskSetManager: Finished task 0.0 in stage 178.0 (TID 303) in 35 ms on 172.18.0.14 (executor 1) (2/2)
26/01/04 17:13:22 INFO TaskSchedulerImpl: Removed TaskSet 178.0, whose tasks have all completed, from pool 
26/01/04 17:13:22 INFO DAGScheduler: ResultStage 178 (start at NativeMethodAccessorImpl.java:0) finished in 0.046 s
26/01/04 17:13:22 INFO DAGScheduler: Job 177 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:13:22 INFO TaskSchedulerImpl: Killing all running tasks in stage 178: Stage finished
26/01/04 17:13:22 INFO DAGScheduler: Job 177 finished: start at NativeMethodAccessorImpl.java:0, took 0.048650 s
26/01/04 17:13:22 INFO MemoryStore: Block broadcast_265_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.3 MiB)
26/01/04 17:13:22 INFO BlockManagerInfo: Added broadcast_265_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:13:22 INFO SparkContext: Created broadcast 265 from start at NativeMethodAccessorImpl.java:0
26/01/04 17:13:22 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 87, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@701cbadf]. The input RDD has 1 partitions.
26/01/04 17:13:22 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:13:22 INFO DAGScheduler: Got job 178 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/01/04 17:13:22 INFO DAGScheduler: Final stage: ResultStage 179 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:13:22 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:13:22 INFO DAGScheduler: Missing parents: List()
26/01/04 17:13:22 INFO DAGScheduler: Submitting ResultStage 179 (MapPartitionsRDD[897] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:13:22 INFO MemoryStore: Block broadcast_266 stored as values in memory (estimated size 52.9 KiB, free 434.3 MiB)
26/01/04 17:13:22 INFO MemoryStore: Block broadcast_266_piece0 stored as bytes in memory (estimated size 20.2 KiB, free 434.3 MiB)
26/01/04 17:13:23 INFO BlockManagerInfo: Added broadcast_266_piece0 in memory on spark-master:33535 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:13:23 INFO SparkContext: Created broadcast 266 from broadcast at DAGScheduler.scala:1585
26/01/04 17:13:23 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 179 (MapPartitionsRDD[897] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/01/04 17:13:23 INFO TaskSchedulerImpl: Adding task set 179.0 with 1 tasks resource profile 0
26/01/04 17:13:23 INFO TaskSetManager: Starting task 0.0 in stage 179.0 (TID 304) (172.18.0.13, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:13:23 INFO BlockManagerInfo: Added broadcast_266_piece0 in memory on 172.18.0.13:44401 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:13:23 INFO BlockManagerInfo: Added broadcast_265_piece0 in memory on 172.18.0.13:44401 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:13:24 INFO TaskSetManager: Finished task 0.0 in stage 179.0 (TID 304) in 807 ms on 172.18.0.13 (executor 0) (1/1)
26/01/04 17:13:24 INFO TaskSchedulerImpl: Removed TaskSet 179.0, whose tasks have all completed, from pool 
26/01/04 17:13:24 INFO DAGScheduler: ResultStage 179 (start at NativeMethodAccessorImpl.java:0) finished in 1.188 s
26/01/04 17:13:24 INFO DAGScheduler: Job 178 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:13:24 INFO TaskSchedulerImpl: Killing all running tasks in stage 179: Stage finished
26/01/04 17:13:24 INFO DAGScheduler: Job 178 finished: start at NativeMethodAccessorImpl.java:0, took 1.189020 s
26/01/04 17:13:24 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 87, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@701cbadf] is committing.
26/01/04 17:13:24 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 87, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@701cbadf] committed.
26/01/04 17:13:24 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/87 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.87.59b34f6e-6385-4618-bce1-1714401be5b3.tmp
26/01/04 17:13:24 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.87.59b34f6e-6385-4618-bce1-1714401be5b3.tmp to file:/tmp/spark-checkpoint-enrichment/commits/87
26/01/04 17:13:24 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T17:13:22.670Z",
  "batchId" : 87,
  "numInputRows" : 30,
  "inputRowsPerSecond" : 2500.0,
  "processedRowsPerSecond" : 19.63350785340314,
  "durationMs" : {
    "addBatch" : 1302,
    "commitOffsets" : 96,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 29,
    "triggerExecution" : 1528,
    "walCommit" : 100
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "0" : 1869
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "0" : 1899
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "0" : 1899
      }
    },
    "numInputRows" : 30,
    "inputRowsPerSecond" : 2500.0,
    "processedRowsPerSecond" : 19.63350785340314,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 31
  }
}
26/01/04 17:13:33 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/88 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.88.242af01b-465c-467c-a600-1def0911cb2f.tmp
26/01/04 17:13:33 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.88.242af01b-465c-467c-a600-1def0911cb2f.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/88
26/01/04 17:13:33 INFO MicroBatchExecution: Committed offsets for batch 88. Metadata OffsetSeqMetadata(0,1767546813688,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 17:13:33 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:13:33 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:13:33 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:13:33 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:13:33 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:13:33 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:13:33 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#72235 - airline_prefix.nullCount#72234) > 0)
26/01/04 17:13:33 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#72270 - min_flight_num.nullCount#72269) > 0)
26/01/04 17:13:33 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#72265 - max_flight_num.nullCount#72264) > 0)
26/01/04 17:13:33 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:13:33 INFO DAGScheduler: Got job 179 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 17:13:33 INFO DAGScheduler: Final stage: ResultStage 180 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:13:33 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:13:33 INFO DAGScheduler: Missing parents: List()
26/01/04 17:13:33 INFO DAGScheduler: Submitting ResultStage 180 (MapPartitionsRDD[902] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:13:33 INFO MemoryStore: Block broadcast_267 stored as values in memory (estimated size 38.5 KiB, free 434.2 MiB)
26/01/04 17:13:33 INFO MemoryStore: Block broadcast_267_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.2 MiB)
26/01/04 17:13:33 INFO BlockManagerInfo: Added broadcast_267_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:13:33 INFO SparkContext: Created broadcast 267 from broadcast at DAGScheduler.scala:1585
26/01/04 17:13:33 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 180 (MapPartitionsRDD[902] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 17:13:33 INFO TaskSchedulerImpl: Adding task set 180.0 with 2 tasks resource profile 0
26/01/04 17:13:33 INFO TaskSetManager: Starting task 1.0 in stage 180.0 (TID 305) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 17:13:33 INFO TaskSetManager: Starting task 0.0 in stage 180.0 (TID 306) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 17:13:33 INFO BlockManagerInfo: Added broadcast_267_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:13:33 INFO BlockManagerInfo: Added broadcast_267_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:13:33 INFO TaskSetManager: Finished task 1.0 in stage 180.0 (TID 305) in 26 ms on 172.18.0.13 (executor 0) (1/2)
26/01/04 17:13:33 INFO TaskSetManager: Finished task 0.0 in stage 180.0 (TID 306) in 39 ms on 172.18.0.14 (executor 1) (2/2)
26/01/04 17:13:33 INFO TaskSchedulerImpl: Removed TaskSet 180.0, whose tasks have all completed, from pool 
26/01/04 17:13:33 INFO DAGScheduler: ResultStage 180 (start at NativeMethodAccessorImpl.java:0) finished in 0.048 s
26/01/04 17:13:33 INFO DAGScheduler: Job 179 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:13:33 INFO TaskSchedulerImpl: Killing all running tasks in stage 180: Stage finished
26/01/04 17:13:33 INFO DAGScheduler: Job 179 finished: start at NativeMethodAccessorImpl.java:0, took 0.050204 s
26/01/04 17:13:33 INFO MemoryStore: Block broadcast_268_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.2 MiB)
26/01/04 17:13:33 INFO BlockManagerInfo: Added broadcast_268_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:13:33 INFO SparkContext: Created broadcast 268 from start at NativeMethodAccessorImpl.java:0
26/01/04 17:13:33 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 88, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@6698961e]. The input RDD has 1 partitions.
26/01/04 17:13:33 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:13:33 INFO DAGScheduler: Got job 180 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/01/04 17:13:33 INFO DAGScheduler: Final stage: ResultStage 181 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:13:33 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:13:33 INFO DAGScheduler: Missing parents: List()
26/01/04 17:13:33 INFO DAGScheduler: Submitting ResultStage 181 (MapPartitionsRDD[907] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:13:33 INFO MemoryStore: Block broadcast_269 stored as values in memory (estimated size 52.9 KiB, free 434.2 MiB)
26/01/04 17:13:33 INFO MemoryStore: Block broadcast_269_piece0 stored as bytes in memory (estimated size 20.2 KiB, free 434.1 MiB)
26/01/04 17:13:33 INFO BlockManagerInfo: Added broadcast_269_piece0 in memory on spark-master:33535 (size: 20.2 KiB, free: 434.3 MiB)
26/01/04 17:13:33 INFO SparkContext: Created broadcast 269 from broadcast at DAGScheduler.scala:1585
26/01/04 17:13:33 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 181 (MapPartitionsRDD[907] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/01/04 17:13:33 INFO TaskSchedulerImpl: Adding task set 181.0 with 1 tasks resource profile 0
26/01/04 17:13:33 INFO TaskSetManager: Starting task 0.0 in stage 181.0 (TID 307) (172.18.0.13, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:13:33 INFO BlockManagerInfo: Added broadcast_269_piece0 in memory on 172.18.0.13:44401 (size: 20.2 KiB, free: 434.3 MiB)
26/01/04 17:13:33 INFO BlockManagerInfo: Added broadcast_268_piece0 in memory on 172.18.0.13:44401 (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:13:34 INFO TaskSetManager: Finished task 0.0 in stage 181.0 (TID 307) in 557 ms on 172.18.0.13 (executor 0) (1/1)
26/01/04 17:13:34 INFO TaskSchedulerImpl: Removed TaskSet 181.0, whose tasks have all completed, from pool 
26/01/04 17:13:34 INFO DAGScheduler: ResultStage 181 (start at NativeMethodAccessorImpl.java:0) finished in 0.563 s
26/01/04 17:13:34 INFO DAGScheduler: Job 180 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:13:34 INFO TaskSchedulerImpl: Killing all running tasks in stage 181: Stage finished
26/01/04 17:13:34 INFO DAGScheduler: Job 180 finished: start at NativeMethodAccessorImpl.java:0, took 0.564260 s
26/01/04 17:13:34 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 88, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@6698961e] is committing.
26/01/04 17:13:34 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 88, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@6698961e] committed.
26/01/04 17:13:34 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/88 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.88.bfae536d-9902-4f8b-934b-e3e5c2fc52b7.tmp
26/01/04 17:13:34 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.88.bfae536d-9902-4f8b-934b-e3e5c2fc52b7.tmp to file:/tmp/spark-checkpoint-enrichment/commits/88
26/01/04 17:13:34 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T17:13:33.686Z",
  "batchId" : 88,
  "numInputRows" : 30,
  "inputRowsPerSecond" : 2500.0,
  "processedRowsPerSecond" : 35.587188612099645,
  "durationMs" : {
    "addBatch" : 675,
    "commitOffsets" : 59,
    "getBatch" : 0,
    "latestOffset" : 2,
    "queryPlanning" : 19,
    "triggerExecution" : 843,
    "walCommit" : 87
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "0" : 1899
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "0" : 1929
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "0" : 1929
      }
    },
    "numInputRows" : 30,
    "inputRowsPerSecond" : 2500.0,
    "processedRowsPerSecond" : 35.587188612099645,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 31
  }
}
26/01/04 17:13:36 INFO BlockManagerInfo: Removed broadcast_269_piece0 on spark-master:33535 in memory (size: 20.2 KiB, free: 434.3 MiB)
26/01/04 17:13:36 INFO BlockManagerInfo: Removed broadcast_269_piece0 on 172.18.0.13:44401 in memory (size: 20.2 KiB, free: 434.3 MiB)
26/01/04 17:13:36 INFO BlockManagerInfo: Removed broadcast_267_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:13:36 INFO BlockManagerInfo: Removed broadcast_267_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:13:36 INFO BlockManagerInfo: Removed broadcast_267_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:13:36 INFO BlockManagerInfo: Removed broadcast_265_piece0 on spark-master:33535 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:13:36 INFO BlockManagerInfo: Removed broadcast_265_piece0 on 172.18.0.13:44401 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:13:36 INFO BlockManagerInfo: Removed broadcast_266_piece0 on spark-master:33535 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:13:36 INFO BlockManagerInfo: Removed broadcast_266_piece0 on 172.18.0.13:44401 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:13:36 INFO BlockManagerInfo: Removed broadcast_264_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:13:36 INFO BlockManagerInfo: Removed broadcast_264_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:13:36 INFO BlockManagerInfo: Removed broadcast_264_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:13:44 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/01/04 17:13:44 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/89 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.89.8fcbd3a5-89f2-4ca1-b083-afdc9cd2ac66.tmp
26/01/04 17:13:44 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.89.8fcbd3a5-89f2-4ca1-b083-afdc9cd2ac66.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/89
26/01/04 17:13:44 INFO MicroBatchExecution: Committed offsets for batch 89. Metadata OffsetSeqMetadata(0,1767546824699,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 17:13:44 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:13:44 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:13:44 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:13:44 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:13:44 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:13:44 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:13:44 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#73039 - airline_prefix.nullCount#73038) > 0)
26/01/04 17:13:44 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#73074 - min_flight_num.nullCount#73073) > 0)
26/01/04 17:13:44 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#73069 - max_flight_num.nullCount#73068) > 0)
26/01/04 17:13:44 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:13:44 INFO DAGScheduler: Got job 181 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 17:13:44 INFO DAGScheduler: Final stage: ResultStage 182 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:13:44 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:13:44 INFO DAGScheduler: Missing parents: List()
26/01/04 17:13:44 INFO DAGScheduler: Submitting ResultStage 182 (MapPartitionsRDD[912] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:13:44 INFO MemoryStore: Block broadcast_270 stored as values in memory (estimated size 38.5 KiB, free 434.4 MiB)
26/01/04 17:13:44 INFO BlockManagerInfo: Removed broadcast_268_piece0 on spark-master:33535 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:13:44 INFO BlockManagerInfo: Removed broadcast_268_piece0 on 172.18.0.13:44401 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:13:44 INFO MemoryStore: Block broadcast_270_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.3 MiB)
26/01/04 17:13:44 INFO BlockManagerInfo: Added broadcast_270_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:13:44 INFO SparkContext: Created broadcast 270 from broadcast at DAGScheduler.scala:1585
26/01/04 17:13:44 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 182 (MapPartitionsRDD[912] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 17:13:44 INFO TaskSchedulerImpl: Adding task set 182.0 with 2 tasks resource profile 0
26/01/04 17:13:44 INFO TaskSetManager: Starting task 1.0 in stage 182.0 (TID 308) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 17:13:44 INFO TaskSetManager: Starting task 0.0 in stage 182.0 (TID 309) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 17:13:44 INFO BlockManagerInfo: Added broadcast_270_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:13:44 INFO BlockManagerInfo: Added broadcast_270_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:13:44 INFO TaskSetManager: Finished task 1.0 in stage 182.0 (TID 308) in 32 ms on 172.18.0.13 (executor 0) (1/2)
26/01/04 17:13:45 INFO TaskSetManager: Finished task 0.0 in stage 182.0 (TID 309) in 79 ms on 172.18.0.14 (executor 1) (2/2)
26/01/04 17:13:45 INFO TaskSchedulerImpl: Removed TaskSet 182.0, whose tasks have all completed, from pool 
26/01/04 17:13:45 INFO DAGScheduler: ResultStage 182 (start at NativeMethodAccessorImpl.java:0) finished in 0.104 s
26/01/04 17:13:45 INFO DAGScheduler: Job 181 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:13:45 INFO TaskSchedulerImpl: Killing all running tasks in stage 182: Stage finished
26/01/04 17:13:45 INFO DAGScheduler: Job 181 finished: start at NativeMethodAccessorImpl.java:0, took 0.107057 s
26/01/04 17:13:45 INFO MemoryStore: Block broadcast_271_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.3 MiB)
26/01/04 17:13:45 INFO BlockManagerInfo: Added broadcast_271_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:13:45 INFO SparkContext: Created broadcast 271 from start at NativeMethodAccessorImpl.java:0
26/01/04 17:13:45 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 89, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@2603c9e4]. The input RDD has 1 partitions.
26/01/04 17:13:45 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:13:45 INFO DAGScheduler: Got job 182 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/01/04 17:13:45 INFO DAGScheduler: Final stage: ResultStage 183 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:13:45 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:13:45 INFO DAGScheduler: Missing parents: List()
26/01/04 17:13:45 INFO DAGScheduler: Submitting ResultStage 183 (MapPartitionsRDD[917] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:13:45 INFO MemoryStore: Block broadcast_272 stored as values in memory (estimated size 52.9 KiB, free 434.3 MiB)
26/01/04 17:13:45 INFO MemoryStore: Block broadcast_272_piece0 stored as bytes in memory (estimated size 20.2 KiB, free 434.3 MiB)
26/01/04 17:13:45 INFO BlockManagerInfo: Added broadcast_272_piece0 in memory on spark-master:33535 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:13:45 INFO SparkContext: Created broadcast 272 from broadcast at DAGScheduler.scala:1585
26/01/04 17:13:45 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 183 (MapPartitionsRDD[917] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/01/04 17:13:45 INFO TaskSchedulerImpl: Adding task set 183.0 with 1 tasks resource profile 0
26/01/04 17:13:45 INFO TaskSetManager: Starting task 0.0 in stage 183.0 (TID 310) (172.18.0.13, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:13:45 INFO BlockManagerInfo: Added broadcast_272_piece0 in memory on 172.18.0.13:44401 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:13:45 INFO BlockManagerInfo: Added broadcast_271_piece0 in memory on 172.18.0.13:44401 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:13:45 INFO TaskSetManager: Finished task 0.0 in stage 183.0 (TID 310) in 564 ms on 172.18.0.13 (executor 0) (1/1)
26/01/04 17:13:45 INFO TaskSchedulerImpl: Removed TaskSet 183.0, whose tasks have all completed, from pool 
26/01/04 17:13:45 INFO DAGScheduler: ResultStage 183 (start at NativeMethodAccessorImpl.java:0) finished in 0.572 s
26/01/04 17:13:45 INFO DAGScheduler: Job 182 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:13:45 INFO TaskSchedulerImpl: Killing all running tasks in stage 183: Stage finished
26/01/04 17:13:45 INFO DAGScheduler: Job 182 finished: start at NativeMethodAccessorImpl.java:0, took 0.573938 s
26/01/04 17:13:45 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 89, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@2603c9e4] is committing.
26/01/04 17:13:45 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 89, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@2603c9e4] committed.
26/01/04 17:13:45 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/89 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.89.ee1b1be4-90dc-4e61-9c01-2f8eca6ef7b2.tmp
26/01/04 17:13:45 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.89.ee1b1be4-90dc-4e61-9c01-2f8eca6ef7b2.tmp to file:/tmp/spark-checkpoint-enrichment/commits/89
26/01/04 17:13:45 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T17:13:44.698Z",
  "batchId" : 89,
  "numInputRows" : 30,
  "inputRowsPerSecond" : 2500.0,
  "processedRowsPerSecond" : 29.821073558648113,
  "durationMs" : {
    "addBatch" : 788,
    "commitOffsets" : 75,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 45,
    "triggerExecution" : 1006,
    "walCommit" : 97
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "0" : 1929
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "0" : 1959
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "0" : 1959
      }
    },
    "numInputRows" : 30,
    "inputRowsPerSecond" : 2500.0,
    "processedRowsPerSecond" : 29.821073558648113,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 31
  }
}
26/01/04 17:13:55 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/01/04 17:13:55 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/90 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.90.f1694464-17dd-4eee-9fd8-1849f75bdb84.tmp
26/01/04 17:13:55 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.90.f1694464-17dd-4eee-9fd8-1849f75bdb84.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/90
26/01/04 17:13:55 INFO MicroBatchExecution: Committed offsets for batch 90. Metadata OffsetSeqMetadata(0,1767546835717,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 17:13:55 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:13:55 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:13:55 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:13:55 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:13:55 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:13:55 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:13:55 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#73843 - airline_prefix.nullCount#73842) > 0)
26/01/04 17:13:55 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#73878 - min_flight_num.nullCount#73877) > 0)
26/01/04 17:13:55 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#73873 - max_flight_num.nullCount#73872) > 0)
26/01/04 17:13:55 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:13:55 INFO DAGScheduler: Got job 183 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 17:13:55 INFO DAGScheduler: Final stage: ResultStage 184 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:13:55 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:13:55 INFO DAGScheduler: Missing parents: List()
26/01/04 17:13:55 INFO DAGScheduler: Submitting ResultStage 184 (MapPartitionsRDD[922] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:13:55 INFO MemoryStore: Block broadcast_273 stored as values in memory (estimated size 38.5 KiB, free 434.2 MiB)
26/01/04 17:13:55 INFO MemoryStore: Block broadcast_273_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.2 MiB)
26/01/04 17:13:55 INFO BlockManagerInfo: Added broadcast_273_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:13:55 INFO SparkContext: Created broadcast 273 from broadcast at DAGScheduler.scala:1585
26/01/04 17:13:55 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 184 (MapPartitionsRDD[922] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 17:13:55 INFO TaskSchedulerImpl: Adding task set 184.0 with 2 tasks resource profile 0
26/01/04 17:13:55 INFO TaskSetManager: Starting task 1.0 in stage 184.0 (TID 311) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 17:13:55 INFO TaskSetManager: Starting task 0.0 in stage 184.0 (TID 312) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 17:13:55 INFO BlockManagerInfo: Added broadcast_273_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:13:55 INFO BlockManagerInfo: Added broadcast_273_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:13:55 INFO TaskSetManager: Finished task 1.0 in stage 184.0 (TID 311) in 16 ms on 172.18.0.13 (executor 0) (1/2)
26/01/04 17:13:55 INFO TaskSetManager: Finished task 0.0 in stage 184.0 (TID 312) in 28 ms on 172.18.0.14 (executor 1) (2/2)
26/01/04 17:13:55 INFO TaskSchedulerImpl: Removed TaskSet 184.0, whose tasks have all completed, from pool 
26/01/04 17:13:55 INFO DAGScheduler: ResultStage 184 (start at NativeMethodAccessorImpl.java:0) finished in 0.033 s
26/01/04 17:13:55 INFO DAGScheduler: Job 183 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:13:55 INFO TaskSchedulerImpl: Killing all running tasks in stage 184: Stage finished
26/01/04 17:13:55 INFO DAGScheduler: Job 183 finished: start at NativeMethodAccessorImpl.java:0, took 0.035020 s
26/01/04 17:13:55 INFO MemoryStore: Block broadcast_274_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.2 MiB)
26/01/04 17:13:55 INFO BlockManagerInfo: Added broadcast_274_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:13:55 INFO SparkContext: Created broadcast 274 from start at NativeMethodAccessorImpl.java:0
26/01/04 17:13:55 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 90, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@41a8fa2b]. The input RDD has 1 partitions.
26/01/04 17:13:55 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:13:55 INFO DAGScheduler: Got job 184 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/01/04 17:13:55 INFO DAGScheduler: Final stage: ResultStage 185 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:13:55 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:13:55 INFO DAGScheduler: Missing parents: List()
26/01/04 17:13:55 INFO DAGScheduler: Submitting ResultStage 185 (MapPartitionsRDD[927] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:13:55 INFO MemoryStore: Block broadcast_275 stored as values in memory (estimated size 52.9 KiB, free 434.2 MiB)
26/01/04 17:13:55 INFO MemoryStore: Block broadcast_275_piece0 stored as bytes in memory (estimated size 20.2 KiB, free 434.1 MiB)
26/01/04 17:13:55 INFO BlockManagerInfo: Added broadcast_275_piece0 in memory on spark-master:33535 (size: 20.2 KiB, free: 434.3 MiB)
26/01/04 17:13:55 INFO SparkContext: Created broadcast 275 from broadcast at DAGScheduler.scala:1585
26/01/04 17:13:55 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 185 (MapPartitionsRDD[927] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/01/04 17:13:55 INFO TaskSchedulerImpl: Adding task set 185.0 with 1 tasks resource profile 0
26/01/04 17:13:55 INFO TaskSetManager: Starting task 0.0 in stage 185.0 (TID 313) (172.18.0.13, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:13:55 INFO BlockManagerInfo: Added broadcast_275_piece0 in memory on 172.18.0.13:44401 (size: 20.2 KiB, free: 434.3 MiB)
26/01/04 17:13:55 INFO BlockManagerInfo: Added broadcast_274_piece0 in memory on 172.18.0.13:44401 (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:13:56 INFO TaskSetManager: Finished task 0.0 in stage 185.0 (TID 313) in 549 ms on 172.18.0.13 (executor 0) (1/1)
26/01/04 17:13:56 INFO TaskSchedulerImpl: Removed TaskSet 185.0, whose tasks have all completed, from pool 
26/01/04 17:13:56 INFO DAGScheduler: ResultStage 185 (start at NativeMethodAccessorImpl.java:0) finished in 0.554 s
26/01/04 17:13:56 INFO DAGScheduler: Job 184 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:13:56 INFO TaskSchedulerImpl: Killing all running tasks in stage 185: Stage finished
26/01/04 17:13:56 INFO DAGScheduler: Job 184 finished: start at NativeMethodAccessorImpl.java:0, took 0.556189 s
26/01/04 17:13:56 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 90, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@41a8fa2b] is committing.
26/01/04 17:13:56 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 90, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@41a8fa2b] committed.
26/01/04 17:13:56 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/90 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.90.aa1aea21-b622-490f-ad4d-5c530218f2e3.tmp
26/01/04 17:13:56 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.90.aa1aea21-b622-490f-ad4d-5c530218f2e3.tmp to file:/tmp/spark-checkpoint-enrichment/commits/90
26/01/04 17:13:56 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T17:13:55.716Z",
  "batchId" : 90,
  "numInputRows" : 30,
  "inputRowsPerSecond" : 2500.0,
  "processedRowsPerSecond" : 35.046728971962615,
  "durationMs" : {
    "addBatch" : 653,
    "commitOffsets" : 67,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 27,
    "triggerExecution" : 856,
    "walCommit" : 107
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "0" : 1959
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "0" : 1989
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "0" : 1989
      }
    },
    "numInputRows" : 30,
    "inputRowsPerSecond" : 2500.0,
    "processedRowsPerSecond" : 35.046728971962615,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 31
  }
}
26/01/04 17:13:58 INFO BlockManagerInfo: Removed broadcast_270_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:13:58 INFO BlockManagerInfo: Removed broadcast_270_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:13:58 INFO BlockManagerInfo: Removed broadcast_270_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:13:58 INFO BlockManagerInfo: Removed broadcast_272_piece0 on spark-master:33535 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:13:58 INFO BlockManagerInfo: Removed broadcast_272_piece0 on 172.18.0.13:44401 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:13:58 INFO BlockManagerInfo: Removed broadcast_273_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:13:58 INFO BlockManagerInfo: Removed broadcast_273_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:13:58 INFO BlockManagerInfo: Removed broadcast_273_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:13:58 INFO BlockManagerInfo: Removed broadcast_275_piece0 on spark-master:33535 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:13:58 INFO BlockManagerInfo: Removed broadcast_275_piece0 on 172.18.0.13:44401 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:13:58 INFO BlockManagerInfo: Removed broadcast_271_piece0 on spark-master:33535 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:13:58 INFO BlockManagerInfo: Removed broadcast_271_piece0 on 172.18.0.13:44401 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:14:06 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/01/04 17:14:06 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/91 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.91.34c1304d-fa1f-4851-83a9-142398aee84b.tmp
26/01/04 17:14:06 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.91.34c1304d-fa1f-4851-83a9-142398aee84b.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/91
26/01/04 17:14:06 INFO MicroBatchExecution: Committed offsets for batch 91. Metadata OffsetSeqMetadata(0,1767546846730,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 17:14:06 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:14:06 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:14:06 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:14:06 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:14:06 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:14:06 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:14:06 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#74647 - airline_prefix.nullCount#74646) > 0)
26/01/04 17:14:06 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#74682 - min_flight_num.nullCount#74681) > 0)
26/01/04 17:14:06 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#74677 - max_flight_num.nullCount#74676) > 0)
26/01/04 17:14:06 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:14:06 INFO DAGScheduler: Got job 185 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 17:14:06 INFO DAGScheduler: Final stage: ResultStage 186 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:14:06 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:14:06 INFO DAGScheduler: Missing parents: List()
26/01/04 17:14:06 INFO DAGScheduler: Submitting ResultStage 186 (MapPartitionsRDD[932] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:14:06 INFO MemoryStore: Block broadcast_276 stored as values in memory (estimated size 38.5 KiB, free 434.4 MiB)
26/01/04 17:14:06 INFO MemoryStore: Block broadcast_276_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.3 MiB)
26/01/04 17:14:06 INFO BlockManagerInfo: Removed broadcast_274_piece0 on spark-master:33535 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:14:06 INFO BlockManagerInfo: Added broadcast_276_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:14:06 INFO SparkContext: Created broadcast 276 from broadcast at DAGScheduler.scala:1585
26/01/04 17:14:06 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 186 (MapPartitionsRDD[932] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 17:14:06 INFO TaskSchedulerImpl: Adding task set 186.0 with 2 tasks resource profile 0
26/01/04 17:14:06 INFO BlockManagerInfo: Removed broadcast_274_piece0 on 172.18.0.13:44401 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:14:06 INFO TaskSetManager: Starting task 0.0 in stage 186.0 (TID 314) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 17:14:06 INFO TaskSetManager: Starting task 1.0 in stage 186.0 (TID 315) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 17:14:06 INFO BlockManagerInfo: Added broadcast_276_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:14:06 INFO BlockManagerInfo: Added broadcast_276_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:14:06 INFO TaskSetManager: Finished task 1.0 in stage 186.0 (TID 315) in 17 ms on 172.18.0.13 (executor 0) (1/2)
26/01/04 17:14:06 INFO TaskSetManager: Finished task 0.0 in stage 186.0 (TID 314) in 43 ms on 172.18.0.14 (executor 1) (2/2)
26/01/04 17:14:06 INFO TaskSchedulerImpl: Removed TaskSet 186.0, whose tasks have all completed, from pool 
26/01/04 17:14:06 INFO DAGScheduler: ResultStage 186 (start at NativeMethodAccessorImpl.java:0) finished in 0.054 s
26/01/04 17:14:06 INFO DAGScheduler: Job 185 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:14:06 INFO TaskSchedulerImpl: Killing all running tasks in stage 186: Stage finished
26/01/04 17:14:06 INFO DAGScheduler: Job 185 finished: start at NativeMethodAccessorImpl.java:0, took 0.056589 s
26/01/04 17:14:06 INFO MemoryStore: Block broadcast_277_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.3 MiB)
26/01/04 17:14:06 INFO BlockManagerInfo: Added broadcast_277_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:14:06 INFO SparkContext: Created broadcast 277 from start at NativeMethodAccessorImpl.java:0
26/01/04 17:14:06 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 91, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@2103704b]. The input RDD has 1 partitions.
26/01/04 17:14:06 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:14:06 INFO DAGScheduler: Got job 186 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/01/04 17:14:06 INFO DAGScheduler: Final stage: ResultStage 187 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:14:06 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:14:06 INFO DAGScheduler: Missing parents: List()
26/01/04 17:14:06 INFO DAGScheduler: Submitting ResultStage 187 (MapPartitionsRDD[937] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:14:06 INFO MemoryStore: Block broadcast_278 stored as values in memory (estimated size 52.9 KiB, free 434.3 MiB)
26/01/04 17:14:06 INFO MemoryStore: Block broadcast_278_piece0 stored as bytes in memory (estimated size 20.2 KiB, free 434.3 MiB)
26/01/04 17:14:06 INFO BlockManagerInfo: Added broadcast_278_piece0 in memory on spark-master:33535 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:14:06 INFO SparkContext: Created broadcast 278 from broadcast at DAGScheduler.scala:1585
26/01/04 17:14:06 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 187 (MapPartitionsRDD[937] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/01/04 17:14:06 INFO TaskSchedulerImpl: Adding task set 187.0 with 1 tasks resource profile 0
26/01/04 17:14:06 INFO TaskSetManager: Starting task 0.0 in stage 187.0 (TID 316) (172.18.0.13, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:14:06 INFO BlockManagerInfo: Added broadcast_278_piece0 in memory on 172.18.0.13:44401 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:14:06 INFO BlockManagerInfo: Added broadcast_277_piece0 in memory on 172.18.0.13:44401 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:14:07 INFO TaskSetManager: Finished task 0.0 in stage 187.0 (TID 316) in 534 ms on 172.18.0.13 (executor 0) (1/1)
26/01/04 17:14:07 INFO TaskSchedulerImpl: Removed TaskSet 187.0, whose tasks have all completed, from pool 
26/01/04 17:14:07 INFO DAGScheduler: ResultStage 187 (start at NativeMethodAccessorImpl.java:0) finished in 0.539 s
26/01/04 17:14:07 INFO DAGScheduler: Job 186 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:14:07 INFO TaskSchedulerImpl: Killing all running tasks in stage 187: Stage finished
26/01/04 17:14:07 INFO DAGScheduler: Job 186 finished: start at NativeMethodAccessorImpl.java:0, took 0.539468 s
26/01/04 17:14:07 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 91, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@2103704b] is committing.
26/01/04 17:14:07 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 91, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@2103704b] committed.
26/01/04 17:14:07 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/91 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.91.607d3fdb-853e-4b44-a3ac-78455521bc2c.tmp
26/01/04 17:14:07 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.91.607d3fdb-853e-4b44-a3ac-78455521bc2c.tmp to file:/tmp/spark-checkpoint-enrichment/commits/91
26/01/04 17:14:07 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T17:14:06.728Z",
  "batchId" : 91,
  "numInputRows" : 30,
  "inputRowsPerSecond" : 2500.0,
  "processedRowsPerSecond" : 36.014405762304925,
  "durationMs" : {
    "addBatch" : 671,
    "commitOffsets" : 66,
    "getBatch" : 0,
    "latestOffset" : 2,
    "queryPlanning" : 35,
    "triggerExecution" : 833,
    "walCommit" : 57
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "0" : 1989
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "0" : 2019
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "0" : 2019
      }
    },
    "numInputRows" : 30,
    "inputRowsPerSecond" : 2500.0,
    "processedRowsPerSecond" : 36.014405762304925,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 31
  }
}
26/01/04 17:14:17 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/01/04 17:14:17 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/92 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.92.17151712-db70-47fc-aab1-8ddfc5aaa6d0.tmp
26/01/04 17:14:17 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.92.17151712-db70-47fc-aab1-8ddfc5aaa6d0.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/92
26/01/04 17:14:17 INFO MicroBatchExecution: Committed offsets for batch 92. Metadata OffsetSeqMetadata(0,1767546857747,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 17:14:17 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:14:17 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:14:17 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:14:17 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:14:17 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:14:17 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:14:17 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#75451 - airline_prefix.nullCount#75450) > 0)
26/01/04 17:14:17 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#75486 - min_flight_num.nullCount#75485) > 0)
26/01/04 17:14:17 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#75481 - max_flight_num.nullCount#75480) > 0)
26/01/04 17:14:17 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:14:17 INFO DAGScheduler: Got job 187 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 17:14:17 INFO DAGScheduler: Final stage: ResultStage 188 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:14:17 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:14:17 INFO DAGScheduler: Missing parents: List()
26/01/04 17:14:17 INFO DAGScheduler: Submitting ResultStage 188 (MapPartitionsRDD[942] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:14:17 INFO MemoryStore: Block broadcast_279 stored as values in memory (estimated size 38.5 KiB, free 434.2 MiB)
26/01/04 17:14:17 INFO MemoryStore: Block broadcast_279_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.2 MiB)
26/01/04 17:14:17 INFO BlockManagerInfo: Added broadcast_279_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:14:17 INFO SparkContext: Created broadcast 279 from broadcast at DAGScheduler.scala:1585
26/01/04 17:14:17 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 188 (MapPartitionsRDD[942] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 17:14:17 INFO TaskSchedulerImpl: Adding task set 188.0 with 2 tasks resource profile 0
26/01/04 17:14:17 INFO TaskSetManager: Starting task 1.0 in stage 188.0 (TID 317) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 17:14:17 INFO TaskSetManager: Starting task 0.0 in stage 188.0 (TID 318) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 17:14:17 INFO BlockManagerInfo: Added broadcast_279_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:14:17 INFO BlockManagerInfo: Added broadcast_279_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:14:17 INFO TaskSetManager: Finished task 1.0 in stage 188.0 (TID 317) in 23 ms on 172.18.0.13 (executor 0) (1/2)
26/01/04 17:14:17 INFO TaskSetManager: Finished task 0.0 in stage 188.0 (TID 318) in 39 ms on 172.18.0.14 (executor 1) (2/2)
26/01/04 17:14:17 INFO TaskSchedulerImpl: Removed TaskSet 188.0, whose tasks have all completed, from pool 
26/01/04 17:14:17 INFO DAGScheduler: ResultStage 188 (start at NativeMethodAccessorImpl.java:0) finished in 0.046 s
26/01/04 17:14:17 INFO DAGScheduler: Job 187 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:14:17 INFO TaskSchedulerImpl: Killing all running tasks in stage 188: Stage finished
26/01/04 17:14:17 INFO DAGScheduler: Job 187 finished: start at NativeMethodAccessorImpl.java:0, took 0.048045 s
26/01/04 17:14:17 INFO MemoryStore: Block broadcast_280_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.2 MiB)
26/01/04 17:14:17 INFO BlockManagerInfo: Added broadcast_280_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:14:17 INFO SparkContext: Created broadcast 280 from start at NativeMethodAccessorImpl.java:0
26/01/04 17:14:17 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 92, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@772d100f]. The input RDD has 1 partitions.
26/01/04 17:14:17 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:14:17 INFO DAGScheduler: Got job 188 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/01/04 17:14:17 INFO DAGScheduler: Final stage: ResultStage 189 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:14:17 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:14:17 INFO DAGScheduler: Missing parents: List()
26/01/04 17:14:17 INFO DAGScheduler: Submitting ResultStage 189 (MapPartitionsRDD[947] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:14:17 INFO MemoryStore: Block broadcast_281 stored as values in memory (estimated size 52.9 KiB, free 434.2 MiB)
26/01/04 17:14:17 INFO MemoryStore: Block broadcast_281_piece0 stored as bytes in memory (estimated size 20.2 KiB, free 434.1 MiB)
26/01/04 17:14:17 INFO BlockManagerInfo: Added broadcast_281_piece0 in memory on spark-master:33535 (size: 20.2 KiB, free: 434.3 MiB)
26/01/04 17:14:17 INFO SparkContext: Created broadcast 281 from broadcast at DAGScheduler.scala:1585
26/01/04 17:14:17 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 189 (MapPartitionsRDD[947] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/01/04 17:14:17 INFO TaskSchedulerImpl: Adding task set 189.0 with 1 tasks resource profile 0
26/01/04 17:14:17 INFO TaskSetManager: Starting task 0.0 in stage 189.0 (TID 319) (172.18.0.13, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:14:17 INFO BlockManagerInfo: Added broadcast_281_piece0 in memory on 172.18.0.13:44401 (size: 20.2 KiB, free: 434.3 MiB)
26/01/04 17:14:17 INFO BlockManagerInfo: Added broadcast_280_piece0 in memory on 172.18.0.13:44401 (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:14:18 INFO TaskSetManager: Finished task 0.0 in stage 189.0 (TID 319) in 573 ms on 172.18.0.13 (executor 0) (1/1)
26/01/04 17:14:18 INFO TaskSchedulerImpl: Removed TaskSet 189.0, whose tasks have all completed, from pool 
26/01/04 17:14:18 INFO DAGScheduler: ResultStage 189 (start at NativeMethodAccessorImpl.java:0) finished in 0.579 s
26/01/04 17:14:18 INFO DAGScheduler: Job 188 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:14:18 INFO TaskSchedulerImpl: Killing all running tasks in stage 189: Stage finished
26/01/04 17:14:18 INFO DAGScheduler: Job 188 finished: start at NativeMethodAccessorImpl.java:0, took 0.581120 s
26/01/04 17:14:18 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 92, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@772d100f] is committing.
26/01/04 17:14:18 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 92, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@772d100f] committed.
26/01/04 17:14:18 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/92 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.92.d62411d9-112b-4d1c-858c-ceb39055b3a6.tmp
26/01/04 17:14:18 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.92.d62411d9-112b-4d1c-858c-ceb39055b3a6.tmp to file:/tmp/spark-checkpoint-enrichment/commits/92
26/01/04 17:14:18 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T17:14:17.746Z",
  "batchId" : 92,
  "numInputRows" : 30,
  "inputRowsPerSecond" : 2500.0,
  "processedRowsPerSecond" : 35.37735849056604,
  "durationMs" : {
    "addBatch" : 694,
    "commitOffsets" : 66,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 18,
    "triggerExecution" : 848,
    "walCommit" : 66
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "0" : 2019
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "0" : 2049
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "0" : 2049
      }
    },
    "numInputRows" : 30,
    "inputRowsPerSecond" : 2500.0,
    "processedRowsPerSecond" : 35.37735849056604,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 31
  }
}
26/01/04 17:14:20 INFO BlockManagerInfo: Removed broadcast_277_piece0 on spark-master:33535 in memory (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:14:20 INFO BlockManagerInfo: Removed broadcast_277_piece0 on 172.18.0.13:44401 in memory (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:14:20 INFO BlockManagerInfo: Removed broadcast_278_piece0 on spark-master:33535 in memory (size: 20.2 KiB, free: 434.3 MiB)
26/01/04 17:14:20 INFO BlockManagerInfo: Removed broadcast_278_piece0 on 172.18.0.13:44401 in memory (size: 20.2 KiB, free: 434.3 MiB)
26/01/04 17:14:20 INFO BlockManagerInfo: Removed broadcast_279_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:14:20 INFO BlockManagerInfo: Removed broadcast_279_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:14:20 INFO BlockManagerInfo: Removed broadcast_279_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:14:20 INFO BlockManagerInfo: Removed broadcast_281_piece0 on spark-master:33535 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:14:20 INFO BlockManagerInfo: Removed broadcast_281_piece0 on 172.18.0.13:44401 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:14:20 INFO BlockManagerInfo: Removed broadcast_276_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:14:20 INFO BlockManagerInfo: Removed broadcast_276_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:14:20 INFO BlockManagerInfo: Removed broadcast_276_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:14:28 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/01/04 17:14:28 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/93 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.93.9eed840a-772b-446d-b690-a51b75315ed9.tmp
26/01/04 17:14:28 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.93.9eed840a-772b-446d-b690-a51b75315ed9.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/93
26/01/04 17:14:28 INFO MicroBatchExecution: Committed offsets for batch 93. Metadata OffsetSeqMetadata(0,1767546868773,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 17:14:28 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:14:28 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:14:28 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:14:28 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:14:28 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:14:28 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:14:28 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#76255 - airline_prefix.nullCount#76254) > 0)
26/01/04 17:14:28 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#76290 - min_flight_num.nullCount#76289) > 0)
26/01/04 17:14:28 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#76285 - max_flight_num.nullCount#76284) > 0)
26/01/04 17:14:28 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:14:28 INFO DAGScheduler: Got job 189 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 17:14:28 INFO DAGScheduler: Final stage: ResultStage 190 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:14:28 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:14:28 INFO DAGScheduler: Missing parents: List()
26/01/04 17:14:28 INFO DAGScheduler: Submitting ResultStage 190 (MapPartitionsRDD[952] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:14:28 INFO MemoryStore: Block broadcast_282 stored as values in memory (estimated size 38.5 KiB, free 434.4 MiB)
26/01/04 17:14:28 INFO MemoryStore: Block broadcast_282_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.3 MiB)
26/01/04 17:14:28 INFO BlockManagerInfo: Removed broadcast_280_piece0 on spark-master:33535 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:14:28 INFO BlockManagerInfo: Added broadcast_282_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:14:28 INFO SparkContext: Created broadcast 282 from broadcast at DAGScheduler.scala:1585
26/01/04 17:14:28 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 190 (MapPartitionsRDD[952] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 17:14:28 INFO TaskSchedulerImpl: Adding task set 190.0 with 2 tasks resource profile 0
26/01/04 17:14:28 INFO BlockManagerInfo: Removed broadcast_280_piece0 on 172.18.0.13:44401 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:14:28 INFO TaskSetManager: Starting task 0.0 in stage 190.0 (TID 320) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 17:14:28 INFO TaskSetManager: Starting task 1.0 in stage 190.0 (TID 321) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 17:14:28 INFO BlockManagerInfo: Added broadcast_282_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:14:28 INFO BlockManagerInfo: Added broadcast_282_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:14:28 INFO TaskSetManager: Finished task 1.0 in stage 190.0 (TID 321) in 26 ms on 172.18.0.13 (executor 0) (1/2)
26/01/04 17:14:29 INFO TaskSetManager: Finished task 0.0 in stage 190.0 (TID 320) in 52 ms on 172.18.0.14 (executor 1) (2/2)
26/01/04 17:14:29 INFO TaskSchedulerImpl: Removed TaskSet 190.0, whose tasks have all completed, from pool 
26/01/04 17:14:29 INFO DAGScheduler: ResultStage 190 (start at NativeMethodAccessorImpl.java:0) finished in 0.064 s
26/01/04 17:14:29 INFO DAGScheduler: Job 189 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:14:29 INFO TaskSchedulerImpl: Killing all running tasks in stage 190: Stage finished
26/01/04 17:14:29 INFO DAGScheduler: Job 189 finished: start at NativeMethodAccessorImpl.java:0, took 0.067432 s
26/01/04 17:14:29 INFO MemoryStore: Block broadcast_283_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.3 MiB)
26/01/04 17:14:29 INFO BlockManagerInfo: Added broadcast_283_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:14:29 INFO SparkContext: Created broadcast 283 from start at NativeMethodAccessorImpl.java:0
26/01/04 17:14:29 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 93, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@641a10fc]. The input RDD has 1 partitions.
26/01/04 17:14:29 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:14:29 INFO DAGScheduler: Got job 190 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/01/04 17:14:29 INFO DAGScheduler: Final stage: ResultStage 191 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:14:29 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:14:29 INFO DAGScheduler: Missing parents: List()
26/01/04 17:14:29 INFO DAGScheduler: Submitting ResultStage 191 (MapPartitionsRDD[957] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:14:29 INFO MemoryStore: Block broadcast_284 stored as values in memory (estimated size 52.9 KiB, free 434.3 MiB)
26/01/04 17:14:29 INFO MemoryStore: Block broadcast_284_piece0 stored as bytes in memory (estimated size 20.2 KiB, free 434.3 MiB)
26/01/04 17:14:29 INFO BlockManagerInfo: Added broadcast_284_piece0 in memory on spark-master:33535 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:14:29 INFO SparkContext: Created broadcast 284 from broadcast at DAGScheduler.scala:1585
26/01/04 17:14:29 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 191 (MapPartitionsRDD[957] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/01/04 17:14:29 INFO TaskSchedulerImpl: Adding task set 191.0 with 1 tasks resource profile 0
26/01/04 17:14:29 INFO TaskSetManager: Starting task 0.0 in stage 191.0 (TID 322) (172.18.0.13, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:14:29 INFO BlockManagerInfo: Added broadcast_284_piece0 in memory on 172.18.0.13:44401 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:14:29 INFO BlockManagerInfo: Added broadcast_283_piece0 in memory on 172.18.0.13:44401 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:14:29 INFO TaskSetManager: Finished task 0.0 in stage 191.0 (TID 322) in 550 ms on 172.18.0.13 (executor 0) (1/1)
26/01/04 17:14:29 INFO TaskSchedulerImpl: Removed TaskSet 191.0, whose tasks have all completed, from pool 
26/01/04 17:14:29 INFO DAGScheduler: ResultStage 191 (start at NativeMethodAccessorImpl.java:0) finished in 0.555 s
26/01/04 17:14:29 INFO DAGScheduler: Job 190 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:14:29 INFO TaskSchedulerImpl: Killing all running tasks in stage 191: Stage finished
26/01/04 17:14:29 INFO DAGScheduler: Job 190 finished: start at NativeMethodAccessorImpl.java:0, took 0.557182 s
26/01/04 17:14:29 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 93, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@641a10fc] is committing.
26/01/04 17:14:29 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 93, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@641a10fc] committed.
26/01/04 17:14:29 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/93 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.93.426947e0-8ba7-4c7b-b793-66a92cc9eae0.tmp
26/01/04 17:14:29 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.93.426947e0-8ba7-4c7b-b793-66a92cc9eae0.tmp to file:/tmp/spark-checkpoint-enrichment/commits/93
26/01/04 17:14:29 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T17:14:28.771Z",
  "batchId" : 93,
  "numInputRows" : 30,
  "inputRowsPerSecond" : 2307.6923076923076,
  "processedRowsPerSecond" : 33.482142857142854,
  "durationMs" : {
    "addBatch" : 714,
    "commitOffsets" : 63,
    "getBatch" : 0,
    "latestOffset" : 2,
    "queryPlanning" : 28,
    "triggerExecution" : 896,
    "walCommit" : 89
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "0" : 2049
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "0" : 2079
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "0" : 2079
      }
    },
    "numInputRows" : 30,
    "inputRowsPerSecond" : 2307.6923076923076,
    "processedRowsPerSecond" : 33.482142857142854,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 31
  }
}
26/01/04 17:14:39 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/01/04 17:14:39 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/94 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.94.c177e12c-78f4-4378-8ae8-a705f6f3e610.tmp
26/01/04 17:14:39 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.94.c177e12c-78f4-4378-8ae8-a705f6f3e610.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/94
26/01/04 17:14:39 INFO MicroBatchExecution: Committed offsets for batch 94. Metadata OffsetSeqMetadata(0,1767546879780,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 17:14:39 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:14:39 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:14:39 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:14:39 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:14:39 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:14:39 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:14:39 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#77059 - airline_prefix.nullCount#77058) > 0)
26/01/04 17:14:39 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#77094 - min_flight_num.nullCount#77093) > 0)
26/01/04 17:14:39 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#77089 - max_flight_num.nullCount#77088) > 0)
26/01/04 17:14:39 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:14:39 INFO DAGScheduler: Got job 191 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 17:14:39 INFO DAGScheduler: Final stage: ResultStage 192 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:14:39 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:14:39 INFO DAGScheduler: Missing parents: List()
26/01/04 17:14:39 INFO DAGScheduler: Submitting ResultStage 192 (MapPartitionsRDD[962] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:14:39 INFO MemoryStore: Block broadcast_285 stored as values in memory (estimated size 38.5 KiB, free 434.2 MiB)
26/01/04 17:14:39 INFO MemoryStore: Block broadcast_285_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.2 MiB)
26/01/04 17:14:39 INFO BlockManagerInfo: Added broadcast_285_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:14:39 INFO SparkContext: Created broadcast 285 from broadcast at DAGScheduler.scala:1585
26/01/04 17:14:39 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 192 (MapPartitionsRDD[962] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 17:14:39 INFO TaskSchedulerImpl: Adding task set 192.0 with 2 tasks resource profile 0
26/01/04 17:14:39 INFO TaskSetManager: Starting task 1.0 in stage 192.0 (TID 323) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 17:14:39 INFO TaskSetManager: Starting task 0.0 in stage 192.0 (TID 324) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 17:14:39 INFO BlockManagerInfo: Added broadcast_285_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:14:39 INFO BlockManagerInfo: Added broadcast_285_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:14:39 INFO TaskSetManager: Finished task 1.0 in stage 192.0 (TID 323) in 19 ms on 172.18.0.13 (executor 0) (1/2)
26/01/04 17:14:39 INFO TaskSetManager: Finished task 0.0 in stage 192.0 (TID 324) in 41 ms on 172.18.0.14 (executor 1) (2/2)
26/01/04 17:14:39 INFO TaskSchedulerImpl: Removed TaskSet 192.0, whose tasks have all completed, from pool 
26/01/04 17:14:39 INFO DAGScheduler: ResultStage 192 (start at NativeMethodAccessorImpl.java:0) finished in 0.045 s
26/01/04 17:14:39 INFO DAGScheduler: Job 191 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:14:39 INFO TaskSchedulerImpl: Killing all running tasks in stage 192: Stage finished
26/01/04 17:14:39 INFO DAGScheduler: Job 191 finished: start at NativeMethodAccessorImpl.java:0, took 0.047415 s
26/01/04 17:14:39 INFO MemoryStore: Block broadcast_286_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.2 MiB)
26/01/04 17:14:39 INFO BlockManagerInfo: Added broadcast_286_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:14:39 INFO SparkContext: Created broadcast 286 from start at NativeMethodAccessorImpl.java:0
26/01/04 17:14:39 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 94, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@e31babe]. The input RDD has 1 partitions.
26/01/04 17:14:39 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:14:39 INFO DAGScheduler: Got job 192 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/01/04 17:14:39 INFO DAGScheduler: Final stage: ResultStage 193 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:14:39 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:14:39 INFO DAGScheduler: Missing parents: List()
26/01/04 17:14:39 INFO DAGScheduler: Submitting ResultStage 193 (MapPartitionsRDD[967] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:14:39 INFO MemoryStore: Block broadcast_287 stored as values in memory (estimated size 52.9 KiB, free 434.2 MiB)
26/01/04 17:14:39 INFO MemoryStore: Block broadcast_287_piece0 stored as bytes in memory (estimated size 20.2 KiB, free 434.1 MiB)
26/01/04 17:14:39 INFO BlockManagerInfo: Added broadcast_287_piece0 in memory on spark-master:33535 (size: 20.2 KiB, free: 434.3 MiB)
26/01/04 17:14:39 INFO SparkContext: Created broadcast 287 from broadcast at DAGScheduler.scala:1585
26/01/04 17:14:39 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 193 (MapPartitionsRDD[967] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/01/04 17:14:39 INFO TaskSchedulerImpl: Adding task set 193.0 with 1 tasks resource profile 0
26/01/04 17:14:39 INFO TaskSetManager: Starting task 0.0 in stage 193.0 (TID 325) (172.18.0.13, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:14:40 INFO BlockManagerInfo: Added broadcast_287_piece0 in memory on 172.18.0.13:44401 (size: 20.2 KiB, free: 434.3 MiB)
26/01/04 17:14:40 INFO BlockManagerInfo: Added broadcast_286_piece0 in memory on 172.18.0.13:44401 (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:14:40 INFO TaskSetManager: Finished task 0.0 in stage 193.0 (TID 325) in 550 ms on 172.18.0.13 (executor 0) (1/1)
26/01/04 17:14:40 INFO TaskSchedulerImpl: Removed TaskSet 193.0, whose tasks have all completed, from pool 
26/01/04 17:14:40 INFO DAGScheduler: ResultStage 193 (start at NativeMethodAccessorImpl.java:0) finished in 0.555 s
26/01/04 17:14:40 INFO DAGScheduler: Job 192 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:14:40 INFO TaskSchedulerImpl: Killing all running tasks in stage 193: Stage finished
26/01/04 17:14:40 INFO DAGScheduler: Job 192 finished: start at NativeMethodAccessorImpl.java:0, took 0.557583 s
26/01/04 17:14:40 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 94, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@e31babe] is committing.
26/01/04 17:14:40 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 94, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@e31babe] committed.
26/01/04 17:14:40 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/94 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.94.80d6f1ae-98ec-4128-a703-711d24e6ba54.tmp
26/01/04 17:14:40 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.94.80d6f1ae-98ec-4128-a703-711d24e6ba54.tmp to file:/tmp/spark-checkpoint-enrichment/commits/94
26/01/04 17:14:40 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T17:14:39.779Z",
  "batchId" : 94,
  "numInputRows" : 30,
  "inputRowsPerSecond" : 2500.0,
  "processedRowsPerSecond" : 33.59462486002239,
  "durationMs" : {
    "addBatch" : 675,
    "commitOffsets" : 123,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 20,
    "triggerExecution" : 893,
    "walCommit" : 72
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "0" : 2079
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "0" : 2109
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "0" : 2109
      }
    },
    "numInputRows" : 30,
    "inputRowsPerSecond" : 2500.0,
    "processedRowsPerSecond" : 33.59462486002239,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 31
  }
}
26/01/04 17:14:42 INFO BlockManagerInfo: Removed broadcast_283_piece0 on spark-master:33535 in memory (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:14:42 INFO BlockManagerInfo: Removed broadcast_283_piece0 on 172.18.0.13:44401 in memory (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:14:42 INFO BlockManagerInfo: Removed broadcast_287_piece0 on spark-master:33535 in memory (size: 20.2 KiB, free: 434.3 MiB)
26/01/04 17:14:42 INFO BlockManagerInfo: Removed broadcast_287_piece0 on 172.18.0.13:44401 in memory (size: 20.2 KiB, free: 434.3 MiB)
26/01/04 17:14:42 INFO BlockManagerInfo: Removed broadcast_285_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:14:42 INFO BlockManagerInfo: Removed broadcast_285_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:14:42 INFO BlockManagerInfo: Removed broadcast_285_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:14:42 INFO BlockManagerInfo: Removed broadcast_282_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:14:42 INFO BlockManagerInfo: Removed broadcast_282_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:14:42 INFO BlockManagerInfo: Removed broadcast_282_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:14:42 INFO BlockManagerInfo: Removed broadcast_284_piece0 on spark-master:33535 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:14:42 INFO BlockManagerInfo: Removed broadcast_284_piece0 on 172.18.0.13:44401 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:14:50 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/01/04 17:14:50 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/95 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.95.018dc590-f4a0-427a-8652-f0071c666b05.tmp
26/01/04 17:14:50 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.95.018dc590-f4a0-427a-8652-f0071c666b05.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/95
26/01/04 17:14:50 INFO MicroBatchExecution: Committed offsets for batch 95. Metadata OffsetSeqMetadata(0,1767546890796,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 17:14:50 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:14:50 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:14:50 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:14:50 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:14:51 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:14:51 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:14:51 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#77863 - airline_prefix.nullCount#77862) > 0)
26/01/04 17:14:51 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#77898 - min_flight_num.nullCount#77897) > 0)
26/01/04 17:14:51 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#77893 - max_flight_num.nullCount#77892) > 0)
26/01/04 17:14:51 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:14:51 INFO DAGScheduler: Got job 193 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 17:14:51 INFO DAGScheduler: Final stage: ResultStage 194 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:14:51 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:14:51 INFO DAGScheduler: Missing parents: List()
26/01/04 17:14:51 INFO DAGScheduler: Submitting ResultStage 194 (MapPartitionsRDD[972] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:14:51 INFO MemoryStore: Block broadcast_288 stored as values in memory (estimated size 38.5 KiB, free 434.4 MiB)
26/01/04 17:14:51 INFO MemoryStore: Block broadcast_288_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.3 MiB)
26/01/04 17:14:51 INFO BlockManagerInfo: Added broadcast_288_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:14:51 INFO SparkContext: Created broadcast 288 from broadcast at DAGScheduler.scala:1585
26/01/04 17:14:51 INFO BlockManagerInfo: Removed broadcast_286_piece0 on spark-master:33535 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:14:51 INFO BlockManagerInfo: Removed broadcast_286_piece0 on 172.18.0.13:44401 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:14:51 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 194 (MapPartitionsRDD[972] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 17:14:51 INFO TaskSchedulerImpl: Adding task set 194.0 with 2 tasks resource profile 0
26/01/04 17:14:51 INFO TaskSetManager: Starting task 0.0 in stage 194.0 (TID 326) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 17:14:51 INFO TaskSetManager: Starting task 1.0 in stage 194.0 (TID 327) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 17:14:51 INFO BlockManagerInfo: Added broadcast_288_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:14:51 INFO BlockManagerInfo: Added broadcast_288_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:14:51 INFO TaskSetManager: Finished task 1.0 in stage 194.0 (TID 327) in 119 ms on 172.18.0.13 (executor 0) (1/2)
26/01/04 17:14:51 INFO TaskSetManager: Finished task 0.0 in stage 194.0 (TID 326) in 143 ms on 172.18.0.14 (executor 1) (2/2)
26/01/04 17:14:51 INFO TaskSchedulerImpl: Removed TaskSet 194.0, whose tasks have all completed, from pool 
26/01/04 17:14:51 INFO DAGScheduler: ResultStage 194 (start at NativeMethodAccessorImpl.java:0) finished in 0.192 s
26/01/04 17:14:51 INFO DAGScheduler: Job 193 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:14:51 INFO TaskSchedulerImpl: Killing all running tasks in stage 194: Stage finished
26/01/04 17:14:51 INFO DAGScheduler: Job 193 finished: start at NativeMethodAccessorImpl.java:0, took 0.195596 s
26/01/04 17:14:51 INFO MemoryStore: Block broadcast_289_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.3 MiB)
26/01/04 17:14:51 INFO BlockManagerInfo: Added broadcast_289_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:14:51 INFO SparkContext: Created broadcast 289 from start at NativeMethodAccessorImpl.java:0
26/01/04 17:14:51 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 95, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@6600351b]. The input RDD has 1 partitions.
26/01/04 17:14:51 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:14:51 INFO DAGScheduler: Got job 194 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/01/04 17:14:51 INFO DAGScheduler: Final stage: ResultStage 195 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:14:51 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:14:51 INFO DAGScheduler: Missing parents: List()
26/01/04 17:14:51 INFO DAGScheduler: Submitting ResultStage 195 (MapPartitionsRDD[977] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:14:51 INFO MemoryStore: Block broadcast_290 stored as values in memory (estimated size 52.9 KiB, free 434.3 MiB)
26/01/04 17:14:51 INFO MemoryStore: Block broadcast_290_piece0 stored as bytes in memory (estimated size 20.2 KiB, free 434.3 MiB)
26/01/04 17:14:51 INFO BlockManagerInfo: Added broadcast_290_piece0 in memory on spark-master:33535 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:14:51 INFO SparkContext: Created broadcast 290 from broadcast at DAGScheduler.scala:1585
26/01/04 17:14:51 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 195 (MapPartitionsRDD[977] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/01/04 17:14:51 INFO TaskSchedulerImpl: Adding task set 195.0 with 1 tasks resource profile 0
26/01/04 17:14:51 INFO TaskSetManager: Starting task 0.0 in stage 195.0 (TID 328) (172.18.0.13, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:14:51 INFO BlockManagerInfo: Added broadcast_290_piece0 in memory on 172.18.0.13:44401 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:14:51 INFO BlockManagerInfo: Added broadcast_289_piece0 in memory on 172.18.0.13:44401 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:14:51 INFO TaskSetManager: Finished task 0.0 in stage 195.0 (TID 328) in 594 ms on 172.18.0.13 (executor 0) (1/1)
26/01/04 17:14:51 INFO TaskSchedulerImpl: Removed TaskSet 195.0, whose tasks have all completed, from pool 
26/01/04 17:14:51 INFO DAGScheduler: ResultStage 195 (start at NativeMethodAccessorImpl.java:0) finished in 0.603 s
26/01/04 17:14:51 INFO DAGScheduler: Job 194 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:14:51 INFO TaskSchedulerImpl: Killing all running tasks in stage 195: Stage finished
26/01/04 17:14:51 INFO DAGScheduler: Job 194 finished: start at NativeMethodAccessorImpl.java:0, took 0.609217 s
26/01/04 17:14:51 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 95, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@6600351b] is committing.
26/01/04 17:14:51 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 95, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@6600351b] committed.
26/01/04 17:14:51 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/95 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.95.4a56cab0-f043-48b0-95a4-7f9185c8a88e.tmp
26/01/04 17:14:51 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.95.4a56cab0-f043-48b0-95a4-7f9185c8a88e.tmp to file:/tmp/spark-checkpoint-enrichment/commits/95
26/01/04 17:14:51 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T17:14:50.794Z",
  "batchId" : 95,
  "numInputRows" : 30,
  "inputRowsPerSecond" : 2307.6923076923076,
  "processedRowsPerSecond" : 25.59726962457338,
  "durationMs" : {
    "addBatch" : 947,
    "commitOffsets" : 69,
    "getBatch" : 0,
    "latestOffset" : 2,
    "queryPlanning" : 44,
    "triggerExecution" : 1172,
    "walCommit" : 109
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "0" : 2109
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "0" : 2139
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "0" : 2139
      }
    },
    "numInputRows" : 30,
    "inputRowsPerSecond" : 2307.6923076923076,
    "processedRowsPerSecond" : 25.59726962457338,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 31
  }
}
26/01/04 17:15:01 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/96 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.96.05e642f7-dc22-4d73-bbfb-d0fb72f803ad.tmp
26/01/04 17:15:01 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.96.05e642f7-dc22-4d73-bbfb-d0fb72f803ad.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/96
26/01/04 17:15:01 INFO MicroBatchExecution: Committed offsets for batch 96. Metadata OffsetSeqMetadata(0,1767546901816,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 17:15:01 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:15:01 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:15:01 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:15:01 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:15:01 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:15:01 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:15:02 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#78667 - airline_prefix.nullCount#78666) > 0)
26/01/04 17:15:02 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#78702 - min_flight_num.nullCount#78701) > 0)
26/01/04 17:15:02 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#78697 - max_flight_num.nullCount#78696) > 0)
26/01/04 17:15:02 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:15:02 INFO DAGScheduler: Got job 195 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 17:15:02 INFO DAGScheduler: Final stage: ResultStage 196 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:15:02 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:15:02 INFO DAGScheduler: Missing parents: List()
26/01/04 17:15:02 INFO DAGScheduler: Submitting ResultStage 196 (MapPartitionsRDD[982] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:15:02 INFO MemoryStore: Block broadcast_291 stored as values in memory (estimated size 38.5 KiB, free 434.2 MiB)
26/01/04 17:15:02 INFO MemoryStore: Block broadcast_291_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.2 MiB)
26/01/04 17:15:02 INFO BlockManagerInfo: Added broadcast_291_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:15:02 INFO SparkContext: Created broadcast 291 from broadcast at DAGScheduler.scala:1585
26/01/04 17:15:02 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 196 (MapPartitionsRDD[982] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 17:15:02 INFO TaskSchedulerImpl: Adding task set 196.0 with 2 tasks resource profile 0
26/01/04 17:15:02 INFO TaskSetManager: Starting task 0.0 in stage 196.0 (TID 329) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 17:15:02 INFO TaskSetManager: Starting task 1.0 in stage 196.0 (TID 330) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 17:15:02 INFO BlockManagerInfo: Added broadcast_291_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:15:02 INFO BlockManagerInfo: Added broadcast_291_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:15:02 INFO TaskSetManager: Finished task 1.0 in stage 196.0 (TID 330) in 15 ms on 172.18.0.13 (executor 0) (1/2)
26/01/04 17:15:02 INFO TaskSetManager: Finished task 0.0 in stage 196.0 (TID 329) in 29 ms on 172.18.0.14 (executor 1) (2/2)
26/01/04 17:15:02 INFO TaskSchedulerImpl: Removed TaskSet 196.0, whose tasks have all completed, from pool 
26/01/04 17:15:02 INFO DAGScheduler: ResultStage 196 (start at NativeMethodAccessorImpl.java:0) finished in 0.034 s
26/01/04 17:15:02 INFO DAGScheduler: Job 195 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:15:02 INFO TaskSchedulerImpl: Killing all running tasks in stage 196: Stage finished
26/01/04 17:15:02 INFO DAGScheduler: Job 195 finished: start at NativeMethodAccessorImpl.java:0, took 0.035914 s
26/01/04 17:15:02 INFO MemoryStore: Block broadcast_292_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.2 MiB)
26/01/04 17:15:02 INFO BlockManagerInfo: Added broadcast_292_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:15:02 INFO SparkContext: Created broadcast 292 from start at NativeMethodAccessorImpl.java:0
26/01/04 17:15:02 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 96, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@4dbdd292]. The input RDD has 1 partitions.
26/01/04 17:15:02 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:15:02 INFO DAGScheduler: Got job 196 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/01/04 17:15:02 INFO DAGScheduler: Final stage: ResultStage 197 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:15:02 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:15:02 INFO DAGScheduler: Missing parents: List()
26/01/04 17:15:02 INFO DAGScheduler: Submitting ResultStage 197 (MapPartitionsRDD[987] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:15:02 INFO MemoryStore: Block broadcast_293 stored as values in memory (estimated size 52.9 KiB, free 434.2 MiB)
26/01/04 17:15:02 INFO MemoryStore: Block broadcast_293_piece0 stored as bytes in memory (estimated size 20.2 KiB, free 434.1 MiB)
26/01/04 17:15:02 INFO BlockManagerInfo: Added broadcast_293_piece0 in memory on spark-master:33535 (size: 20.2 KiB, free: 434.3 MiB)
26/01/04 17:15:02 INFO SparkContext: Created broadcast 293 from broadcast at DAGScheduler.scala:1585
26/01/04 17:15:02 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 197 (MapPartitionsRDD[987] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/01/04 17:15:02 INFO TaskSchedulerImpl: Adding task set 197.0 with 1 tasks resource profile 0
26/01/04 17:15:02 INFO TaskSetManager: Starting task 0.0 in stage 197.0 (TID 331) (172.18.0.13, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:15:02 INFO BlockManagerInfo: Added broadcast_293_piece0 in memory on 172.18.0.13:44401 (size: 20.2 KiB, free: 434.3 MiB)
26/01/04 17:15:02 INFO BlockManagerInfo: Added broadcast_292_piece0 in memory on 172.18.0.13:44401 (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:15:02 INFO TaskSetManager: Finished task 0.0 in stage 197.0 (TID 331) in 575 ms on 172.18.0.13 (executor 0) (1/1)
26/01/04 17:15:02 INFO TaskSchedulerImpl: Removed TaskSet 197.0, whose tasks have all completed, from pool 
26/01/04 17:15:02 INFO DAGScheduler: ResultStage 197 (start at NativeMethodAccessorImpl.java:0) finished in 0.581 s
26/01/04 17:15:02 INFO DAGScheduler: Job 196 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:15:02 INFO TaskSchedulerImpl: Killing all running tasks in stage 197: Stage finished
26/01/04 17:15:02 INFO DAGScheduler: Job 196 finished: start at NativeMethodAccessorImpl.java:0, took 0.583264 s
26/01/04 17:15:02 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 96, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@4dbdd292] is committing.
26/01/04 17:15:02 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 96, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@4dbdd292] committed.
26/01/04 17:15:02 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/96 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.96.107ca2f6-1e0e-406d-bbc8-ddac47b81473.tmp
26/01/04 17:15:02 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.96.107ca2f6-1e0e-406d-bbc8-ddac47b81473.tmp to file:/tmp/spark-checkpoint-enrichment/commits/96
26/01/04 17:15:02 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T17:15:01.815Z",
  "batchId" : 96,
  "numInputRows" : 30,
  "inputRowsPerSecond" : 2500.0,
  "processedRowsPerSecond" : 32.29278794402583,
  "durationMs" : {
    "addBatch" : 687,
    "commitOffsets" : 97,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 24,
    "triggerExecution" : 929,
    "walCommit" : 118
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "0" : 2139
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "0" : 2169
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "0" : 2169
      }
    },
    "numInputRows" : 30,
    "inputRowsPerSecond" : 2500.0,
    "processedRowsPerSecond" : 32.29278794402583,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 31
  }
}
26/01/04 17:15:04 INFO BlockManagerInfo: Removed broadcast_293_piece0 on spark-master:33535 in memory (size: 20.2 KiB, free: 434.3 MiB)
26/01/04 17:15:04 INFO BlockManagerInfo: Removed broadcast_293_piece0 on 172.18.0.13:44401 in memory (size: 20.2 KiB, free: 434.3 MiB)
26/01/04 17:15:04 INFO BlockManagerInfo: Removed broadcast_289_piece0 on spark-master:33535 in memory (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:15:04 INFO BlockManagerInfo: Removed broadcast_289_piece0 on 172.18.0.13:44401 in memory (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:15:04 INFO BlockManagerInfo: Removed broadcast_290_piece0 on spark-master:33535 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:15:04 INFO BlockManagerInfo: Removed broadcast_290_piece0 on 172.18.0.13:44401 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:15:04 INFO BlockManagerInfo: Removed broadcast_291_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:15:04 INFO BlockManagerInfo: Removed broadcast_291_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:15:04 INFO BlockManagerInfo: Removed broadcast_291_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:15:04 INFO BlockManagerInfo: Removed broadcast_288_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:15:04 INFO BlockManagerInfo: Removed broadcast_288_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:15:04 INFO BlockManagerInfo: Removed broadcast_288_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:15:12 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/01/04 17:15:12 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/97 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.97.02a77565-e507-4012-826d-6fe3a5af505c.tmp
26/01/04 17:15:12 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.97.02a77565-e507-4012-826d-6fe3a5af505c.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/97
26/01/04 17:15:12 INFO MicroBatchExecution: Committed offsets for batch 97. Metadata OffsetSeqMetadata(0,1767546912821,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 17:15:12 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:15:12 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:15:12 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:15:12 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:15:13 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:15:13 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:15:13 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#79471 - airline_prefix.nullCount#79470) > 0)
26/01/04 17:15:13 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#79506 - min_flight_num.nullCount#79505) > 0)
26/01/04 17:15:13 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#79501 - max_flight_num.nullCount#79500) > 0)
26/01/04 17:15:13 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:15:13 INFO DAGScheduler: Got job 197 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 17:15:13 INFO DAGScheduler: Final stage: ResultStage 198 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:15:13 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:15:13 INFO DAGScheduler: Missing parents: List()
26/01/04 17:15:13 INFO DAGScheduler: Submitting ResultStage 198 (MapPartitionsRDD[992] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:15:13 INFO MemoryStore: Block broadcast_294 stored as values in memory (estimated size 38.5 KiB, free 434.4 MiB)
26/01/04 17:15:13 INFO MemoryStore: Block broadcast_294_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.3 MiB)
26/01/04 17:15:13 INFO BlockManagerInfo: Added broadcast_294_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:15:13 INFO SparkContext: Created broadcast 294 from broadcast at DAGScheduler.scala:1585
26/01/04 17:15:13 INFO BlockManagerInfo: Removed broadcast_292_piece0 on spark-master:33535 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:15:13 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 198 (MapPartitionsRDD[992] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 17:15:13 INFO TaskSchedulerImpl: Adding task set 198.0 with 2 tasks resource profile 0
26/01/04 17:15:13 INFO BlockManagerInfo: Removed broadcast_292_piece0 on 172.18.0.13:44401 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:15:13 INFO TaskSetManager: Starting task 1.0 in stage 198.0 (TID 332) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 17:15:13 INFO TaskSetManager: Starting task 0.0 in stage 198.0 (TID 333) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 17:15:13 INFO BlockManagerInfo: Added broadcast_294_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:15:13 INFO BlockManagerInfo: Added broadcast_294_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:15:13 INFO TaskSetManager: Finished task 1.0 in stage 198.0 (TID 332) in 41 ms on 172.18.0.13 (executor 0) (1/2)
26/01/04 17:15:13 INFO TaskSetManager: Finished task 0.0 in stage 198.0 (TID 333) in 88 ms on 172.18.0.14 (executor 1) (2/2)
26/01/04 17:15:13 INFO TaskSchedulerImpl: Removed TaskSet 198.0, whose tasks have all completed, from pool 
26/01/04 17:15:13 INFO DAGScheduler: ResultStage 198 (start at NativeMethodAccessorImpl.java:0) finished in 0.106 s
26/01/04 17:15:13 INFO DAGScheduler: Job 197 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:15:13 INFO TaskSchedulerImpl: Killing all running tasks in stage 198: Stage finished
26/01/04 17:15:13 INFO DAGScheduler: Job 197 finished: start at NativeMethodAccessorImpl.java:0, took 0.109924 s
26/01/04 17:15:13 INFO MemoryStore: Block broadcast_295_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.3 MiB)
26/01/04 17:15:13 INFO BlockManagerInfo: Added broadcast_295_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:15:13 INFO SparkContext: Created broadcast 295 from start at NativeMethodAccessorImpl.java:0
26/01/04 17:15:13 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 97, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@3b4ce4c0]. The input RDD has 1 partitions.
26/01/04 17:15:13 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:15:13 INFO DAGScheduler: Got job 198 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/01/04 17:15:13 INFO DAGScheduler: Final stage: ResultStage 199 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:15:13 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:15:13 INFO DAGScheduler: Missing parents: List()
26/01/04 17:15:13 INFO DAGScheduler: Submitting ResultStage 199 (MapPartitionsRDD[997] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:15:13 INFO MemoryStore: Block broadcast_296 stored as values in memory (estimated size 52.9 KiB, free 434.3 MiB)
26/01/04 17:15:13 INFO MemoryStore: Block broadcast_296_piece0 stored as bytes in memory (estimated size 20.2 KiB, free 434.3 MiB)
26/01/04 17:15:13 INFO BlockManagerInfo: Added broadcast_296_piece0 in memory on spark-master:33535 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:15:13 INFO SparkContext: Created broadcast 296 from broadcast at DAGScheduler.scala:1585
26/01/04 17:15:13 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 199 (MapPartitionsRDD[997] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/01/04 17:15:13 INFO TaskSchedulerImpl: Adding task set 199.0 with 1 tasks resource profile 0
26/01/04 17:15:13 INFO TaskSetManager: Starting task 0.0 in stage 199.0 (TID 334) (172.18.0.13, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:15:13 INFO BlockManagerInfo: Added broadcast_296_piece0 in memory on 172.18.0.13:44401 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:15:13 INFO BlockManagerInfo: Added broadcast_295_piece0 in memory on 172.18.0.13:44401 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:15:13 INFO TaskSetManager: Finished task 0.0 in stage 199.0 (TID 334) in 591 ms on 172.18.0.13 (executor 0) (1/1)
26/01/04 17:15:13 INFO TaskSchedulerImpl: Removed TaskSet 199.0, whose tasks have all completed, from pool 
26/01/04 17:15:13 INFO DAGScheduler: ResultStage 199 (start at NativeMethodAccessorImpl.java:0) finished in 0.601 s
26/01/04 17:15:13 INFO DAGScheduler: Job 198 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:15:13 INFO TaskSchedulerImpl: Killing all running tasks in stage 199: Stage finished
26/01/04 17:15:13 INFO DAGScheduler: Job 198 finished: start at NativeMethodAccessorImpl.java:0, took 0.605157 s
26/01/04 17:15:13 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 97, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@3b4ce4c0] is committing.
26/01/04 17:15:13 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 97, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@3b4ce4c0] committed.
26/01/04 17:15:13 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/97 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.97.7d93248d-02f7-4080-a1e6-2f1b075faf97.tmp
26/01/04 17:15:13 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.97.7d93248d-02f7-4080-a1e6-2f1b075faf97.tmp to file:/tmp/spark-checkpoint-enrichment/commits/97
26/01/04 17:15:13 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T17:15:12.817Z",
  "batchId" : 97,
  "numInputRows" : 30,
  "inputRowsPerSecond" : 2500.0,
  "processedRowsPerSecond" : 27.573529411764703,
  "durationMs" : {
    "addBatch" : 858,
    "commitOffsets" : 92,
    "getBatch" : 0,
    "latestOffset" : 4,
    "queryPlanning" : 44,
    "triggerExecution" : 1088,
    "walCommit" : 90
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "0" : 2169
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "0" : 2199
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "0" : 2199
      }
    },
    "numInputRows" : 30,
    "inputRowsPerSecond" : 2500.0,
    "processedRowsPerSecond" : 27.573529411764703,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 31
  }
}
26/01/04 17:15:23 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/98 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.98.cd9f87b7-d8cf-426a-9a72-5f93d42dba96.tmp
26/01/04 17:15:23 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.98.cd9f87b7-d8cf-426a-9a72-5f93d42dba96.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/98
26/01/04 17:15:23 INFO MicroBatchExecution: Committed offsets for batch 98. Metadata OffsetSeqMetadata(0,1767546923837,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 17:15:23 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:15:23 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:15:24 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:15:24 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:15:24 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:15:24 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:15:24 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#80275 - airline_prefix.nullCount#80274) > 0)
26/01/04 17:15:24 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#80310 - min_flight_num.nullCount#80309) > 0)
26/01/04 17:15:24 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#80305 - max_flight_num.nullCount#80304) > 0)
26/01/04 17:15:24 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:15:24 INFO DAGScheduler: Got job 199 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 17:15:24 INFO DAGScheduler: Final stage: ResultStage 200 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:15:24 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:15:24 INFO DAGScheduler: Missing parents: List()
26/01/04 17:15:24 INFO DAGScheduler: Submitting ResultStage 200 (MapPartitionsRDD[1002] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:15:24 INFO MemoryStore: Block broadcast_297 stored as values in memory (estimated size 38.5 KiB, free 434.2 MiB)
26/01/04 17:15:24 INFO MemoryStore: Block broadcast_297_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.2 MiB)
26/01/04 17:15:24 INFO BlockManagerInfo: Added broadcast_297_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:15:24 INFO SparkContext: Created broadcast 297 from broadcast at DAGScheduler.scala:1585
26/01/04 17:15:24 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 200 (MapPartitionsRDD[1002] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 17:15:24 INFO TaskSchedulerImpl: Adding task set 200.0 with 2 tasks resource profile 0
26/01/04 17:15:24 INFO TaskSetManager: Starting task 0.0 in stage 200.0 (TID 335) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 17:15:24 INFO TaskSetManager: Starting task 1.0 in stage 200.0 (TID 336) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 17:15:24 INFO BlockManagerInfo: Added broadcast_297_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:15:24 INFO BlockManagerInfo: Added broadcast_297_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:15:24 INFO TaskSetManager: Finished task 1.0 in stage 200.0 (TID 336) in 32 ms on 172.18.0.13 (executor 0) (1/2)
26/01/04 17:15:24 INFO TaskSetManager: Finished task 0.0 in stage 200.0 (TID 335) in 56 ms on 172.18.0.14 (executor 1) (2/2)
26/01/04 17:15:24 INFO TaskSchedulerImpl: Removed TaskSet 200.0, whose tasks have all completed, from pool 
26/01/04 17:15:24 INFO DAGScheduler: ResultStage 200 (start at NativeMethodAccessorImpl.java:0) finished in 0.064 s
26/01/04 17:15:24 INFO DAGScheduler: Job 199 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:15:24 INFO TaskSchedulerImpl: Killing all running tasks in stage 200: Stage finished
26/01/04 17:15:24 INFO DAGScheduler: Job 199 finished: start at NativeMethodAccessorImpl.java:0, took 0.067591 s
26/01/04 17:15:24 INFO MemoryStore: Block broadcast_298_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.2 MiB)
26/01/04 17:15:24 INFO BlockManagerInfo: Added broadcast_298_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:15:24 INFO SparkContext: Created broadcast 298 from start at NativeMethodAccessorImpl.java:0
26/01/04 17:15:24 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 98, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@435c30b3]. The input RDD has 1 partitions.
26/01/04 17:15:24 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:15:24 INFO DAGScheduler: Got job 200 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/01/04 17:15:24 INFO DAGScheduler: Final stage: ResultStage 201 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:15:24 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:15:24 INFO DAGScheduler: Missing parents: List()
26/01/04 17:15:24 INFO DAGScheduler: Submitting ResultStage 201 (MapPartitionsRDD[1007] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:15:24 INFO MemoryStore: Block broadcast_299 stored as values in memory (estimated size 52.9 KiB, free 434.2 MiB)
26/01/04 17:15:24 INFO MemoryStore: Block broadcast_299_piece0 stored as bytes in memory (estimated size 20.2 KiB, free 434.1 MiB)
26/01/04 17:15:24 INFO BlockManagerInfo: Added broadcast_299_piece0 in memory on spark-master:33535 (size: 20.2 KiB, free: 434.3 MiB)
26/01/04 17:15:24 INFO SparkContext: Created broadcast 299 from broadcast at DAGScheduler.scala:1585
26/01/04 17:15:24 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 201 (MapPartitionsRDD[1007] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/01/04 17:15:24 INFO TaskSchedulerImpl: Adding task set 201.0 with 1 tasks resource profile 0
26/01/04 17:15:24 INFO TaskSetManager: Starting task 0.0 in stage 201.0 (TID 337) (172.18.0.13, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:15:24 INFO BlockManagerInfo: Added broadcast_299_piece0 in memory on 172.18.0.13:44401 (size: 20.2 KiB, free: 434.3 MiB)
26/01/04 17:15:24 INFO BlockManagerInfo: Added broadcast_298_piece0 in memory on 172.18.0.13:44401 (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:15:24 INFO TaskSetManager: Finished task 0.0 in stage 201.0 (TID 337) in 579 ms on 172.18.0.13 (executor 0) (1/1)
26/01/04 17:15:24 INFO TaskSchedulerImpl: Removed TaskSet 201.0, whose tasks have all completed, from pool 
26/01/04 17:15:24 INFO DAGScheduler: ResultStage 201 (start at NativeMethodAccessorImpl.java:0) finished in 0.586 s
26/01/04 17:15:24 INFO DAGScheduler: Job 200 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:15:24 INFO TaskSchedulerImpl: Killing all running tasks in stage 201: Stage finished
26/01/04 17:15:24 INFO DAGScheduler: Job 200 finished: start at NativeMethodAccessorImpl.java:0, took 0.588678 s
26/01/04 17:15:24 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 98, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@435c30b3] is committing.
26/01/04 17:15:24 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 98, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@435c30b3] committed.
26/01/04 17:15:24 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/98 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.98.5bda571b-f833-4b91-9d57-07362e2f2774.tmp
26/01/04 17:15:24 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.98.5bda571b-f833-4b91-9d57-07362e2f2774.tmp to file:/tmp/spark-checkpoint-enrichment/commits/98
26/01/04 17:15:24 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T17:15:23.831Z",
  "batchId" : 98,
  "numInputRows" : 30,
  "inputRowsPerSecond" : 2142.8571428571427,
  "processedRowsPerSecond" : 29.615004935834158,
  "durationMs" : {
    "addBatch" : 766,
    "commitOffsets" : 99,
    "getBatch" : 0,
    "latestOffset" : 6,
    "queryPlanning" : 28,
    "triggerExecution" : 1013,
    "walCommit" : 114
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "0" : 2199
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "0" : 2229
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "0" : 2229
      }
    },
    "numInputRows" : 30,
    "inputRowsPerSecond" : 2142.8571428571427,
    "processedRowsPerSecond" : 29.615004935834158,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 31
  }
}
26/01/04 17:15:27 INFO BlockManagerInfo: Removed broadcast_294_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:15:27 INFO BlockManagerInfo: Removed broadcast_294_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:15:27 INFO BlockManagerInfo: Removed broadcast_294_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:15:27 INFO BlockManagerInfo: Removed broadcast_297_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:15:27 INFO BlockManagerInfo: Removed broadcast_297_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:15:27 INFO BlockManagerInfo: Removed broadcast_297_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:15:27 INFO BlockManagerInfo: Removed broadcast_296_piece0 on spark-master:33535 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:15:27 INFO BlockManagerInfo: Removed broadcast_296_piece0 on 172.18.0.13:44401 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:15:27 INFO BlockManagerInfo: Removed broadcast_299_piece0 on spark-master:33535 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:15:27 INFO BlockManagerInfo: Removed broadcast_299_piece0 on 172.18.0.13:44401 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:15:27 INFO BlockManagerInfo: Removed broadcast_295_piece0 on spark-master:33535 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:15:27 INFO BlockManagerInfo: Removed broadcast_295_piece0 on 172.18.0.13:44401 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:15:34 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/99 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.99.f502cc75-c5f9-4b93-a85e-ba4c28881a68.tmp
26/01/04 17:15:34 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.99.f502cc75-c5f9-4b93-a85e-ba4c28881a68.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/99
26/01/04 17:15:34 INFO MicroBatchExecution: Committed offsets for batch 99. Metadata OffsetSeqMetadata(0,1767546934854,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 17:15:34 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:15:34 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:15:34 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:15:34 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:15:35 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:15:35 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:15:35 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#81079 - airline_prefix.nullCount#81078) > 0)
26/01/04 17:15:35 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#81114 - min_flight_num.nullCount#81113) > 0)
26/01/04 17:15:35 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#81109 - max_flight_num.nullCount#81108) > 0)
26/01/04 17:15:35 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:15:35 INFO DAGScheduler: Got job 201 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 17:15:35 INFO DAGScheduler: Final stage: ResultStage 202 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:15:35 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:15:35 INFO DAGScheduler: Missing parents: List()
26/01/04 17:15:35 INFO DAGScheduler: Submitting ResultStage 202 (MapPartitionsRDD[1012] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:15:35 INFO MemoryStore: Block broadcast_300 stored as values in memory (estimated size 38.5 KiB, free 434.4 MiB)
26/01/04 17:15:35 INFO BlockManagerInfo: Removed broadcast_298_piece0 on spark-master:33535 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:15:35 INFO MemoryStore: Block broadcast_300_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.3 MiB)
26/01/04 17:15:35 INFO BlockManagerInfo: Added broadcast_300_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:15:35 INFO BlockManagerInfo: Removed broadcast_298_piece0 on 172.18.0.13:44401 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:15:35 INFO SparkContext: Created broadcast 300 from broadcast at DAGScheduler.scala:1585
26/01/04 17:15:35 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 202 (MapPartitionsRDD[1012] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 17:15:35 INFO TaskSchedulerImpl: Adding task set 202.0 with 2 tasks resource profile 0
26/01/04 17:15:35 INFO TaskSetManager: Starting task 1.0 in stage 202.0 (TID 338) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 17:15:35 INFO TaskSetManager: Starting task 0.0 in stage 202.0 (TID 339) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 17:15:35 INFO BlockManagerInfo: Added broadcast_300_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:15:35 INFO BlockManagerInfo: Added broadcast_300_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:15:35 INFO TaskSetManager: Finished task 1.0 in stage 202.0 (TID 338) in 22 ms on 172.18.0.13 (executor 0) (1/2)
26/01/04 17:15:35 INFO TaskSetManager: Finished task 0.0 in stage 202.0 (TID 339) in 49 ms on 172.18.0.14 (executor 1) (2/2)
26/01/04 17:15:35 INFO TaskSchedulerImpl: Removed TaskSet 202.0, whose tasks have all completed, from pool 
26/01/04 17:15:35 INFO DAGScheduler: ResultStage 202 (start at NativeMethodAccessorImpl.java:0) finished in 0.061 s
26/01/04 17:15:35 INFO DAGScheduler: Job 201 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:15:35 INFO TaskSchedulerImpl: Killing all running tasks in stage 202: Stage finished
26/01/04 17:15:35 INFO DAGScheduler: Job 201 finished: start at NativeMethodAccessorImpl.java:0, took 0.062535 s
26/01/04 17:15:35 INFO MemoryStore: Block broadcast_301_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.3 MiB)
26/01/04 17:15:35 INFO BlockManagerInfo: Added broadcast_301_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:15:35 INFO SparkContext: Created broadcast 301 from start at NativeMethodAccessorImpl.java:0
26/01/04 17:15:35 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 99, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@7163db6b]. The input RDD has 1 partitions.
26/01/04 17:15:35 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:15:35 INFO DAGScheduler: Got job 202 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/01/04 17:15:35 INFO DAGScheduler: Final stage: ResultStage 203 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:15:35 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:15:35 INFO DAGScheduler: Missing parents: List()
26/01/04 17:15:35 INFO DAGScheduler: Submitting ResultStage 203 (MapPartitionsRDD[1017] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:15:35 INFO MemoryStore: Block broadcast_302 stored as values in memory (estimated size 52.9 KiB, free 434.3 MiB)
26/01/04 17:15:35 INFO MemoryStore: Block broadcast_302_piece0 stored as bytes in memory (estimated size 20.2 KiB, free 434.3 MiB)
26/01/04 17:15:35 INFO BlockManagerInfo: Added broadcast_302_piece0 in memory on spark-master:33535 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:15:35 INFO SparkContext: Created broadcast 302 from broadcast at DAGScheduler.scala:1585
26/01/04 17:15:35 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 203 (MapPartitionsRDD[1017] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/01/04 17:15:35 INFO TaskSchedulerImpl: Adding task set 203.0 with 1 tasks resource profile 0
26/01/04 17:15:35 INFO TaskSetManager: Starting task 0.0 in stage 203.0 (TID 340) (172.18.0.13, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:15:35 INFO BlockManagerInfo: Added broadcast_302_piece0 in memory on 172.18.0.13:44401 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:15:35 INFO BlockManagerInfo: Added broadcast_301_piece0 in memory on 172.18.0.13:44401 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:15:35 INFO TaskSetManager: Finished task 0.0 in stage 203.0 (TID 340) in 571 ms on 172.18.0.13 (executor 0) (1/1)
26/01/04 17:15:35 INFO TaskSchedulerImpl: Removed TaskSet 203.0, whose tasks have all completed, from pool 
26/01/04 17:15:35 INFO DAGScheduler: ResultStage 203 (start at NativeMethodAccessorImpl.java:0) finished in 0.575 s
26/01/04 17:15:35 INFO DAGScheduler: Job 202 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:15:35 INFO TaskSchedulerImpl: Killing all running tasks in stage 203: Stage finished
26/01/04 17:15:35 INFO DAGScheduler: Job 202 finished: start at NativeMethodAccessorImpl.java:0, took 0.576722 s
26/01/04 17:15:35 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 99, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@7163db6b] is committing.
26/01/04 17:15:35 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 99, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@7163db6b] committed.
26/01/04 17:15:35 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/99 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.99.1c2efcb6-b9f1-4808-8fe9-71b0694139db.tmp
26/01/04 17:15:35 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.99.1c2efcb6-b9f1-4808-8fe9-71b0694139db.tmp to file:/tmp/spark-checkpoint-enrichment/commits/99
26/01/04 17:15:35 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T17:15:34.852Z",
  "batchId" : 99,
  "numInputRows" : 30,
  "inputRowsPerSecond" : 2500.0,
  "processedRowsPerSecond" : 32.93084522502744,
  "durationMs" : {
    "addBatch" : 703,
    "commitOffsets" : 77,
    "getBatch" : 0,
    "latestOffset" : 2,
    "queryPlanning" : 32,
    "triggerExecution" : 911,
    "walCommit" : 96
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "0" : 2229
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "0" : 2259
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "0" : 2259
      }
    },
    "numInputRows" : 30,
    "inputRowsPerSecond" : 2500.0,
    "processedRowsPerSecond" : 32.93084522502744,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 31
  }
}
26/01/04 17:15:45 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/01/04 17:15:45 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/100 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.100.16fab7f7-c12c-404b-a9d2-d4c4a6afa95e.tmp
26/01/04 17:15:45 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.100.16fab7f7-c12c-404b-a9d2-d4c4a6afa95e.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/100
26/01/04 17:15:45 INFO MicroBatchExecution: Committed offsets for batch 100. Metadata OffsetSeqMetadata(0,1767546945870,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 17:15:45 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:15:45 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:15:46 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:15:46 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:15:46 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:15:46 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:15:46 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#81883 - airline_prefix.nullCount#81882) > 0)
26/01/04 17:15:46 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#81918 - min_flight_num.nullCount#81917) > 0)
26/01/04 17:15:46 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#81913 - max_flight_num.nullCount#81912) > 0)
26/01/04 17:15:46 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:15:46 INFO DAGScheduler: Got job 203 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 17:15:46 INFO DAGScheduler: Final stage: ResultStage 204 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:15:46 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:15:46 INFO DAGScheduler: Missing parents: List()
26/01/04 17:15:46 INFO DAGScheduler: Submitting ResultStage 204 (MapPartitionsRDD[1022] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:15:46 INFO MemoryStore: Block broadcast_303 stored as values in memory (estimated size 38.5 KiB, free 434.2 MiB)
26/01/04 17:15:46 INFO MemoryStore: Block broadcast_303_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.2 MiB)
26/01/04 17:15:46 INFO BlockManagerInfo: Added broadcast_303_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:15:46 INFO SparkContext: Created broadcast 303 from broadcast at DAGScheduler.scala:1585
26/01/04 17:15:46 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 204 (MapPartitionsRDD[1022] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 17:15:46 INFO TaskSchedulerImpl: Adding task set 204.0 with 2 tasks resource profile 0
26/01/04 17:15:46 INFO TaskSetManager: Starting task 1.0 in stage 204.0 (TID 341) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 17:15:46 INFO TaskSetManager: Starting task 0.0 in stage 204.0 (TID 342) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 17:15:46 INFO BlockManagerInfo: Added broadcast_303_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:15:46 INFO BlockManagerInfo: Added broadcast_303_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:15:46 INFO TaskSetManager: Finished task 1.0 in stage 204.0 (TID 341) in 25 ms on 172.18.0.13 (executor 0) (1/2)
26/01/04 17:15:46 INFO TaskSetManager: Finished task 0.0 in stage 204.0 (TID 342) in 45 ms on 172.18.0.14 (executor 1) (2/2)
26/01/04 17:15:46 INFO TaskSchedulerImpl: Removed TaskSet 204.0, whose tasks have all completed, from pool 
26/01/04 17:15:46 INFO DAGScheduler: ResultStage 204 (start at NativeMethodAccessorImpl.java:0) finished in 0.052 s
26/01/04 17:15:46 INFO DAGScheduler: Job 203 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:15:46 INFO TaskSchedulerImpl: Killing all running tasks in stage 204: Stage finished
26/01/04 17:15:46 INFO DAGScheduler: Job 203 finished: start at NativeMethodAccessorImpl.java:0, took 0.054952 s
26/01/04 17:15:46 INFO MemoryStore: Block broadcast_304_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.2 MiB)
26/01/04 17:15:46 INFO BlockManagerInfo: Added broadcast_304_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:15:46 INFO SparkContext: Created broadcast 304 from start at NativeMethodAccessorImpl.java:0
26/01/04 17:15:46 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 100, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@7548e60a]. The input RDD has 1 partitions.
26/01/04 17:15:46 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:15:46 INFO DAGScheduler: Got job 204 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/01/04 17:15:46 INFO DAGScheduler: Final stage: ResultStage 205 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:15:46 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:15:46 INFO DAGScheduler: Missing parents: List()
26/01/04 17:15:46 INFO DAGScheduler: Submitting ResultStage 205 (MapPartitionsRDD[1027] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:15:46 INFO MemoryStore: Block broadcast_305 stored as values in memory (estimated size 52.9 KiB, free 434.2 MiB)
26/01/04 17:15:46 INFO MemoryStore: Block broadcast_305_piece0 stored as bytes in memory (estimated size 20.2 KiB, free 434.1 MiB)
26/01/04 17:15:46 INFO BlockManagerInfo: Added broadcast_305_piece0 in memory on spark-master:33535 (size: 20.2 KiB, free: 434.3 MiB)
26/01/04 17:15:46 INFO SparkContext: Created broadcast 305 from broadcast at DAGScheduler.scala:1585
26/01/04 17:15:46 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 205 (MapPartitionsRDD[1027] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/01/04 17:15:46 INFO TaskSchedulerImpl: Adding task set 205.0 with 1 tasks resource profile 0
26/01/04 17:15:46 INFO TaskSetManager: Starting task 0.0 in stage 205.0 (TID 343) (172.18.0.13, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:15:46 INFO BlockManagerInfo: Added broadcast_305_piece0 in memory on 172.18.0.13:44401 (size: 20.2 KiB, free: 434.3 MiB)
26/01/04 17:15:46 INFO BlockManagerInfo: Added broadcast_304_piece0 in memory on 172.18.0.13:44401 (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:15:46 INFO TaskSetManager: Finished task 0.0 in stage 205.0 (TID 343) in 577 ms on 172.18.0.13 (executor 0) (1/1)
26/01/04 17:15:46 INFO TaskSchedulerImpl: Removed TaskSet 205.0, whose tasks have all completed, from pool 
26/01/04 17:15:46 INFO DAGScheduler: ResultStage 205 (start at NativeMethodAccessorImpl.java:0) finished in 0.585 s
26/01/04 17:15:46 INFO DAGScheduler: Job 204 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:15:46 INFO TaskSchedulerImpl: Killing all running tasks in stage 205: Stage finished
26/01/04 17:15:46 INFO DAGScheduler: Job 204 finished: start at NativeMethodAccessorImpl.java:0, took 0.587130 s
26/01/04 17:15:46 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 100, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@7548e60a] is committing.
26/01/04 17:15:46 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 100, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@7548e60a] committed.
26/01/04 17:15:46 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/100 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.100.20c4cd4c-071c-487c-9f42-d7108b6af876.tmp
26/01/04 17:15:46 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.100.20c4cd4c-071c-487c-9f42-d7108b6af876.tmp to file:/tmp/spark-checkpoint-enrichment/commits/100
26/01/04 17:15:46 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T17:15:45.869Z",
  "batchId" : 100,
  "numInputRows" : 30,
  "inputRowsPerSecond" : 2307.6923076923076,
  "processedRowsPerSecond" : 32.43243243243243,
  "durationMs" : {
    "addBatch" : 741,
    "commitOffsets" : 69,
    "getBatch" : 1,
    "latestOffset" : 1,
    "queryPlanning" : 35,
    "triggerExecution" : 925,
    "walCommit" : 78
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "0" : 2259
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "0" : 2289
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "0" : 2289
      }
    },
    "numInputRows" : 30,
    "inputRowsPerSecond" : 2307.6923076923076,
    "processedRowsPerSecond" : 32.43243243243243,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 31
  }
}
26/01/04 17:15:49 INFO BlockManagerInfo: Removed broadcast_300_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:15:49 INFO BlockManagerInfo: Removed broadcast_300_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:15:49 INFO BlockManagerInfo: Removed broadcast_300_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:15:49 INFO BlockManagerInfo: Removed broadcast_301_piece0 on spark-master:33535 in memory (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:15:49 INFO BlockManagerInfo: Removed broadcast_301_piece0 on 172.18.0.13:44401 in memory (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:15:49 INFO BlockManagerInfo: Removed broadcast_302_piece0 on spark-master:33535 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:15:49 INFO BlockManagerInfo: Removed broadcast_302_piece0 on 172.18.0.13:44401 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:15:49 INFO BlockManagerInfo: Removed broadcast_303_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:15:49 INFO BlockManagerInfo: Removed broadcast_303_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:15:49 INFO BlockManagerInfo: Removed broadcast_303_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:15:49 INFO BlockManagerInfo: Removed broadcast_305_piece0 on spark-master:33535 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:15:49 INFO BlockManagerInfo: Removed broadcast_305_piece0 on 172.18.0.13:44401 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:15:56 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/01/04 17:15:56 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/101 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.101.3f8b4779-d7ff-41b3-a918-5dfda0bc4bfa.tmp
26/01/04 17:15:56 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.101.3f8b4779-d7ff-41b3-a918-5dfda0bc4bfa.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/101
26/01/04 17:15:56 INFO MicroBatchExecution: Committed offsets for batch 101. Metadata OffsetSeqMetadata(0,1767546956878,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 17:15:56 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:15:56 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:15:56 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:15:56 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:15:57 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:15:57 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:15:57 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#82687 - airline_prefix.nullCount#82686) > 0)
26/01/04 17:15:57 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#82722 - min_flight_num.nullCount#82721) > 0)
26/01/04 17:15:57 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#82717 - max_flight_num.nullCount#82716) > 0)
26/01/04 17:15:57 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:15:57 INFO DAGScheduler: Got job 205 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 17:15:57 INFO DAGScheduler: Final stage: ResultStage 206 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:15:57 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:15:57 INFO DAGScheduler: Missing parents: List()
26/01/04 17:15:57 INFO DAGScheduler: Submitting ResultStage 206 (MapPartitionsRDD[1032] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:15:57 INFO MemoryStore: Block broadcast_306 stored as values in memory (estimated size 38.5 KiB, free 434.4 MiB)
26/01/04 17:15:57 INFO MemoryStore: Block broadcast_306_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.3 MiB)
26/01/04 17:15:57 INFO BlockManagerInfo: Added broadcast_306_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:15:57 INFO SparkContext: Created broadcast 306 from broadcast at DAGScheduler.scala:1585
26/01/04 17:15:57 INFO BlockManagerInfo: Removed broadcast_304_piece0 on spark-master:33535 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:15:57 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 206 (MapPartitionsRDD[1032] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 17:15:57 INFO TaskSchedulerImpl: Adding task set 206.0 with 2 tasks resource profile 0
26/01/04 17:15:57 INFO BlockManagerInfo: Removed broadcast_304_piece0 on 172.18.0.13:44401 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:15:57 INFO TaskSetManager: Starting task 0.0 in stage 206.0 (TID 344) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 17:15:57 INFO TaskSetManager: Starting task 1.0 in stage 206.0 (TID 345) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 17:15:57 INFO BlockManagerInfo: Added broadcast_306_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:15:57 INFO TaskSetManager: Finished task 1.0 in stage 206.0 (TID 345) in 32 ms on 172.18.0.13 (executor 0) (1/2)
26/01/04 17:15:57 INFO BlockManagerInfo: Added broadcast_306_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:15:57 INFO TaskSetManager: Finished task 0.0 in stage 206.0 (TID 344) in 91 ms on 172.18.0.14 (executor 1) (2/2)
26/01/04 17:15:57 INFO TaskSchedulerImpl: Removed TaskSet 206.0, whose tasks have all completed, from pool 
26/01/04 17:15:57 INFO DAGScheduler: ResultStage 206 (start at NativeMethodAccessorImpl.java:0) finished in 0.104 s
26/01/04 17:15:57 INFO DAGScheduler: Job 205 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:15:57 INFO TaskSchedulerImpl: Killing all running tasks in stage 206: Stage finished
26/01/04 17:15:57 INFO DAGScheduler: Job 205 finished: start at NativeMethodAccessorImpl.java:0, took 0.107543 s
26/01/04 17:15:57 INFO MemoryStore: Block broadcast_307_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.3 MiB)
26/01/04 17:15:57 INFO BlockManagerInfo: Added broadcast_307_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:15:57 INFO SparkContext: Created broadcast 307 from start at NativeMethodAccessorImpl.java:0
26/01/04 17:15:57 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 101, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@577b704d]. The input RDD has 1 partitions.
26/01/04 17:15:57 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:15:57 INFO DAGScheduler: Got job 206 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/01/04 17:15:57 INFO DAGScheduler: Final stage: ResultStage 207 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:15:57 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:15:57 INFO DAGScheduler: Missing parents: List()
26/01/04 17:15:57 INFO DAGScheduler: Submitting ResultStage 207 (MapPartitionsRDD[1037] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:15:57 INFO MemoryStore: Block broadcast_308 stored as values in memory (estimated size 52.9 KiB, free 434.3 MiB)
26/01/04 17:15:57 INFO MemoryStore: Block broadcast_308_piece0 stored as bytes in memory (estimated size 20.1 KiB, free 434.3 MiB)
26/01/04 17:15:57 INFO BlockManagerInfo: Added broadcast_308_piece0 in memory on spark-master:33535 (size: 20.1 KiB, free: 434.4 MiB)
26/01/04 17:15:57 INFO SparkContext: Created broadcast 308 from broadcast at DAGScheduler.scala:1585
26/01/04 17:15:57 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 207 (MapPartitionsRDD[1037] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/01/04 17:15:57 INFO TaskSchedulerImpl: Adding task set 207.0 with 1 tasks resource profile 0
26/01/04 17:15:57 INFO TaskSetManager: Starting task 0.0 in stage 207.0 (TID 346) (172.18.0.13, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:15:57 INFO BlockManagerInfo: Added broadcast_308_piece0 in memory on 172.18.0.13:44401 (size: 20.1 KiB, free: 434.4 MiB)
26/01/04 17:15:57 INFO BlockManagerInfo: Added broadcast_307_piece0 in memory on 172.18.0.13:44401 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:15:57 INFO TaskSetManager: Finished task 0.0 in stage 207.0 (TID 346) in 557 ms on 172.18.0.13 (executor 0) (1/1)
26/01/04 17:15:57 INFO TaskSchedulerImpl: Removed TaskSet 207.0, whose tasks have all completed, from pool 
26/01/04 17:15:57 INFO DAGScheduler: ResultStage 207 (start at NativeMethodAccessorImpl.java:0) finished in 0.565 s
26/01/04 17:15:57 INFO DAGScheduler: Job 206 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:15:57 INFO TaskSchedulerImpl: Killing all running tasks in stage 207: Stage finished
26/01/04 17:15:57 INFO DAGScheduler: Job 206 finished: start at NativeMethodAccessorImpl.java:0, took 0.567529 s
26/01/04 17:15:57 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 101, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@577b704d] is committing.
26/01/04 17:15:57 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 101, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@577b704d] committed.
26/01/04 17:15:57 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/101 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.101.70d75a95-81c6-4668-a07d-10ea5fa9029f.tmp
26/01/04 17:15:57 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.101.70d75a95-81c6-4668-a07d-10ea5fa9029f.tmp to file:/tmp/spark-checkpoint-enrichment/commits/101
26/01/04 17:15:57 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T17:15:56.877Z",
  "batchId" : 101,
  "numInputRows" : 30,
  "inputRowsPerSecond" : 2500.0,
  "processedRowsPerSecond" : 31.088082901554404,
  "durationMs" : {
    "addBatch" : 754,
    "commitOffsets" : 109,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 25,
    "triggerExecution" : 965,
    "walCommit" : 75
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "0" : 2289
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "0" : 2319
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "0" : 2319
      }
    },
    "numInputRows" : 30,
    "inputRowsPerSecond" : 2500.0,
    "processedRowsPerSecond" : 31.088082901554404,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 31
  }
}
26/01/04 17:16:07 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/01/04 17:16:07 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/102 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.102.3c55ae8e-d09f-4c75-b8b5-e01dcd281ef1.tmp
26/01/04 17:16:07 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.102.3c55ae8e-d09f-4c75-b8b5-e01dcd281ef1.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/102
26/01/04 17:16:07 INFO MicroBatchExecution: Committed offsets for batch 102. Metadata OffsetSeqMetadata(0,1767546967895,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 17:16:08 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:16:08 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:16:08 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:16:08 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:16:08 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:16:08 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:16:08 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#83491 - airline_prefix.nullCount#83490) > 0)
26/01/04 17:16:08 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#83526 - min_flight_num.nullCount#83525) > 0)
26/01/04 17:16:08 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#83521 - max_flight_num.nullCount#83520) > 0)
26/01/04 17:16:08 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:16:08 INFO DAGScheduler: Got job 207 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 17:16:08 INFO DAGScheduler: Final stage: ResultStage 208 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:16:08 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:16:08 INFO DAGScheduler: Missing parents: List()
26/01/04 17:16:08 INFO DAGScheduler: Submitting ResultStage 208 (MapPartitionsRDD[1042] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:16:08 INFO MemoryStore: Block broadcast_309 stored as values in memory (estimated size 38.5 KiB, free 434.2 MiB)
26/01/04 17:16:08 INFO MemoryStore: Block broadcast_309_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.2 MiB)
26/01/04 17:16:08 INFO BlockManagerInfo: Added broadcast_309_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:16:08 INFO SparkContext: Created broadcast 309 from broadcast at DAGScheduler.scala:1585
26/01/04 17:16:08 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 208 (MapPartitionsRDD[1042] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 17:16:08 INFO TaskSchedulerImpl: Adding task set 208.0 with 2 tasks resource profile 0
26/01/04 17:16:08 INFO TaskSetManager: Starting task 1.0 in stage 208.0 (TID 347) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 17:16:08 INFO TaskSetManager: Starting task 0.0 in stage 208.0 (TID 348) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 17:16:08 INFO BlockManagerInfo: Added broadcast_309_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:16:08 INFO BlockManagerInfo: Added broadcast_309_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:16:08 INFO TaskSetManager: Finished task 1.0 in stage 208.0 (TID 347) in 30 ms on 172.18.0.13 (executor 0) (1/2)
26/01/04 17:16:08 INFO TaskSetManager: Finished task 0.0 in stage 208.0 (TID 348) in 51 ms on 172.18.0.14 (executor 1) (2/2)
26/01/04 17:16:08 INFO TaskSchedulerImpl: Removed TaskSet 208.0, whose tasks have all completed, from pool 
26/01/04 17:16:08 INFO DAGScheduler: ResultStage 208 (start at NativeMethodAccessorImpl.java:0) finished in 0.060 s
26/01/04 17:16:08 INFO DAGScheduler: Job 207 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:16:08 INFO TaskSchedulerImpl: Killing all running tasks in stage 208: Stage finished
26/01/04 17:16:08 INFO DAGScheduler: Job 207 finished: start at NativeMethodAccessorImpl.java:0, took 0.062255 s
26/01/04 17:16:08 INFO MemoryStore: Block broadcast_310_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.2 MiB)
26/01/04 17:16:08 INFO BlockManagerInfo: Added broadcast_310_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:16:08 INFO SparkContext: Created broadcast 310 from start at NativeMethodAccessorImpl.java:0
26/01/04 17:16:08 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 102, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@5bea4c9f]. The input RDD has 1 partitions.
26/01/04 17:16:08 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:16:08 INFO DAGScheduler: Got job 208 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/01/04 17:16:08 INFO DAGScheduler: Final stage: ResultStage 209 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:16:08 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:16:08 INFO DAGScheduler: Missing parents: List()
26/01/04 17:16:08 INFO DAGScheduler: Submitting ResultStage 209 (MapPartitionsRDD[1047] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:16:08 INFO MemoryStore: Block broadcast_311 stored as values in memory (estimated size 52.9 KiB, free 434.2 MiB)
26/01/04 17:16:08 INFO MemoryStore: Block broadcast_311_piece0 stored as bytes in memory (estimated size 20.1 KiB, free 434.1 MiB)
26/01/04 17:16:08 INFO BlockManagerInfo: Added broadcast_311_piece0 in memory on spark-master:33535 (size: 20.1 KiB, free: 434.3 MiB)
26/01/04 17:16:08 INFO SparkContext: Created broadcast 311 from broadcast at DAGScheduler.scala:1585
26/01/04 17:16:08 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 209 (MapPartitionsRDD[1047] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/01/04 17:16:08 INFO TaskSchedulerImpl: Adding task set 209.0 with 1 tasks resource profile 0
26/01/04 17:16:08 INFO TaskSetManager: Starting task 0.0 in stage 209.0 (TID 349) (172.18.0.13, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:16:08 INFO BlockManagerInfo: Added broadcast_311_piece0 in memory on 172.18.0.13:44401 (size: 20.1 KiB, free: 434.3 MiB)
26/01/04 17:16:08 INFO BlockManagerInfo: Added broadcast_310_piece0 in memory on 172.18.0.13:44401 (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:16:08 INFO TaskSetManager: Finished task 0.0 in stage 209.0 (TID 349) in 610 ms on 172.18.0.13 (executor 0) (1/1)
26/01/04 17:16:08 INFO TaskSchedulerImpl: Removed TaskSet 209.0, whose tasks have all completed, from pool 
26/01/04 17:16:08 INFO DAGScheduler: ResultStage 209 (start at NativeMethodAccessorImpl.java:0) finished in 0.635 s
26/01/04 17:16:08 INFO DAGScheduler: Job 208 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:16:08 INFO TaskSchedulerImpl: Killing all running tasks in stage 209: Stage finished
26/01/04 17:16:08 INFO DAGScheduler: Job 208 finished: start at NativeMethodAccessorImpl.java:0, took 0.636548 s
26/01/04 17:16:08 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 102, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@5bea4c9f] is committing.
26/01/04 17:16:08 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 102, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@5bea4c9f] committed.
26/01/04 17:16:08 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/102 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.102.accb493c-e072-4232-b5c9-af554396d063.tmp
26/01/04 17:16:08 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.102.accb493c-e072-4232-b5c9-af554396d063.tmp to file:/tmp/spark-checkpoint-enrichment/commits/102
26/01/04 17:16:08 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T17:16:07.892Z",
  "batchId" : 102,
  "numInputRows" : 30,
  "inputRowsPerSecond" : 1578.9473684210527,
  "processedRowsPerSecond" : 28.409090909090907,
  "durationMs" : {
    "addBatch" : 791,
    "commitOffsets" : 147,
    "getBatch" : 1,
    "latestOffset" : 3,
    "queryPlanning" : 29,
    "triggerExecution" : 1056,
    "walCommit" : 84
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "0" : 2319
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "0" : 2349
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "0" : 2349
      }
    },
    "numInputRows" : 30,
    "inputRowsPerSecond" : 1578.9473684210527,
    "processedRowsPerSecond" : 28.409090909090907,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 31
  }
}
26/01/04 17:16:12 INFO BlockManagerInfo: Removed broadcast_306_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:16:12 INFO BlockManagerInfo: Removed broadcast_306_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:16:12 INFO BlockManagerInfo: Removed broadcast_306_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:16:12 INFO BlockManagerInfo: Removed broadcast_307_piece0 on spark-master:33535 in memory (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:16:12 INFO BlockManagerInfo: Removed broadcast_307_piece0 on 172.18.0.13:44401 in memory (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:16:12 INFO BlockManagerInfo: Removed broadcast_311_piece0 on spark-master:33535 in memory (size: 20.1 KiB, free: 434.4 MiB)
26/01/04 17:16:12 INFO BlockManagerInfo: Removed broadcast_311_piece0 on 172.18.0.13:44401 in memory (size: 20.1 KiB, free: 434.4 MiB)
26/01/04 17:16:12 INFO BlockManagerInfo: Removed broadcast_308_piece0 on spark-master:33535 in memory (size: 20.1 KiB, free: 434.4 MiB)
26/01/04 17:16:12 INFO BlockManagerInfo: Removed broadcast_308_piece0 on 172.18.0.13:44401 in memory (size: 20.1 KiB, free: 434.4 MiB)
26/01/04 17:16:12 INFO BlockManagerInfo: Removed broadcast_309_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:16:12 INFO BlockManagerInfo: Removed broadcast_309_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:16:12 INFO BlockManagerInfo: Removed broadcast_309_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:16:18 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/103 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.103.d280b8b6-20b3-407f-9703-4eb19dc7d6e5.tmp
26/01/04 17:16:19 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.103.d280b8b6-20b3-407f-9703-4eb19dc7d6e5.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/103
26/01/04 17:16:19 INFO MicroBatchExecution: Committed offsets for batch 103. Metadata OffsetSeqMetadata(0,1767546978916,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 17:16:19 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:16:19 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:16:19 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:16:19 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:16:19 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:16:19 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:16:19 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#84295 - airline_prefix.nullCount#84294) > 0)
26/01/04 17:16:19 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#84330 - min_flight_num.nullCount#84329) > 0)
26/01/04 17:16:19 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#84325 - max_flight_num.nullCount#84324) > 0)
26/01/04 17:16:19 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:16:19 INFO DAGScheduler: Got job 209 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 17:16:19 INFO DAGScheduler: Final stage: ResultStage 210 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:16:19 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:16:19 INFO DAGScheduler: Missing parents: List()
26/01/04 17:16:19 INFO DAGScheduler: Submitting ResultStage 210 (MapPartitionsRDD[1052] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:16:19 INFO MemoryStore: Block broadcast_312 stored as values in memory (estimated size 38.5 KiB, free 434.4 MiB)
26/01/04 17:16:19 INFO MemoryStore: Block broadcast_312_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.3 MiB)
26/01/04 17:16:19 INFO BlockManagerInfo: Removed broadcast_310_piece0 on spark-master:33535 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:16:19 INFO BlockManagerInfo: Added broadcast_312_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:16:19 INFO SparkContext: Created broadcast 312 from broadcast at DAGScheduler.scala:1585
26/01/04 17:16:19 INFO BlockManagerInfo: Removed broadcast_310_piece0 on 172.18.0.13:44401 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:16:19 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 210 (MapPartitionsRDD[1052] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 17:16:19 INFO TaskSchedulerImpl: Adding task set 210.0 with 2 tasks resource profile 0
26/01/04 17:16:19 INFO TaskSetManager: Starting task 0.0 in stage 210.0 (TID 350) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 17:16:19 INFO TaskSetManager: Starting task 1.0 in stage 210.0 (TID 351) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 17:16:19 INFO BlockManagerInfo: Added broadcast_312_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:16:19 INFO BlockManagerInfo: Added broadcast_312_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:16:19 INFO TaskSetManager: Finished task 1.0 in stage 210.0 (TID 351) in 76 ms on 172.18.0.13 (executor 0) (1/2)
26/01/04 17:16:19 INFO TaskSetManager: Finished task 0.0 in stage 210.0 (TID 350) in 218 ms on 172.18.0.14 (executor 1) (2/2)
26/01/04 17:16:19 INFO TaskSchedulerImpl: Removed TaskSet 210.0, whose tasks have all completed, from pool 
26/01/04 17:16:19 INFO DAGScheduler: ResultStage 210 (start at NativeMethodAccessorImpl.java:0) finished in 0.250 s
26/01/04 17:16:19 INFO DAGScheduler: Job 209 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:16:19 INFO TaskSchedulerImpl: Killing all running tasks in stage 210: Stage finished
26/01/04 17:16:19 INFO DAGScheduler: Job 209 finished: start at NativeMethodAccessorImpl.java:0, took 0.256854 s
26/01/04 17:16:19 INFO MemoryStore: Block broadcast_313_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.3 MiB)
26/01/04 17:16:19 INFO BlockManagerInfo: Added broadcast_313_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:16:19 INFO SparkContext: Created broadcast 313 from start at NativeMethodAccessorImpl.java:0
26/01/04 17:16:19 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 103, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@55d53d20]. The input RDD has 1 partitions.
26/01/04 17:16:19 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:16:19 INFO DAGScheduler: Got job 210 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/01/04 17:16:19 INFO DAGScheduler: Final stage: ResultStage 211 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:16:19 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:16:19 INFO DAGScheduler: Missing parents: List()
26/01/04 17:16:19 INFO DAGScheduler: Submitting ResultStage 211 (MapPartitionsRDD[1057] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:16:19 INFO MemoryStore: Block broadcast_314 stored as values in memory (estimated size 52.9 KiB, free 434.3 MiB)
26/01/04 17:16:19 INFO MemoryStore: Block broadcast_314_piece0 stored as bytes in memory (estimated size 20.2 KiB, free 434.3 MiB)
26/01/04 17:16:19 INFO BlockManagerInfo: Added broadcast_314_piece0 in memory on spark-master:33535 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:16:19 INFO SparkContext: Created broadcast 314 from broadcast at DAGScheduler.scala:1585
26/01/04 17:16:19 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 211 (MapPartitionsRDD[1057] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/01/04 17:16:19 INFO TaskSchedulerImpl: Adding task set 211.0 with 1 tasks resource profile 0
26/01/04 17:16:19 INFO TaskSetManager: Starting task 0.0 in stage 211.0 (TID 352) (172.18.0.13, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:16:19 INFO BlockManagerInfo: Added broadcast_314_piece0 in memory on 172.18.0.13:44401 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:16:19 INFO BlockManagerInfo: Added broadcast_313_piece0 in memory on 172.18.0.13:44401 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:16:20 INFO TaskSetManager: Finished task 0.0 in stage 211.0 (TID 352) in 640 ms on 172.18.0.13 (executor 0) (1/1)
26/01/04 17:16:20 INFO TaskSchedulerImpl: Removed TaskSet 211.0, whose tasks have all completed, from pool 
26/01/04 17:16:20 INFO DAGScheduler: ResultStage 211 (start at NativeMethodAccessorImpl.java:0) finished in 0.654 s
26/01/04 17:16:20 INFO DAGScheduler: Job 210 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:16:20 INFO TaskSchedulerImpl: Killing all running tasks in stage 211: Stage finished
26/01/04 17:16:20 INFO DAGScheduler: Job 210 finished: start at NativeMethodAccessorImpl.java:0, took 0.659217 s
26/01/04 17:16:20 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 103, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@55d53d20] is committing.
26/01/04 17:16:20 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 103, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@55d53d20] committed.
26/01/04 17:16:20 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/103 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.103.64312d8e-c0bb-42c8-a148-89ec12da565d.tmp
26/01/04 17:16:20 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.103.64312d8e-c0bb-42c8-a148-89ec12da565d.tmp to file:/tmp/spark-checkpoint-enrichment/commits/103
26/01/04 17:16:20 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T17:16:18.901Z",
  "batchId" : 103,
  "numInputRows" : 30,
  "inputRowsPerSecond" : 1764.705882352941,
  "processedRowsPerSecond" : 18.90359168241966,
  "durationMs" : {
    "addBatch" : 1118,
    "commitOffsets" : 153,
    "getBatch" : 0,
    "latestOffset" : 15,
    "queryPlanning" : 64,
    "triggerExecution" : 1587,
    "walCommit" : 236
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "0" : 2349
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "0" : 2379
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "0" : 2379
      }
    },
    "numInputRows" : 30,
    "inputRowsPerSecond" : 1764.705882352941,
    "processedRowsPerSecond" : 18.90359168241966,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 31
  }
}
26/01/04 17:16:29 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/104 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.104.c6394da8-23a7-41bc-807f-5d975cd29c5b.tmp
26/01/04 17:16:30 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.104.c6394da8-23a7-41bc-807f-5d975cd29c5b.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/104
26/01/04 17:16:30 INFO MicroBatchExecution: Committed offsets for batch 104. Metadata OffsetSeqMetadata(0,1767546989941,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 17:16:30 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:16:30 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:16:30 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:16:30 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:16:30 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:16:30 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:16:30 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#85099 - airline_prefix.nullCount#85098) > 0)
26/01/04 17:16:30 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#85134 - min_flight_num.nullCount#85133) > 0)
26/01/04 17:16:30 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#85129 - max_flight_num.nullCount#85128) > 0)
26/01/04 17:16:30 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:16:30 INFO DAGScheduler: Got job 211 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 17:16:30 INFO DAGScheduler: Final stage: ResultStage 212 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:16:30 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:16:30 INFO DAGScheduler: Missing parents: List()
26/01/04 17:16:30 INFO DAGScheduler: Submitting ResultStage 212 (MapPartitionsRDD[1062] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:16:30 INFO MemoryStore: Block broadcast_315 stored as values in memory (estimated size 38.5 KiB, free 434.2 MiB)
26/01/04 17:16:30 INFO MemoryStore: Block broadcast_315_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.2 MiB)
26/01/04 17:16:30 INFO BlockManagerInfo: Added broadcast_315_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:16:30 INFO SparkContext: Created broadcast 315 from broadcast at DAGScheduler.scala:1585
26/01/04 17:16:30 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 212 (MapPartitionsRDD[1062] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 17:16:30 INFO TaskSchedulerImpl: Adding task set 212.0 with 2 tasks resource profile 0
26/01/04 17:16:30 INFO TaskSetManager: Starting task 1.0 in stage 212.0 (TID 353) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 17:16:30 INFO TaskSetManager: Starting task 0.0 in stage 212.0 (TID 354) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 17:16:30 INFO BlockManagerInfo: Added broadcast_315_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:16:30 INFO BlockManagerInfo: Added broadcast_315_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:16:30 INFO TaskSetManager: Finished task 1.0 in stage 212.0 (TID 353) in 36 ms on 172.18.0.13 (executor 0) (1/2)
26/01/04 17:16:30 INFO TaskSetManager: Finished task 0.0 in stage 212.0 (TID 354) in 68 ms on 172.18.0.14 (executor 1) (2/2)
26/01/04 17:16:30 INFO TaskSchedulerImpl: Removed TaskSet 212.0, whose tasks have all completed, from pool 
26/01/04 17:16:30 INFO DAGScheduler: ResultStage 212 (start at NativeMethodAccessorImpl.java:0) finished in 0.076 s
26/01/04 17:16:30 INFO DAGScheduler: Job 211 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:16:30 INFO TaskSchedulerImpl: Killing all running tasks in stage 212: Stage finished
26/01/04 17:16:30 INFO DAGScheduler: Job 211 finished: start at NativeMethodAccessorImpl.java:0, took 0.079577 s
26/01/04 17:16:30 INFO MemoryStore: Block broadcast_316_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.2 MiB)
26/01/04 17:16:30 INFO BlockManagerInfo: Added broadcast_316_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:16:30 INFO SparkContext: Created broadcast 316 from start at NativeMethodAccessorImpl.java:0
26/01/04 17:16:30 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 104, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@35d6c0e2]. The input RDD has 1 partitions.
26/01/04 17:16:30 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:16:30 INFO DAGScheduler: Got job 212 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/01/04 17:16:30 INFO DAGScheduler: Final stage: ResultStage 213 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:16:30 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:16:30 INFO DAGScheduler: Missing parents: List()
26/01/04 17:16:30 INFO DAGScheduler: Submitting ResultStage 213 (MapPartitionsRDD[1067] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:16:30 INFO MemoryStore: Block broadcast_317 stored as values in memory (estimated size 52.9 KiB, free 434.2 MiB)
26/01/04 17:16:30 INFO MemoryStore: Block broadcast_317_piece0 stored as bytes in memory (estimated size 20.1 KiB, free 434.1 MiB)
26/01/04 17:16:30 INFO BlockManagerInfo: Added broadcast_317_piece0 in memory on spark-master:33535 (size: 20.1 KiB, free: 434.3 MiB)
26/01/04 17:16:30 INFO SparkContext: Created broadcast 317 from broadcast at DAGScheduler.scala:1585
26/01/04 17:16:30 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 213 (MapPartitionsRDD[1067] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/01/04 17:16:30 INFO TaskSchedulerImpl: Adding task set 213.0 with 1 tasks resource profile 0
26/01/04 17:16:30 INFO TaskSetManager: Starting task 0.0 in stage 213.0 (TID 355) (172.18.0.13, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:16:30 INFO BlockManagerInfo: Added broadcast_317_piece0 in memory on 172.18.0.13:44401 (size: 20.1 KiB, free: 434.3 MiB)
26/01/04 17:16:30 INFO BlockManagerInfo: Added broadcast_316_piece0 in memory on 172.18.0.13:44401 (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:16:30 INFO TaskSetManager: Finished task 0.0 in stage 213.0 (TID 355) in 709 ms on 172.18.0.13 (executor 0) (1/1)
26/01/04 17:16:30 INFO TaskSchedulerImpl: Removed TaskSet 213.0, whose tasks have all completed, from pool 
26/01/04 17:16:30 INFO DAGScheduler: ResultStage 213 (start at NativeMethodAccessorImpl.java:0) finished in 0.724 s
26/01/04 17:16:30 INFO DAGScheduler: Job 212 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:16:30 INFO TaskSchedulerImpl: Killing all running tasks in stage 213: Stage finished
26/01/04 17:16:30 INFO DAGScheduler: Job 212 finished: start at NativeMethodAccessorImpl.java:0, took 0.725740 s
26/01/04 17:16:30 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 104, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@35d6c0e2] is committing.
26/01/04 17:16:30 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 104, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@35d6c0e2] committed.
26/01/04 17:16:31 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/104 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.104.aa08475b-051d-48a3-8640-2d5c500f2502.tmp
26/01/04 17:16:31 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.104.aa08475b-051d-48a3-8640-2d5c500f2502.tmp to file:/tmp/spark-checkpoint-enrichment/commits/104
26/01/04 17:16:31 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T17:16:29.939Z",
  "batchId" : 104,
  "numInputRows" : 30,
  "inputRowsPerSecond" : 2727.2727272727275,
  "processedRowsPerSecond" : 24.77291494632535,
  "durationMs" : {
    "addBatch" : 900,
    "commitOffsets" : 159,
    "getBatch" : 0,
    "latestOffset" : 2,
    "queryPlanning" : 37,
    "triggerExecution" : 1211,
    "walCommit" : 112
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "0" : 2379
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "0" : 2409
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "0" : 2409
      }
    },
    "numInputRows" : 30,
    "inputRowsPerSecond" : 2727.2727272727275,
    "processedRowsPerSecond" : 24.77291494632535,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 31
  }
}
26/01/04 17:16:34 INFO BlockManagerInfo: Removed broadcast_314_piece0 on spark-master:33535 in memory (size: 20.2 KiB, free: 434.3 MiB)
26/01/04 17:16:34 INFO BlockManagerInfo: Removed broadcast_314_piece0 on 172.18.0.13:44401 in memory (size: 20.2 KiB, free: 434.3 MiB)
26/01/04 17:16:34 INFO BlockManagerInfo: Removed broadcast_313_piece0 on 172.18.0.13:44401 in memory (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:16:34 INFO BlockManagerInfo: Removed broadcast_313_piece0 on spark-master:33535 in memory (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:16:34 INFO BlockManagerInfo: Removed broadcast_315_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:16:34 INFO BlockManagerInfo: Removed broadcast_315_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:16:34 INFO BlockManagerInfo: Removed broadcast_315_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:16:34 INFO BlockManagerInfo: Removed broadcast_312_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:16:34 INFO BlockManagerInfo: Removed broadcast_312_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:16:34 INFO BlockManagerInfo: Removed broadcast_312_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:16:34 INFO BlockManagerInfo: Removed broadcast_317_piece0 on spark-master:33535 in memory (size: 20.1 KiB, free: 434.4 MiB)
26/01/04 17:16:34 INFO BlockManagerInfo: Removed broadcast_317_piece0 on 172.18.0.13:44401 in memory (size: 20.1 KiB, free: 434.4 MiB)
26/01/04 17:16:40 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/105 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.105.84383fdc-2022-46e5-8b61-119be7ec25a8.tmp
26/01/04 17:16:41 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.105.84383fdc-2022-46e5-8b61-119be7ec25a8.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/105
26/01/04 17:16:41 INFO MicroBatchExecution: Committed offsets for batch 105. Metadata OffsetSeqMetadata(0,1767547000952,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 17:16:41 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:16:41 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:16:41 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:16:41 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:16:41 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:16:41 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:16:41 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#85903 - airline_prefix.nullCount#85902) > 0)
26/01/04 17:16:41 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#85938 - min_flight_num.nullCount#85937) > 0)
26/01/04 17:16:41 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#85933 - max_flight_num.nullCount#85932) > 0)
26/01/04 17:16:41 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:16:41 INFO DAGScheduler: Got job 213 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 17:16:41 INFO DAGScheduler: Final stage: ResultStage 214 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:16:41 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:16:41 INFO DAGScheduler: Missing parents: List()
26/01/04 17:16:41 INFO DAGScheduler: Submitting ResultStage 214 (MapPartitionsRDD[1072] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:16:41 INFO MemoryStore: Block broadcast_318 stored as values in memory (estimated size 38.5 KiB, free 434.4 MiB)
26/01/04 17:16:41 INFO MemoryStore: Block broadcast_318_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.3 MiB)
26/01/04 17:16:41 INFO BlockManagerInfo: Removed broadcast_316_piece0 on spark-master:33535 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:16:41 INFO BlockManagerInfo: Added broadcast_318_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:16:41 INFO SparkContext: Created broadcast 318 from broadcast at DAGScheduler.scala:1585
26/01/04 17:16:41 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 214 (MapPartitionsRDD[1072] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 17:16:41 INFO TaskSchedulerImpl: Adding task set 214.0 with 2 tasks resource profile 0
26/01/04 17:16:41 INFO BlockManagerInfo: Removed broadcast_316_piece0 on 172.18.0.13:44401 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:16:41 INFO TaskSetManager: Starting task 0.0 in stage 214.0 (TID 356) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 17:16:41 INFO TaskSetManager: Starting task 1.0 in stage 214.0 (TID 357) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 17:16:41 INFO BlockManagerInfo: Added broadcast_318_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:16:41 INFO BlockManagerInfo: Added broadcast_318_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:16:41 INFO TaskSetManager: Finished task 1.0 in stage 214.0 (TID 357) in 19 ms on 172.18.0.13 (executor 0) (1/2)
26/01/04 17:16:41 INFO TaskSetManager: Finished task 0.0 in stage 214.0 (TID 356) in 40 ms on 172.18.0.14 (executor 1) (2/2)
26/01/04 17:16:41 INFO TaskSchedulerImpl: Removed TaskSet 214.0, whose tasks have all completed, from pool 
26/01/04 17:16:41 INFO DAGScheduler: ResultStage 214 (start at NativeMethodAccessorImpl.java:0) finished in 0.048 s
26/01/04 17:16:41 INFO DAGScheduler: Job 213 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:16:41 INFO TaskSchedulerImpl: Killing all running tasks in stage 214: Stage finished
26/01/04 17:16:41 INFO DAGScheduler: Job 213 finished: start at NativeMethodAccessorImpl.java:0, took 0.049810 s
26/01/04 17:16:41 INFO MemoryStore: Block broadcast_319_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.3 MiB)
26/01/04 17:16:41 INFO BlockManagerInfo: Added broadcast_319_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:16:41 INFO SparkContext: Created broadcast 319 from start at NativeMethodAccessorImpl.java:0
26/01/04 17:16:41 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 105, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@12cdf552]. The input RDD has 1 partitions.
26/01/04 17:16:41 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:16:41 INFO DAGScheduler: Got job 214 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/01/04 17:16:41 INFO DAGScheduler: Final stage: ResultStage 215 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:16:41 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:16:41 INFO DAGScheduler: Missing parents: List()
26/01/04 17:16:41 INFO DAGScheduler: Submitting ResultStage 215 (MapPartitionsRDD[1077] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:16:41 INFO MemoryStore: Block broadcast_320 stored as values in memory (estimated size 52.9 KiB, free 434.3 MiB)
26/01/04 17:16:41 INFO MemoryStore: Block broadcast_320_piece0 stored as bytes in memory (estimated size 20.2 KiB, free 434.3 MiB)
26/01/04 17:16:41 INFO BlockManagerInfo: Added broadcast_320_piece0 in memory on spark-master:33535 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:16:41 INFO SparkContext: Created broadcast 320 from broadcast at DAGScheduler.scala:1585
26/01/04 17:16:41 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 215 (MapPartitionsRDD[1077] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/01/04 17:16:41 INFO TaskSchedulerImpl: Adding task set 215.0 with 1 tasks resource profile 0
26/01/04 17:16:41 INFO TaskSetManager: Starting task 0.0 in stage 215.0 (TID 358) (172.18.0.13, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:16:41 INFO BlockManagerInfo: Added broadcast_320_piece0 in memory on 172.18.0.13:44401 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:16:41 INFO BlockManagerInfo: Added broadcast_319_piece0 in memory on 172.18.0.13:44401 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:16:41 INFO TaskSetManager: Finished task 0.0 in stage 215.0 (TID 358) in 549 ms on 172.18.0.13 (executor 0) (1/1)
26/01/04 17:16:41 INFO TaskSchedulerImpl: Removed TaskSet 215.0, whose tasks have all completed, from pool 
26/01/04 17:16:41 INFO DAGScheduler: ResultStage 215 (start at NativeMethodAccessorImpl.java:0) finished in 0.554 s
26/01/04 17:16:41 INFO DAGScheduler: Job 214 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:16:41 INFO TaskSchedulerImpl: Killing all running tasks in stage 215: Stage finished
26/01/04 17:16:41 INFO DAGScheduler: Job 214 finished: start at NativeMethodAccessorImpl.java:0, took 0.555309 s
26/01/04 17:16:41 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 105, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@12cdf552] is committing.
26/01/04 17:16:41 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 105, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@12cdf552] committed.
26/01/04 17:16:41 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/105 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.105.13959eba-4df1-42c5-b6bb-c7dc337b5f93.tmp
26/01/04 17:16:41 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.105.13959eba-4df1-42c5-b6bb-c7dc337b5f93.tmp to file:/tmp/spark-checkpoint-enrichment/commits/105
26/01/04 17:16:41 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T17:16:40.951Z",
  "batchId" : 105,
  "numInputRows" : 30,
  "inputRowsPerSecond" : 2500.0,
  "processedRowsPerSecond" : 35.37735849056604,
  "durationMs" : {
    "addBatch" : 667,
    "commitOffsets" : 60,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 36,
    "triggerExecution" : 848,
    "walCommit" : 83
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "0" : 2409
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "0" : 2439
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "0" : 2439
      }
    },
    "numInputRows" : 30,
    "inputRowsPerSecond" : 2500.0,
    "processedRowsPerSecond" : 35.37735849056604,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 31
  }
}
26/01/04 17:16:51 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/01/04 17:16:51 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/106 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.106.12984334-7176-43a0-9c6f-636e33b12a7a.tmp
26/01/04 17:16:52 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.106.12984334-7176-43a0-9c6f-636e33b12a7a.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/106
26/01/04 17:16:52 INFO MicroBatchExecution: Committed offsets for batch 106. Metadata OffsetSeqMetadata(0,1767547011973,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 17:16:52 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:16:52 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:16:52 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:16:52 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:16:52 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:16:52 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:16:52 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#86707 - airline_prefix.nullCount#86706) > 0)
26/01/04 17:16:52 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#86742 - min_flight_num.nullCount#86741) > 0)
26/01/04 17:16:52 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#86737 - max_flight_num.nullCount#86736) > 0)
26/01/04 17:16:52 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:16:52 INFO DAGScheduler: Got job 215 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 17:16:52 INFO DAGScheduler: Final stage: ResultStage 216 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:16:52 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:16:52 INFO DAGScheduler: Missing parents: List()
26/01/04 17:16:52 INFO DAGScheduler: Submitting ResultStage 216 (MapPartitionsRDD[1082] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:16:52 INFO MemoryStore: Block broadcast_321 stored as values in memory (estimated size 38.5 KiB, free 434.2 MiB)
26/01/04 17:16:52 INFO MemoryStore: Block broadcast_321_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.2 MiB)
26/01/04 17:16:52 INFO BlockManagerInfo: Added broadcast_321_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:16:52 INFO SparkContext: Created broadcast 321 from broadcast at DAGScheduler.scala:1585
26/01/04 17:16:52 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 216 (MapPartitionsRDD[1082] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 17:16:52 INFO TaskSchedulerImpl: Adding task set 216.0 with 2 tasks resource profile 0
26/01/04 17:16:52 INFO TaskSetManager: Starting task 1.0 in stage 216.0 (TID 359) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 17:16:52 INFO TaskSetManager: Starting task 0.0 in stage 216.0 (TID 360) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 17:16:52 INFO BlockManagerInfo: Added broadcast_321_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:16:52 INFO BlockManagerInfo: Added broadcast_321_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:16:52 INFO TaskSetManager: Finished task 1.0 in stage 216.0 (TID 359) in 32 ms on 172.18.0.13 (executor 0) (1/2)
26/01/04 17:16:52 INFO TaskSetManager: Finished task 0.0 in stage 216.0 (TID 360) in 73 ms on 172.18.0.14 (executor 1) (2/2)
26/01/04 17:16:52 INFO TaskSchedulerImpl: Removed TaskSet 216.0, whose tasks have all completed, from pool 
26/01/04 17:16:52 INFO DAGScheduler: ResultStage 216 (start at NativeMethodAccessorImpl.java:0) finished in 0.085 s
26/01/04 17:16:52 INFO DAGScheduler: Job 215 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:16:52 INFO TaskSchedulerImpl: Killing all running tasks in stage 216: Stage finished
26/01/04 17:16:52 INFO DAGScheduler: Job 215 finished: start at NativeMethodAccessorImpl.java:0, took 0.086752 s
26/01/04 17:16:52 INFO MemoryStore: Block broadcast_322_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.2 MiB)
26/01/04 17:16:52 INFO BlockManagerInfo: Added broadcast_322_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:16:52 INFO SparkContext: Created broadcast 322 from start at NativeMethodAccessorImpl.java:0
26/01/04 17:16:52 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 106, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@414ba694]. The input RDD has 1 partitions.
26/01/04 17:16:52 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:16:52 INFO DAGScheduler: Got job 216 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/01/04 17:16:52 INFO DAGScheduler: Final stage: ResultStage 217 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:16:52 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:16:52 INFO DAGScheduler: Missing parents: List()
26/01/04 17:16:52 INFO DAGScheduler: Submitting ResultStage 217 (MapPartitionsRDD[1087] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:16:52 INFO MemoryStore: Block broadcast_323 stored as values in memory (estimated size 52.9 KiB, free 434.2 MiB)
26/01/04 17:16:52 INFO MemoryStore: Block broadcast_323_piece0 stored as bytes in memory (estimated size 20.1 KiB, free 434.1 MiB)
26/01/04 17:16:52 INFO BlockManagerInfo: Added broadcast_323_piece0 in memory on spark-master:33535 (size: 20.1 KiB, free: 434.3 MiB)
26/01/04 17:16:52 INFO SparkContext: Created broadcast 323 from broadcast at DAGScheduler.scala:1585
26/01/04 17:16:52 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 217 (MapPartitionsRDD[1087] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/01/04 17:16:52 INFO TaskSchedulerImpl: Adding task set 217.0 with 1 tasks resource profile 0
26/01/04 17:16:52 INFO TaskSetManager: Starting task 0.0 in stage 217.0 (TID 361) (172.18.0.13, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:16:52 INFO BlockManagerInfo: Added broadcast_323_piece0 in memory on 172.18.0.13:44401 (size: 20.1 KiB, free: 434.3 MiB)
26/01/04 17:16:52 INFO BlockManagerInfo: Added broadcast_322_piece0 in memory on 172.18.0.13:44401 (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:16:52 INFO TaskSetManager: Finished task 0.0 in stage 217.0 (TID 361) in 579 ms on 172.18.0.13 (executor 0) (1/1)
26/01/04 17:16:52 INFO TaskSchedulerImpl: Removed TaskSet 217.0, whose tasks have all completed, from pool 
26/01/04 17:16:52 INFO DAGScheduler: ResultStage 217 (start at NativeMethodAccessorImpl.java:0) finished in 0.584 s
26/01/04 17:16:52 INFO DAGScheduler: Job 216 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:16:52 INFO TaskSchedulerImpl: Killing all running tasks in stage 217: Stage finished
26/01/04 17:16:52 INFO DAGScheduler: Job 216 finished: start at NativeMethodAccessorImpl.java:0, took 0.586096 s
26/01/04 17:16:52 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 106, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@414ba694] is committing.
26/01/04 17:16:52 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 106, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@414ba694] committed.
26/01/04 17:16:52 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/106 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.106.57e5e591-7ce8-4ede-9797-2653dbd1a7b7.tmp
26/01/04 17:16:52 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.106.57e5e591-7ce8-4ede-9797-2653dbd1a7b7.tmp to file:/tmp/spark-checkpoint-enrichment/commits/106
26/01/04 17:16:52 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T17:16:51.971Z",
  "batchId" : 106,
  "numInputRows" : 30,
  "inputRowsPerSecond" : 2500.0,
  "processedRowsPerSecond" : 30.706243602865918,
  "durationMs" : {
    "addBatch" : 761,
    "commitOffsets" : 85,
    "getBatch" : 0,
    "latestOffset" : 2,
    "queryPlanning" : 14,
    "triggerExecution" : 977,
    "walCommit" : 114
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "0" : 2439
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "0" : 2469
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "0" : 2469
      }
    },
    "numInputRows" : 30,
    "inputRowsPerSecond" : 2500.0,
    "processedRowsPerSecond" : 30.706243602865918,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 31
  }
}
26/01/04 17:16:54 INFO BlockManagerInfo: Removed broadcast_320_piece0 on spark-master:33535 in memory (size: 20.2 KiB, free: 434.3 MiB)
26/01/04 17:16:54 INFO BlockManagerInfo: Removed broadcast_320_piece0 on 172.18.0.13:44401 in memory (size: 20.2 KiB, free: 434.3 MiB)
26/01/04 17:16:54 INFO BlockManagerInfo: Removed broadcast_319_piece0 on spark-master:33535 in memory (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:16:54 INFO BlockManagerInfo: Removed broadcast_319_piece0 on 172.18.0.13:44401 in memory (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:16:54 INFO BlockManagerInfo: Removed broadcast_323_piece0 on spark-master:33535 in memory (size: 20.1 KiB, free: 434.4 MiB)
26/01/04 17:16:54 INFO BlockManagerInfo: Removed broadcast_323_piece0 on 172.18.0.13:44401 in memory (size: 20.1 KiB, free: 434.4 MiB)
26/01/04 17:16:54 INFO BlockManagerInfo: Removed broadcast_318_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:16:54 INFO BlockManagerInfo: Removed broadcast_318_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:16:54 INFO BlockManagerInfo: Removed broadcast_318_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:16:54 INFO BlockManagerInfo: Removed broadcast_321_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:16:54 INFO BlockManagerInfo: Removed broadcast_321_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:16:54 INFO BlockManagerInfo: Removed broadcast_321_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:17:02 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/01/04 17:17:03 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/107 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.107.61c02a16-4a6d-44a5-8d45-51f2f81b5c63.tmp
26/01/04 17:17:03 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.107.61c02a16-4a6d-44a5-8d45-51f2f81b5c63.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/107
26/01/04 17:17:03 INFO MicroBatchExecution: Committed offsets for batch 107. Metadata OffsetSeqMetadata(0,1767547022982,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 17:17:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:17:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:17:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:17:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:17:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:17:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:17:03 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#87511 - airline_prefix.nullCount#87510) > 0)
26/01/04 17:17:03 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#87546 - min_flight_num.nullCount#87545) > 0)
26/01/04 17:17:03 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#87541 - max_flight_num.nullCount#87540) > 0)
26/01/04 17:17:03 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:17:03 INFO DAGScheduler: Got job 217 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 17:17:03 INFO DAGScheduler: Final stage: ResultStage 218 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:17:03 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:17:03 INFO DAGScheduler: Missing parents: List()
26/01/04 17:17:03 INFO DAGScheduler: Submitting ResultStage 218 (MapPartitionsRDD[1092] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:17:03 INFO MemoryStore: Block broadcast_324 stored as values in memory (estimated size 38.5 KiB, free 434.4 MiB)
26/01/04 17:17:03 INFO MemoryStore: Block broadcast_324_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.3 MiB)
26/01/04 17:17:03 INFO BlockManagerInfo: Removed broadcast_322_piece0 on spark-master:33535 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:17:03 INFO BlockManagerInfo: Added broadcast_324_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:17:03 INFO SparkContext: Created broadcast 324 from broadcast at DAGScheduler.scala:1585
26/01/04 17:17:03 INFO BlockManagerInfo: Removed broadcast_322_piece0 on 172.18.0.13:44401 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:17:03 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 218 (MapPartitionsRDD[1092] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 17:17:03 INFO TaskSchedulerImpl: Adding task set 218.0 with 2 tasks resource profile 0
26/01/04 17:17:03 INFO TaskSetManager: Starting task 1.0 in stage 218.0 (TID 362) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 17:17:03 INFO TaskSetManager: Starting task 0.0 in stage 218.0 (TID 363) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 17:17:03 INFO BlockManagerInfo: Added broadcast_324_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:17:03 INFO BlockManagerInfo: Added broadcast_324_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:17:03 INFO TaskSetManager: Finished task 0.0 in stage 218.0 (TID 363) in 113 ms on 172.18.0.14 (executor 1) (1/2)
26/01/04 17:17:03 INFO TaskSetManager: Finished task 1.0 in stage 218.0 (TID 362) in 137 ms on 172.18.0.13 (executor 0) (2/2)
26/01/04 17:17:03 INFO TaskSchedulerImpl: Removed TaskSet 218.0, whose tasks have all completed, from pool 
26/01/04 17:17:03 INFO DAGScheduler: ResultStage 218 (start at NativeMethodAccessorImpl.java:0) finished in 0.168 s
26/01/04 17:17:03 INFO DAGScheduler: Job 217 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:17:03 INFO TaskSchedulerImpl: Killing all running tasks in stage 218: Stage finished
26/01/04 17:17:03 INFO DAGScheduler: Job 217 finished: start at NativeMethodAccessorImpl.java:0, took 0.206815 s
26/01/04 17:17:03 INFO MemoryStore: Block broadcast_325_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.3 MiB)
26/01/04 17:17:03 INFO BlockManagerInfo: Added broadcast_325_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:17:03 INFO SparkContext: Created broadcast 325 from start at NativeMethodAccessorImpl.java:0
26/01/04 17:17:03 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 107, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@de8fb9b]. The input RDD has 1 partitions.
26/01/04 17:17:03 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:17:03 INFO DAGScheduler: Got job 218 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/01/04 17:17:03 INFO DAGScheduler: Final stage: ResultStage 219 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:17:03 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:17:03 INFO DAGScheduler: Missing parents: List()
26/01/04 17:17:03 INFO DAGScheduler: Submitting ResultStage 219 (MapPartitionsRDD[1097] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:17:03 INFO MemoryStore: Block broadcast_326 stored as values in memory (estimated size 52.9 KiB, free 434.3 MiB)
26/01/04 17:17:03 INFO MemoryStore: Block broadcast_326_piece0 stored as bytes in memory (estimated size 20.2 KiB, free 434.3 MiB)
26/01/04 17:17:03 INFO BlockManagerInfo: Added broadcast_326_piece0 in memory on spark-master:33535 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:17:03 INFO SparkContext: Created broadcast 326 from broadcast at DAGScheduler.scala:1585
26/01/04 17:17:03 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 219 (MapPartitionsRDD[1097] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/01/04 17:17:03 INFO TaskSchedulerImpl: Adding task set 219.0 with 1 tasks resource profile 0
26/01/04 17:17:03 INFO TaskSetManager: Starting task 0.0 in stage 219.0 (TID 364) (172.18.0.13, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:17:03 INFO BlockManagerInfo: Added broadcast_326_piece0 in memory on 172.18.0.13:44401 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:17:03 INFO BlockManagerInfo: Added broadcast_325_piece0 in memory on 172.18.0.13:44401 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:17:04 INFO TaskSetManager: Finished task 0.0 in stage 219.0 (TID 364) in 746 ms on 172.18.0.13 (executor 0) (1/1)
26/01/04 17:17:04 INFO TaskSchedulerImpl: Removed TaskSet 219.0, whose tasks have all completed, from pool 
26/01/04 17:17:04 INFO DAGScheduler: ResultStage 219 (start at NativeMethodAccessorImpl.java:0) finished in 0.787 s
26/01/04 17:17:04 INFO DAGScheduler: Job 218 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:17:04 INFO TaskSchedulerImpl: Killing all running tasks in stage 219: Stage finished
26/01/04 17:17:04 INFO DAGScheduler: Job 218 finished: start at NativeMethodAccessorImpl.java:0, took 0.800779 s
26/01/04 17:17:04 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 107, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@de8fb9b] is committing.
26/01/04 17:17:04 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 107, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@de8fb9b] committed.
26/01/04 17:17:04 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/107 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.107.b96bc5df-5492-4db4-9601-d67d8938870b.tmp
26/01/04 17:17:04 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.107.b96bc5df-5492-4db4-9601-d67d8938870b.tmp to file:/tmp/spark-checkpoint-enrichment/commits/107
26/01/04 17:17:04 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T17:17:02.975Z",
  "batchId" : 107,
  "numInputRows" : 30,
  "inputRowsPerSecond" : 857.1428571428571,
  "processedRowsPerSecond" : 17.411491584445734,
  "durationMs" : {
    "addBatch" : 1203,
    "commitOffsets" : 215,
    "getBatch" : 0,
    "latestOffset" : 7,
    "queryPlanning" : 64,
    "triggerExecution" : 1723,
    "walCommit" : 233
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "0" : 2469
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "0" : 2499
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "0" : 2499
      }
    },
    "numInputRows" : 30,
    "inputRowsPerSecond" : 857.1428571428571,
    "processedRowsPerSecond" : 17.411491584445734,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 31
  }
}
26/01/04 17:17:14 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/108 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.108.67e22e65-dac2-4e67-965b-0436104dbd98.tmp
26/01/04 17:17:14 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.108.67e22e65-dac2-4e67-965b-0436104dbd98.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/108
26/01/04 17:17:14 INFO MicroBatchExecution: Committed offsets for batch 108. Metadata OffsetSeqMetadata(0,1767547034057,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 17:17:14 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:17:14 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:17:14 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:17:14 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:17:14 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:17:14 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:17:14 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#88315 - airline_prefix.nullCount#88314) > 0)
26/01/04 17:17:14 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#88350 - min_flight_num.nullCount#88349) > 0)
26/01/04 17:17:14 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#88345 - max_flight_num.nullCount#88344) > 0)
26/01/04 17:17:14 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:17:14 INFO DAGScheduler: Got job 219 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 17:17:14 INFO DAGScheduler: Final stage: ResultStage 220 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:17:14 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:17:14 INFO DAGScheduler: Missing parents: List()
26/01/04 17:17:14 INFO DAGScheduler: Submitting ResultStage 220 (MapPartitionsRDD[1102] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:17:14 INFO MemoryStore: Block broadcast_327 stored as values in memory (estimated size 38.5 KiB, free 434.2 MiB)
26/01/04 17:17:14 INFO MemoryStore: Block broadcast_327_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.2 MiB)
26/01/04 17:17:14 INFO BlockManagerInfo: Added broadcast_327_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:17:14 INFO SparkContext: Created broadcast 327 from broadcast at DAGScheduler.scala:1585
26/01/04 17:17:14 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 220 (MapPartitionsRDD[1102] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 17:17:14 INFO TaskSchedulerImpl: Adding task set 220.0 with 2 tasks resource profile 0
26/01/04 17:17:14 INFO TaskSetManager: Starting task 1.0 in stage 220.0 (TID 365) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 17:17:14 INFO TaskSetManager: Starting task 0.0 in stage 220.0 (TID 366) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 17:17:14 INFO BlockManagerInfo: Added broadcast_327_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:17:14 INFO TaskSetManager: Finished task 1.0 in stage 220.0 (TID 365) in 44 ms on 172.18.0.13 (executor 0) (1/2)
26/01/04 17:17:14 INFO BlockManagerInfo: Added broadcast_327_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:17:14 INFO TaskSetManager: Finished task 0.0 in stage 220.0 (TID 366) in 117 ms on 172.18.0.14 (executor 1) (2/2)
26/01/04 17:17:14 INFO TaskSchedulerImpl: Removed TaskSet 220.0, whose tasks have all completed, from pool 
26/01/04 17:17:14 INFO DAGScheduler: ResultStage 220 (start at NativeMethodAccessorImpl.java:0) finished in 0.126 s
26/01/04 17:17:14 INFO DAGScheduler: Job 219 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:17:14 INFO TaskSchedulerImpl: Killing all running tasks in stage 220: Stage finished
26/01/04 17:17:14 INFO DAGScheduler: Job 219 finished: start at NativeMethodAccessorImpl.java:0, took 0.164276 s
26/01/04 17:17:14 INFO MemoryStore: Block broadcast_328_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.2 MiB)
26/01/04 17:17:14 INFO BlockManagerInfo: Added broadcast_328_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:17:14 INFO SparkContext: Created broadcast 328 from start at NativeMethodAccessorImpl.java:0
26/01/04 17:17:14 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 108, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@6a9c4cb5]. The input RDD has 1 partitions.
26/01/04 17:17:14 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:17:14 INFO DAGScheduler: Got job 220 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/01/04 17:17:14 INFO DAGScheduler: Final stage: ResultStage 221 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:17:14 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:17:14 INFO DAGScheduler: Missing parents: List()
26/01/04 17:17:14 INFO DAGScheduler: Submitting ResultStage 221 (MapPartitionsRDD[1107] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:17:14 INFO MemoryStore: Block broadcast_329 stored as values in memory (estimated size 52.9 KiB, free 434.2 MiB)
26/01/04 17:17:14 INFO MemoryStore: Block broadcast_329_piece0 stored as bytes in memory (estimated size 20.2 KiB, free 434.1 MiB)
26/01/04 17:17:14 INFO BlockManagerInfo: Added broadcast_329_piece0 in memory on spark-master:33535 (size: 20.2 KiB, free: 434.3 MiB)
26/01/04 17:17:14 INFO SparkContext: Created broadcast 329 from broadcast at DAGScheduler.scala:1585
26/01/04 17:17:14 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 221 (MapPartitionsRDD[1107] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/01/04 17:17:14 INFO TaskSchedulerImpl: Adding task set 221.0 with 1 tasks resource profile 0
26/01/04 17:17:14 INFO TaskSetManager: Starting task 0.0 in stage 221.0 (TID 367) (172.18.0.13, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:17:14 INFO BlockManagerInfo: Added broadcast_329_piece0 in memory on 172.18.0.13:44401 (size: 20.2 KiB, free: 434.3 MiB)
26/01/04 17:17:14 INFO BlockManagerInfo: Added broadcast_328_piece0 in memory on 172.18.0.13:44401 (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:17:15 INFO TaskSetManager: Finished task 0.0 in stage 221.0 (TID 367) in 752 ms on 172.18.0.13 (executor 0) (1/1)
26/01/04 17:17:15 INFO DAGScheduler: ResultStage 221 (start at NativeMethodAccessorImpl.java:0) finished in 0.766 s
26/01/04 17:17:15 INFO TaskSchedulerImpl: Removed TaskSet 221.0, whose tasks have all completed, from pool 
26/01/04 17:17:15 INFO DAGScheduler: Job 220 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:17:15 INFO TaskSchedulerImpl: Killing all running tasks in stage 221: Stage finished
26/01/04 17:17:15 INFO DAGScheduler: Job 220 finished: start at NativeMethodAccessorImpl.java:0, took 0.780154 s
26/01/04 17:17:15 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 108, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@6a9c4cb5] is committing.
26/01/04 17:17:15 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 108, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@6a9c4cb5] committed.
26/01/04 17:17:15 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/108 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.108.8052f7f2-776f-473b-8cfd-89503c7dddfc.tmp
26/01/04 17:17:15 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.108.8052f7f2-776f-473b-8cfd-89503c7dddfc.tmp to file:/tmp/spark-checkpoint-enrichment/commits/108
26/01/04 17:17:15 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T17:17:13.984Z",
  "batchId" : 108,
  "numInputRows" : 30,
  "inputRowsPerSecond" : 2142.8571428571427,
  "processedRowsPerSecond" : 16.741071428571427,
  "durationMs" : {
    "addBatch" : 1069,
    "commitOffsets" : 280,
    "getBatch" : 0,
    "latestOffset" : 73,
    "queryPlanning" : 123,
    "triggerExecution" : 1792,
    "walCommit" : 246
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "0" : 2499
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "0" : 2529
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "0" : 2529
      }
    },
    "numInputRows" : 30,
    "inputRowsPerSecond" : 2142.8571428571427,
    "processedRowsPerSecond" : 16.741071428571427,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 31
  }
}
26/01/04 17:17:21 INFO BlockManagerInfo: Removed broadcast_326_piece0 on spark-master:33535 in memory (size: 20.2 KiB, free: 434.3 MiB)
26/01/04 17:17:21 INFO BlockManagerInfo: Removed broadcast_326_piece0 on 172.18.0.13:44401 in memory (size: 20.2 KiB, free: 434.3 MiB)
26/01/04 17:17:21 INFO BlockManagerInfo: Removed broadcast_325_piece0 on spark-master:33535 in memory (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:17:21 INFO BlockManagerInfo: Removed broadcast_325_piece0 on 172.18.0.13:44401 in memory (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:17:21 INFO BlockManagerInfo: Removed broadcast_329_piece0 on spark-master:33535 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:17:21 INFO BlockManagerInfo: Removed broadcast_329_piece0 on 172.18.0.13:44401 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:17:21 INFO BlockManagerInfo: Removed broadcast_324_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:17:21 INFO BlockManagerInfo: Removed broadcast_324_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:17:21 INFO BlockManagerInfo: Removed broadcast_324_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:17:21 INFO BlockManagerInfo: Removed broadcast_327_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:17:21 INFO BlockManagerInfo: Removed broadcast_327_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:17:21 INFO BlockManagerInfo: Removed broadcast_327_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:17:25 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/109 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.109.ea335cdd-a69d-4dcd-8b4b-4b4cf22b4c30.tmp
26/01/04 17:17:25 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.109.ea335cdd-a69d-4dcd-8b4b-4b4cf22b4c30.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/109
26/01/04 17:17:25 INFO MicroBatchExecution: Committed offsets for batch 109. Metadata OffsetSeqMetadata(0,1767547045073,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 17:17:25 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:17:25 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:17:25 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:17:25 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:17:25 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:17:25 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:17:25 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#89119 - airline_prefix.nullCount#89118) > 0)
26/01/04 17:17:25 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#89154 - min_flight_num.nullCount#89153) > 0)
26/01/04 17:17:25 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#89149 - max_flight_num.nullCount#89148) > 0)
26/01/04 17:17:25 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:17:25 INFO DAGScheduler: Got job 221 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 17:17:25 INFO DAGScheduler: Final stage: ResultStage 222 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:17:25 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:17:25 INFO DAGScheduler: Missing parents: List()
26/01/04 17:17:25 INFO DAGScheduler: Submitting ResultStage 222 (MapPartitionsRDD[1112] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:17:25 INFO MemoryStore: Block broadcast_330 stored as values in memory (estimated size 38.5 KiB, free 434.4 MiB)
26/01/04 17:17:25 INFO MemoryStore: Block broadcast_330_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.3 MiB)
26/01/04 17:17:25 INFO BlockManagerInfo: Added broadcast_330_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:17:25 INFO BlockManagerInfo: Removed broadcast_328_piece0 on spark-master:33535 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:17:25 INFO SparkContext: Created broadcast 330 from broadcast at DAGScheduler.scala:1585
26/01/04 17:17:25 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 222 (MapPartitionsRDD[1112] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 17:17:25 INFO TaskSchedulerImpl: Adding task set 222.0 with 2 tasks resource profile 0
26/01/04 17:17:25 INFO TaskSetManager: Starting task 1.0 in stage 222.0 (TID 368) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 17:17:25 INFO TaskSetManager: Starting task 0.0 in stage 222.0 (TID 369) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 17:17:25 INFO BlockManagerInfo: Added broadcast_330_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:17:25 INFO TaskSetManager: Finished task 1.0 in stage 222.0 (TID 368) in 48 ms on 172.18.0.13 (executor 0) (1/2)
26/01/04 17:17:25 INFO BlockManagerInfo: Removed broadcast_328_piece0 on 172.18.0.13:44401 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:17:25 INFO BlockManagerInfo: Added broadcast_330_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:17:25 INFO TaskSetManager: Finished task 0.0 in stage 222.0 (TID 369) in 221 ms on 172.18.0.14 (executor 1) (2/2)
26/01/04 17:17:25 INFO TaskSchedulerImpl: Removed TaskSet 222.0, whose tasks have all completed, from pool 
26/01/04 17:17:25 INFO DAGScheduler: ResultStage 222 (start at NativeMethodAccessorImpl.java:0) finished in 0.261 s
26/01/04 17:17:25 INFO DAGScheduler: Job 221 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:17:25 INFO TaskSchedulerImpl: Killing all running tasks in stage 222: Stage finished
26/01/04 17:17:25 INFO DAGScheduler: Job 221 finished: start at NativeMethodAccessorImpl.java:0, took 0.269881 s
26/01/04 17:17:25 INFO MemoryStore: Block broadcast_331_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.3 MiB)
26/01/04 17:17:25 INFO BlockManagerInfo: Added broadcast_331_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:17:25 INFO SparkContext: Created broadcast 331 from start at NativeMethodAccessorImpl.java:0
26/01/04 17:17:25 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 109, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@42733dd4]. The input RDD has 1 partitions.
26/01/04 17:17:25 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:17:25 INFO DAGScheduler: Got job 222 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/01/04 17:17:25 INFO DAGScheduler: Final stage: ResultStage 223 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:17:25 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:17:25 INFO DAGScheduler: Missing parents: List()
26/01/04 17:17:25 INFO DAGScheduler: Submitting ResultStage 223 (MapPartitionsRDD[1117] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:17:25 INFO MemoryStore: Block broadcast_332 stored as values in memory (estimated size 52.9 KiB, free 434.3 MiB)
26/01/04 17:17:25 INFO MemoryStore: Block broadcast_332_piece0 stored as bytes in memory (estimated size 20.1 KiB, free 434.3 MiB)
26/01/04 17:17:25 INFO BlockManagerInfo: Added broadcast_332_piece0 in memory on spark-master:33535 (size: 20.1 KiB, free: 434.4 MiB)
26/01/04 17:17:25 INFO SparkContext: Created broadcast 332 from broadcast at DAGScheduler.scala:1585
26/01/04 17:17:25 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 223 (MapPartitionsRDD[1117] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/01/04 17:17:25 INFO TaskSchedulerImpl: Adding task set 223.0 with 1 tasks resource profile 0
26/01/04 17:17:25 INFO TaskSetManager: Starting task 0.0 in stage 223.0 (TID 370) (172.18.0.13, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:17:25 INFO BlockManagerInfo: Added broadcast_332_piece0 in memory on 172.18.0.13:44401 (size: 20.1 KiB, free: 434.4 MiB)
26/01/04 17:17:25 INFO BlockManagerInfo: Added broadcast_331_piece0 in memory on 172.18.0.13:44401 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:17:26 INFO TaskSetManager: Finished task 0.0 in stage 223.0 (TID 370) in 679 ms on 172.18.0.13 (executor 0) (1/1)
26/01/04 17:17:26 INFO TaskSchedulerImpl: Removed TaskSet 223.0, whose tasks have all completed, from pool 
26/01/04 17:17:26 INFO DAGScheduler: ResultStage 223 (start at NativeMethodAccessorImpl.java:0) finished in 0.694 s
26/01/04 17:17:26 INFO DAGScheduler: Job 222 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:17:26 INFO TaskSchedulerImpl: Killing all running tasks in stage 223: Stage finished
26/01/04 17:17:26 INFO DAGScheduler: Job 222 finished: start at NativeMethodAccessorImpl.java:0, took 0.698894 s
26/01/04 17:17:26 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 109, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@42733dd4] is committing.
26/01/04 17:17:26 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 109, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@42733dd4] committed.
26/01/04 17:17:26 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/109 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.109.b3ad5f7a-a009-408e-97b8-d7f1e95a400b.tmp
26/01/04 17:17:26 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.109.b3ad5f7a-a009-408e-97b8-d7f1e95a400b.tmp to file:/tmp/spark-checkpoint-enrichment/commits/109
26/01/04 17:17:26 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T17:17:25.070Z",
  "batchId" : 109,
  "numInputRows" : 30,
  "inputRowsPerSecond" : 2000.0,
  "processedRowsPerSecond" : 18.726591760299623,
  "durationMs" : {
    "addBatch" : 1121,
    "commitOffsets" : 219,
    "getBatch" : 0,
    "latestOffset" : 3,
    "queryPlanning" : 46,
    "triggerExecution" : 1602,
    "walCommit" : 212
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "0" : 2529
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "0" : 2559
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "0" : 2559
      }
    },
    "numInputRows" : 30,
    "inputRowsPerSecond" : 2000.0,
    "processedRowsPerSecond" : 18.726591760299623,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 31
  }
}
26/01/04 17:17:36 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/110 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.110.f60274f8-a4dd-4647-99e0-9fb146a448d3.tmp
26/01/04 17:17:36 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.110.f60274f8-a4dd-4647-99e0-9fb146a448d3.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/110
26/01/04 17:17:36 INFO MicroBatchExecution: Committed offsets for batch 110. Metadata OffsetSeqMetadata(0,1767547056101,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 17:17:36 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:17:36 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:17:36 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:17:36 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:17:36 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:17:36 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:17:36 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#89923 - airline_prefix.nullCount#89922) > 0)
26/01/04 17:17:36 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#89958 - min_flight_num.nullCount#89957) > 0)
26/01/04 17:17:36 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#89953 - max_flight_num.nullCount#89952) > 0)
26/01/04 17:17:36 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:17:36 INFO DAGScheduler: Got job 223 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 17:17:36 INFO DAGScheduler: Final stage: ResultStage 224 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:17:36 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:17:36 INFO DAGScheduler: Missing parents: List()
26/01/04 17:17:36 INFO DAGScheduler: Submitting ResultStage 224 (MapPartitionsRDD[1122] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:17:36 INFO MemoryStore: Block broadcast_333 stored as values in memory (estimated size 38.5 KiB, free 434.2 MiB)
26/01/04 17:17:36 INFO MemoryStore: Block broadcast_333_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.2 MiB)
26/01/04 17:17:36 INFO BlockManagerInfo: Added broadcast_333_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:17:36 INFO SparkContext: Created broadcast 333 from broadcast at DAGScheduler.scala:1585
26/01/04 17:17:36 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 224 (MapPartitionsRDD[1122] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 17:17:36 INFO TaskSchedulerImpl: Adding task set 224.0 with 2 tasks resource profile 0
26/01/04 17:17:36 INFO TaskSetManager: Starting task 0.0 in stage 224.0 (TID 371) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 17:17:36 INFO TaskSetManager: Starting task 1.0 in stage 224.0 (TID 372) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 17:17:36 INFO BlockManagerInfo: Added broadcast_333_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:17:36 INFO BlockManagerInfo: Added broadcast_333_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:17:36 INFO TaskSetManager: Finished task 1.0 in stage 224.0 (TID 372) in 20 ms on 172.18.0.13 (executor 0) (1/2)
26/01/04 17:17:36 INFO TaskSetManager: Finished task 0.0 in stage 224.0 (TID 371) in 41 ms on 172.18.0.14 (executor 1) (2/2)
26/01/04 17:17:36 INFO TaskSchedulerImpl: Removed TaskSet 224.0, whose tasks have all completed, from pool 
26/01/04 17:17:36 INFO DAGScheduler: ResultStage 224 (start at NativeMethodAccessorImpl.java:0) finished in 0.047 s
26/01/04 17:17:36 INFO DAGScheduler: Job 223 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:17:36 INFO TaskSchedulerImpl: Killing all running tasks in stage 224: Stage finished
26/01/04 17:17:36 INFO DAGScheduler: Job 223 finished: start at NativeMethodAccessorImpl.java:0, took 0.055648 s
26/01/04 17:17:36 INFO MemoryStore: Block broadcast_334_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.2 MiB)
26/01/04 17:17:36 INFO BlockManagerInfo: Added broadcast_334_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:17:36 INFO SparkContext: Created broadcast 334 from start at NativeMethodAccessorImpl.java:0
26/01/04 17:17:36 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 110, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@4f3a1cda]. The input RDD has 1 partitions.
26/01/04 17:17:36 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:17:36 INFO DAGScheduler: Got job 224 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/01/04 17:17:36 INFO DAGScheduler: Final stage: ResultStage 225 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:17:36 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:17:36 INFO DAGScheduler: Missing parents: List()
26/01/04 17:17:36 INFO DAGScheduler: Submitting ResultStage 225 (MapPartitionsRDD[1127] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:17:36 INFO MemoryStore: Block broadcast_335 stored as values in memory (estimated size 52.9 KiB, free 434.2 MiB)
26/01/04 17:17:36 INFO MemoryStore: Block broadcast_335_piece0 stored as bytes in memory (estimated size 20.1 KiB, free 434.1 MiB)
26/01/04 17:17:36 INFO BlockManagerInfo: Added broadcast_335_piece0 in memory on spark-master:33535 (size: 20.1 KiB, free: 434.3 MiB)
26/01/04 17:17:36 INFO SparkContext: Created broadcast 335 from broadcast at DAGScheduler.scala:1585
26/01/04 17:17:36 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 225 (MapPartitionsRDD[1127] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/01/04 17:17:36 INFO TaskSchedulerImpl: Adding task set 225.0 with 1 tasks resource profile 0
26/01/04 17:17:36 INFO TaskSetManager: Starting task 0.0 in stage 225.0 (TID 373) (172.18.0.13, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:17:36 INFO BlockManagerInfo: Added broadcast_335_piece0 in memory on 172.18.0.13:44401 (size: 20.1 KiB, free: 434.3 MiB)
26/01/04 17:17:36 INFO BlockManagerInfo: Added broadcast_334_piece0 in memory on 172.18.0.13:44401 (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:17:36 INFO TaskSetManager: Finished task 0.0 in stage 225.0 (TID 373) in 566 ms on 172.18.0.13 (executor 0) (1/1)
26/01/04 17:17:36 INFO TaskSchedulerImpl: Removed TaskSet 225.0, whose tasks have all completed, from pool 
26/01/04 17:17:36 INFO DAGScheduler: ResultStage 225 (start at NativeMethodAccessorImpl.java:0) finished in 0.572 s
26/01/04 17:17:36 INFO DAGScheduler: Job 224 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:17:36 INFO TaskSchedulerImpl: Killing all running tasks in stage 225: Stage finished
26/01/04 17:17:36 INFO DAGScheduler: Job 224 finished: start at NativeMethodAccessorImpl.java:0, took 0.574195 s
26/01/04 17:17:36 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 110, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@4f3a1cda] is committing.
26/01/04 17:17:36 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 110, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@4f3a1cda] committed.
26/01/04 17:17:36 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/110 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.110.b17b8209-cd0e-4fea-8449-3a265fdf0b96.tmp
26/01/04 17:17:36 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.110.b17b8209-cd0e-4fea-8449-3a265fdf0b96.tmp to file:/tmp/spark-checkpoint-enrichment/commits/110
26/01/04 17:17:36 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T17:17:36.099Z",
  "batchId" : 110,
  "numInputRows" : 30,
  "inputRowsPerSecond" : 2500.0,
  "processedRowsPerSecond" : 34.92433061699651,
  "durationMs" : {
    "addBatch" : 692,
    "commitOffsets" : 69,
    "getBatch" : 0,
    "latestOffset" : 2,
    "queryPlanning" : 17,
    "triggerExecution" : 859,
    "walCommit" : 78
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "0" : 2559
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "0" : 2589
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "0" : 2589
      }
    },
    "numInputRows" : 30,
    "inputRowsPerSecond" : 2500.0,
    "processedRowsPerSecond" : 34.92433061699651,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 31
  }
}
26/01/04 17:17:41 INFO BlockManagerInfo: Removed broadcast_331_piece0 on spark-master:33535 in memory (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:17:41 INFO BlockManagerInfo: Removed broadcast_331_piece0 on 172.18.0.13:44401 in memory (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:17:41 INFO BlockManagerInfo: Removed broadcast_333_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:17:41 INFO BlockManagerInfo: Removed broadcast_333_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:17:41 INFO BlockManagerInfo: Removed broadcast_333_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:17:41 INFO BlockManagerInfo: Removed broadcast_335_piece0 on spark-master:33535 in memory (size: 20.1 KiB, free: 434.4 MiB)
26/01/04 17:17:41 INFO BlockManagerInfo: Removed broadcast_335_piece0 on 172.18.0.13:44401 in memory (size: 20.1 KiB, free: 434.4 MiB)
26/01/04 17:17:41 INFO BlockManagerInfo: Removed broadcast_332_piece0 on spark-master:33535 in memory (size: 20.1 KiB, free: 434.4 MiB)
26/01/04 17:17:41 INFO BlockManagerInfo: Removed broadcast_332_piece0 on 172.18.0.13:44401 in memory (size: 20.1 KiB, free: 434.4 MiB)
26/01/04 17:17:41 INFO BlockManagerInfo: Removed broadcast_330_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:17:41 INFO BlockManagerInfo: Removed broadcast_330_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:17:41 INFO BlockManagerInfo: Removed broadcast_330_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:17:46 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/01/04 17:17:47 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/111 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.111.0765ce2d-eaed-400d-944f-0637d480e73e.tmp
26/01/04 17:17:47 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.111.0765ce2d-eaed-400d-944f-0637d480e73e.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/111
26/01/04 17:17:47 INFO MicroBatchExecution: Committed offsets for batch 111. Metadata OffsetSeqMetadata(0,1767547067105,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 17:17:47 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:17:47 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:17:47 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:17:47 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:17:47 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:17:47 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:17:47 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#90727 - airline_prefix.nullCount#90726) > 0)
26/01/04 17:17:47 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#90762 - min_flight_num.nullCount#90761) > 0)
26/01/04 17:17:47 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#90757 - max_flight_num.nullCount#90756) > 0)
26/01/04 17:17:47 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:17:47 INFO DAGScheduler: Got job 225 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 17:17:47 INFO DAGScheduler: Final stage: ResultStage 226 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:17:47 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:17:47 INFO DAGScheduler: Missing parents: List()
26/01/04 17:17:47 INFO DAGScheduler: Submitting ResultStage 226 (MapPartitionsRDD[1132] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:17:47 INFO MemoryStore: Block broadcast_336 stored as values in memory (estimated size 38.5 KiB, free 434.4 MiB)
26/01/04 17:17:47 INFO BlockManagerInfo: Removed broadcast_334_piece0 on spark-master:33535 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:17:47 INFO MemoryStore: Block broadcast_336_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.3 MiB)
26/01/04 17:17:47 INFO BlockManagerInfo: Removed broadcast_334_piece0 on 172.18.0.13:44401 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:17:47 INFO BlockManagerInfo: Added broadcast_336_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:17:47 INFO SparkContext: Created broadcast 336 from broadcast at DAGScheduler.scala:1585
26/01/04 17:17:47 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 226 (MapPartitionsRDD[1132] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 17:17:47 INFO TaskSchedulerImpl: Adding task set 226.0 with 2 tasks resource profile 0
26/01/04 17:17:47 INFO TaskSetManager: Starting task 1.0 in stage 226.0 (TID 374) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 17:17:47 INFO TaskSetManager: Starting task 0.0 in stage 226.0 (TID 375) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 17:17:47 INFO BlockManagerInfo: Added broadcast_336_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:17:47 INFO BlockManagerInfo: Added broadcast_336_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:17:47 INFO TaskSetManager: Finished task 1.0 in stage 226.0 (TID 374) in 36 ms on 172.18.0.13 (executor 0) (1/2)
26/01/04 17:17:47 INFO TaskSetManager: Finished task 0.0 in stage 226.0 (TID 375) in 73 ms on 172.18.0.14 (executor 1) (2/2)
26/01/04 17:17:47 INFO TaskSchedulerImpl: Removed TaskSet 226.0, whose tasks have all completed, from pool 
26/01/04 17:17:47 INFO DAGScheduler: ResultStage 226 (start at NativeMethodAccessorImpl.java:0) finished in 0.099 s
26/01/04 17:17:47 INFO DAGScheduler: Job 225 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:17:47 INFO TaskSchedulerImpl: Killing all running tasks in stage 226: Stage finished
26/01/04 17:17:47 INFO DAGScheduler: Job 225 finished: start at NativeMethodAccessorImpl.java:0, took 0.102360 s
26/01/04 17:17:47 INFO MemoryStore: Block broadcast_337_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.3 MiB)
26/01/04 17:17:47 INFO BlockManagerInfo: Added broadcast_337_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:17:47 INFO SparkContext: Created broadcast 337 from start at NativeMethodAccessorImpl.java:0
26/01/04 17:17:47 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 111, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@461aba19]. The input RDD has 1 partitions.
26/01/04 17:17:47 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:17:47 INFO DAGScheduler: Got job 226 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/01/04 17:17:47 INFO DAGScheduler: Final stage: ResultStage 227 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:17:47 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:17:47 INFO DAGScheduler: Missing parents: List()
26/01/04 17:17:47 INFO DAGScheduler: Submitting ResultStage 227 (MapPartitionsRDD[1137] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:17:47 INFO MemoryStore: Block broadcast_338 stored as values in memory (estimated size 52.9 KiB, free 434.3 MiB)
26/01/04 17:17:47 INFO MemoryStore: Block broadcast_338_piece0 stored as bytes in memory (estimated size 20.1 KiB, free 434.3 MiB)
26/01/04 17:17:47 INFO BlockManagerInfo: Added broadcast_338_piece0 in memory on spark-master:33535 (size: 20.1 KiB, free: 434.4 MiB)
26/01/04 17:17:47 INFO SparkContext: Created broadcast 338 from broadcast at DAGScheduler.scala:1585
26/01/04 17:17:47 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 227 (MapPartitionsRDD[1137] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/01/04 17:17:47 INFO TaskSchedulerImpl: Adding task set 227.0 with 1 tasks resource profile 0
26/01/04 17:17:47 INFO TaskSetManager: Starting task 0.0 in stage 227.0 (TID 376) (172.18.0.13, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:17:47 INFO BlockManagerInfo: Added broadcast_338_piece0 in memory on 172.18.0.13:44401 (size: 20.1 KiB, free: 434.4 MiB)
26/01/04 17:17:47 INFO BlockManagerInfo: Added broadcast_337_piece0 in memory on 172.18.0.13:44401 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:17:47 INFO TaskSetManager: Finished task 0.0 in stage 227.0 (TID 376) in 555 ms on 172.18.0.13 (executor 0) (1/1)
26/01/04 17:17:47 INFO TaskSchedulerImpl: Removed TaskSet 227.0, whose tasks have all completed, from pool 
26/01/04 17:17:47 INFO DAGScheduler: ResultStage 227 (start at NativeMethodAccessorImpl.java:0) finished in 0.563 s
26/01/04 17:17:47 INFO DAGScheduler: Job 226 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:17:47 INFO TaskSchedulerImpl: Killing all running tasks in stage 227: Stage finished
26/01/04 17:17:47 INFO DAGScheduler: Job 226 finished: start at NativeMethodAccessorImpl.java:0, took 0.564430 s
26/01/04 17:17:47 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 111, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@461aba19] is committing.
26/01/04 17:17:47 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 111, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@461aba19] committed.
26/01/04 17:17:48 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/111 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.111.66fd1391-c7dd-403b-958e-56fd73ff976f.tmp
26/01/04 17:17:48 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.111.66fd1391-c7dd-403b-958e-56fd73ff976f.tmp to file:/tmp/spark-checkpoint-enrichment/commits/111
26/01/04 17:17:48 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T17:17:47.104Z",
  "batchId" : 111,
  "numInputRows" : 30,
  "inputRowsPerSecond" : 2727.2727272727275,
  "processedRowsPerSecond" : 30.51881993896236,
  "durationMs" : {
    "addBatch" : 771,
    "commitOffsets" : 104,
    "getBatch" : 1,
    "latestOffset" : 1,
    "queryPlanning" : 35,
    "triggerExecution" : 983,
    "walCommit" : 71
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "0" : 2589
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "0" : 2619
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "0" : 2619
      }
    },
    "numInputRows" : 30,
    "inputRowsPerSecond" : 2727.2727272727275,
    "processedRowsPerSecond" : 30.51881993896236,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 31
  }
}
26/01/04 17:17:58 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/01/04 17:17:58 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/112 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.112.eda30878-dc23-4314-a4d6-18ff63f46707.tmp
26/01/04 17:17:58 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.112.eda30878-dc23-4314-a4d6-18ff63f46707.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/112
26/01/04 17:17:58 INFO MicroBatchExecution: Committed offsets for batch 112. Metadata OffsetSeqMetadata(0,1767547078115,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 17:17:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:17:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:17:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:17:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:17:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:17:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:17:58 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#91531 - airline_prefix.nullCount#91530) > 0)
26/01/04 17:17:58 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#91566 - min_flight_num.nullCount#91565) > 0)
26/01/04 17:17:58 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#91561 - max_flight_num.nullCount#91560) > 0)
26/01/04 17:17:58 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:17:58 INFO DAGScheduler: Got job 227 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 17:17:58 INFO DAGScheduler: Final stage: ResultStage 228 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:17:58 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:17:58 INFO DAGScheduler: Missing parents: List()
26/01/04 17:17:58 INFO DAGScheduler: Submitting ResultStage 228 (MapPartitionsRDD[1142] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:17:58 INFO MemoryStore: Block broadcast_339 stored as values in memory (estimated size 38.5 KiB, free 434.2 MiB)
26/01/04 17:17:58 INFO MemoryStore: Block broadcast_339_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.2 MiB)
26/01/04 17:17:58 INFO BlockManagerInfo: Added broadcast_339_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:17:58 INFO SparkContext: Created broadcast 339 from broadcast at DAGScheduler.scala:1585
26/01/04 17:17:58 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 228 (MapPartitionsRDD[1142] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 17:17:58 INFO TaskSchedulerImpl: Adding task set 228.0 with 2 tasks resource profile 0
26/01/04 17:17:58 INFO TaskSetManager: Starting task 1.0 in stage 228.0 (TID 377) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 17:17:58 INFO TaskSetManager: Starting task 0.0 in stage 228.0 (TID 378) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 17:17:58 INFO BlockManagerInfo: Added broadcast_339_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:17:58 INFO BlockManagerInfo: Added broadcast_339_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:17:58 INFO TaskSetManager: Finished task 1.0 in stage 228.0 (TID 377) in 18 ms on 172.18.0.13 (executor 0) (1/2)
26/01/04 17:17:58 INFO TaskSetManager: Finished task 0.0 in stage 228.0 (TID 378) in 35 ms on 172.18.0.14 (executor 1) (2/2)
26/01/04 17:17:58 INFO TaskSchedulerImpl: Removed TaskSet 228.0, whose tasks have all completed, from pool 
26/01/04 17:17:58 INFO DAGScheduler: ResultStage 228 (start at NativeMethodAccessorImpl.java:0) finished in 0.040 s
26/01/04 17:17:58 INFO DAGScheduler: Job 227 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:17:58 INFO TaskSchedulerImpl: Killing all running tasks in stage 228: Stage finished
26/01/04 17:17:58 INFO DAGScheduler: Job 227 finished: start at NativeMethodAccessorImpl.java:0, took 0.041713 s
26/01/04 17:17:58 INFO MemoryStore: Block broadcast_340_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.2 MiB)
26/01/04 17:17:58 INFO BlockManagerInfo: Added broadcast_340_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:17:58 INFO SparkContext: Created broadcast 340 from start at NativeMethodAccessorImpl.java:0
26/01/04 17:17:58 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 112, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@316bcf8a]. The input RDD has 1 partitions.
26/01/04 17:17:58 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:17:58 INFO DAGScheduler: Got job 228 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/01/04 17:17:58 INFO DAGScheduler: Final stage: ResultStage 229 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:17:58 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:17:58 INFO DAGScheduler: Missing parents: List()
26/01/04 17:17:58 INFO DAGScheduler: Submitting ResultStage 229 (MapPartitionsRDD[1147] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:17:58 INFO MemoryStore: Block broadcast_341 stored as values in memory (estimated size 52.9 KiB, free 434.2 MiB)
26/01/04 17:17:58 INFO MemoryStore: Block broadcast_341_piece0 stored as bytes in memory (estimated size 20.2 KiB, free 434.1 MiB)
26/01/04 17:17:58 INFO BlockManagerInfo: Added broadcast_341_piece0 in memory on spark-master:33535 (size: 20.2 KiB, free: 434.3 MiB)
26/01/04 17:17:58 INFO SparkContext: Created broadcast 341 from broadcast at DAGScheduler.scala:1585
26/01/04 17:17:58 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 229 (MapPartitionsRDD[1147] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/01/04 17:17:58 INFO TaskSchedulerImpl: Adding task set 229.0 with 1 tasks resource profile 0
26/01/04 17:17:58 INFO TaskSetManager: Starting task 0.0 in stage 229.0 (TID 379) (172.18.0.13, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:17:58 INFO BlockManagerInfo: Added broadcast_341_piece0 in memory on 172.18.0.13:44401 (size: 20.2 KiB, free: 434.3 MiB)
26/01/04 17:17:58 INFO BlockManagerInfo: Added broadcast_340_piece0 in memory on 172.18.0.13:44401 (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:17:58 INFO TaskSetManager: Finished task 0.0 in stage 229.0 (TID 379) in 563 ms on 172.18.0.13 (executor 0) (1/1)
26/01/04 17:17:58 INFO TaskSchedulerImpl: Removed TaskSet 229.0, whose tasks have all completed, from pool 
26/01/04 17:17:58 INFO DAGScheduler: ResultStage 229 (start at NativeMethodAccessorImpl.java:0) finished in 0.567 s
26/01/04 17:17:58 INFO DAGScheduler: Job 228 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:17:58 INFO TaskSchedulerImpl: Killing all running tasks in stage 229: Stage finished
26/01/04 17:17:58 INFO DAGScheduler: Job 228 finished: start at NativeMethodAccessorImpl.java:0, took 0.568347 s
26/01/04 17:17:58 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 112, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@316bcf8a] is committing.
26/01/04 17:17:58 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 112, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@316bcf8a] committed.
26/01/04 17:17:58 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/112 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.112.925cf497-c268-4de6-857f-0620ab9fd317.tmp
26/01/04 17:17:58 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.112.925cf497-c268-4de6-857f-0620ab9fd317.tmp to file:/tmp/spark-checkpoint-enrichment/commits/112
26/01/04 17:17:58 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T17:17:58.113Z",
  "batchId" : 112,
  "numInputRows" : 30,
  "inputRowsPerSecond" : 2500.0,
  "processedRowsPerSecond" : 34.24657534246575,
  "durationMs" : {
    "addBatch" : 680,
    "commitOffsets" : 65,
    "getBatch" : 0,
    "latestOffset" : 2,
    "queryPlanning" : 28,
    "triggerExecution" : 876,
    "walCommit" : 100
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "0" : 2619
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "0" : 2649
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "0" : 2649
      }
    },
    "numInputRows" : 30,
    "inputRowsPerSecond" : 2500.0,
    "processedRowsPerSecond" : 34.24657534246575,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 31
  }
}
26/01/04 17:18:01 INFO BlockManagerInfo: Removed broadcast_337_piece0 on spark-master:33535 in memory (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:18:01 INFO BlockManagerInfo: Removed broadcast_337_piece0 on 172.18.0.13:44401 in memory (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:18:01 INFO BlockManagerInfo: Removed broadcast_336_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:18:01 INFO BlockManagerInfo: Removed broadcast_336_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:18:01 INFO BlockManagerInfo: Removed broadcast_336_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:18:01 INFO BlockManagerInfo: Removed broadcast_339_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:18:01 INFO BlockManagerInfo: Removed broadcast_339_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:18:01 INFO BlockManagerInfo: Removed broadcast_339_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:18:01 INFO BlockManagerInfo: Removed broadcast_338_piece0 on spark-master:33535 in memory (size: 20.1 KiB, free: 434.4 MiB)
26/01/04 17:18:01 INFO BlockManagerInfo: Removed broadcast_338_piece0 on 172.18.0.13:44401 in memory (size: 20.1 KiB, free: 434.4 MiB)
26/01/04 17:18:01 INFO BlockManagerInfo: Removed broadcast_341_piece0 on spark-master:33535 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:18:01 INFO BlockManagerInfo: Removed broadcast_341_piece0 on 172.18.0.13:44401 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:18:08 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/01/04 17:18:09 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/113 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.113.6fd7708c-7a8f-49e6-a2ce-2e20a14c5ce6.tmp
26/01/04 17:18:09 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.113.6fd7708c-7a8f-49e6-a2ce-2e20a14c5ce6.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/113
26/01/04 17:18:09 INFO MicroBatchExecution: Committed offsets for batch 113. Metadata OffsetSeqMetadata(0,1767547089134,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 17:18:09 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:18:09 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:18:09 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:18:09 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:18:09 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:18:09 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:18:09 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#92335 - airline_prefix.nullCount#92334) > 0)
26/01/04 17:18:09 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#92370 - min_flight_num.nullCount#92369) > 0)
26/01/04 17:18:09 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#92365 - max_flight_num.nullCount#92364) > 0)
26/01/04 17:18:09 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:18:09 INFO DAGScheduler: Got job 229 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 17:18:09 INFO DAGScheduler: Final stage: ResultStage 230 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:18:09 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:18:09 INFO DAGScheduler: Missing parents: List()
26/01/04 17:18:09 INFO DAGScheduler: Submitting ResultStage 230 (MapPartitionsRDD[1152] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:18:09 INFO MemoryStore: Block broadcast_342 stored as values in memory (estimated size 38.5 KiB, free 434.4 MiB)
26/01/04 17:18:09 INFO MemoryStore: Block broadcast_342_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.3 MiB)
26/01/04 17:18:09 INFO BlockManagerInfo: Removed broadcast_340_piece0 on spark-master:33535 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:18:09 INFO BlockManagerInfo: Added broadcast_342_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:18:09 INFO SparkContext: Created broadcast 342 from broadcast at DAGScheduler.scala:1585
26/01/04 17:18:09 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 230 (MapPartitionsRDD[1152] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 17:18:09 INFO TaskSchedulerImpl: Adding task set 230.0 with 2 tasks resource profile 0
26/01/04 17:18:09 INFO BlockManagerInfo: Removed broadcast_340_piece0 on 172.18.0.13:44401 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:18:09 INFO TaskSetManager: Starting task 1.0 in stage 230.0 (TID 380) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 17:18:09 INFO TaskSetManager: Starting task 0.0 in stage 230.0 (TID 381) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 17:18:09 INFO BlockManagerInfo: Added broadcast_342_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:18:09 INFO BlockManagerInfo: Added broadcast_342_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:18:09 INFO TaskSetManager: Finished task 1.0 in stage 230.0 (TID 380) in 25 ms on 172.18.0.13 (executor 0) (1/2)
26/01/04 17:18:09 INFO TaskSetManager: Finished task 0.0 in stage 230.0 (TID 381) in 48 ms on 172.18.0.14 (executor 1) (2/2)
26/01/04 17:18:09 INFO TaskSchedulerImpl: Removed TaskSet 230.0, whose tasks have all completed, from pool 
26/01/04 17:18:09 INFO DAGScheduler: ResultStage 230 (start at NativeMethodAccessorImpl.java:0) finished in 0.061 s
26/01/04 17:18:09 INFO DAGScheduler: Job 229 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:18:09 INFO TaskSchedulerImpl: Killing all running tasks in stage 230: Stage finished
26/01/04 17:18:09 INFO DAGScheduler: Job 229 finished: start at NativeMethodAccessorImpl.java:0, took 0.064819 s
26/01/04 17:18:09 INFO MemoryStore: Block broadcast_343_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.3 MiB)
26/01/04 17:18:09 INFO BlockManagerInfo: Added broadcast_343_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:18:09 INFO SparkContext: Created broadcast 343 from start at NativeMethodAccessorImpl.java:0
26/01/04 17:18:09 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 113, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@e1d71f1]. The input RDD has 1 partitions.
26/01/04 17:18:09 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:18:09 INFO DAGScheduler: Got job 230 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/01/04 17:18:09 INFO DAGScheduler: Final stage: ResultStage 231 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:18:09 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:18:09 INFO DAGScheduler: Missing parents: List()
26/01/04 17:18:09 INFO DAGScheduler: Submitting ResultStage 231 (MapPartitionsRDD[1157] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:18:09 INFO MemoryStore: Block broadcast_344 stored as values in memory (estimated size 52.9 KiB, free 434.3 MiB)
26/01/04 17:18:09 INFO MemoryStore: Block broadcast_344_piece0 stored as bytes in memory (estimated size 20.2 KiB, free 434.3 MiB)
26/01/04 17:18:09 INFO BlockManagerInfo: Added broadcast_344_piece0 in memory on spark-master:33535 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:18:09 INFO SparkContext: Created broadcast 344 from broadcast at DAGScheduler.scala:1585
26/01/04 17:18:09 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 231 (MapPartitionsRDD[1157] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/01/04 17:18:09 INFO TaskSchedulerImpl: Adding task set 231.0 with 1 tasks resource profile 0
26/01/04 17:18:09 INFO TaskSetManager: Starting task 0.0 in stage 231.0 (TID 382) (172.18.0.13, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:18:09 INFO BlockManagerInfo: Added broadcast_344_piece0 in memory on 172.18.0.13:44401 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:18:09 INFO BlockManagerInfo: Added broadcast_343_piece0 in memory on 172.18.0.13:44401 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:18:09 INFO TaskSetManager: Finished task 0.0 in stage 231.0 (TID 382) in 554 ms on 172.18.0.13 (executor 0) (1/1)
26/01/04 17:18:09 INFO TaskSchedulerImpl: Removed TaskSet 231.0, whose tasks have all completed, from pool 
26/01/04 17:18:09 INFO DAGScheduler: ResultStage 231 (start at NativeMethodAccessorImpl.java:0) finished in 0.561 s
26/01/04 17:18:09 INFO DAGScheduler: Job 230 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:18:09 INFO TaskSchedulerImpl: Killing all running tasks in stage 231: Stage finished
26/01/04 17:18:09 INFO DAGScheduler: Job 230 finished: start at NativeMethodAccessorImpl.java:0, took 0.564982 s
26/01/04 17:18:09 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 113, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@e1d71f1] is committing.
26/01/04 17:18:09 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 113, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@e1d71f1] committed.
26/01/04 17:18:09 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/113 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.113.2761c2db-ef5b-4ccc-bc3e-68e12e1cdb5d.tmp
26/01/04 17:18:10 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.113.2761c2db-ef5b-4ccc-bc3e-68e12e1cdb5d.tmp to file:/tmp/spark-checkpoint-enrichment/commits/113
26/01/04 17:18:10 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T17:18:09.133Z",
  "batchId" : 113,
  "numInputRows" : 30,
  "inputRowsPerSecond" : 2500.0,
  "processedRowsPerSecond" : 33.1858407079646,
  "durationMs" : {
    "addBatch" : 701,
    "commitOffsets" : 76,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 28,
    "triggerExecution" : 904,
    "walCommit" : 98
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "0" : 2649
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "0" : 2679
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "0" : 2679
      }
    },
    "numInputRows" : 30,
    "inputRowsPerSecond" : 2500.0,
    "processedRowsPerSecond" : 33.1858407079646,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 31
  }
}
26/01/04 17:18:20 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/01/04 17:18:20 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/114 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.114.f55e54f2-cd1d-4b5e-a0cd-635bbdae04b4.tmp
26/01/04 17:18:20 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.114.f55e54f2-cd1d-4b5e-a0cd-635bbdae04b4.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/114
26/01/04 17:18:20 INFO MicroBatchExecution: Committed offsets for batch 114. Metadata OffsetSeqMetadata(0,1767547100149,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 17:18:20 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:18:20 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:18:20 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:18:20 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:18:20 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:18:20 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:18:20 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#93139 - airline_prefix.nullCount#93138) > 0)
26/01/04 17:18:20 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#93174 - min_flight_num.nullCount#93173) > 0)
26/01/04 17:18:20 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#93169 - max_flight_num.nullCount#93168) > 0)
26/01/04 17:18:20 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:18:20 INFO DAGScheduler: Got job 231 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 17:18:20 INFO DAGScheduler: Final stage: ResultStage 232 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:18:20 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:18:20 INFO DAGScheduler: Missing parents: List()
26/01/04 17:18:20 INFO DAGScheduler: Submitting ResultStage 232 (MapPartitionsRDD[1162] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:18:20 INFO MemoryStore: Block broadcast_345 stored as values in memory (estimated size 38.5 KiB, free 434.2 MiB)
26/01/04 17:18:20 INFO MemoryStore: Block broadcast_345_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.2 MiB)
26/01/04 17:18:20 INFO BlockManagerInfo: Added broadcast_345_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:18:20 INFO SparkContext: Created broadcast 345 from broadcast at DAGScheduler.scala:1585
26/01/04 17:18:20 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 232 (MapPartitionsRDD[1162] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 17:18:20 INFO TaskSchedulerImpl: Adding task set 232.0 with 2 tasks resource profile 0
26/01/04 17:18:20 INFO TaskSetManager: Starting task 0.0 in stage 232.0 (TID 383) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 17:18:20 INFO TaskSetManager: Starting task 1.0 in stage 232.0 (TID 384) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 17:18:20 INFO BlockManagerInfo: Added broadcast_345_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:18:20 INFO BlockManagerInfo: Added broadcast_345_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:18:20 INFO TaskSetManager: Finished task 1.0 in stage 232.0 (TID 384) in 17 ms on 172.18.0.13 (executor 0) (1/2)
26/01/04 17:18:20 INFO TaskSetManager: Finished task 0.0 in stage 232.0 (TID 383) in 30 ms on 172.18.0.14 (executor 1) (2/2)
26/01/04 17:18:20 INFO TaskSchedulerImpl: Removed TaskSet 232.0, whose tasks have all completed, from pool 
26/01/04 17:18:20 INFO DAGScheduler: ResultStage 232 (start at NativeMethodAccessorImpl.java:0) finished in 0.035 s
26/01/04 17:18:20 INFO DAGScheduler: Job 231 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:18:20 INFO TaskSchedulerImpl: Killing all running tasks in stage 232: Stage finished
26/01/04 17:18:20 INFO DAGScheduler: Job 231 finished: start at NativeMethodAccessorImpl.java:0, took 0.037262 s
26/01/04 17:18:20 INFO MemoryStore: Block broadcast_346_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.2 MiB)
26/01/04 17:18:20 INFO BlockManagerInfo: Added broadcast_346_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:18:20 INFO SparkContext: Created broadcast 346 from start at NativeMethodAccessorImpl.java:0
26/01/04 17:18:20 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 114, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@2cf23006]. The input RDD has 1 partitions.
26/01/04 17:18:20 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:18:20 INFO DAGScheduler: Got job 232 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/01/04 17:18:20 INFO DAGScheduler: Final stage: ResultStage 233 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:18:20 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:18:20 INFO DAGScheduler: Missing parents: List()
26/01/04 17:18:20 INFO DAGScheduler: Submitting ResultStage 233 (MapPartitionsRDD[1167] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:18:20 INFO MemoryStore: Block broadcast_347 stored as values in memory (estimated size 52.9 KiB, free 434.2 MiB)
26/01/04 17:18:20 INFO MemoryStore: Block broadcast_347_piece0 stored as bytes in memory (estimated size 20.1 KiB, free 434.1 MiB)
26/01/04 17:18:20 INFO BlockManagerInfo: Added broadcast_347_piece0 in memory on spark-master:33535 (size: 20.1 KiB, free: 434.3 MiB)
26/01/04 17:18:20 INFO SparkContext: Created broadcast 347 from broadcast at DAGScheduler.scala:1585
26/01/04 17:18:20 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 233 (MapPartitionsRDD[1167] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/01/04 17:18:20 INFO TaskSchedulerImpl: Adding task set 233.0 with 1 tasks resource profile 0
26/01/04 17:18:20 INFO TaskSetManager: Starting task 0.0 in stage 233.0 (TID 385) (172.18.0.13, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:18:20 INFO BlockManagerInfo: Added broadcast_347_piece0 in memory on 172.18.0.13:44401 (size: 20.1 KiB, free: 434.3 MiB)
26/01/04 17:18:20 INFO BlockManagerInfo: Added broadcast_346_piece0 in memory on 172.18.0.13:44401 (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:18:20 INFO TaskSetManager: Finished task 0.0 in stage 233.0 (TID 385) in 542 ms on 172.18.0.13 (executor 0) (1/1)
26/01/04 17:18:20 INFO TaskSchedulerImpl: Removed TaskSet 233.0, whose tasks have all completed, from pool 
26/01/04 17:18:20 INFO DAGScheduler: ResultStage 233 (start at NativeMethodAccessorImpl.java:0) finished in 0.546 s
26/01/04 17:18:20 INFO DAGScheduler: Job 232 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:18:20 INFO TaskSchedulerImpl: Killing all running tasks in stage 233: Stage finished
26/01/04 17:18:20 INFO DAGScheduler: Job 232 finished: start at NativeMethodAccessorImpl.java:0, took 0.547875 s
26/01/04 17:18:20 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 114, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@2cf23006] is committing.
26/01/04 17:18:20 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 114, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@2cf23006] committed.
26/01/04 17:18:20 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/114 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.114.4bfcf373-32b9-4f99-831b-36b506bea176.tmp
26/01/04 17:18:21 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.114.4bfcf373-32b9-4f99-831b-36b506bea176.tmp to file:/tmp/spark-checkpoint-enrichment/commits/114
26/01/04 17:18:21 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T17:18:20.148Z",
  "batchId" : 114,
  "numInputRows" : 30,
  "inputRowsPerSecond" : 2500.0,
  "processedRowsPerSecond" : 34.285714285714285,
  "durationMs" : {
    "addBatch" : 656,
    "commitOffsets" : 71,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 29,
    "triggerExecution" : 875,
    "walCommit" : 116
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "0" : 2679
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "0" : 2709
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "0" : 2709
      }
    },
    "numInputRows" : 30,
    "inputRowsPerSecond" : 2500.0,
    "processedRowsPerSecond" : 34.285714285714285,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 31
  }
}
26/01/04 17:18:22 INFO BlockManagerInfo: Removed broadcast_344_piece0 on spark-master:33535 in memory (size: 20.2 KiB, free: 434.3 MiB)
26/01/04 17:18:22 INFO BlockManagerInfo: Removed broadcast_344_piece0 on 172.18.0.13:44401 in memory (size: 20.2 KiB, free: 434.3 MiB)
26/01/04 17:18:22 INFO BlockManagerInfo: Removed broadcast_342_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:18:22 INFO BlockManagerInfo: Removed broadcast_342_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:18:22 INFO BlockManagerInfo: Removed broadcast_342_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:18:22 INFO BlockManagerInfo: Removed broadcast_343_piece0 on spark-master:33535 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:18:22 INFO BlockManagerInfo: Removed broadcast_343_piece0 on 172.18.0.13:44401 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:18:22 INFO BlockManagerInfo: Removed broadcast_347_piece0 on spark-master:33535 in memory (size: 20.1 KiB, free: 434.4 MiB)
26/01/04 17:18:22 INFO BlockManagerInfo: Removed broadcast_347_piece0 on 172.18.0.13:44401 in memory (size: 20.1 KiB, free: 434.4 MiB)
26/01/04 17:18:22 INFO BlockManagerInfo: Removed broadcast_345_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:18:22 INFO BlockManagerInfo: Removed broadcast_345_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:18:22 INFO BlockManagerInfo: Removed broadcast_345_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:18:31 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/01/04 17:18:31 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/115 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.115.ea10e7e6-7890-4a47-847c-07ed9f7237a2.tmp
26/01/04 17:18:31 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.115.ea10e7e6-7890-4a47-847c-07ed9f7237a2.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/115
26/01/04 17:18:31 INFO MicroBatchExecution: Committed offsets for batch 115. Metadata OffsetSeqMetadata(0,1767547111168,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 17:18:31 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:18:31 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:18:31 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:18:31 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:18:31 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:18:31 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:18:31 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#93943 - airline_prefix.nullCount#93942) > 0)
26/01/04 17:18:31 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#93978 - min_flight_num.nullCount#93977) > 0)
26/01/04 17:18:31 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#93973 - max_flight_num.nullCount#93972) > 0)
26/01/04 17:18:31 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:18:31 INFO DAGScheduler: Got job 233 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 17:18:31 INFO DAGScheduler: Final stage: ResultStage 234 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:18:31 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:18:31 INFO DAGScheduler: Missing parents: List()
26/01/04 17:18:31 INFO DAGScheduler: Submitting ResultStage 234 (MapPartitionsRDD[1172] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:18:31 INFO MemoryStore: Block broadcast_348 stored as values in memory (estimated size 38.5 KiB, free 434.4 MiB)
26/01/04 17:18:31 INFO MemoryStore: Block broadcast_348_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.3 MiB)
26/01/04 17:18:31 INFO BlockManagerInfo: Added broadcast_348_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:18:31 INFO BlockManagerInfo: Removed broadcast_346_piece0 on spark-master:33535 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:18:31 INFO SparkContext: Created broadcast 348 from broadcast at DAGScheduler.scala:1585
26/01/04 17:18:31 INFO BlockManagerInfo: Removed broadcast_346_piece0 on 172.18.0.13:44401 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:18:31 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 234 (MapPartitionsRDD[1172] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 17:18:31 INFO TaskSchedulerImpl: Adding task set 234.0 with 2 tasks resource profile 0
26/01/04 17:18:31 INFO TaskSetManager: Starting task 0.0 in stage 234.0 (TID 386) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 17:18:31 INFO TaskSetManager: Starting task 1.0 in stage 234.0 (TID 387) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 17:18:31 INFO BlockManagerInfo: Added broadcast_348_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:18:31 INFO BlockManagerInfo: Added broadcast_348_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:18:31 INFO TaskSetManager: Finished task 1.0 in stage 234.0 (TID 387) in 18 ms on 172.18.0.13 (executor 0) (1/2)
26/01/04 17:18:31 INFO TaskSetManager: Finished task 0.0 in stage 234.0 (TID 386) in 40 ms on 172.18.0.14 (executor 1) (2/2)
26/01/04 17:18:31 INFO TaskSchedulerImpl: Removed TaskSet 234.0, whose tasks have all completed, from pool 
26/01/04 17:18:31 INFO DAGScheduler: ResultStage 234 (start at NativeMethodAccessorImpl.java:0) finished in 0.052 s
26/01/04 17:18:31 INFO DAGScheduler: Job 233 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:18:31 INFO TaskSchedulerImpl: Killing all running tasks in stage 234: Stage finished
26/01/04 17:18:31 INFO DAGScheduler: Job 233 finished: start at NativeMethodAccessorImpl.java:0, took 0.053834 s
26/01/04 17:18:31 INFO MemoryStore: Block broadcast_349_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.3 MiB)
26/01/04 17:18:31 INFO BlockManagerInfo: Added broadcast_349_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:18:31 INFO SparkContext: Created broadcast 349 from start at NativeMethodAccessorImpl.java:0
26/01/04 17:18:31 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 115, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@4d728077]. The input RDD has 1 partitions.
26/01/04 17:18:31 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:18:31 INFO DAGScheduler: Got job 234 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/01/04 17:18:31 INFO DAGScheduler: Final stage: ResultStage 235 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:18:31 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:18:31 INFO DAGScheduler: Missing parents: List()
26/01/04 17:18:31 INFO DAGScheduler: Submitting ResultStage 235 (MapPartitionsRDD[1177] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:18:31 INFO MemoryStore: Block broadcast_350 stored as values in memory (estimated size 52.9 KiB, free 434.3 MiB)
26/01/04 17:18:31 INFO MemoryStore: Block broadcast_350_piece0 stored as bytes in memory (estimated size 20.2 KiB, free 434.3 MiB)
26/01/04 17:18:31 INFO BlockManagerInfo: Added broadcast_350_piece0 in memory on spark-master:33535 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:18:31 INFO SparkContext: Created broadcast 350 from broadcast at DAGScheduler.scala:1585
26/01/04 17:18:31 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 235 (MapPartitionsRDD[1177] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/01/04 17:18:31 INFO TaskSchedulerImpl: Adding task set 235.0 with 1 tasks resource profile 0
26/01/04 17:18:31 INFO TaskSetManager: Starting task 0.0 in stage 235.0 (TID 388) (172.18.0.13, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:18:31 INFO BlockManagerInfo: Added broadcast_350_piece0 in memory on 172.18.0.13:44401 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:18:31 INFO BlockManagerInfo: Added broadcast_349_piece0 in memory on 172.18.0.13:44401 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:18:32 INFO TaskSetManager: Finished task 0.0 in stage 235.0 (TID 388) in 604 ms on 172.18.0.13 (executor 0) (1/1)
26/01/04 17:18:32 INFO TaskSchedulerImpl: Removed TaskSet 235.0, whose tasks have all completed, from pool 
26/01/04 17:18:32 INFO DAGScheduler: ResultStage 235 (start at NativeMethodAccessorImpl.java:0) finished in 0.610 s
26/01/04 17:18:32 INFO DAGScheduler: Job 234 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:18:32 INFO TaskSchedulerImpl: Killing all running tasks in stage 235: Stage finished
26/01/04 17:18:32 INFO DAGScheduler: Job 234 finished: start at NativeMethodAccessorImpl.java:0, took 0.613587 s
26/01/04 17:18:32 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 115, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@4d728077] is committing.
26/01/04 17:18:32 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 115, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@4d728077] committed.
26/01/04 17:18:32 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/115 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.115.892354cc-2582-4347-9713-2bb115023e2a.tmp
26/01/04 17:18:32 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.115.892354cc-2582-4347-9713-2bb115023e2a.tmp to file:/tmp/spark-checkpoint-enrichment/commits/115
26/01/04 17:18:32 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T17:18:31.167Z",
  "batchId" : 115,
  "numInputRows" : 30,
  "inputRowsPerSecond" : 2500.0,
  "processedRowsPerSecond" : 30.54989816700611,
  "durationMs" : {
    "addBatch" : 737,
    "commitOffsets" : 125,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 30,
    "triggerExecution" : 982,
    "walCommit" : 88
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "0" : 2709
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "0" : 2739
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "0" : 2739
      }
    },
    "numInputRows" : 30,
    "inputRowsPerSecond" : 2500.0,
    "processedRowsPerSecond" : 30.54989816700611,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 31
  }
}
26/01/04 17:18:42 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/01/04 17:18:42 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/116 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.116.1f7acd69-ba51-44a4-90ca-dfc26ceeee7f.tmp
26/01/04 17:18:42 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.116.1f7acd69-ba51-44a4-90ca-dfc26ceeee7f.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/116
26/01/04 17:18:42 INFO MicroBatchExecution: Committed offsets for batch 116. Metadata OffsetSeqMetadata(0,1767547122175,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 17:18:42 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:18:42 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:18:42 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:18:42 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:18:42 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:18:42 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:18:42 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#94747 - airline_prefix.nullCount#94746) > 0)
26/01/04 17:18:42 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#94782 - min_flight_num.nullCount#94781) > 0)
26/01/04 17:18:42 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#94777 - max_flight_num.nullCount#94776) > 0)
26/01/04 17:18:42 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:18:42 INFO DAGScheduler: Got job 235 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 17:18:42 INFO DAGScheduler: Final stage: ResultStage 236 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:18:42 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:18:42 INFO DAGScheduler: Missing parents: List()
26/01/04 17:18:42 INFO DAGScheduler: Submitting ResultStage 236 (MapPartitionsRDD[1182] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:18:42 INFO MemoryStore: Block broadcast_351 stored as values in memory (estimated size 38.5 KiB, free 434.2 MiB)
26/01/04 17:18:42 INFO MemoryStore: Block broadcast_351_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.2 MiB)
26/01/04 17:18:42 INFO BlockManagerInfo: Added broadcast_351_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:18:42 INFO SparkContext: Created broadcast 351 from broadcast at DAGScheduler.scala:1585
26/01/04 17:18:42 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 236 (MapPartitionsRDD[1182] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 17:18:42 INFO TaskSchedulerImpl: Adding task set 236.0 with 2 tasks resource profile 0
26/01/04 17:18:42 INFO TaskSetManager: Starting task 1.0 in stage 236.0 (TID 389) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 17:18:42 INFO TaskSetManager: Starting task 0.0 in stage 236.0 (TID 390) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 17:18:42 INFO BlockManagerInfo: Added broadcast_351_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:18:42 INFO BlockManagerInfo: Added broadcast_351_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:18:42 INFO TaskSetManager: Finished task 1.0 in stage 236.0 (TID 389) in 19 ms on 172.18.0.13 (executor 0) (1/2)
26/01/04 17:18:42 INFO TaskSetManager: Finished task 0.0 in stage 236.0 (TID 390) in 116 ms on 172.18.0.14 (executor 1) (2/2)
26/01/04 17:18:42 INFO TaskSchedulerImpl: Removed TaskSet 236.0, whose tasks have all completed, from pool 
26/01/04 17:18:42 INFO DAGScheduler: ResultStage 236 (start at NativeMethodAccessorImpl.java:0) finished in 0.122 s
26/01/04 17:18:42 INFO DAGScheduler: Job 235 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:18:42 INFO TaskSchedulerImpl: Killing all running tasks in stage 236: Stage finished
26/01/04 17:18:42 INFO DAGScheduler: Job 235 finished: start at NativeMethodAccessorImpl.java:0, took 0.123227 s
26/01/04 17:18:42 INFO MemoryStore: Block broadcast_352_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.2 MiB)
26/01/04 17:18:42 INFO BlockManagerInfo: Added broadcast_352_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:18:42 INFO SparkContext: Created broadcast 352 from start at NativeMethodAccessorImpl.java:0
26/01/04 17:18:42 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 116, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@7dccdf76]. The input RDD has 1 partitions.
26/01/04 17:18:42 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:18:42 INFO DAGScheduler: Got job 236 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/01/04 17:18:42 INFO DAGScheduler: Final stage: ResultStage 237 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:18:42 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:18:42 INFO DAGScheduler: Missing parents: List()
26/01/04 17:18:42 INFO DAGScheduler: Submitting ResultStage 237 (MapPartitionsRDD[1187] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:18:42 INFO MemoryStore: Block broadcast_353 stored as values in memory (estimated size 52.9 KiB, free 434.2 MiB)
26/01/04 17:18:42 INFO MemoryStore: Block broadcast_353_piece0 stored as bytes in memory (estimated size 20.1 KiB, free 434.1 MiB)
26/01/04 17:18:42 INFO BlockManagerInfo: Added broadcast_353_piece0 in memory on spark-master:33535 (size: 20.1 KiB, free: 434.3 MiB)
26/01/04 17:18:42 INFO SparkContext: Created broadcast 353 from broadcast at DAGScheduler.scala:1585
26/01/04 17:18:42 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 237 (MapPartitionsRDD[1187] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/01/04 17:18:42 INFO TaskSchedulerImpl: Adding task set 237.0 with 1 tasks resource profile 0
26/01/04 17:18:42 INFO TaskSetManager: Starting task 0.0 in stage 237.0 (TID 391) (172.18.0.13, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:18:42 INFO BlockManagerInfo: Added broadcast_353_piece0 in memory on 172.18.0.13:44401 (size: 20.1 KiB, free: 434.3 MiB)
26/01/04 17:18:42 INFO BlockManagerInfo: Added broadcast_352_piece0 in memory on 172.18.0.13:44401 (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:18:43 INFO TaskSetManager: Finished task 0.0 in stage 237.0 (TID 391) in 552 ms on 172.18.0.13 (executor 0) (1/1)
26/01/04 17:18:43 INFO TaskSchedulerImpl: Removed TaskSet 237.0, whose tasks have all completed, from pool 
26/01/04 17:18:43 INFO DAGScheduler: ResultStage 237 (start at NativeMethodAccessorImpl.java:0) finished in 0.556 s
26/01/04 17:18:43 INFO DAGScheduler: Job 236 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:18:43 INFO TaskSchedulerImpl: Killing all running tasks in stage 237: Stage finished
26/01/04 17:18:43 INFO DAGScheduler: Job 236 finished: start at NativeMethodAccessorImpl.java:0, took 0.557043 s
26/01/04 17:18:43 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 116, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@7dccdf76] is committing.
26/01/04 17:18:43 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 116, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@7dccdf76] committed.
26/01/04 17:18:43 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/116 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.116.0d46ff3e-ba8f-454d-ab2e-4cfc46082ce1.tmp
26/01/04 17:18:43 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.116.0d46ff3e-ba8f-454d-ab2e-4cfc46082ce1.tmp to file:/tmp/spark-checkpoint-enrichment/commits/116
26/01/04 17:18:43 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T17:18:42.172Z",
  "batchId" : 116,
  "numInputRows" : 30,
  "inputRowsPerSecond" : 2727.2727272727275,
  "processedRowsPerSecond" : 32.15434083601286,
  "durationMs" : {
    "addBatch" : 733,
    "commitOffsets" : 79,
    "getBatch" : 0,
    "latestOffset" : 3,
    "queryPlanning" : 30,
    "triggerExecution" : 933,
    "walCommit" : 88
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "0" : 2739
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "0" : 2769
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "0" : 2769
      }
    },
    "numInputRows" : 30,
    "inputRowsPerSecond" : 2727.2727272727275,
    "processedRowsPerSecond" : 32.15434083601286,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 31
  }
}
26/01/04 17:18:45 INFO BlockManagerInfo: Removed broadcast_350_piece0 on spark-master:33535 in memory (size: 20.2 KiB, free: 434.3 MiB)
26/01/04 17:18:45 INFO BlockManagerInfo: Removed broadcast_350_piece0 on 172.18.0.13:44401 in memory (size: 20.2 KiB, free: 434.3 MiB)
26/01/04 17:18:45 INFO BlockManagerInfo: Removed broadcast_349_piece0 on spark-master:33535 in memory (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:18:45 INFO BlockManagerInfo: Removed broadcast_349_piece0 on 172.18.0.13:44401 in memory (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:18:45 INFO BlockManagerInfo: Removed broadcast_353_piece0 on spark-master:33535 in memory (size: 20.1 KiB, free: 434.4 MiB)
26/01/04 17:18:45 INFO BlockManagerInfo: Removed broadcast_353_piece0 on 172.18.0.13:44401 in memory (size: 20.1 KiB, free: 434.4 MiB)
26/01/04 17:18:45 INFO BlockManagerInfo: Removed broadcast_351_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:18:45 INFO BlockManagerInfo: Removed broadcast_351_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:18:45 INFO BlockManagerInfo: Removed broadcast_351_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:18:45 INFO BlockManagerInfo: Removed broadcast_348_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:18:45 INFO BlockManagerInfo: Removed broadcast_348_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:18:45 INFO BlockManagerInfo: Removed broadcast_348_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:18:53 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/01/04 17:18:53 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/117 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.117.5674fc43-a762-4d27-b911-a9edfa9d4d39.tmp
26/01/04 17:18:53 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.117.5674fc43-a762-4d27-b911-a9edfa9d4d39.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/117
26/01/04 17:18:53 INFO MicroBatchExecution: Committed offsets for batch 117. Metadata OffsetSeqMetadata(0,1767547133198,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 17:18:53 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:18:53 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:18:53 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:18:53 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:18:53 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:18:53 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:18:53 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#95551 - airline_prefix.nullCount#95550) > 0)
26/01/04 17:18:53 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#95586 - min_flight_num.nullCount#95585) > 0)
26/01/04 17:18:53 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#95581 - max_flight_num.nullCount#95580) > 0)
26/01/04 17:18:53 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:18:53 INFO DAGScheduler: Got job 237 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 17:18:53 INFO DAGScheduler: Final stage: ResultStage 238 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:18:53 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:18:53 INFO DAGScheduler: Missing parents: List()
26/01/04 17:18:53 INFO DAGScheduler: Submitting ResultStage 238 (MapPartitionsRDD[1192] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:18:53 INFO MemoryStore: Block broadcast_354 stored as values in memory (estimated size 38.5 KiB, free 434.4 MiB)
26/01/04 17:18:53 INFO MemoryStore: Block broadcast_354_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.3 MiB)
26/01/04 17:18:53 INFO BlockManagerInfo: Added broadcast_354_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:18:53 INFO SparkContext: Created broadcast 354 from broadcast at DAGScheduler.scala:1585
26/01/04 17:18:53 INFO BlockManagerInfo: Removed broadcast_352_piece0 on spark-master:33535 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:18:53 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 238 (MapPartitionsRDD[1192] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 17:18:53 INFO TaskSchedulerImpl: Adding task set 238.0 with 2 tasks resource profile 0
26/01/04 17:18:53 INFO BlockManagerInfo: Removed broadcast_352_piece0 on 172.18.0.13:44401 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:18:53 INFO TaskSetManager: Starting task 1.0 in stage 238.0 (TID 392) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 17:18:53 INFO TaskSetManager: Starting task 0.0 in stage 238.0 (TID 393) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 17:18:53 INFO BlockManagerInfo: Added broadcast_354_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:18:53 INFO BlockManagerInfo: Added broadcast_354_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:18:53 INFO TaskSetManager: Finished task 1.0 in stage 238.0 (TID 392) in 24 ms on 172.18.0.13 (executor 0) (1/2)
26/01/04 17:18:53 INFO TaskSetManager: Finished task 0.0 in stage 238.0 (TID 393) in 44 ms on 172.18.0.14 (executor 1) (2/2)
26/01/04 17:18:53 INFO TaskSchedulerImpl: Removed TaskSet 238.0, whose tasks have all completed, from pool 
26/01/04 17:18:53 INFO DAGScheduler: ResultStage 238 (start at NativeMethodAccessorImpl.java:0) finished in 0.054 s
26/01/04 17:18:53 INFO DAGScheduler: Job 237 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:18:53 INFO TaskSchedulerImpl: Killing all running tasks in stage 238: Stage finished
26/01/04 17:18:53 INFO DAGScheduler: Job 237 finished: start at NativeMethodAccessorImpl.java:0, took 0.056529 s
26/01/04 17:18:53 INFO MemoryStore: Block broadcast_355_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.3 MiB)
26/01/04 17:18:53 INFO BlockManagerInfo: Added broadcast_355_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:18:53 INFO SparkContext: Created broadcast 355 from start at NativeMethodAccessorImpl.java:0
26/01/04 17:18:53 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 117, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@4a6eee99]. The input RDD has 1 partitions.
26/01/04 17:18:53 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:18:53 INFO DAGScheduler: Got job 238 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/01/04 17:18:53 INFO DAGScheduler: Final stage: ResultStage 239 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:18:53 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:18:53 INFO DAGScheduler: Missing parents: List()
26/01/04 17:18:53 INFO DAGScheduler: Submitting ResultStage 239 (MapPartitionsRDD[1197] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:18:53 INFO MemoryStore: Block broadcast_356 stored as values in memory (estimated size 52.9 KiB, free 434.3 MiB)
26/01/04 17:18:53 INFO MemoryStore: Block broadcast_356_piece0 stored as bytes in memory (estimated size 20.2 KiB, free 434.3 MiB)
26/01/04 17:18:53 INFO BlockManagerInfo: Added broadcast_356_piece0 in memory on spark-master:33535 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:18:53 INFO SparkContext: Created broadcast 356 from broadcast at DAGScheduler.scala:1585
26/01/04 17:18:53 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 239 (MapPartitionsRDD[1197] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/01/04 17:18:53 INFO TaskSchedulerImpl: Adding task set 239.0 with 1 tasks resource profile 0
26/01/04 17:18:53 INFO TaskSetManager: Starting task 0.0 in stage 239.0 (TID 394) (172.18.0.13, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:18:53 INFO BlockManagerInfo: Added broadcast_356_piece0 in memory on 172.18.0.13:44401 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:18:53 INFO BlockManagerInfo: Added broadcast_355_piece0 in memory on 172.18.0.13:44401 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:18:54 INFO TaskSetManager: Finished task 0.0 in stage 239.0 (TID 394) in 556 ms on 172.18.0.13 (executor 0) (1/1)
26/01/04 17:18:54 INFO TaskSchedulerImpl: Removed TaskSet 239.0, whose tasks have all completed, from pool 
26/01/04 17:18:54 INFO DAGScheduler: ResultStage 239 (start at NativeMethodAccessorImpl.java:0) finished in 0.562 s
26/01/04 17:18:54 INFO DAGScheduler: Job 238 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:18:54 INFO TaskSchedulerImpl: Killing all running tasks in stage 239: Stage finished
26/01/04 17:18:54 INFO DAGScheduler: Job 238 finished: start at NativeMethodAccessorImpl.java:0, took 0.563626 s
26/01/04 17:18:54 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 117, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@4a6eee99] is committing.
26/01/04 17:18:54 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 117, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@4a6eee99] committed.
26/01/04 17:18:54 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/117 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.117.f5b39efe-3fe2-4493-9e6e-95fcb04e9925.tmp
26/01/04 17:18:54 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.117.f5b39efe-3fe2-4493-9e6e-95fcb04e9925.tmp to file:/tmp/spark-checkpoint-enrichment/commits/117
26/01/04 17:18:54 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T17:18:53.197Z",
  "batchId" : 117,
  "numInputRows" : 30,
  "inputRowsPerSecond" : 2500.0,
  "processedRowsPerSecond" : 32.25806451612903,
  "durationMs" : {
    "addBatch" : 689,
    "commitOffsets" : 83,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 34,
    "triggerExecution" : 930,
    "walCommit" : 122
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "0" : 2769
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "0" : 2799
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "0" : 2799
      }
    },
    "numInputRows" : 30,
    "inputRowsPerSecond" : 2500.0,
    "processedRowsPerSecond" : 32.25806451612903,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 31
  }
}
26/01/04 17:19:04 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/01/04 17:19:04 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/118 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.118.5105f3c9-ca3d-483f-b38f-810a6b3eb72d.tmp
26/01/04 17:19:04 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.118.5105f3c9-ca3d-483f-b38f-810a6b3eb72d.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/118
26/01/04 17:19:04 INFO MicroBatchExecution: Committed offsets for batch 118. Metadata OffsetSeqMetadata(0,1767547144210,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 17:19:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:19:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:19:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:19:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:19:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:19:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:19:04 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#96355 - airline_prefix.nullCount#96354) > 0)
26/01/04 17:19:04 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#96390 - min_flight_num.nullCount#96389) > 0)
26/01/04 17:19:04 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#96385 - max_flight_num.nullCount#96384) > 0)
26/01/04 17:19:04 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:19:04 INFO DAGScheduler: Got job 239 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 17:19:04 INFO DAGScheduler: Final stage: ResultStage 240 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:19:04 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:19:04 INFO DAGScheduler: Missing parents: List()
26/01/04 17:19:04 INFO DAGScheduler: Submitting ResultStage 240 (MapPartitionsRDD[1202] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:19:04 INFO MemoryStore: Block broadcast_357 stored as values in memory (estimated size 38.5 KiB, free 434.2 MiB)
26/01/04 17:19:04 INFO MemoryStore: Block broadcast_357_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.2 MiB)
26/01/04 17:19:04 INFO BlockManagerInfo: Added broadcast_357_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:19:04 INFO SparkContext: Created broadcast 357 from broadcast at DAGScheduler.scala:1585
26/01/04 17:19:04 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 240 (MapPartitionsRDD[1202] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 17:19:04 INFO TaskSchedulerImpl: Adding task set 240.0 with 2 tasks resource profile 0
26/01/04 17:19:04 INFO TaskSetManager: Starting task 0.0 in stage 240.0 (TID 395) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 17:19:04 INFO TaskSetManager: Starting task 1.0 in stage 240.0 (TID 396) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 17:19:04 INFO BlockManagerInfo: Added broadcast_357_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:19:04 INFO BlockManagerInfo: Added broadcast_357_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:19:04 INFO TaskSetManager: Finished task 1.0 in stage 240.0 (TID 396) in 13 ms on 172.18.0.13 (executor 0) (1/2)
26/01/04 17:19:04 INFO TaskSetManager: Finished task 0.0 in stage 240.0 (TID 395) in 33 ms on 172.18.0.14 (executor 1) (2/2)
26/01/04 17:19:04 INFO TaskSchedulerImpl: Removed TaskSet 240.0, whose tasks have all completed, from pool 
26/01/04 17:19:04 INFO DAGScheduler: ResultStage 240 (start at NativeMethodAccessorImpl.java:0) finished in 0.037 s
26/01/04 17:19:04 INFO DAGScheduler: Job 239 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:19:04 INFO TaskSchedulerImpl: Killing all running tasks in stage 240: Stage finished
26/01/04 17:19:04 INFO DAGScheduler: Job 239 finished: start at NativeMethodAccessorImpl.java:0, took 0.039241 s
26/01/04 17:19:04 INFO MemoryStore: Block broadcast_358_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.2 MiB)
26/01/04 17:19:04 INFO BlockManagerInfo: Added broadcast_358_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:19:04 INFO SparkContext: Created broadcast 358 from start at NativeMethodAccessorImpl.java:0
26/01/04 17:19:04 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 118, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@31a7ca8]. The input RDD has 1 partitions.
26/01/04 17:19:04 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:19:04 INFO DAGScheduler: Got job 240 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/01/04 17:19:04 INFO DAGScheduler: Final stage: ResultStage 241 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:19:04 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:19:04 INFO DAGScheduler: Missing parents: List()
26/01/04 17:19:04 INFO DAGScheduler: Submitting ResultStage 241 (MapPartitionsRDD[1207] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:19:04 INFO MemoryStore: Block broadcast_359 stored as values in memory (estimated size 52.9 KiB, free 434.2 MiB)
26/01/04 17:19:04 INFO MemoryStore: Block broadcast_359_piece0 stored as bytes in memory (estimated size 20.2 KiB, free 434.1 MiB)
26/01/04 17:19:04 INFO BlockManagerInfo: Added broadcast_359_piece0 in memory on spark-master:33535 (size: 20.2 KiB, free: 434.3 MiB)
26/01/04 17:19:04 INFO SparkContext: Created broadcast 359 from broadcast at DAGScheduler.scala:1585
26/01/04 17:19:04 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 241 (MapPartitionsRDD[1207] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/01/04 17:19:04 INFO TaskSchedulerImpl: Adding task set 241.0 with 1 tasks resource profile 0
26/01/04 17:19:04 INFO TaskSetManager: Starting task 0.0 in stage 241.0 (TID 397) (172.18.0.13, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:19:04 INFO BlockManagerInfo: Added broadcast_359_piece0 in memory on 172.18.0.13:44401 (size: 20.2 KiB, free: 434.3 MiB)
26/01/04 17:19:04 INFO BlockManagerInfo: Added broadcast_358_piece0 in memory on 172.18.0.13:44401 (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:19:04 INFO TaskSetManager: Finished task 0.0 in stage 241.0 (TID 397) in 551 ms on 172.18.0.13 (executor 0) (1/1)
26/01/04 17:19:04 INFO TaskSchedulerImpl: Removed TaskSet 241.0, whose tasks have all completed, from pool 
26/01/04 17:19:04 INFO DAGScheduler: ResultStage 241 (start at NativeMethodAccessorImpl.java:0) finished in 0.557 s
26/01/04 17:19:04 INFO DAGScheduler: Job 240 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:19:04 INFO TaskSchedulerImpl: Killing all running tasks in stage 241: Stage finished
26/01/04 17:19:04 INFO DAGScheduler: Job 240 finished: start at NativeMethodAccessorImpl.java:0, took 0.558412 s
26/01/04 17:19:04 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 118, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@31a7ca8] is committing.
26/01/04 17:19:04 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 118, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@31a7ca8] committed.
26/01/04 17:19:04 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/118 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.118.fda14afc-2aac-4a92-a2f0-c4857848b1c2.tmp
26/01/04 17:19:05 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.118.fda14afc-2aac-4a92-a2f0-c4857848b1c2.tmp to file:/tmp/spark-checkpoint-enrichment/commits/118
26/01/04 17:19:05 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T17:19:04.208Z",
  "batchId" : 118,
  "numInputRows" : 30,
  "inputRowsPerSecond" : 2500.0,
  "processedRowsPerSecond" : 36.85503685503686,
  "durationMs" : {
    "addBatch" : 664,
    "commitOffsets" : 67,
    "getBatch" : 0,
    "latestOffset" : 2,
    "queryPlanning" : 16,
    "triggerExecution" : 814,
    "walCommit" : 65
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "0" : 2799
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "0" : 2829
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "0" : 2829
      }
    },
    "numInputRows" : 30,
    "inputRowsPerSecond" : 2500.0,
    "processedRowsPerSecond" : 36.85503685503686,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 31
  }
}
26/01/04 17:19:06 INFO BlockManagerInfo: Removed broadcast_359_piece0 on spark-master:33535 in memory (size: 20.2 KiB, free: 434.3 MiB)
26/01/04 17:19:06 INFO BlockManagerInfo: Removed broadcast_359_piece0 on 172.18.0.13:44401 in memory (size: 20.2 KiB, free: 434.3 MiB)
26/01/04 17:19:06 INFO BlockManagerInfo: Removed broadcast_356_piece0 on spark-master:33535 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:19:06 INFO BlockManagerInfo: Removed broadcast_356_piece0 on 172.18.0.13:44401 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:19:06 INFO BlockManagerInfo: Removed broadcast_357_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:19:06 INFO BlockManagerInfo: Removed broadcast_357_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:19:06 INFO BlockManagerInfo: Removed broadcast_357_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:19:06 INFO BlockManagerInfo: Removed broadcast_355_piece0 on spark-master:33535 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:19:06 INFO BlockManagerInfo: Removed broadcast_355_piece0 on 172.18.0.13:44401 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:19:06 INFO BlockManagerInfo: Removed broadcast_354_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:19:06 INFO BlockManagerInfo: Removed broadcast_354_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:19:06 INFO BlockManagerInfo: Removed broadcast_354_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:19:15 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/01/04 17:19:15 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/119 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.119.57bc7269-02d0-44ed-92eb-50f801ffd8df.tmp
26/01/04 17:19:15 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.119.57bc7269-02d0-44ed-92eb-50f801ffd8df.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/119
26/01/04 17:19:15 INFO MicroBatchExecution: Committed offsets for batch 119. Metadata OffsetSeqMetadata(0,1767547155227,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 17:19:15 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:19:15 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:19:15 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:19:15 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:19:15 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:19:15 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:19:15 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#97159 - airline_prefix.nullCount#97158) > 0)
26/01/04 17:19:15 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#97194 - min_flight_num.nullCount#97193) > 0)
26/01/04 17:19:15 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#97189 - max_flight_num.nullCount#97188) > 0)
26/01/04 17:19:15 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:19:15 INFO DAGScheduler: Got job 241 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 17:19:15 INFO DAGScheduler: Final stage: ResultStage 242 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:19:15 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:19:15 INFO DAGScheduler: Missing parents: List()
26/01/04 17:19:15 INFO DAGScheduler: Submitting ResultStage 242 (MapPartitionsRDD[1212] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:19:15 INFO MemoryStore: Block broadcast_360 stored as values in memory (estimated size 38.5 KiB, free 434.4 MiB)
26/01/04 17:19:15 INFO MemoryStore: Block broadcast_360_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.3 MiB)
26/01/04 17:19:15 INFO BlockManagerInfo: Added broadcast_360_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:19:15 INFO BlockManagerInfo: Removed broadcast_358_piece0 on spark-master:33535 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:19:15 INFO SparkContext: Created broadcast 360 from broadcast at DAGScheduler.scala:1585
26/01/04 17:19:15 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 242 (MapPartitionsRDD[1212] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 17:19:15 INFO TaskSchedulerImpl: Adding task set 242.0 with 2 tasks resource profile 0
26/01/04 17:19:15 INFO BlockManagerInfo: Removed broadcast_358_piece0 on 172.18.0.13:44401 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:19:15 INFO TaskSetManager: Starting task 0.0 in stage 242.0 (TID 398) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 17:19:15 INFO TaskSetManager: Starting task 1.0 in stage 242.0 (TID 399) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 17:19:15 INFO BlockManagerInfo: Added broadcast_360_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:19:15 INFO BlockManagerInfo: Added broadcast_360_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:19:15 INFO TaskSetManager: Finished task 1.0 in stage 242.0 (TID 399) in 22 ms on 172.18.0.13 (executor 0) (1/2)
26/01/04 17:19:15 INFO TaskSetManager: Finished task 0.0 in stage 242.0 (TID 398) in 39 ms on 172.18.0.14 (executor 1) (2/2)
26/01/04 17:19:15 INFO TaskSchedulerImpl: Removed TaskSet 242.0, whose tasks have all completed, from pool 
26/01/04 17:19:15 INFO DAGScheduler: ResultStage 242 (start at NativeMethodAccessorImpl.java:0) finished in 0.054 s
26/01/04 17:19:15 INFO DAGScheduler: Job 241 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:19:15 INFO TaskSchedulerImpl: Killing all running tasks in stage 242: Stage finished
26/01/04 17:19:15 INFO DAGScheduler: Job 241 finished: start at NativeMethodAccessorImpl.java:0, took 0.055507 s
26/01/04 17:19:15 INFO MemoryStore: Block broadcast_361_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.3 MiB)
26/01/04 17:19:15 INFO BlockManagerInfo: Added broadcast_361_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:19:15 INFO SparkContext: Created broadcast 361 from start at NativeMethodAccessorImpl.java:0
26/01/04 17:19:15 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 119, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@4a830b9c]. The input RDD has 1 partitions.
26/01/04 17:19:15 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:19:15 INFO DAGScheduler: Got job 242 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/01/04 17:19:15 INFO DAGScheduler: Final stage: ResultStage 243 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:19:15 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:19:15 INFO DAGScheduler: Missing parents: List()
26/01/04 17:19:15 INFO DAGScheduler: Submitting ResultStage 243 (MapPartitionsRDD[1217] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:19:15 INFO MemoryStore: Block broadcast_362 stored as values in memory (estimated size 52.9 KiB, free 434.3 MiB)
26/01/04 17:19:15 INFO MemoryStore: Block broadcast_362_piece0 stored as bytes in memory (estimated size 20.1 KiB, free 434.3 MiB)
26/01/04 17:19:15 INFO BlockManagerInfo: Added broadcast_362_piece0 in memory on spark-master:33535 (size: 20.1 KiB, free: 434.4 MiB)
26/01/04 17:19:15 INFO SparkContext: Created broadcast 362 from broadcast at DAGScheduler.scala:1585
26/01/04 17:19:15 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 243 (MapPartitionsRDD[1217] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/01/04 17:19:15 INFO TaskSchedulerImpl: Adding task set 243.0 with 1 tasks resource profile 0
26/01/04 17:19:15 INFO TaskSetManager: Starting task 0.0 in stage 243.0 (TID 400) (172.18.0.13, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:19:15 INFO BlockManagerInfo: Added broadcast_362_piece0 in memory on 172.18.0.13:44401 (size: 20.1 KiB, free: 434.4 MiB)
26/01/04 17:19:15 INFO BlockManagerInfo: Added broadcast_361_piece0 in memory on 172.18.0.13:44401 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:19:16 INFO TaskSetManager: Finished task 0.0 in stage 243.0 (TID 400) in 550 ms on 172.18.0.13 (executor 0) (1/1)
26/01/04 17:19:16 INFO TaskSchedulerImpl: Removed TaskSet 243.0, whose tasks have all completed, from pool 
26/01/04 17:19:16 INFO DAGScheduler: ResultStage 243 (start at NativeMethodAccessorImpl.java:0) finished in 0.554 s
26/01/04 17:19:16 INFO DAGScheduler: Job 242 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:19:16 INFO TaskSchedulerImpl: Killing all running tasks in stage 243: Stage finished
26/01/04 17:19:16 INFO DAGScheduler: Job 242 finished: start at NativeMethodAccessorImpl.java:0, took 0.556812 s
26/01/04 17:19:16 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 119, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@4a830b9c] is committing.
26/01/04 17:19:16 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 119, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@4a830b9c] committed.
26/01/04 17:19:16 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/119 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.119.45155f50-2043-4c13-b5b3-3f59b8a18621.tmp
26/01/04 17:19:16 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.119.45155f50-2043-4c13-b5b3-3f59b8a18621.tmp to file:/tmp/spark-checkpoint-enrichment/commits/119
26/01/04 17:19:16 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T17:19:15.226Z",
  "batchId" : 119,
  "numInputRows" : 30,
  "inputRowsPerSecond" : 2307.6923076923076,
  "processedRowsPerSecond" : 34.0522133938706,
  "durationMs" : {
    "addBatch" : 672,
    "commitOffsets" : 70,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 26,
    "triggerExecution" : 881,
    "walCommit" : 112
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "0" : 2829
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "0" : 2859
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "0" : 2859
      }
    },
    "numInputRows" : 30,
    "inputRowsPerSecond" : 2307.6923076923076,
    "processedRowsPerSecond" : 34.0522133938706,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 31
  }
}
26/01/04 17:19:26 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/01/04 17:19:26 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/120 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.120.43be097e-66fa-47fa-be41-ede82a0e26bb.tmp
26/01/04 17:19:26 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.120.43be097e-66fa-47fa-be41-ede82a0e26bb.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/120
26/01/04 17:19:26 INFO MicroBatchExecution: Committed offsets for batch 120. Metadata OffsetSeqMetadata(0,1767547166257,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 17:19:26 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:19:26 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:19:26 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:19:26 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:19:26 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:19:26 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:19:26 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#97963 - airline_prefix.nullCount#97962) > 0)
26/01/04 17:19:26 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#97998 - min_flight_num.nullCount#97997) > 0)
26/01/04 17:19:26 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#97993 - max_flight_num.nullCount#97992) > 0)
26/01/04 17:19:26 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:19:26 INFO DAGScheduler: Got job 243 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 17:19:26 INFO DAGScheduler: Final stage: ResultStage 244 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:19:26 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:19:26 INFO DAGScheduler: Missing parents: List()
26/01/04 17:19:26 INFO DAGScheduler: Submitting ResultStage 244 (MapPartitionsRDD[1222] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:19:26 INFO MemoryStore: Block broadcast_363 stored as values in memory (estimated size 38.5 KiB, free 434.2 MiB)
26/01/04 17:19:26 INFO MemoryStore: Block broadcast_363_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.2 MiB)
26/01/04 17:19:26 INFO BlockManagerInfo: Added broadcast_363_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:19:26 INFO SparkContext: Created broadcast 363 from broadcast at DAGScheduler.scala:1585
26/01/04 17:19:26 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 244 (MapPartitionsRDD[1222] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 17:19:26 INFO TaskSchedulerImpl: Adding task set 244.0 with 2 tasks resource profile 0
26/01/04 17:19:26 INFO TaskSetManager: Starting task 0.0 in stage 244.0 (TID 401) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 17:19:26 INFO TaskSetManager: Starting task 1.0 in stage 244.0 (TID 402) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 17:19:26 INFO BlockManagerInfo: Added broadcast_363_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:19:26 INFO BlockManagerInfo: Added broadcast_363_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:19:26 INFO TaskSetManager: Finished task 1.0 in stage 244.0 (TID 402) in 22 ms on 172.18.0.13 (executor 0) (1/2)
26/01/04 17:19:26 INFO TaskSetManager: Finished task 0.0 in stage 244.0 (TID 401) in 49 ms on 172.18.0.14 (executor 1) (2/2)
26/01/04 17:19:26 INFO TaskSchedulerImpl: Removed TaskSet 244.0, whose tasks have all completed, from pool 
26/01/04 17:19:26 INFO DAGScheduler: ResultStage 244 (start at NativeMethodAccessorImpl.java:0) finished in 0.055 s
26/01/04 17:19:26 INFO DAGScheduler: Job 243 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:19:26 INFO TaskSchedulerImpl: Killing all running tasks in stage 244: Stage finished
26/01/04 17:19:26 INFO DAGScheduler: Job 243 finished: start at NativeMethodAccessorImpl.java:0, took 0.056671 s
26/01/04 17:19:26 INFO MemoryStore: Block broadcast_364_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.2 MiB)
26/01/04 17:19:26 INFO BlockManagerInfo: Added broadcast_364_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:19:26 INFO SparkContext: Created broadcast 364 from start at NativeMethodAccessorImpl.java:0
26/01/04 17:19:26 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 120, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@1120a6db]. The input RDD has 1 partitions.
26/01/04 17:19:26 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:19:26 INFO DAGScheduler: Got job 244 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/01/04 17:19:26 INFO DAGScheduler: Final stage: ResultStage 245 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:19:26 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:19:26 INFO DAGScheduler: Missing parents: List()
26/01/04 17:19:26 INFO DAGScheduler: Submitting ResultStage 245 (MapPartitionsRDD[1227] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:19:26 INFO MemoryStore: Block broadcast_365 stored as values in memory (estimated size 52.9 KiB, free 434.2 MiB)
26/01/04 17:19:26 INFO MemoryStore: Block broadcast_365_piece0 stored as bytes in memory (estimated size 20.2 KiB, free 434.1 MiB)
26/01/04 17:19:26 INFO BlockManagerInfo: Added broadcast_365_piece0 in memory on spark-master:33535 (size: 20.2 KiB, free: 434.3 MiB)
26/01/04 17:19:26 INFO SparkContext: Created broadcast 365 from broadcast at DAGScheduler.scala:1585
26/01/04 17:19:26 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 245 (MapPartitionsRDD[1227] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/01/04 17:19:26 INFO TaskSchedulerImpl: Adding task set 245.0 with 1 tasks resource profile 0
26/01/04 17:19:26 INFO TaskSetManager: Starting task 0.0 in stage 245.0 (TID 403) (172.18.0.13, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:19:26 INFO BlockManagerInfo: Added broadcast_365_piece0 in memory on 172.18.0.13:44401 (size: 20.2 KiB, free: 434.3 MiB)
26/01/04 17:19:26 INFO BlockManagerInfo: Added broadcast_364_piece0 in memory on 172.18.0.13:44401 (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:19:27 INFO TaskSetManager: Finished task 0.0 in stage 245.0 (TID 403) in 562 ms on 172.18.0.13 (executor 0) (1/1)
26/01/04 17:19:27 INFO TaskSchedulerImpl: Removed TaskSet 245.0, whose tasks have all completed, from pool 
26/01/04 17:19:27 INFO DAGScheduler: ResultStage 245 (start at NativeMethodAccessorImpl.java:0) finished in 0.568 s
26/01/04 17:19:27 INFO DAGScheduler: Job 244 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:19:27 INFO TaskSchedulerImpl: Killing all running tasks in stage 245: Stage finished
26/01/04 17:19:27 INFO DAGScheduler: Job 244 finished: start at NativeMethodAccessorImpl.java:0, took 0.568988 s
26/01/04 17:19:27 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 120, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@1120a6db] is committing.
26/01/04 17:19:27 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 120, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@1120a6db] committed.
26/01/04 17:19:27 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/120 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.120.84c47ef4-0efb-4c50-85a3-1087d73e448f.tmp
26/01/04 17:19:27 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.120.84c47ef4-0efb-4c50-85a3-1087d73e448f.tmp to file:/tmp/spark-checkpoint-enrichment/commits/120
26/01/04 17:19:27 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T17:19:26.255Z",
  "batchId" : 120,
  "numInputRows" : 30,
  "inputRowsPerSecond" : 2500.0,
  "processedRowsPerSecond" : 34.32494279176201,
  "durationMs" : {
    "addBatch" : 699,
    "commitOffsets" : 62,
    "getBatch" : 0,
    "latestOffset" : 2,
    "queryPlanning" : 32,
    "triggerExecution" : 874,
    "walCommit" : 79
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "0" : 2859
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "0" : 2889
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "0" : 2889
      }
    },
    "numInputRows" : 30,
    "inputRowsPerSecond" : 2500.0,
    "processedRowsPerSecond" : 34.32494279176201,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 31
  }
}
26/01/04 17:19:29 INFO BlockManagerInfo: Removed broadcast_363_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:19:29 INFO BlockManagerInfo: Removed broadcast_363_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:19:29 INFO BlockManagerInfo: Removed broadcast_363_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:19:29 INFO BlockManagerInfo: Removed broadcast_362_piece0 on spark-master:33535 in memory (size: 20.1 KiB, free: 434.4 MiB)
26/01/04 17:19:29 INFO BlockManagerInfo: Removed broadcast_362_piece0 on 172.18.0.13:44401 in memory (size: 20.1 KiB, free: 434.4 MiB)
26/01/04 17:19:29 INFO BlockManagerInfo: Removed broadcast_361_piece0 on spark-master:33535 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:19:29 INFO BlockManagerInfo: Removed broadcast_361_piece0 on 172.18.0.13:44401 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:19:29 INFO BlockManagerInfo: Removed broadcast_365_piece0 on spark-master:33535 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:19:29 INFO BlockManagerInfo: Removed broadcast_365_piece0 on 172.18.0.13:44401 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:19:29 INFO BlockManagerInfo: Removed broadcast_360_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:19:29 INFO BlockManagerInfo: Removed broadcast_360_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:19:29 INFO BlockManagerInfo: Removed broadcast_360_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:19:37 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/01/04 17:19:37 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/121 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.121.05798982-3c84-414c-8596-a4334d005074.tmp
26/01/04 17:19:37 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.121.05798982-3c84-414c-8596-a4334d005074.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/121
26/01/04 17:19:37 INFO MicroBatchExecution: Committed offsets for batch 121. Metadata OffsetSeqMetadata(0,1767547177269,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 17:19:37 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:19:37 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:19:37 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:19:37 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:19:37 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:19:37 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:19:37 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#98767 - airline_prefix.nullCount#98766) > 0)
26/01/04 17:19:37 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#98802 - min_flight_num.nullCount#98801) > 0)
26/01/04 17:19:37 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#98797 - max_flight_num.nullCount#98796) > 0)
26/01/04 17:19:37 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:19:37 INFO DAGScheduler: Got job 245 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 17:19:37 INFO DAGScheduler: Final stage: ResultStage 246 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:19:37 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:19:37 INFO DAGScheduler: Missing parents: List()
26/01/04 17:19:37 INFO DAGScheduler: Submitting ResultStage 246 (MapPartitionsRDD[1232] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:19:37 INFO MemoryStore: Block broadcast_366 stored as values in memory (estimated size 38.5 KiB, free 434.4 MiB)
26/01/04 17:19:37 INFO BlockManagerInfo: Removed broadcast_364_piece0 on spark-master:33535 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:19:37 INFO MemoryStore: Block broadcast_366_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.3 MiB)
26/01/04 17:19:37 INFO BlockManagerInfo: Added broadcast_366_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:19:37 INFO SparkContext: Created broadcast 366 from broadcast at DAGScheduler.scala:1585
26/01/04 17:19:37 INFO BlockManagerInfo: Removed broadcast_364_piece0 on 172.18.0.13:44401 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:19:37 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 246 (MapPartitionsRDD[1232] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 17:19:37 INFO TaskSchedulerImpl: Adding task set 246.0 with 2 tasks resource profile 0
26/01/04 17:19:37 INFO TaskSetManager: Starting task 0.0 in stage 246.0 (TID 404) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 17:19:37 INFO TaskSetManager: Starting task 1.0 in stage 246.0 (TID 405) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 17:19:37 INFO BlockManagerInfo: Added broadcast_366_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:19:37 INFO BlockManagerInfo: Added broadcast_366_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:19:37 INFO TaskSetManager: Finished task 1.0 in stage 246.0 (TID 405) in 26 ms on 172.18.0.13 (executor 0) (1/2)
26/01/04 17:19:37 INFO TaskSetManager: Finished task 0.0 in stage 246.0 (TID 404) in 49 ms on 172.18.0.14 (executor 1) (2/2)
26/01/04 17:19:37 INFO TaskSchedulerImpl: Removed TaskSet 246.0, whose tasks have all completed, from pool 
26/01/04 17:19:37 INFO DAGScheduler: ResultStage 246 (start at NativeMethodAccessorImpl.java:0) finished in 0.061 s
26/01/04 17:19:37 INFO DAGScheduler: Job 245 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:19:37 INFO TaskSchedulerImpl: Killing all running tasks in stage 246: Stage finished
26/01/04 17:19:37 INFO DAGScheduler: Job 245 finished: start at NativeMethodAccessorImpl.java:0, took 0.062989 s
26/01/04 17:19:37 INFO MemoryStore: Block broadcast_367_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.3 MiB)
26/01/04 17:19:37 INFO BlockManagerInfo: Added broadcast_367_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:19:37 INFO SparkContext: Created broadcast 367 from start at NativeMethodAccessorImpl.java:0
26/01/04 17:19:37 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 121, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@1b2e6f0a]. The input RDD has 1 partitions.
26/01/04 17:19:37 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:19:37 INFO DAGScheduler: Got job 246 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/01/04 17:19:37 INFO DAGScheduler: Final stage: ResultStage 247 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:19:37 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:19:37 INFO DAGScheduler: Missing parents: List()
26/01/04 17:19:37 INFO DAGScheduler: Submitting ResultStage 247 (MapPartitionsRDD[1237] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:19:37 INFO MemoryStore: Block broadcast_368 stored as values in memory (estimated size 52.9 KiB, free 434.3 MiB)
26/01/04 17:19:37 INFO MemoryStore: Block broadcast_368_piece0 stored as bytes in memory (estimated size 20.2 KiB, free 434.3 MiB)
26/01/04 17:19:37 INFO BlockManagerInfo: Added broadcast_368_piece0 in memory on spark-master:33535 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:19:37 INFO SparkContext: Created broadcast 368 from broadcast at DAGScheduler.scala:1585
26/01/04 17:19:37 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 247 (MapPartitionsRDD[1237] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/01/04 17:19:37 INFO TaskSchedulerImpl: Adding task set 247.0 with 1 tasks resource profile 0
26/01/04 17:19:37 INFO TaskSetManager: Starting task 0.0 in stage 247.0 (TID 406) (172.18.0.13, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:19:37 INFO BlockManagerInfo: Added broadcast_368_piece0 in memory on 172.18.0.13:44401 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:19:37 INFO BlockManagerInfo: Added broadcast_367_piece0 in memory on 172.18.0.13:44401 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:19:38 INFO TaskSetManager: Finished task 0.0 in stage 247.0 (TID 406) in 558 ms on 172.18.0.13 (executor 0) (1/1)
26/01/04 17:19:38 INFO TaskSchedulerImpl: Removed TaskSet 247.0, whose tasks have all completed, from pool 
26/01/04 17:19:38 INFO DAGScheduler: ResultStage 247 (start at NativeMethodAccessorImpl.java:0) finished in 0.564 s
26/01/04 17:19:38 INFO DAGScheduler: Job 246 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:19:38 INFO TaskSchedulerImpl: Killing all running tasks in stage 247: Stage finished
26/01/04 17:19:38 INFO DAGScheduler: Job 246 finished: start at NativeMethodAccessorImpl.java:0, took 0.565645 s
26/01/04 17:19:38 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 121, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@1b2e6f0a] is committing.
26/01/04 17:19:38 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 121, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@1b2e6f0a] committed.
26/01/04 17:19:38 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/121 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.121.6a610c21-bdc9-4739-aa52-e92dcdde9e89.tmp
26/01/04 17:19:38 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.121.6a610c21-bdc9-4739-aa52-e92dcdde9e89.tmp to file:/tmp/spark-checkpoint-enrichment/commits/121
26/01/04 17:19:38 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T17:19:37.267Z",
  "batchId" : 121,
  "numInputRows" : 30,
  "inputRowsPerSecond" : 2500.0,
  "processedRowsPerSecond" : 32.6797385620915,
  "durationMs" : {
    "addBatch" : 698,
    "commitOffsets" : 67,
    "getBatch" : 0,
    "latestOffset" : 2,
    "queryPlanning" : 33,
    "triggerExecution" : 918,
    "walCommit" : 117
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "0" : 2889
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "0" : 2919
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "0" : 2919
      }
    },
    "numInputRows" : 30,
    "inputRowsPerSecond" : 2500.0,
    "processedRowsPerSecond" : 32.6797385620915,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 31
  }
}
26/01/04 17:19:48 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/01/04 17:19:48 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/122 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.122.bdc475e4-a667-438d-a074-b9ea2eeb57ff.tmp
26/01/04 17:19:48 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.122.bdc475e4-a667-438d-a074-b9ea2eeb57ff.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/122
26/01/04 17:19:48 INFO MicroBatchExecution: Committed offsets for batch 122. Metadata OffsetSeqMetadata(0,1767547188278,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 17:19:48 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:19:48 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:19:48 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:19:48 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:19:48 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:19:48 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:19:48 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#99571 - airline_prefix.nullCount#99570) > 0)
26/01/04 17:19:48 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#99606 - min_flight_num.nullCount#99605) > 0)
26/01/04 17:19:48 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#99601 - max_flight_num.nullCount#99600) > 0)
26/01/04 17:19:48 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:19:48 INFO DAGScheduler: Got job 247 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 17:19:48 INFO DAGScheduler: Final stage: ResultStage 248 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:19:48 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:19:48 INFO DAGScheduler: Missing parents: List()
26/01/04 17:19:48 INFO DAGScheduler: Submitting ResultStage 248 (MapPartitionsRDD[1242] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:19:48 INFO MemoryStore: Block broadcast_369 stored as values in memory (estimated size 38.5 KiB, free 434.2 MiB)
26/01/04 17:19:48 INFO MemoryStore: Block broadcast_369_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.2 MiB)
26/01/04 17:19:48 INFO BlockManagerInfo: Added broadcast_369_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:19:48 INFO SparkContext: Created broadcast 369 from broadcast at DAGScheduler.scala:1585
26/01/04 17:19:48 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 248 (MapPartitionsRDD[1242] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 17:19:48 INFO TaskSchedulerImpl: Adding task set 248.0 with 2 tasks resource profile 0
26/01/04 17:19:48 INFO TaskSetManager: Starting task 0.0 in stage 248.0 (TID 407) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 17:19:48 INFO TaskSetManager: Starting task 1.0 in stage 248.0 (TID 408) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 17:19:48 INFO BlockManagerInfo: Added broadcast_369_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:19:48 INFO BlockManagerInfo: Added broadcast_369_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:19:48 INFO TaskSetManager: Finished task 1.0 in stage 248.0 (TID 408) in 18 ms on 172.18.0.13 (executor 0) (1/2)
26/01/04 17:19:48 INFO TaskSetManager: Finished task 0.0 in stage 248.0 (TID 407) in 46 ms on 172.18.0.14 (executor 1) (2/2)
26/01/04 17:19:48 INFO TaskSchedulerImpl: Removed TaskSet 248.0, whose tasks have all completed, from pool 
26/01/04 17:19:48 INFO DAGScheduler: ResultStage 248 (start at NativeMethodAccessorImpl.java:0) finished in 0.052 s
26/01/04 17:19:48 INFO DAGScheduler: Job 247 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:19:48 INFO TaskSchedulerImpl: Killing all running tasks in stage 248: Stage finished
26/01/04 17:19:48 INFO DAGScheduler: Job 247 finished: start at NativeMethodAccessorImpl.java:0, took 0.054512 s
26/01/04 17:19:48 INFO MemoryStore: Block broadcast_370_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.2 MiB)
26/01/04 17:19:48 INFO BlockManagerInfo: Added broadcast_370_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:19:48 INFO SparkContext: Created broadcast 370 from start at NativeMethodAccessorImpl.java:0
26/01/04 17:19:48 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 122, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@202c81d6]. The input RDD has 1 partitions.
26/01/04 17:19:48 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:19:48 INFO DAGScheduler: Got job 248 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/01/04 17:19:48 INFO DAGScheduler: Final stage: ResultStage 249 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:19:48 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:19:48 INFO DAGScheduler: Missing parents: List()
26/01/04 17:19:48 INFO DAGScheduler: Submitting ResultStage 249 (MapPartitionsRDD[1247] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:19:48 INFO MemoryStore: Block broadcast_371 stored as values in memory (estimated size 52.9 KiB, free 434.2 MiB)
26/01/04 17:19:48 INFO MemoryStore: Block broadcast_371_piece0 stored as bytes in memory (estimated size 20.1 KiB, free 434.1 MiB)
26/01/04 17:19:48 INFO BlockManagerInfo: Added broadcast_371_piece0 in memory on spark-master:33535 (size: 20.1 KiB, free: 434.3 MiB)
26/01/04 17:19:48 INFO SparkContext: Created broadcast 371 from broadcast at DAGScheduler.scala:1585
26/01/04 17:19:48 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 249 (MapPartitionsRDD[1247] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/01/04 17:19:48 INFO TaskSchedulerImpl: Adding task set 249.0 with 1 tasks resource profile 0
26/01/04 17:19:48 INFO TaskSetManager: Starting task 0.0 in stage 249.0 (TID 409) (172.18.0.13, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:19:48 INFO BlockManagerInfo: Added broadcast_371_piece0 in memory on 172.18.0.13:44401 (size: 20.1 KiB, free: 434.3 MiB)
26/01/04 17:19:48 INFO BlockManagerInfo: Added broadcast_370_piece0 in memory on 172.18.0.13:44401 (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:19:49 INFO TaskSetManager: Finished task 0.0 in stage 249.0 (TID 409) in 558 ms on 172.18.0.13 (executor 0) (1/1)
26/01/04 17:19:49 INFO TaskSchedulerImpl: Removed TaskSet 249.0, whose tasks have all completed, from pool 
26/01/04 17:19:49 INFO DAGScheduler: ResultStage 249 (start at NativeMethodAccessorImpl.java:0) finished in 0.565 s
26/01/04 17:19:49 INFO DAGScheduler: Job 248 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:19:49 INFO TaskSchedulerImpl: Killing all running tasks in stage 249: Stage finished
26/01/04 17:19:49 INFO DAGScheduler: Job 248 finished: start at NativeMethodAccessorImpl.java:0, took 0.566908 s
26/01/04 17:19:49 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 122, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@202c81d6] is committing.
26/01/04 17:19:49 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 122, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@202c81d6] committed.
26/01/04 17:19:49 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/122 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.122.cb8b7235-d856-45a7-b859-96b4dd8669ca.tmp
26/01/04 17:19:49 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.122.cb8b7235-d856-45a7-b859-96b4dd8669ca.tmp to file:/tmp/spark-checkpoint-enrichment/commits/122
26/01/04 17:19:49 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T17:19:48.277Z",
  "batchId" : 122,
  "numInputRows" : 30,
  "inputRowsPerSecond" : 2727.2727272727275,
  "processedRowsPerSecond" : 34.44316877152698,
  "durationMs" : {
    "addBatch" : 686,
    "commitOffsets" : 77,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 31,
    "triggerExecution" : 871,
    "walCommit" : 75
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "0" : 2919
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "0" : 2949
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "0" : 2949
      }
    },
    "numInputRows" : 30,
    "inputRowsPerSecond" : 2727.2727272727275,
    "processedRowsPerSecond" : 34.44316877152698,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 31
  }
}
26/01/04 17:19:50 INFO BlockManagerInfo: Removed broadcast_368_piece0 on spark-master:33535 in memory (size: 20.2 KiB, free: 434.3 MiB)
26/01/04 17:19:50 INFO BlockManagerInfo: Removed broadcast_368_piece0 on 172.18.0.13:44401 in memory (size: 20.2 KiB, free: 434.3 MiB)
26/01/04 17:19:50 INFO BlockManagerInfo: Removed broadcast_371_piece0 on spark-master:33535 in memory (size: 20.1 KiB, free: 434.4 MiB)
26/01/04 17:19:50 INFO BlockManagerInfo: Removed broadcast_371_piece0 on 172.18.0.13:44401 in memory (size: 20.1 KiB, free: 434.4 MiB)
26/01/04 17:19:50 INFO BlockManagerInfo: Removed broadcast_366_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:19:50 INFO BlockManagerInfo: Removed broadcast_366_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:19:50 INFO BlockManagerInfo: Removed broadcast_366_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:19:50 INFO BlockManagerInfo: Removed broadcast_367_piece0 on spark-master:33535 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:19:50 INFO BlockManagerInfo: Removed broadcast_367_piece0 on 172.18.0.13:44401 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:19:50 INFO BlockManagerInfo: Removed broadcast_369_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:19:50 INFO BlockManagerInfo: Removed broadcast_369_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:19:50 INFO BlockManagerInfo: Removed broadcast_369_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:19:59 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/01/04 17:19:59 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/123 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.123.a1929019-bb1f-4134-8ba2-469f35c057b0.tmp
26/01/04 17:19:59 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.123.a1929019-bb1f-4134-8ba2-469f35c057b0.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/123
26/01/04 17:19:59 INFO MicroBatchExecution: Committed offsets for batch 123. Metadata OffsetSeqMetadata(0,1767547199283,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 17:19:59 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:19:59 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:19:59 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:19:59 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:19:59 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:19:59 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:19:59 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#100375 - airline_prefix.nullCount#100374) > 0)
26/01/04 17:19:59 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#100410 - min_flight_num.nullCount#100409) > 0)
26/01/04 17:19:59 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#100405 - max_flight_num.nullCount#100404) > 0)
26/01/04 17:19:59 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:19:59 INFO DAGScheduler: Got job 249 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 17:19:59 INFO DAGScheduler: Final stage: ResultStage 250 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:19:59 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:19:59 INFO DAGScheduler: Missing parents: List()
26/01/04 17:19:59 INFO DAGScheduler: Submitting ResultStage 250 (MapPartitionsRDD[1252] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:19:59 INFO MemoryStore: Block broadcast_372 stored as values in memory (estimated size 38.5 KiB, free 434.4 MiB)
26/01/04 17:19:59 INFO MemoryStore: Block broadcast_372_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.3 MiB)
26/01/04 17:19:59 INFO BlockManagerInfo: Removed broadcast_370_piece0 on spark-master:33535 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:19:59 INFO BlockManagerInfo: Added broadcast_372_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:19:59 INFO SparkContext: Created broadcast 372 from broadcast at DAGScheduler.scala:1585
26/01/04 17:19:59 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 250 (MapPartitionsRDD[1252] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 17:19:59 INFO TaskSchedulerImpl: Adding task set 250.0 with 2 tasks resource profile 0
26/01/04 17:19:59 INFO BlockManagerInfo: Removed broadcast_370_piece0 on 172.18.0.13:44401 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:19:59 INFO TaskSetManager: Starting task 0.0 in stage 250.0 (TID 410) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 17:19:59 INFO TaskSetManager: Starting task 1.0 in stage 250.0 (TID 411) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 17:19:59 INFO BlockManagerInfo: Added broadcast_372_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:19:59 INFO BlockManagerInfo: Added broadcast_372_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:19:59 INFO TaskSetManager: Finished task 1.0 in stage 250.0 (TID 411) in 28 ms on 172.18.0.13 (executor 0) (1/2)
26/01/04 17:19:59 INFO TaskSetManager: Finished task 0.0 in stage 250.0 (TID 410) in 53 ms on 172.18.0.14 (executor 1) (2/2)
26/01/04 17:19:59 INFO TaskSchedulerImpl: Removed TaskSet 250.0, whose tasks have all completed, from pool 
26/01/04 17:19:59 INFO DAGScheduler: ResultStage 250 (start at NativeMethodAccessorImpl.java:0) finished in 0.078 s
26/01/04 17:19:59 INFO DAGScheduler: Job 249 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:19:59 INFO TaskSchedulerImpl: Killing all running tasks in stage 250: Stage finished
26/01/04 17:19:59 INFO DAGScheduler: Job 249 finished: start at NativeMethodAccessorImpl.java:0, took 0.080760 s
26/01/04 17:19:59 INFO MemoryStore: Block broadcast_373_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.3 MiB)
26/01/04 17:19:59 INFO BlockManagerInfo: Added broadcast_373_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:19:59 INFO SparkContext: Created broadcast 373 from start at NativeMethodAccessorImpl.java:0
26/01/04 17:19:59 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 123, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@aab7f94]. The input RDD has 1 partitions.
26/01/04 17:19:59 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:19:59 INFO DAGScheduler: Got job 250 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/01/04 17:19:59 INFO DAGScheduler: Final stage: ResultStage 251 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:19:59 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:19:59 INFO DAGScheduler: Missing parents: List()
26/01/04 17:19:59 INFO DAGScheduler: Submitting ResultStage 251 (MapPartitionsRDD[1257] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:19:59 INFO MemoryStore: Block broadcast_374 stored as values in memory (estimated size 52.9 KiB, free 434.3 MiB)
26/01/04 17:19:59 INFO MemoryStore: Block broadcast_374_piece0 stored as bytes in memory (estimated size 20.1 KiB, free 434.3 MiB)
26/01/04 17:19:59 INFO BlockManagerInfo: Added broadcast_374_piece0 in memory on spark-master:33535 (size: 20.1 KiB, free: 434.4 MiB)
26/01/04 17:19:59 INFO SparkContext: Created broadcast 374 from broadcast at DAGScheduler.scala:1585
26/01/04 17:19:59 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 251 (MapPartitionsRDD[1257] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/01/04 17:19:59 INFO TaskSchedulerImpl: Adding task set 251.0 with 1 tasks resource profile 0
26/01/04 17:19:59 INFO TaskSetManager: Starting task 0.0 in stage 251.0 (TID 412) (172.18.0.13, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:19:59 INFO BlockManagerInfo: Added broadcast_374_piece0 in memory on 172.18.0.13:44401 (size: 20.1 KiB, free: 434.4 MiB)
26/01/04 17:19:59 INFO BlockManagerInfo: Added broadcast_373_piece0 in memory on 172.18.0.13:44401 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:20:00 INFO TaskSetManager: Finished task 0.0 in stage 251.0 (TID 412) in 556 ms on 172.18.0.13 (executor 0) (1/1)
26/01/04 17:20:00 INFO TaskSchedulerImpl: Removed TaskSet 251.0, whose tasks have all completed, from pool 
26/01/04 17:20:00 INFO DAGScheduler: ResultStage 251 (start at NativeMethodAccessorImpl.java:0) finished in 0.561 s
26/01/04 17:20:00 INFO DAGScheduler: Job 250 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:20:00 INFO TaskSchedulerImpl: Killing all running tasks in stage 251: Stage finished
26/01/04 17:20:00 INFO DAGScheduler: Job 250 finished: start at NativeMethodAccessorImpl.java:0, took 0.566027 s
26/01/04 17:20:00 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 123, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@aab7f94] is committing.
26/01/04 17:20:00 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 123, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@aab7f94] committed.
26/01/04 17:20:00 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/123 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.123.d9151e61-03c2-493b-8ad8-bfaec13fbfa9.tmp
26/01/04 17:20:00 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.123.d9151e61-03c2-493b-8ad8-bfaec13fbfa9.tmp to file:/tmp/spark-checkpoint-enrichment/commits/123
26/01/04 17:20:00 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T17:19:59.281Z",
  "batchId" : 123,
  "numInputRows" : 30,
  "inputRowsPerSecond" : 2727.2727272727275,
  "processedRowsPerSecond" : 34.285714285714285,
  "durationMs" : {
    "addBatch" : 714,
    "commitOffsets" : 56,
    "getBatch" : 0,
    "latestOffset" : 2,
    "queryPlanning" : 22,
    "triggerExecution" : 875,
    "walCommit" : 79
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "0" : 2949
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "0" : 2979
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "0" : 2979
      }
    },
    "numInputRows" : 30,
    "inputRowsPerSecond" : 2727.2727272727275,
    "processedRowsPerSecond" : 34.285714285714285,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 31
  }
}
26/01/04 17:20:10 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/01/04 17:20:10 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/124 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.124.54c4cb98-e9a5-480d-855e-e1eb434a0f43.tmp
26/01/04 17:20:10 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.124.54c4cb98-e9a5-480d-855e-e1eb434a0f43.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/124
26/01/04 17:20:10 INFO MicroBatchExecution: Committed offsets for batch 124. Metadata OffsetSeqMetadata(0,1767547210310,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 17:20:10 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:20:10 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:20:10 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:20:10 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:20:10 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:20:10 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:20:10 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#101179 - airline_prefix.nullCount#101178) > 0)
26/01/04 17:20:10 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#101214 - min_flight_num.nullCount#101213) > 0)
26/01/04 17:20:10 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#101209 - max_flight_num.nullCount#101208) > 0)
26/01/04 17:20:10 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:20:10 INFO DAGScheduler: Got job 251 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 17:20:10 INFO DAGScheduler: Final stage: ResultStage 252 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:20:10 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:20:10 INFO DAGScheduler: Missing parents: List()
26/01/04 17:20:10 INFO DAGScheduler: Submitting ResultStage 252 (MapPartitionsRDD[1262] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:20:10 INFO MemoryStore: Block broadcast_375 stored as values in memory (estimated size 38.5 KiB, free 434.2 MiB)
26/01/04 17:20:10 INFO MemoryStore: Block broadcast_375_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.2 MiB)
26/01/04 17:20:10 INFO BlockManagerInfo: Added broadcast_375_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:20:10 INFO SparkContext: Created broadcast 375 from broadcast at DAGScheduler.scala:1585
26/01/04 17:20:10 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 252 (MapPartitionsRDD[1262] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 17:20:10 INFO TaskSchedulerImpl: Adding task set 252.0 with 2 tasks resource profile 0
26/01/04 17:20:10 INFO TaskSetManager: Starting task 0.0 in stage 252.0 (TID 413) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 17:20:10 INFO TaskSetManager: Starting task 1.0 in stage 252.0 (TID 414) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 17:20:10 INFO BlockManagerInfo: Added broadcast_375_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:20:10 INFO BlockManagerInfo: Added broadcast_375_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:20:10 INFO TaskSetManager: Finished task 1.0 in stage 252.0 (TID 414) in 15 ms on 172.18.0.13 (executor 0) (1/2)
26/01/04 17:20:10 INFO TaskSetManager: Finished task 0.0 in stage 252.0 (TID 413) in 33 ms on 172.18.0.14 (executor 1) (2/2)
26/01/04 17:20:10 INFO TaskSchedulerImpl: Removed TaskSet 252.0, whose tasks have all completed, from pool 
26/01/04 17:20:10 INFO DAGScheduler: ResultStage 252 (start at NativeMethodAccessorImpl.java:0) finished in 0.037 s
26/01/04 17:20:10 INFO DAGScheduler: Job 251 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:20:10 INFO TaskSchedulerImpl: Killing all running tasks in stage 252: Stage finished
26/01/04 17:20:10 INFO DAGScheduler: Job 251 finished: start at NativeMethodAccessorImpl.java:0, took 0.038834 s
26/01/04 17:20:10 INFO MemoryStore: Block broadcast_376_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.2 MiB)
26/01/04 17:20:10 INFO BlockManagerInfo: Added broadcast_376_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:20:10 INFO SparkContext: Created broadcast 376 from start at NativeMethodAccessorImpl.java:0
26/01/04 17:20:10 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 124, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@693b4555]. The input RDD has 1 partitions.
26/01/04 17:20:10 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:20:10 INFO DAGScheduler: Got job 252 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/01/04 17:20:10 INFO DAGScheduler: Final stage: ResultStage 253 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:20:10 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:20:10 INFO DAGScheduler: Missing parents: List()
26/01/04 17:20:10 INFO DAGScheduler: Submitting ResultStage 253 (MapPartitionsRDD[1267] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:20:10 INFO MemoryStore: Block broadcast_377 stored as values in memory (estimated size 52.9 KiB, free 434.2 MiB)
26/01/04 17:20:10 INFO MemoryStore: Block broadcast_377_piece0 stored as bytes in memory (estimated size 20.2 KiB, free 434.1 MiB)
26/01/04 17:20:10 INFO BlockManagerInfo: Added broadcast_377_piece0 in memory on spark-master:33535 (size: 20.2 KiB, free: 434.3 MiB)
26/01/04 17:20:10 INFO SparkContext: Created broadcast 377 from broadcast at DAGScheduler.scala:1585
26/01/04 17:20:10 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 253 (MapPartitionsRDD[1267] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/01/04 17:20:10 INFO TaskSchedulerImpl: Adding task set 253.0 with 1 tasks resource profile 0
26/01/04 17:20:10 INFO TaskSetManager: Starting task 0.0 in stage 253.0 (TID 415) (172.18.0.13, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:20:10 INFO BlockManagerInfo: Added broadcast_377_piece0 in memory on 172.18.0.13:44401 (size: 20.2 KiB, free: 434.3 MiB)
26/01/04 17:20:10 INFO BlockManagerInfo: Added broadcast_376_piece0 in memory on 172.18.0.13:44401 (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:20:11 INFO TaskSetManager: Finished task 0.0 in stage 253.0 (TID 415) in 560 ms on 172.18.0.13 (executor 0) (1/1)
26/01/04 17:20:11 INFO TaskSchedulerImpl: Removed TaskSet 253.0, whose tasks have all completed, from pool 
26/01/04 17:20:11 INFO DAGScheduler: ResultStage 253 (start at NativeMethodAccessorImpl.java:0) finished in 0.566 s
26/01/04 17:20:11 INFO DAGScheduler: Job 252 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:20:11 INFO TaskSchedulerImpl: Killing all running tasks in stage 253: Stage finished
26/01/04 17:20:11 INFO DAGScheduler: Job 252 finished: start at NativeMethodAccessorImpl.java:0, took 0.567111 s
26/01/04 17:20:11 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 124, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@693b4555] is committing.
26/01/04 17:20:11 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 124, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@693b4555] committed.
26/01/04 17:20:11 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/124 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.124.b14e9db7-e608-4830-92e3-13bfc4569749.tmp
26/01/04 17:20:11 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.124.b14e9db7-e608-4830-92e3-13bfc4569749.tmp to file:/tmp/spark-checkpoint-enrichment/commits/124
26/01/04 17:20:11 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T17:20:10.308Z",
  "batchId" : 124,
  "numInputRows" : 30,
  "inputRowsPerSecond" : 2500.0,
  "processedRowsPerSecond" : 34.403669724770644,
  "durationMs" : {
    "addBatch" : 673,
    "commitOffsets" : 102,
    "getBatch" : 0,
    "latestOffset" : 2,
    "queryPlanning" : 25,
    "triggerExecution" : 872,
    "walCommit" : 70
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "0" : 2979
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "0" : 3009
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "0" : 3009
      }
    },
    "numInputRows" : 30,
    "inputRowsPerSecond" : 2500.0,
    "processedRowsPerSecond" : 34.403669724770644,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 31
  }
}
26/01/04 17:20:13 INFO BlockManagerInfo: Removed broadcast_375_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:20:13 INFO BlockManagerInfo: Removed broadcast_375_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:20:13 INFO BlockManagerInfo: Removed broadcast_375_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:20:13 INFO BlockManagerInfo: Removed broadcast_374_piece0 on spark-master:33535 in memory (size: 20.1 KiB, free: 434.4 MiB)
26/01/04 17:20:13 INFO BlockManagerInfo: Removed broadcast_374_piece0 on 172.18.0.13:44401 in memory (size: 20.1 KiB, free: 434.4 MiB)
26/01/04 17:20:13 INFO BlockManagerInfo: Removed broadcast_373_piece0 on spark-master:33535 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:20:13 INFO BlockManagerInfo: Removed broadcast_373_piece0 on 172.18.0.13:44401 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:20:13 INFO BlockManagerInfo: Removed broadcast_377_piece0 on spark-master:33535 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:20:13 INFO BlockManagerInfo: Removed broadcast_377_piece0 on 172.18.0.13:44401 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:20:13 INFO BlockManagerInfo: Removed broadcast_372_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:20:13 INFO BlockManagerInfo: Removed broadcast_372_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:20:13 INFO BlockManagerInfo: Removed broadcast_372_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:20:21 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/01/04 17:20:21 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/125 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.125.2916d432-6ef1-4fba-8ed6-e02c9d2d7e98.tmp
26/01/04 17:20:21 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.125.2916d432-6ef1-4fba-8ed6-e02c9d2d7e98.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/125
26/01/04 17:20:21 INFO MicroBatchExecution: Committed offsets for batch 125. Metadata OffsetSeqMetadata(0,1767547221320,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 17:20:21 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:20:21 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:20:21 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:20:21 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:20:21 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:20:21 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:20:21 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#101983 - airline_prefix.nullCount#101982) > 0)
26/01/04 17:20:21 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#102018 - min_flight_num.nullCount#102017) > 0)
26/01/04 17:20:21 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#102013 - max_flight_num.nullCount#102012) > 0)
26/01/04 17:20:21 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:20:21 INFO DAGScheduler: Got job 253 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 17:20:21 INFO DAGScheduler: Final stage: ResultStage 254 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:20:21 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:20:21 INFO DAGScheduler: Missing parents: List()
26/01/04 17:20:21 INFO DAGScheduler: Submitting ResultStage 254 (MapPartitionsRDD[1272] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:20:21 INFO MemoryStore: Block broadcast_378 stored as values in memory (estimated size 38.5 KiB, free 434.4 MiB)
26/01/04 17:20:21 INFO MemoryStore: Block broadcast_378_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.3 MiB)
26/01/04 17:20:21 INFO BlockManagerInfo: Added broadcast_378_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:20:21 INFO BlockManagerInfo: Removed broadcast_376_piece0 on spark-master:33535 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:20:21 INFO SparkContext: Created broadcast 378 from broadcast at DAGScheduler.scala:1585
26/01/04 17:20:21 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 254 (MapPartitionsRDD[1272] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 17:20:21 INFO TaskSchedulerImpl: Adding task set 254.0 with 2 tasks resource profile 0
26/01/04 17:20:21 INFO TaskSetManager: Starting task 1.0 in stage 254.0 (TID 416) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 17:20:21 INFO TaskSetManager: Starting task 0.0 in stage 254.0 (TID 417) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 17:20:21 INFO BlockManagerInfo: Removed broadcast_376_piece0 on 172.18.0.13:44401 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:20:21 INFO BlockManagerInfo: Added broadcast_378_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:20:21 INFO BlockManagerInfo: Added broadcast_378_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:20:21 INFO TaskSetManager: Finished task 1.0 in stage 254.0 (TID 416) in 39 ms on 172.18.0.13 (executor 0) (1/2)
26/01/04 17:20:21 INFO TaskSetManager: Finished task 0.0 in stage 254.0 (TID 417) in 84 ms on 172.18.0.14 (executor 1) (2/2)
26/01/04 17:20:21 INFO TaskSchedulerImpl: Removed TaskSet 254.0, whose tasks have all completed, from pool 
26/01/04 17:20:21 INFO DAGScheduler: ResultStage 254 (start at NativeMethodAccessorImpl.java:0) finished in 0.097 s
26/01/04 17:20:21 INFO DAGScheduler: Job 253 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:20:21 INFO TaskSchedulerImpl: Killing all running tasks in stage 254: Stage finished
26/01/04 17:20:21 INFO DAGScheduler: Job 253 finished: start at NativeMethodAccessorImpl.java:0, took 0.098391 s
26/01/04 17:20:21 INFO MemoryStore: Block broadcast_379_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.3 MiB)
26/01/04 17:20:21 INFO BlockManagerInfo: Added broadcast_379_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:20:21 INFO SparkContext: Created broadcast 379 from start at NativeMethodAccessorImpl.java:0
26/01/04 17:20:21 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 125, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@33ed6c16]. The input RDD has 1 partitions.
26/01/04 17:20:21 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:20:21 INFO DAGScheduler: Got job 254 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/01/04 17:20:21 INFO DAGScheduler: Final stage: ResultStage 255 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:20:21 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:20:21 INFO DAGScheduler: Missing parents: List()
26/01/04 17:20:21 INFO DAGScheduler: Submitting ResultStage 255 (MapPartitionsRDD[1277] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:20:21 INFO MemoryStore: Block broadcast_380 stored as values in memory (estimated size 52.9 KiB, free 434.3 MiB)
26/01/04 17:20:21 INFO MemoryStore: Block broadcast_380_piece0 stored as bytes in memory (estimated size 20.2 KiB, free 434.3 MiB)
26/01/04 17:20:21 INFO BlockManagerInfo: Added broadcast_380_piece0 in memory on spark-master:33535 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:20:21 INFO SparkContext: Created broadcast 380 from broadcast at DAGScheduler.scala:1585
26/01/04 17:20:21 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 255 (MapPartitionsRDD[1277] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/01/04 17:20:21 INFO TaskSchedulerImpl: Adding task set 255.0 with 1 tasks resource profile 0
26/01/04 17:20:21 INFO TaskSetManager: Starting task 0.0 in stage 255.0 (TID 418) (172.18.0.13, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:20:21 INFO BlockManagerInfo: Added broadcast_380_piece0 in memory on 172.18.0.13:44401 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:20:21 INFO BlockManagerInfo: Added broadcast_379_piece0 in memory on 172.18.0.13:44401 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:20:22 INFO TaskSetManager: Finished task 0.0 in stage 255.0 (TID 418) in 532 ms on 172.18.0.13 (executor 0) (1/1)
26/01/04 17:20:22 INFO TaskSchedulerImpl: Removed TaskSet 255.0, whose tasks have all completed, from pool 
26/01/04 17:20:22 INFO DAGScheduler: ResultStage 255 (start at NativeMethodAccessorImpl.java:0) finished in 0.535 s
26/01/04 17:20:22 INFO DAGScheduler: Job 254 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:20:22 INFO TaskSchedulerImpl: Killing all running tasks in stage 255: Stage finished
26/01/04 17:20:22 INFO DAGScheduler: Job 254 finished: start at NativeMethodAccessorImpl.java:0, took 0.537007 s
26/01/04 17:20:22 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 125, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@33ed6c16] is committing.
26/01/04 17:20:22 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 125, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@33ed6c16] committed.
26/01/04 17:20:22 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/125 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.125.033f343e-0aa2-4549-895b-1fc9f3b91729.tmp
26/01/04 17:20:22 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.125.033f343e-0aa2-4549-895b-1fc9f3b91729.tmp to file:/tmp/spark-checkpoint-enrichment/commits/125
26/01/04 17:20:22 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T17:20:21.318Z",
  "batchId" : 125,
  "numInputRows" : 30,
  "inputRowsPerSecond" : 2500.0,
  "processedRowsPerSecond" : 34.72222222222222,
  "durationMs" : {
    "addBatch" : 694,
    "commitOffsets" : 73,
    "getBatch" : 0,
    "latestOffset" : 2,
    "queryPlanning" : 27,
    "triggerExecution" : 864,
    "walCommit" : 68
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "0" : 3009
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "0" : 3039
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "0" : 3039
      }
    },
    "numInputRows" : 30,
    "inputRowsPerSecond" : 2500.0,
    "processedRowsPerSecond" : 34.72222222222222,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 31
  }
}
26/01/04 17:20:32 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/01/04 17:20:32 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/126 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.126.770e3eb9-329c-496b-85f8-c38d192050e8.tmp
26/01/04 17:20:32 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.126.770e3eb9-329c-496b-85f8-c38d192050e8.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/126
26/01/04 17:20:32 INFO MicroBatchExecution: Committed offsets for batch 126. Metadata OffsetSeqMetadata(0,1767547232339,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 17:20:32 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:20:32 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:20:32 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:20:32 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:20:32 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:20:32 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:20:32 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#102787 - airline_prefix.nullCount#102786) > 0)
26/01/04 17:20:32 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#102822 - min_flight_num.nullCount#102821) > 0)
26/01/04 17:20:32 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#102817 - max_flight_num.nullCount#102816) > 0)
26/01/04 17:20:32 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:20:32 INFO DAGScheduler: Got job 255 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 17:20:32 INFO DAGScheduler: Final stage: ResultStage 256 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:20:32 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:20:32 INFO DAGScheduler: Missing parents: List()
26/01/04 17:20:32 INFO DAGScheduler: Submitting ResultStage 256 (MapPartitionsRDD[1282] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:20:32 INFO MemoryStore: Block broadcast_381 stored as values in memory (estimated size 38.5 KiB, free 434.2 MiB)
26/01/04 17:20:32 INFO MemoryStore: Block broadcast_381_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.2 MiB)
26/01/04 17:20:32 INFO BlockManagerInfo: Added broadcast_381_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:20:32 INFO SparkContext: Created broadcast 381 from broadcast at DAGScheduler.scala:1585
26/01/04 17:20:32 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 256 (MapPartitionsRDD[1282] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 17:20:32 INFO TaskSchedulerImpl: Adding task set 256.0 with 2 tasks resource profile 0
26/01/04 17:20:32 INFO TaskSetManager: Starting task 0.0 in stage 256.0 (TID 419) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 17:20:32 INFO TaskSetManager: Starting task 1.0 in stage 256.0 (TID 420) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 17:20:32 INFO BlockManagerInfo: Added broadcast_381_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:20:32 INFO BlockManagerInfo: Added broadcast_381_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:20:32 INFO TaskSetManager: Finished task 1.0 in stage 256.0 (TID 420) in 21 ms on 172.18.0.13 (executor 0) (1/2)
26/01/04 17:20:32 INFO TaskSetManager: Finished task 0.0 in stage 256.0 (TID 419) in 46 ms on 172.18.0.14 (executor 1) (2/2)
26/01/04 17:20:32 INFO TaskSchedulerImpl: Removed TaskSet 256.0, whose tasks have all completed, from pool 
26/01/04 17:20:32 INFO DAGScheduler: ResultStage 256 (start at NativeMethodAccessorImpl.java:0) finished in 0.053 s
26/01/04 17:20:32 INFO DAGScheduler: Job 255 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:20:32 INFO TaskSchedulerImpl: Killing all running tasks in stage 256: Stage finished
26/01/04 17:20:32 INFO DAGScheduler: Job 255 finished: start at NativeMethodAccessorImpl.java:0, took 0.054960 s
26/01/04 17:20:32 INFO MemoryStore: Block broadcast_382_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.2 MiB)
26/01/04 17:20:32 INFO BlockManagerInfo: Added broadcast_382_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:20:32 INFO SparkContext: Created broadcast 382 from start at NativeMethodAccessorImpl.java:0
26/01/04 17:20:32 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 126, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@3a396939]. The input RDD has 1 partitions.
26/01/04 17:20:32 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:20:32 INFO DAGScheduler: Got job 256 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/01/04 17:20:32 INFO DAGScheduler: Final stage: ResultStage 257 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:20:32 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:20:32 INFO DAGScheduler: Missing parents: List()
26/01/04 17:20:32 INFO DAGScheduler: Submitting ResultStage 257 (MapPartitionsRDD[1287] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:20:32 INFO MemoryStore: Block broadcast_383 stored as values in memory (estimated size 52.9 KiB, free 434.2 MiB)
26/01/04 17:20:32 INFO MemoryStore: Block broadcast_383_piece0 stored as bytes in memory (estimated size 20.2 KiB, free 434.1 MiB)
26/01/04 17:20:32 INFO BlockManagerInfo: Added broadcast_383_piece0 in memory on spark-master:33535 (size: 20.2 KiB, free: 434.3 MiB)
26/01/04 17:20:32 INFO SparkContext: Created broadcast 383 from broadcast at DAGScheduler.scala:1585
26/01/04 17:20:32 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 257 (MapPartitionsRDD[1287] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/01/04 17:20:32 INFO TaskSchedulerImpl: Adding task set 257.0 with 1 tasks resource profile 0
26/01/04 17:20:32 INFO TaskSetManager: Starting task 0.0 in stage 257.0 (TID 421) (172.18.0.13, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:20:32 INFO BlockManagerInfo: Added broadcast_383_piece0 in memory on 172.18.0.13:44401 (size: 20.2 KiB, free: 434.3 MiB)
26/01/04 17:20:32 INFO BlockManagerInfo: Added broadcast_382_piece0 in memory on 172.18.0.13:44401 (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:20:33 INFO TaskSetManager: Finished task 0.0 in stage 257.0 (TID 421) in 545 ms on 172.18.0.13 (executor 0) (1/1)
26/01/04 17:20:33 INFO TaskSchedulerImpl: Removed TaskSet 257.0, whose tasks have all completed, from pool 
26/01/04 17:20:33 INFO DAGScheduler: ResultStage 257 (start at NativeMethodAccessorImpl.java:0) finished in 0.549 s
26/01/04 17:20:33 INFO DAGScheduler: Job 256 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:20:33 INFO TaskSchedulerImpl: Killing all running tasks in stage 257: Stage finished
26/01/04 17:20:33 INFO DAGScheduler: Job 256 finished: start at NativeMethodAccessorImpl.java:0, took 0.551759 s
26/01/04 17:20:33 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 126, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@3a396939] is committing.
26/01/04 17:20:33 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 126, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@3a396939] committed.
26/01/04 17:20:33 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/126 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.126.e100b830-e01c-464a-bf9a-49761dddeff8.tmp
26/01/04 17:20:33 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.126.e100b830-e01c-464a-bf9a-49761dddeff8.tmp to file:/tmp/spark-checkpoint-enrichment/commits/126
26/01/04 17:20:33 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T17:20:32.337Z",
  "batchId" : 126,
  "numInputRows" : 30,
  "inputRowsPerSecond" : 2307.6923076923076,
  "processedRowsPerSecond" : 33.898305084745765,
  "durationMs" : {
    "addBatch" : 671,
    "commitOffsets" : 74,
    "getBatch" : 1,
    "latestOffset" : 2,
    "queryPlanning" : 22,
    "triggerExecution" : 885,
    "walCommit" : 115
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "0" : 3039
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "0" : 3069
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "0" : 3069
      }
    },
    "numInputRows" : 30,
    "inputRowsPerSecond" : 2307.6923076923076,
    "processedRowsPerSecond" : 33.898305084745765,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 31
  }
}
26/01/04 17:20:35 INFO BlockManagerInfo: Removed broadcast_379_piece0 on spark-master:33535 in memory (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:20:35 INFO BlockManagerInfo: Removed broadcast_379_piece0 on 172.18.0.13:44401 in memory (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:20:35 INFO BlockManagerInfo: Removed broadcast_378_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:20:35 INFO BlockManagerInfo: Removed broadcast_378_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:20:35 INFO BlockManagerInfo: Removed broadcast_378_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:20:35 INFO BlockManagerInfo: Removed broadcast_383_piece0 on spark-master:33535 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:20:35 INFO BlockManagerInfo: Removed broadcast_383_piece0 on 172.18.0.13:44401 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:20:35 INFO BlockManagerInfo: Removed broadcast_381_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:20:35 INFO BlockManagerInfo: Removed broadcast_381_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:20:35 INFO BlockManagerInfo: Removed broadcast_381_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:20:35 INFO BlockManagerInfo: Removed broadcast_380_piece0 on spark-master:33535 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:20:35 INFO BlockManagerInfo: Removed broadcast_380_piece0 on 172.18.0.13:44401 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:20:43 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/01/04 17:20:43 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/127 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.127.99b462c8-28d9-481a-9930-8df2f92f8ca5.tmp
26/01/04 17:20:43 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.127.99b462c8-28d9-481a-9930-8df2f92f8ca5.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/127
26/01/04 17:20:43 INFO MicroBatchExecution: Committed offsets for batch 127. Metadata OffsetSeqMetadata(0,1767547243366,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 17:20:43 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:20:43 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:20:43 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:20:43 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:20:43 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:20:43 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:20:43 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#103591 - airline_prefix.nullCount#103590) > 0)
26/01/04 17:20:43 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#103626 - min_flight_num.nullCount#103625) > 0)
26/01/04 17:20:43 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#103621 - max_flight_num.nullCount#103620) > 0)
26/01/04 17:20:43 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:20:43 INFO DAGScheduler: Got job 257 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 17:20:43 INFO DAGScheduler: Final stage: ResultStage 258 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:20:43 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:20:43 INFO DAGScheduler: Missing parents: List()
26/01/04 17:20:43 INFO DAGScheduler: Submitting ResultStage 258 (MapPartitionsRDD[1292] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:20:43 INFO MemoryStore: Block broadcast_384 stored as values in memory (estimated size 38.5 KiB, free 434.4 MiB)
26/01/04 17:20:43 INFO MemoryStore: Block broadcast_384_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.3 MiB)
26/01/04 17:20:43 INFO BlockManagerInfo: Removed broadcast_382_piece0 on spark-master:33535 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:20:43 INFO BlockManagerInfo: Added broadcast_384_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:20:43 INFO SparkContext: Created broadcast 384 from broadcast at DAGScheduler.scala:1585
26/01/04 17:20:43 INFO BlockManagerInfo: Removed broadcast_382_piece0 on 172.18.0.13:44401 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:20:43 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 258 (MapPartitionsRDD[1292] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 17:20:43 INFO TaskSchedulerImpl: Adding task set 258.0 with 2 tasks resource profile 0
26/01/04 17:20:43 INFO TaskSetManager: Starting task 1.0 in stage 258.0 (TID 422) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 17:20:43 INFO TaskSetManager: Starting task 0.0 in stage 258.0 (TID 423) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 17:20:43 INFO BlockManagerInfo: Added broadcast_384_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:20:43 INFO BlockManagerInfo: Added broadcast_384_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:20:43 INFO TaskSetManager: Finished task 1.0 in stage 258.0 (TID 422) in 31 ms on 172.18.0.13 (executor 0) (1/2)
26/01/04 17:20:43 INFO TaskSetManager: Finished task 0.0 in stage 258.0 (TID 423) in 56 ms on 172.18.0.14 (executor 1) (2/2)
26/01/04 17:20:43 INFO TaskSchedulerImpl: Removed TaskSet 258.0, whose tasks have all completed, from pool 
26/01/04 17:20:43 INFO DAGScheduler: ResultStage 258 (start at NativeMethodAccessorImpl.java:0) finished in 0.067 s
26/01/04 17:20:43 INFO DAGScheduler: Job 257 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:20:43 INFO TaskSchedulerImpl: Killing all running tasks in stage 258: Stage finished
26/01/04 17:20:43 INFO DAGScheduler: Job 257 finished: start at NativeMethodAccessorImpl.java:0, took 0.069027 s
26/01/04 17:20:43 INFO MemoryStore: Block broadcast_385_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.3 MiB)
26/01/04 17:20:43 INFO BlockManagerInfo: Added broadcast_385_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:20:43 INFO SparkContext: Created broadcast 385 from start at NativeMethodAccessorImpl.java:0
26/01/04 17:20:43 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 127, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@333b3399]. The input RDD has 1 partitions.
26/01/04 17:20:43 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:20:43 INFO DAGScheduler: Got job 258 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/01/04 17:20:43 INFO DAGScheduler: Final stage: ResultStage 259 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:20:43 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:20:43 INFO DAGScheduler: Missing parents: List()
26/01/04 17:20:43 INFO DAGScheduler: Submitting ResultStage 259 (MapPartitionsRDD[1297] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:20:43 INFO MemoryStore: Block broadcast_386 stored as values in memory (estimated size 52.9 KiB, free 434.3 MiB)
26/01/04 17:20:43 INFO MemoryStore: Block broadcast_386_piece0 stored as bytes in memory (estimated size 20.2 KiB, free 434.3 MiB)
26/01/04 17:20:43 INFO BlockManagerInfo: Added broadcast_386_piece0 in memory on spark-master:33535 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:20:43 INFO SparkContext: Created broadcast 386 from broadcast at DAGScheduler.scala:1585
26/01/04 17:20:43 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 259 (MapPartitionsRDD[1297] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/01/04 17:20:43 INFO TaskSchedulerImpl: Adding task set 259.0 with 1 tasks resource profile 0
26/01/04 17:20:43 INFO TaskSetManager: Starting task 0.0 in stage 259.0 (TID 424) (172.18.0.13, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:20:43 INFO BlockManagerInfo: Added broadcast_386_piece0 in memory on 172.18.0.13:44401 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:20:43 INFO BlockManagerInfo: Added broadcast_385_piece0 in memory on 172.18.0.13:44401 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:20:44 INFO TaskSetManager: Finished task 0.0 in stage 259.0 (TID 424) in 541 ms on 172.18.0.13 (executor 0) (1/1)
26/01/04 17:20:44 INFO TaskSchedulerImpl: Removed TaskSet 259.0, whose tasks have all completed, from pool 
26/01/04 17:20:44 INFO DAGScheduler: ResultStage 259 (start at NativeMethodAccessorImpl.java:0) finished in 0.547 s
26/01/04 17:20:44 INFO DAGScheduler: Job 258 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:20:44 INFO TaskSchedulerImpl: Killing all running tasks in stage 259: Stage finished
26/01/04 17:20:44 INFO DAGScheduler: Job 258 finished: start at NativeMethodAccessorImpl.java:0, took 0.548131 s
26/01/04 17:20:44 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 127, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@333b3399] is committing.
26/01/04 17:20:44 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 127, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@333b3399] committed.
26/01/04 17:20:44 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/127 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.127.78fd6fcb-5117-42ef-a545-d3d37dcdcfcc.tmp
26/01/04 17:20:44 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.127.78fd6fcb-5117-42ef-a545-d3d37dcdcfcc.tmp to file:/tmp/spark-checkpoint-enrichment/commits/127
26/01/04 17:20:44 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T17:20:43.362Z",
  "batchId" : 127,
  "numInputRows" : 30,
  "inputRowsPerSecond" : 2500.0,
  "processedRowsPerSecond" : 34.52243958573072,
  "durationMs" : {
    "addBatch" : 688,
    "commitOffsets" : 66,
    "getBatch" : 0,
    "latestOffset" : 3,
    "queryPlanning" : 33,
    "triggerExecution" : 869,
    "walCommit" : 77
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "0" : 3069
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "0" : 3099
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "0" : 3099
      }
    },
    "numInputRows" : 30,
    "inputRowsPerSecond" : 2500.0,
    "processedRowsPerSecond" : 34.52243958573072,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 31
  }
}
26/01/04 17:20:54 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/01/04 17:20:54 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/128 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.128.116eda17-78e0-4586-8fda-dc301d344925.tmp
26/01/04 17:20:54 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.128.116eda17-78e0-4586-8fda-dc301d344925.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/128
26/01/04 17:20:54 INFO MicroBatchExecution: Committed offsets for batch 128. Metadata OffsetSeqMetadata(0,1767547254374,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 17:20:54 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:20:54 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:20:54 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:20:54 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:20:54 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:20:54 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:20:54 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#104395 - airline_prefix.nullCount#104394) > 0)
26/01/04 17:20:54 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#104430 - min_flight_num.nullCount#104429) > 0)
26/01/04 17:20:54 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#104425 - max_flight_num.nullCount#104424) > 0)
26/01/04 17:20:54 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:20:54 INFO DAGScheduler: Got job 259 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 17:20:54 INFO DAGScheduler: Final stage: ResultStage 260 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:20:54 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:20:54 INFO DAGScheduler: Missing parents: List()
26/01/04 17:20:54 INFO DAGScheduler: Submitting ResultStage 260 (MapPartitionsRDD[1302] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:20:54 INFO MemoryStore: Block broadcast_387 stored as values in memory (estimated size 38.5 KiB, free 434.2 MiB)
26/01/04 17:20:54 INFO MemoryStore: Block broadcast_387_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.2 MiB)
26/01/04 17:20:54 INFO BlockManagerInfo: Added broadcast_387_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:20:54 INFO SparkContext: Created broadcast 387 from broadcast at DAGScheduler.scala:1585
26/01/04 17:20:54 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 260 (MapPartitionsRDD[1302] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 17:20:54 INFO TaskSchedulerImpl: Adding task set 260.0 with 2 tasks resource profile 0
26/01/04 17:20:54 INFO TaskSetManager: Starting task 1.0 in stage 260.0 (TID 425) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 17:20:54 INFO TaskSetManager: Starting task 0.0 in stage 260.0 (TID 426) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 17:20:54 INFO BlockManagerInfo: Added broadcast_387_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:20:54 INFO BlockManagerInfo: Added broadcast_387_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:20:54 INFO TaskSetManager: Finished task 1.0 in stage 260.0 (TID 425) in 31 ms on 172.18.0.13 (executor 0) (1/2)
26/01/04 17:20:54 INFO TaskSetManager: Finished task 0.0 in stage 260.0 (TID 426) in 50 ms on 172.18.0.14 (executor 1) (2/2)
26/01/04 17:20:54 INFO TaskSchedulerImpl: Removed TaskSet 260.0, whose tasks have all completed, from pool 
26/01/04 17:20:54 INFO DAGScheduler: ResultStage 260 (start at NativeMethodAccessorImpl.java:0) finished in 0.058 s
26/01/04 17:20:54 INFO DAGScheduler: Job 259 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:20:54 INFO TaskSchedulerImpl: Killing all running tasks in stage 260: Stage finished
26/01/04 17:20:54 INFO DAGScheduler: Job 259 finished: start at NativeMethodAccessorImpl.java:0, took 0.059572 s
26/01/04 17:20:54 INFO MemoryStore: Block broadcast_388_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.2 MiB)
26/01/04 17:20:54 INFO BlockManagerInfo: Added broadcast_388_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:20:54 INFO SparkContext: Created broadcast 388 from start at NativeMethodAccessorImpl.java:0
26/01/04 17:20:54 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 128, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@3798c834]. The input RDD has 1 partitions.
26/01/04 17:20:54 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:20:54 INFO DAGScheduler: Got job 260 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/01/04 17:20:54 INFO DAGScheduler: Final stage: ResultStage 261 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:20:54 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:20:54 INFO DAGScheduler: Missing parents: List()
26/01/04 17:20:54 INFO DAGScheduler: Submitting ResultStage 261 (MapPartitionsRDD[1307] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:20:54 INFO MemoryStore: Block broadcast_389 stored as values in memory (estimated size 52.9 KiB, free 434.2 MiB)
26/01/04 17:20:54 INFO MemoryStore: Block broadcast_389_piece0 stored as bytes in memory (estimated size 20.2 KiB, free 434.1 MiB)
26/01/04 17:20:54 INFO BlockManagerInfo: Added broadcast_389_piece0 in memory on spark-master:33535 (size: 20.2 KiB, free: 434.3 MiB)
26/01/04 17:20:54 INFO SparkContext: Created broadcast 389 from broadcast at DAGScheduler.scala:1585
26/01/04 17:20:54 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 261 (MapPartitionsRDD[1307] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/01/04 17:20:54 INFO TaskSchedulerImpl: Adding task set 261.0 with 1 tasks resource profile 0
26/01/04 17:20:54 INFO TaskSetManager: Starting task 0.0 in stage 261.0 (TID 427) (172.18.0.13, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:20:54 INFO BlockManagerInfo: Added broadcast_389_piece0 in memory on 172.18.0.13:44401 (size: 20.2 KiB, free: 434.3 MiB)
26/01/04 17:20:54 INFO BlockManagerInfo: Added broadcast_388_piece0 in memory on 172.18.0.13:44401 (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:20:55 INFO TaskSetManager: Finished task 0.0 in stage 261.0 (TID 427) in 558 ms on 172.18.0.13 (executor 0) (1/1)
26/01/04 17:20:55 INFO TaskSchedulerImpl: Removed TaskSet 261.0, whose tasks have all completed, from pool 
26/01/04 17:20:55 INFO DAGScheduler: ResultStage 261 (start at NativeMethodAccessorImpl.java:0) finished in 0.566 s
26/01/04 17:20:55 INFO DAGScheduler: Job 260 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:20:55 INFO TaskSchedulerImpl: Killing all running tasks in stage 261: Stage finished
26/01/04 17:20:55 INFO DAGScheduler: Job 260 finished: start at NativeMethodAccessorImpl.java:0, took 0.567130 s
26/01/04 17:20:55 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 128, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@3798c834] is committing.
26/01/04 17:20:55 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 128, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@3798c834] committed.
26/01/04 17:20:55 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/128 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.128.6f71085f-5840-4674-8a84-c68fdb43059c.tmp
26/01/04 17:20:55 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.128.6f71085f-5840-4674-8a84-c68fdb43059c.tmp to file:/tmp/spark-checkpoint-enrichment/commits/128
26/01/04 17:20:55 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T17:20:54.373Z",
  "batchId" : 128,
  "numInputRows" : 30,
  "inputRowsPerSecond" : 2000.0,
  "processedRowsPerSecond" : 34.84320557491289,
  "durationMs" : {
    "addBatch" : 691,
    "commitOffsets" : 74,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 19,
    "triggerExecution" : 861,
    "walCommit" : 75
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "0" : 3099
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "0" : 3129
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "0" : 3129
      }
    },
    "numInputRows" : 30,
    "inputRowsPerSecond" : 2000.0,
    "processedRowsPerSecond" : 34.84320557491289,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 31
  }
}
26/01/04 17:20:57 INFO BlockManagerInfo: Removed broadcast_387_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:20:57 INFO BlockManagerInfo: Removed broadcast_387_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:20:57 INFO BlockManagerInfo: Removed broadcast_387_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:20:57 INFO BlockManagerInfo: Removed broadcast_389_piece0 on spark-master:33535 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:20:57 INFO BlockManagerInfo: Removed broadcast_389_piece0 on 172.18.0.13:44401 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:20:57 INFO BlockManagerInfo: Removed broadcast_385_piece0 on spark-master:33535 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:20:57 INFO BlockManagerInfo: Removed broadcast_385_piece0 on 172.18.0.13:44401 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:20:57 INFO BlockManagerInfo: Removed broadcast_386_piece0 on spark-master:33535 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:20:57 INFO BlockManagerInfo: Removed broadcast_386_piece0 on 172.18.0.13:44401 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:20:57 INFO BlockManagerInfo: Removed broadcast_384_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:20:57 INFO BlockManagerInfo: Removed broadcast_384_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:20:57 INFO BlockManagerInfo: Removed broadcast_384_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:21:05 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/01/04 17:21:05 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/129 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.129.cab36e10-d23e-4724-91e9-513cd07a13b6.tmp
26/01/04 17:21:05 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.129.cab36e10-d23e-4724-91e9-513cd07a13b6.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/129
26/01/04 17:21:05 INFO MicroBatchExecution: Committed offsets for batch 129. Metadata OffsetSeqMetadata(0,1767547265384,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 17:21:05 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:21:05 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:21:05 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:21:05 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:21:05 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:21:05 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:21:05 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#105199 - airline_prefix.nullCount#105198) > 0)
26/01/04 17:21:05 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#105234 - min_flight_num.nullCount#105233) > 0)
26/01/04 17:21:05 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#105229 - max_flight_num.nullCount#105228) > 0)
26/01/04 17:21:05 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:21:05 INFO DAGScheduler: Got job 261 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 17:21:05 INFO DAGScheduler: Final stage: ResultStage 262 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:21:05 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:21:05 INFO DAGScheduler: Missing parents: List()
26/01/04 17:21:05 INFO DAGScheduler: Submitting ResultStage 262 (MapPartitionsRDD[1312] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:21:05 INFO MemoryStore: Block broadcast_390 stored as values in memory (estimated size 38.5 KiB, free 434.4 MiB)
26/01/04 17:21:05 INFO MemoryStore: Block broadcast_390_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.3 MiB)
26/01/04 17:21:05 INFO BlockManagerInfo: Added broadcast_390_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:21:05 INFO BlockManagerInfo: Removed broadcast_388_piece0 on spark-master:33535 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:21:05 INFO SparkContext: Created broadcast 390 from broadcast at DAGScheduler.scala:1585
26/01/04 17:21:05 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 262 (MapPartitionsRDD[1312] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 17:21:05 INFO TaskSchedulerImpl: Adding task set 262.0 with 2 tasks resource profile 0
26/01/04 17:21:05 INFO BlockManagerInfo: Removed broadcast_388_piece0 on 172.18.0.13:44401 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:21:05 INFO TaskSetManager: Starting task 0.0 in stage 262.0 (TID 428) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 17:21:05 INFO TaskSetManager: Starting task 1.0 in stage 262.0 (TID 429) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 17:21:05 INFO BlockManagerInfo: Added broadcast_390_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:21:05 INFO BlockManagerInfo: Added broadcast_390_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:21:05 INFO TaskSetManager: Finished task 1.0 in stage 262.0 (TID 429) in 19 ms on 172.18.0.13 (executor 0) (1/2)
26/01/04 17:21:05 INFO TaskSetManager: Finished task 0.0 in stage 262.0 (TID 428) in 34 ms on 172.18.0.14 (executor 1) (2/2)
26/01/04 17:21:05 INFO TaskSchedulerImpl: Removed TaskSet 262.0, whose tasks have all completed, from pool 
26/01/04 17:21:05 INFO DAGScheduler: ResultStage 262 (start at NativeMethodAccessorImpl.java:0) finished in 0.043 s
26/01/04 17:21:05 INFO DAGScheduler: Job 261 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:21:05 INFO TaskSchedulerImpl: Killing all running tasks in stage 262: Stage finished
26/01/04 17:21:05 INFO DAGScheduler: Job 261 finished: start at NativeMethodAccessorImpl.java:0, took 0.044817 s
26/01/04 17:21:05 INFO MemoryStore: Block broadcast_391_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.3 MiB)
26/01/04 17:21:05 INFO BlockManagerInfo: Added broadcast_391_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:21:05 INFO SparkContext: Created broadcast 391 from start at NativeMethodAccessorImpl.java:0
26/01/04 17:21:05 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 129, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@4f0b18cd]. The input RDD has 1 partitions.
26/01/04 17:21:05 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:21:05 INFO DAGScheduler: Got job 262 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/01/04 17:21:05 INFO DAGScheduler: Final stage: ResultStage 263 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:21:05 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:21:05 INFO DAGScheduler: Missing parents: List()
26/01/04 17:21:05 INFO DAGScheduler: Submitting ResultStage 263 (MapPartitionsRDD[1317] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:21:05 INFO MemoryStore: Block broadcast_392 stored as values in memory (estimated size 52.9 KiB, free 434.3 MiB)
26/01/04 17:21:05 INFO MemoryStore: Block broadcast_392_piece0 stored as bytes in memory (estimated size 20.2 KiB, free 434.3 MiB)
26/01/04 17:21:05 INFO BlockManagerInfo: Added broadcast_392_piece0 in memory on spark-master:33535 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:21:05 INFO SparkContext: Created broadcast 392 from broadcast at DAGScheduler.scala:1585
26/01/04 17:21:05 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 263 (MapPartitionsRDD[1317] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/01/04 17:21:05 INFO TaskSchedulerImpl: Adding task set 263.0 with 1 tasks resource profile 0
26/01/04 17:21:05 INFO TaskSetManager: Starting task 0.0 in stage 263.0 (TID 430) (172.18.0.13, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:21:05 INFO BlockManagerInfo: Added broadcast_392_piece0 in memory on 172.18.0.13:44401 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:21:05 INFO BlockManagerInfo: Added broadcast_391_piece0 in memory on 172.18.0.13:44401 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:21:06 INFO TaskSetManager: Finished task 0.0 in stage 263.0 (TID 430) in 551 ms on 172.18.0.13 (executor 0) (1/1)
26/01/04 17:21:06 INFO TaskSchedulerImpl: Removed TaskSet 263.0, whose tasks have all completed, from pool 
26/01/04 17:21:06 INFO DAGScheduler: ResultStage 263 (start at NativeMethodAccessorImpl.java:0) finished in 0.558 s
26/01/04 17:21:06 INFO DAGScheduler: Job 262 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:21:06 INFO TaskSchedulerImpl: Killing all running tasks in stage 263: Stage finished
26/01/04 17:21:06 INFO DAGScheduler: Job 262 finished: start at NativeMethodAccessorImpl.java:0, took 0.559371 s
26/01/04 17:21:06 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 129, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@4f0b18cd] is committing.
26/01/04 17:21:06 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 129, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@4f0b18cd] committed.
26/01/04 17:21:06 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/129 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.129.c7c6a963-f8af-4245-ba3c-2a80f5344fe9.tmp
26/01/04 17:21:06 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.129.c7c6a963-f8af-4245-ba3c-2a80f5344fe9.tmp to file:/tmp/spark-checkpoint-enrichment/commits/129
26/01/04 17:21:06 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T17:21:05.382Z",
  "batchId" : 129,
  "numInputRows" : 30,
  "inputRowsPerSecond" : 2727.2727272727275,
  "processedRowsPerSecond" : 36.630036630036635,
  "durationMs" : {
    "addBatch" : 667,
    "commitOffsets" : 70,
    "getBatch" : 0,
    "latestOffset" : 2,
    "queryPlanning" : 18,
    "triggerExecution" : 819,
    "walCommit" : 62
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "0" : 3129
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "0" : 3159
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "0" : 3159
      }
    },
    "numInputRows" : 30,
    "inputRowsPerSecond" : 2727.2727272727275,
    "processedRowsPerSecond" : 36.630036630036635,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 31
  }
}
26/01/04 17:21:16 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/01/04 17:21:16 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/130 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.130.b1625697-37ba-4bc8-b174-c94fbdc51243.tmp
26/01/04 17:21:16 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.130.b1625697-37ba-4bc8-b174-c94fbdc51243.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/130
26/01/04 17:21:16 INFO MicroBatchExecution: Committed offsets for batch 130. Metadata OffsetSeqMetadata(0,1767547276397,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 17:21:16 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:21:16 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:21:16 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:21:16 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:21:16 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:21:16 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:21:16 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#106003 - airline_prefix.nullCount#106002) > 0)
26/01/04 17:21:16 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#106038 - min_flight_num.nullCount#106037) > 0)
26/01/04 17:21:16 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#106033 - max_flight_num.nullCount#106032) > 0)
26/01/04 17:21:16 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:21:16 INFO DAGScheduler: Got job 263 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 17:21:16 INFO DAGScheduler: Final stage: ResultStage 264 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:21:16 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:21:16 INFO DAGScheduler: Missing parents: List()
26/01/04 17:21:16 INFO DAGScheduler: Submitting ResultStage 264 (MapPartitionsRDD[1322] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:21:16 INFO MemoryStore: Block broadcast_393 stored as values in memory (estimated size 38.5 KiB, free 434.2 MiB)
26/01/04 17:21:16 INFO MemoryStore: Block broadcast_393_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.2 MiB)
26/01/04 17:21:16 INFO BlockManagerInfo: Added broadcast_393_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:21:16 INFO SparkContext: Created broadcast 393 from broadcast at DAGScheduler.scala:1585
26/01/04 17:21:16 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 264 (MapPartitionsRDD[1322] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 17:21:16 INFO TaskSchedulerImpl: Adding task set 264.0 with 2 tasks resource profile 0
26/01/04 17:21:16 INFO TaskSetManager: Starting task 1.0 in stage 264.0 (TID 431) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 17:21:16 INFO TaskSetManager: Starting task 0.0 in stage 264.0 (TID 432) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 17:21:16 INFO BlockManagerInfo: Added broadcast_393_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:21:16 INFO TaskSetManager: Finished task 1.0 in stage 264.0 (TID 431) in 19 ms on 172.18.0.13 (executor 0) (1/2)
26/01/04 17:21:16 INFO BlockManagerInfo: Added broadcast_393_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:21:16 INFO TaskSetManager: Finished task 0.0 in stage 264.0 (TID 432) in 62 ms on 172.18.0.14 (executor 1) (2/2)
26/01/04 17:21:16 INFO TaskSchedulerImpl: Removed TaskSet 264.0, whose tasks have all completed, from pool 
26/01/04 17:21:16 INFO DAGScheduler: ResultStage 264 (start at NativeMethodAccessorImpl.java:0) finished in 0.066 s
26/01/04 17:21:16 INFO DAGScheduler: Job 263 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:21:16 INFO TaskSchedulerImpl: Killing all running tasks in stage 264: Stage finished
26/01/04 17:21:16 INFO DAGScheduler: Job 263 finished: start at NativeMethodAccessorImpl.java:0, took 0.067777 s
26/01/04 17:21:16 INFO MemoryStore: Block broadcast_394_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.2 MiB)
26/01/04 17:21:16 INFO BlockManagerInfo: Added broadcast_394_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:21:16 INFO SparkContext: Created broadcast 394 from start at NativeMethodAccessorImpl.java:0
26/01/04 17:21:16 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 130, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@9a1a4dd]. The input RDD has 1 partitions.
26/01/04 17:21:16 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:21:16 INFO DAGScheduler: Got job 264 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/01/04 17:21:16 INFO DAGScheduler: Final stage: ResultStage 265 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:21:16 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:21:16 INFO DAGScheduler: Missing parents: List()
26/01/04 17:21:16 INFO DAGScheduler: Submitting ResultStage 265 (MapPartitionsRDD[1327] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:21:16 INFO MemoryStore: Block broadcast_395 stored as values in memory (estimated size 52.9 KiB, free 434.2 MiB)
26/01/04 17:21:16 INFO MemoryStore: Block broadcast_395_piece0 stored as bytes in memory (estimated size 20.2 KiB, free 434.1 MiB)
26/01/04 17:21:16 INFO BlockManagerInfo: Added broadcast_395_piece0 in memory on spark-master:33535 (size: 20.2 KiB, free: 434.3 MiB)
26/01/04 17:21:16 INFO SparkContext: Created broadcast 395 from broadcast at DAGScheduler.scala:1585
26/01/04 17:21:16 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 265 (MapPartitionsRDD[1327] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/01/04 17:21:16 INFO TaskSchedulerImpl: Adding task set 265.0 with 1 tasks resource profile 0
26/01/04 17:21:16 INFO TaskSetManager: Starting task 0.0 in stage 265.0 (TID 433) (172.18.0.13, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:21:16 INFO BlockManagerInfo: Added broadcast_395_piece0 in memory on 172.18.0.13:44401 (size: 20.2 KiB, free: 434.3 MiB)
26/01/04 17:21:16 INFO BlockManagerInfo: Added broadcast_394_piece0 in memory on 172.18.0.13:44401 (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:21:17 INFO TaskSetManager: Finished task 0.0 in stage 265.0 (TID 433) in 552 ms on 172.18.0.13 (executor 0) (1/1)
26/01/04 17:21:17 INFO TaskSchedulerImpl: Removed TaskSet 265.0, whose tasks have all completed, from pool 
26/01/04 17:21:17 INFO DAGScheduler: ResultStage 265 (start at NativeMethodAccessorImpl.java:0) finished in 0.557 s
26/01/04 17:21:17 INFO DAGScheduler: Job 264 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:21:17 INFO TaskSchedulerImpl: Killing all running tasks in stage 265: Stage finished
26/01/04 17:21:17 INFO DAGScheduler: Job 264 finished: start at NativeMethodAccessorImpl.java:0, took 0.558388 s
26/01/04 17:21:17 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 130, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@9a1a4dd] is committing.
26/01/04 17:21:17 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 130, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@9a1a4dd] committed.
26/01/04 17:21:17 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/130 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.130.42c383ed-1b6f-4c86-94d7-dcae5f30e956.tmp
26/01/04 17:21:17 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.130.42c383ed-1b6f-4c86-94d7-dcae5f30e956.tmp to file:/tmp/spark-checkpoint-enrichment/commits/130
26/01/04 17:21:17 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T17:21:16.395Z",
  "batchId" : 130,
  "numInputRows" : 30,
  "inputRowsPerSecond" : 2500.0,
  "processedRowsPerSecond" : 33.149171270718234,
  "durationMs" : {
    "addBatch" : 701,
    "commitOffsets" : 71,
    "getBatch" : 0,
    "latestOffset" : 2,
    "queryPlanning" : 25,
    "triggerExecution" : 905,
    "walCommit" : 105
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "0" : 3159
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "0" : 3189
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "0" : 3189
      }
    },
    "numInputRows" : 30,
    "inputRowsPerSecond" : 2500.0,
    "processedRowsPerSecond" : 33.149171270718234,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 31
  }
}
26/01/04 17:21:19 INFO BlockManagerInfo: Removed broadcast_393_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:21:19 INFO BlockManagerInfo: Removed broadcast_393_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:21:19 INFO BlockManagerInfo: Removed broadcast_393_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:21:19 INFO BlockManagerInfo: Removed broadcast_391_piece0 on spark-master:33535 in memory (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:21:19 INFO BlockManagerInfo: Removed broadcast_391_piece0 on 172.18.0.13:44401 in memory (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:21:19 INFO BlockManagerInfo: Removed broadcast_392_piece0 on spark-master:33535 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:21:19 INFO BlockManagerInfo: Removed broadcast_392_piece0 on 172.18.0.13:44401 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:21:19 INFO BlockManagerInfo: Removed broadcast_390_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:21:19 INFO BlockManagerInfo: Removed broadcast_390_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:21:19 INFO BlockManagerInfo: Removed broadcast_390_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:21:19 INFO BlockManagerInfo: Removed broadcast_395_piece0 on spark-master:33535 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:21:19 INFO BlockManagerInfo: Removed broadcast_395_piece0 on 172.18.0.13:44401 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:21:27 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/01/04 17:21:27 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/131 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.131.3fe64617-8df8-446b-9bf1-cbc6e7237f70.tmp
26/01/04 17:21:27 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.131.3fe64617-8df8-446b-9bf1-cbc6e7237f70.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/131
26/01/04 17:21:27 INFO MicroBatchExecution: Committed offsets for batch 131. Metadata OffsetSeqMetadata(0,1767547287408,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 17:21:27 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:21:27 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:21:27 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:21:27 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:21:27 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:21:27 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:21:27 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#106807 - airline_prefix.nullCount#106806) > 0)
26/01/04 17:21:27 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#106842 - min_flight_num.nullCount#106841) > 0)
26/01/04 17:21:27 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#106837 - max_flight_num.nullCount#106836) > 0)
26/01/04 17:21:27 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:21:27 INFO DAGScheduler: Got job 265 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 17:21:27 INFO DAGScheduler: Final stage: ResultStage 266 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:21:27 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:21:27 INFO DAGScheduler: Missing parents: List()
26/01/04 17:21:27 INFO DAGScheduler: Submitting ResultStage 266 (MapPartitionsRDD[1332] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:21:27 INFO MemoryStore: Block broadcast_396 stored as values in memory (estimated size 38.5 KiB, free 434.4 MiB)
26/01/04 17:21:27 INFO MemoryStore: Block broadcast_396_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.3 MiB)
26/01/04 17:21:27 INFO BlockManagerInfo: Added broadcast_396_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:21:27 INFO BlockManagerInfo: Removed broadcast_394_piece0 on spark-master:33535 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:21:27 INFO SparkContext: Created broadcast 396 from broadcast at DAGScheduler.scala:1585
26/01/04 17:21:27 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 266 (MapPartitionsRDD[1332] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 17:21:27 INFO TaskSchedulerImpl: Adding task set 266.0 with 2 tasks resource profile 0
26/01/04 17:21:27 INFO TaskSetManager: Starting task 1.0 in stage 266.0 (TID 434) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 17:21:27 INFO TaskSetManager: Starting task 0.0 in stage 266.0 (TID 435) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 17:21:27 INFO BlockManagerInfo: Removed broadcast_394_piece0 on 172.18.0.13:44401 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:21:27 INFO BlockManagerInfo: Added broadcast_396_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:21:27 INFO BlockManagerInfo: Added broadcast_396_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:21:27 INFO TaskSetManager: Finished task 1.0 in stage 266.0 (TID 434) in 48 ms on 172.18.0.13 (executor 0) (1/2)
26/01/04 17:21:27 INFO TaskSetManager: Finished task 0.0 in stage 266.0 (TID 435) in 82 ms on 172.18.0.14 (executor 1) (2/2)
26/01/04 17:21:27 INFO TaskSchedulerImpl: Removed TaskSet 266.0, whose tasks have all completed, from pool 
26/01/04 17:21:27 INFO DAGScheduler: ResultStage 266 (start at NativeMethodAccessorImpl.java:0) finished in 0.100 s
26/01/04 17:21:27 INFO DAGScheduler: Job 265 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:21:27 INFO TaskSchedulerImpl: Killing all running tasks in stage 266: Stage finished
26/01/04 17:21:27 INFO DAGScheduler: Job 265 finished: start at NativeMethodAccessorImpl.java:0, took 0.103017 s
26/01/04 17:21:27 INFO MemoryStore: Block broadcast_397_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.3 MiB)
26/01/04 17:21:27 INFO BlockManagerInfo: Added broadcast_397_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:21:27 INFO SparkContext: Created broadcast 397 from start at NativeMethodAccessorImpl.java:0
26/01/04 17:21:27 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 131, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@18154b40]. The input RDD has 1 partitions.
26/01/04 17:21:27 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:21:27 INFO DAGScheduler: Got job 266 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/01/04 17:21:27 INFO DAGScheduler: Final stage: ResultStage 267 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:21:27 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:21:27 INFO DAGScheduler: Missing parents: List()
26/01/04 17:21:27 INFO DAGScheduler: Submitting ResultStage 267 (MapPartitionsRDD[1337] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:21:27 INFO MemoryStore: Block broadcast_398 stored as values in memory (estimated size 52.9 KiB, free 434.3 MiB)
26/01/04 17:21:27 INFO MemoryStore: Block broadcast_398_piece0 stored as bytes in memory (estimated size 20.2 KiB, free 434.3 MiB)
26/01/04 17:21:27 INFO BlockManagerInfo: Added broadcast_398_piece0 in memory on spark-master:33535 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:21:27 INFO SparkContext: Created broadcast 398 from broadcast at DAGScheduler.scala:1585
26/01/04 17:21:27 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 267 (MapPartitionsRDD[1337] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/01/04 17:21:27 INFO TaskSchedulerImpl: Adding task set 267.0 with 1 tasks resource profile 0
26/01/04 17:21:27 INFO TaskSetManager: Starting task 0.0 in stage 267.0 (TID 436) (172.18.0.13, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:21:27 INFO BlockManagerInfo: Added broadcast_398_piece0 in memory on 172.18.0.13:44401 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:21:27 INFO BlockManagerInfo: Added broadcast_397_piece0 in memory on 172.18.0.13:44401 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:21:28 INFO TaskSetManager: Finished task 0.0 in stage 267.0 (TID 436) in 551 ms on 172.18.0.13 (executor 0) (1/1)
26/01/04 17:21:28 INFO TaskSchedulerImpl: Removed TaskSet 267.0, whose tasks have all completed, from pool 
26/01/04 17:21:28 INFO DAGScheduler: ResultStage 267 (start at NativeMethodAccessorImpl.java:0) finished in 0.557 s
26/01/04 17:21:28 INFO DAGScheduler: Job 266 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:21:28 INFO TaskSchedulerImpl: Killing all running tasks in stage 267: Stage finished
26/01/04 17:21:28 INFO DAGScheduler: Job 266 finished: start at NativeMethodAccessorImpl.java:0, took 0.558882 s
26/01/04 17:21:28 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 131, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@18154b40] is committing.
26/01/04 17:21:28 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 131, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@18154b40] committed.
26/01/04 17:21:28 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/131 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.131.e94bfa0c-891c-4413-aeeb-27f62d88f7e5.tmp
26/01/04 17:21:28 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.131.e94bfa0c-891c-4413-aeeb-27f62d88f7e5.tmp to file:/tmp/spark-checkpoint-enrichment/commits/131
26/01/04 17:21:28 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T17:21:27.407Z",
  "batchId" : 131,
  "numInputRows" : 30,
  "inputRowsPerSecond" : 2500.0,
  "processedRowsPerSecond" : 32.0855614973262,
  "durationMs" : {
    "addBatch" : 741,
    "commitOffsets" : 91,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 24,
    "triggerExecution" : 935,
    "walCommit" : 78
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "0" : 3189
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "0" : 3219
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "0" : 3219
      }
    },
    "numInputRows" : 30,
    "inputRowsPerSecond" : 2500.0,
    "processedRowsPerSecond" : 32.0855614973262,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 31
  }
}
26/01/04 17:21:38 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/01/04 17:21:38 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/132 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.132.868f57c6-7cc8-4453-9ab0-b729010b74ec.tmp
26/01/04 17:21:38 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.132.868f57c6-7cc8-4453-9ab0-b729010b74ec.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/132
26/01/04 17:21:38 INFO MicroBatchExecution: Committed offsets for batch 132. Metadata OffsetSeqMetadata(0,1767547298424,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 17:21:38 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:21:38 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:21:38 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:21:38 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:21:38 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:21:38 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:21:38 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#107611 - airline_prefix.nullCount#107610) > 0)
26/01/04 17:21:38 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#107646 - min_flight_num.nullCount#107645) > 0)
26/01/04 17:21:38 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#107641 - max_flight_num.nullCount#107640) > 0)
26/01/04 17:21:38 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:21:38 INFO DAGScheduler: Got job 267 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 17:21:38 INFO DAGScheduler: Final stage: ResultStage 268 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:21:38 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:21:38 INFO DAGScheduler: Missing parents: List()
26/01/04 17:21:38 INFO DAGScheduler: Submitting ResultStage 268 (MapPartitionsRDD[1342] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:21:38 INFO MemoryStore: Block broadcast_399 stored as values in memory (estimated size 38.5 KiB, free 434.2 MiB)
26/01/04 17:21:38 INFO MemoryStore: Block broadcast_399_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.2 MiB)
26/01/04 17:21:38 INFO BlockManagerInfo: Added broadcast_399_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:21:38 INFO SparkContext: Created broadcast 399 from broadcast at DAGScheduler.scala:1585
26/01/04 17:21:38 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 268 (MapPartitionsRDD[1342] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 17:21:38 INFO TaskSchedulerImpl: Adding task set 268.0 with 2 tasks resource profile 0
26/01/04 17:21:38 INFO TaskSetManager: Starting task 0.0 in stage 268.0 (TID 437) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 17:21:38 INFO TaskSetManager: Starting task 1.0 in stage 268.0 (TID 438) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 17:21:38 INFO BlockManagerInfo: Added broadcast_399_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:21:38 INFO BlockManagerInfo: Added broadcast_399_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:21:38 INFO TaskSetManager: Finished task 1.0 in stage 268.0 (TID 438) in 18 ms on 172.18.0.13 (executor 0) (1/2)
26/01/04 17:21:38 INFO TaskSetManager: Finished task 0.0 in stage 268.0 (TID 437) in 37 ms on 172.18.0.14 (executor 1) (2/2)
26/01/04 17:21:38 INFO TaskSchedulerImpl: Removed TaskSet 268.0, whose tasks have all completed, from pool 
26/01/04 17:21:38 INFO DAGScheduler: ResultStage 268 (start at NativeMethodAccessorImpl.java:0) finished in 0.043 s
26/01/04 17:21:38 INFO DAGScheduler: Job 267 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:21:38 INFO TaskSchedulerImpl: Killing all running tasks in stage 268: Stage finished
26/01/04 17:21:38 INFO DAGScheduler: Job 267 finished: start at NativeMethodAccessorImpl.java:0, took 0.044133 s
26/01/04 17:21:38 INFO MemoryStore: Block broadcast_400_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.2 MiB)
26/01/04 17:21:38 INFO BlockManagerInfo: Added broadcast_400_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:21:38 INFO SparkContext: Created broadcast 400 from start at NativeMethodAccessorImpl.java:0
26/01/04 17:21:38 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 132, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@731a193b]. The input RDD has 1 partitions.
26/01/04 17:21:38 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:21:38 INFO DAGScheduler: Got job 268 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/01/04 17:21:38 INFO DAGScheduler: Final stage: ResultStage 269 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:21:38 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:21:38 INFO DAGScheduler: Missing parents: List()
26/01/04 17:21:38 INFO DAGScheduler: Submitting ResultStage 269 (MapPartitionsRDD[1347] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:21:38 INFO MemoryStore: Block broadcast_401 stored as values in memory (estimated size 52.9 KiB, free 434.2 MiB)
26/01/04 17:21:38 INFO MemoryStore: Block broadcast_401_piece0 stored as bytes in memory (estimated size 20.2 KiB, free 434.1 MiB)
26/01/04 17:21:38 INFO BlockManagerInfo: Added broadcast_401_piece0 in memory on spark-master:33535 (size: 20.2 KiB, free: 434.3 MiB)
26/01/04 17:21:38 INFO SparkContext: Created broadcast 401 from broadcast at DAGScheduler.scala:1585
26/01/04 17:21:38 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 269 (MapPartitionsRDD[1347] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/01/04 17:21:38 INFO TaskSchedulerImpl: Adding task set 269.0 with 1 tasks resource profile 0
26/01/04 17:21:38 INFO TaskSetManager: Starting task 0.0 in stage 269.0 (TID 439) (172.18.0.13, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:21:38 INFO BlockManagerInfo: Added broadcast_401_piece0 in memory on 172.18.0.13:44401 (size: 20.2 KiB, free: 434.3 MiB)
26/01/04 17:21:38 INFO BlockManagerInfo: Added broadcast_400_piece0 in memory on 172.18.0.13:44401 (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:21:39 INFO TaskSetManager: Finished task 0.0 in stage 269.0 (TID 439) in 571 ms on 172.18.0.13 (executor 0) (1/1)
26/01/04 17:21:39 INFO TaskSchedulerImpl: Removed TaskSet 269.0, whose tasks have all completed, from pool 
26/01/04 17:21:39 INFO DAGScheduler: ResultStage 269 (start at NativeMethodAccessorImpl.java:0) finished in 0.576 s
26/01/04 17:21:39 INFO DAGScheduler: Job 268 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:21:39 INFO TaskSchedulerImpl: Killing all running tasks in stage 269: Stage finished
26/01/04 17:21:39 INFO DAGScheduler: Job 268 finished: start at NativeMethodAccessorImpl.java:0, took 0.578003 s
26/01/04 17:21:39 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 132, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@731a193b] is committing.
26/01/04 17:21:39 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 132, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@731a193b] committed.
26/01/04 17:21:39 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/132 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.132.0783b553-2e9d-45a9-8781-8e2c6d871e1f.tmp
26/01/04 17:21:39 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.132.0783b553-2e9d-45a9-8781-8e2c6d871e1f.tmp to file:/tmp/spark-checkpoint-enrichment/commits/132
26/01/04 17:21:39 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T17:21:38.423Z",
  "batchId" : 132,
  "numInputRows" : 30,
  "inputRowsPerSecond" : 2307.6923076923076,
  "processedRowsPerSecond" : 34.285714285714285,
  "durationMs" : {
    "addBatch" : 680,
    "commitOffsets" : 86,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 21,
    "triggerExecution" : 875,
    "walCommit" : 86
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "0" : 3219
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "0" : 3249
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "0" : 3249
      }
    },
    "numInputRows" : 30,
    "inputRowsPerSecond" : 2307.6923076923076,
    "processedRowsPerSecond" : 34.285714285714285,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 31
  }
}
26/01/04 17:21:41 INFO BlockManagerInfo: Removed broadcast_401_piece0 on spark-master:33535 in memory (size: 20.2 KiB, free: 434.3 MiB)
26/01/04 17:21:41 INFO BlockManagerInfo: Removed broadcast_401_piece0 on 172.18.0.13:44401 in memory (size: 20.2 KiB, free: 434.3 MiB)
26/01/04 17:21:41 INFO BlockManagerInfo: Removed broadcast_399_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:21:41 INFO BlockManagerInfo: Removed broadcast_399_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:21:41 INFO BlockManagerInfo: Removed broadcast_399_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:21:41 INFO BlockManagerInfo: Removed broadcast_398_piece0 on spark-master:33535 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:21:41 INFO BlockManagerInfo: Removed broadcast_398_piece0 on 172.18.0.13:44401 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:21:41 INFO BlockManagerInfo: Removed broadcast_397_piece0 on spark-master:33535 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:21:41 INFO BlockManagerInfo: Removed broadcast_397_piece0 on 172.18.0.13:44401 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:21:41 INFO BlockManagerInfo: Removed broadcast_396_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:21:41 INFO BlockManagerInfo: Removed broadcast_396_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:21:41 INFO BlockManagerInfo: Removed broadcast_396_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:21:49 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/01/04 17:21:49 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/133 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.133.43662361-e3f4-4e3e-913e-dcd92f0e1e5e.tmp
26/01/04 17:21:49 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.133.43662361-e3f4-4e3e-913e-dcd92f0e1e5e.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/133
26/01/04 17:21:49 INFO MicroBatchExecution: Committed offsets for batch 133. Metadata OffsetSeqMetadata(0,1767547309442,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 17:21:49 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:21:49 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:21:49 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:21:49 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:21:49 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:21:49 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:21:49 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#108415 - airline_prefix.nullCount#108414) > 0)
26/01/04 17:21:49 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#108450 - min_flight_num.nullCount#108449) > 0)
26/01/04 17:21:49 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#108445 - max_flight_num.nullCount#108444) > 0)
26/01/04 17:21:49 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:21:49 INFO DAGScheduler: Got job 269 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 17:21:49 INFO DAGScheduler: Final stage: ResultStage 270 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:21:49 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:21:49 INFO DAGScheduler: Missing parents: List()
26/01/04 17:21:49 INFO DAGScheduler: Submitting ResultStage 270 (MapPartitionsRDD[1352] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:21:49 INFO MemoryStore: Block broadcast_402 stored as values in memory (estimated size 38.5 KiB, free 434.4 MiB)
26/01/04 17:21:49 INFO MemoryStore: Block broadcast_402_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.3 MiB)
26/01/04 17:21:49 INFO BlockManagerInfo: Added broadcast_402_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:21:49 INFO SparkContext: Created broadcast 402 from broadcast at DAGScheduler.scala:1585
26/01/04 17:21:49 INFO BlockManagerInfo: Removed broadcast_400_piece0 on spark-master:33535 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:21:49 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 270 (MapPartitionsRDD[1352] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 17:21:49 INFO TaskSchedulerImpl: Adding task set 270.0 with 2 tasks resource profile 0
26/01/04 17:21:49 INFO BlockManagerInfo: Removed broadcast_400_piece0 on 172.18.0.13:44401 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:21:49 INFO TaskSetManager: Starting task 0.0 in stage 270.0 (TID 440) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 17:21:49 INFO TaskSetManager: Starting task 1.0 in stage 270.0 (TID 441) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 17:21:49 INFO BlockManagerInfo: Added broadcast_402_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:21:49 INFO BlockManagerInfo: Added broadcast_402_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:21:49 INFO TaskSetManager: Finished task 1.0 in stage 270.0 (TID 441) in 50 ms on 172.18.0.13 (executor 0) (1/2)
26/01/04 17:21:49 INFO TaskSetManager: Finished task 0.0 in stage 270.0 (TID 440) in 129 ms on 172.18.0.14 (executor 1) (2/2)
26/01/04 17:21:49 INFO TaskSchedulerImpl: Removed TaskSet 270.0, whose tasks have all completed, from pool 
26/01/04 17:21:49 INFO DAGScheduler: ResultStage 270 (start at NativeMethodAccessorImpl.java:0) finished in 0.160 s
26/01/04 17:21:49 INFO DAGScheduler: Job 269 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:21:49 INFO TaskSchedulerImpl: Killing all running tasks in stage 270: Stage finished
26/01/04 17:21:49 INFO DAGScheduler: Job 269 finished: start at NativeMethodAccessorImpl.java:0, took 0.173021 s
26/01/04 17:21:49 INFO MemoryStore: Block broadcast_403_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.3 MiB)
26/01/04 17:21:49 INFO BlockManagerInfo: Added broadcast_403_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:21:49 INFO SparkContext: Created broadcast 403 from start at NativeMethodAccessorImpl.java:0
26/01/04 17:21:49 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 133, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@450fcaff]. The input RDD has 1 partitions.
26/01/04 17:21:49 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:21:49 INFO DAGScheduler: Got job 270 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/01/04 17:21:49 INFO DAGScheduler: Final stage: ResultStage 271 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:21:49 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:21:49 INFO DAGScheduler: Missing parents: List()
26/01/04 17:21:49 INFO DAGScheduler: Submitting ResultStage 271 (MapPartitionsRDD[1357] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:21:49 INFO MemoryStore: Block broadcast_404 stored as values in memory (estimated size 52.9 KiB, free 434.3 MiB)
26/01/04 17:21:50 INFO MemoryStore: Block broadcast_404_piece0 stored as bytes in memory (estimated size 20.2 KiB, free 434.3 MiB)
26/01/04 17:21:50 INFO BlockManagerInfo: Added broadcast_404_piece0 in memory on spark-master:33535 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:21:50 INFO SparkContext: Created broadcast 404 from broadcast at DAGScheduler.scala:1585
26/01/04 17:21:50 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 271 (MapPartitionsRDD[1357] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/01/04 17:21:50 INFO TaskSchedulerImpl: Adding task set 271.0 with 1 tasks resource profile 0
26/01/04 17:21:50 INFO TaskSetManager: Starting task 0.0 in stage 271.0 (TID 442) (172.18.0.13, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:21:50 INFO BlockManagerInfo: Added broadcast_404_piece0 in memory on 172.18.0.13:44401 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:21:50 INFO BlockManagerInfo: Added broadcast_403_piece0 in memory on 172.18.0.13:44401 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:21:50 INFO TaskSetManager: Finished task 0.0 in stage 271.0 (TID 442) in 621 ms on 172.18.0.13 (executor 0) (1/1)
26/01/04 17:21:50 INFO TaskSchedulerImpl: Removed TaskSet 271.0, whose tasks have all completed, from pool 
26/01/04 17:21:50 INFO DAGScheduler: ResultStage 271 (start at NativeMethodAccessorImpl.java:0) finished in 0.647 s
26/01/04 17:21:50 INFO DAGScheduler: Job 270 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:21:50 INFO TaskSchedulerImpl: Killing all running tasks in stage 271: Stage finished
26/01/04 17:21:50 INFO DAGScheduler: Job 270 finished: start at NativeMethodAccessorImpl.java:0, took 0.654925 s
26/01/04 17:21:50 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 133, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@450fcaff] is committing.
26/01/04 17:21:50 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 133, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@450fcaff] committed.
26/01/04 17:21:50 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/133 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.133.6c0f2694-a93d-4b69-bbbc-45c739df5dc4.tmp
26/01/04 17:21:50 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.133.6c0f2694-a93d-4b69-bbbc-45c739df5dc4.tmp to file:/tmp/spark-checkpoint-enrichment/commits/133
26/01/04 17:21:50 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T17:21:49.438Z",
  "batchId" : 133,
  "numInputRows" : 30,
  "inputRowsPerSecond" : 1875.0,
  "processedRowsPerSecond" : 21.382751247327157,
  "durationMs" : {
    "addBatch" : 993,
    "commitOffsets" : 207,
    "getBatch" : 0,
    "latestOffset" : 4,
    "queryPlanning" : 49,
    "triggerExecution" : 1403,
    "walCommit" : 149
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "0" : 3249
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "0" : 3279
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "0" : 3279
      }
    },
    "numInputRows" : 30,
    "inputRowsPerSecond" : 1875.0,
    "processedRowsPerSecond" : 21.382751247327157,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 31
  }
}
26/01/04 17:22:00 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/134 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.134.4ff727b6-1e54-4eda-bf57-1f4cbabc8fe1.tmp
26/01/04 17:22:00 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.134.4ff727b6-1e54-4eda-bf57-1f4cbabc8fe1.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/134
26/01/04 17:22:00 INFO MicroBatchExecution: Committed offsets for batch 134. Metadata OffsetSeqMetadata(0,1767547320456,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 17:22:00 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:22:00 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:22:00 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:22:00 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:22:00 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:22:00 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:22:00 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#109219 - airline_prefix.nullCount#109218) > 0)
26/01/04 17:22:00 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#109254 - min_flight_num.nullCount#109253) > 0)
26/01/04 17:22:00 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#109249 - max_flight_num.nullCount#109248) > 0)
26/01/04 17:22:00 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:22:00 INFO DAGScheduler: Got job 271 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 17:22:00 INFO DAGScheduler: Final stage: ResultStage 272 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:22:00 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:22:00 INFO DAGScheduler: Missing parents: List()
26/01/04 17:22:00 INFO DAGScheduler: Submitting ResultStage 272 (MapPartitionsRDD[1362] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:22:00 INFO MemoryStore: Block broadcast_405 stored as values in memory (estimated size 38.5 KiB, free 434.2 MiB)
26/01/04 17:22:00 INFO MemoryStore: Block broadcast_405_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.2 MiB)
26/01/04 17:22:00 INFO BlockManagerInfo: Added broadcast_405_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:22:00 INFO SparkContext: Created broadcast 405 from broadcast at DAGScheduler.scala:1585
26/01/04 17:22:00 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 272 (MapPartitionsRDD[1362] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 17:22:00 INFO TaskSchedulerImpl: Adding task set 272.0 with 2 tasks resource profile 0
26/01/04 17:22:00 INFO TaskSetManager: Starting task 0.0 in stage 272.0 (TID 443) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 17:22:00 INFO TaskSetManager: Starting task 1.0 in stage 272.0 (TID 444) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 17:22:00 INFO BlockManagerInfo: Added broadcast_405_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:22:00 INFO BlockManagerInfo: Added broadcast_405_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:22:00 INFO TaskSetManager: Finished task 1.0 in stage 272.0 (TID 444) in 24 ms on 172.18.0.13 (executor 0) (1/2)
26/01/04 17:22:00 INFO TaskSetManager: Finished task 0.0 in stage 272.0 (TID 443) in 43 ms on 172.18.0.14 (executor 1) (2/2)
26/01/04 17:22:00 INFO TaskSchedulerImpl: Removed TaskSet 272.0, whose tasks have all completed, from pool 
26/01/04 17:22:00 INFO DAGScheduler: ResultStage 272 (start at NativeMethodAccessorImpl.java:0) finished in 0.048 s
26/01/04 17:22:00 INFO DAGScheduler: Job 271 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:22:00 INFO TaskSchedulerImpl: Killing all running tasks in stage 272: Stage finished
26/01/04 17:22:00 INFO DAGScheduler: Job 271 finished: start at NativeMethodAccessorImpl.java:0, took 0.048972 s
26/01/04 17:22:00 INFO MemoryStore: Block broadcast_406_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.2 MiB)
26/01/04 17:22:00 INFO BlockManagerInfo: Added broadcast_406_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:22:00 INFO SparkContext: Created broadcast 406 from start at NativeMethodAccessorImpl.java:0
26/01/04 17:22:00 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 134, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@c2b4a57]. The input RDD has 1 partitions.
26/01/04 17:22:00 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:22:00 INFO DAGScheduler: Got job 272 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/01/04 17:22:00 INFO DAGScheduler: Final stage: ResultStage 273 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:22:00 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:22:00 INFO DAGScheduler: Missing parents: List()
26/01/04 17:22:00 INFO DAGScheduler: Submitting ResultStage 273 (MapPartitionsRDD[1367] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:22:00 INFO MemoryStore: Block broadcast_407 stored as values in memory (estimated size 52.9 KiB, free 434.2 MiB)
26/01/04 17:22:00 INFO MemoryStore: Block broadcast_407_piece0 stored as bytes in memory (estimated size 20.2 KiB, free 434.1 MiB)
26/01/04 17:22:00 INFO BlockManagerInfo: Added broadcast_407_piece0 in memory on spark-master:33535 (size: 20.2 KiB, free: 434.3 MiB)
26/01/04 17:22:00 INFO SparkContext: Created broadcast 407 from broadcast at DAGScheduler.scala:1585
26/01/04 17:22:00 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 273 (MapPartitionsRDD[1367] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/01/04 17:22:00 INFO TaskSchedulerImpl: Adding task set 273.0 with 1 tasks resource profile 0
26/01/04 17:22:00 INFO TaskSetManager: Starting task 0.0 in stage 273.0 (TID 445) (172.18.0.13, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:22:00 INFO BlockManagerInfo: Added broadcast_407_piece0 in memory on 172.18.0.13:44401 (size: 20.2 KiB, free: 434.3 MiB)
26/01/04 17:22:00 INFO BlockManagerInfo: Added broadcast_406_piece0 in memory on 172.18.0.13:44401 (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:22:01 INFO TaskSetManager: Finished task 0.0 in stage 273.0 (TID 445) in 556 ms on 172.18.0.13 (executor 0) (1/1)
26/01/04 17:22:01 INFO TaskSchedulerImpl: Removed TaskSet 273.0, whose tasks have all completed, from pool 
26/01/04 17:22:01 INFO DAGScheduler: ResultStage 273 (start at NativeMethodAccessorImpl.java:0) finished in 0.562 s
26/01/04 17:22:01 INFO DAGScheduler: Job 272 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:22:01 INFO TaskSchedulerImpl: Killing all running tasks in stage 273: Stage finished
26/01/04 17:22:01 INFO DAGScheduler: Job 272 finished: start at NativeMethodAccessorImpl.java:0, took 0.564436 s
26/01/04 17:22:01 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 134, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@c2b4a57] is committing.
26/01/04 17:22:01 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 134, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@c2b4a57] committed.
26/01/04 17:22:01 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/134 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.134.15c33e9b-d807-43b9-b954-0a6570f4f7cc.tmp
26/01/04 17:22:01 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.134.15c33e9b-d807-43b9-b954-0a6570f4f7cc.tmp to file:/tmp/spark-checkpoint-enrichment/commits/134
26/01/04 17:22:01 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T17:22:00.454Z",
  "batchId" : 134,
  "numInputRows" : 30,
  "inputRowsPerSecond" : 2500.0,
  "processedRowsPerSecond" : 34.68208092485549,
  "durationMs" : {
    "addBatch" : 675,
    "commitOffsets" : 80,
    "getBatch" : 0,
    "latestOffset" : 2,
    "queryPlanning" : 22,
    "triggerExecution" : 865,
    "walCommit" : 85
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "0" : 3279
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "0" : 3309
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "0" : 3309
      }
    },
    "numInputRows" : 30,
    "inputRowsPerSecond" : 2500.0,
    "processedRowsPerSecond" : 34.68208092485549,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 31
  }
}
26/01/04 17:22:04 INFO BlockManagerInfo: Removed broadcast_402_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:22:04 INFO BlockManagerInfo: Removed broadcast_402_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:22:04 INFO BlockManagerInfo: Removed broadcast_402_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:22:04 INFO BlockManagerInfo: Removed broadcast_407_piece0 on spark-master:33535 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:22:04 INFO BlockManagerInfo: Removed broadcast_407_piece0 on 172.18.0.13:44401 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:22:04 INFO BlockManagerInfo: Removed broadcast_403_piece0 on spark-master:33535 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:22:04 INFO BlockManagerInfo: Removed broadcast_403_piece0 on 172.18.0.13:44401 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:22:04 INFO BlockManagerInfo: Removed broadcast_405_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:22:04 INFO BlockManagerInfo: Removed broadcast_405_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:22:04 INFO BlockManagerInfo: Removed broadcast_405_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:22:04 INFO BlockManagerInfo: Removed broadcast_404_piece0 on spark-master:33535 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:22:04 INFO BlockManagerInfo: Removed broadcast_404_piece0 on 172.18.0.13:44401 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:22:11 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/01/04 17:22:11 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/135 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.135.f0a3cfb3-c2d4-4199-9bb2-901abf738fa3.tmp
26/01/04 17:22:11 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.135.f0a3cfb3-c2d4-4199-9bb2-901abf738fa3.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/135
26/01/04 17:22:11 INFO MicroBatchExecution: Committed offsets for batch 135. Metadata OffsetSeqMetadata(0,1767547331468,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 17:22:11 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:22:11 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:22:11 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:22:11 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:22:11 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:22:11 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:22:11 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#110023 - airline_prefix.nullCount#110022) > 0)
26/01/04 17:22:11 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#110058 - min_flight_num.nullCount#110057) > 0)
26/01/04 17:22:11 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#110053 - max_flight_num.nullCount#110052) > 0)
26/01/04 17:22:11 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:22:11 INFO DAGScheduler: Got job 273 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 17:22:11 INFO DAGScheduler: Final stage: ResultStage 274 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:22:11 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:22:11 INFO DAGScheduler: Missing parents: List()
26/01/04 17:22:11 INFO DAGScheduler: Submitting ResultStage 274 (MapPartitionsRDD[1372] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:22:11 INFO MemoryStore: Block broadcast_408 stored as values in memory (estimated size 38.5 KiB, free 434.4 MiB)
26/01/04 17:22:11 INFO MemoryStore: Block broadcast_408_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.3 MiB)
26/01/04 17:22:11 INFO BlockManagerInfo: Removed broadcast_406_piece0 on spark-master:33535 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:22:11 INFO BlockManagerInfo: Added broadcast_408_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:22:11 INFO SparkContext: Created broadcast 408 from broadcast at DAGScheduler.scala:1585
26/01/04 17:22:11 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 274 (MapPartitionsRDD[1372] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 17:22:11 INFO TaskSchedulerImpl: Adding task set 274.0 with 2 tasks resource profile 0
26/01/04 17:22:11 INFO BlockManagerInfo: Removed broadcast_406_piece0 on 172.18.0.13:44401 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:22:11 INFO TaskSetManager: Starting task 1.0 in stage 274.0 (TID 446) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 17:22:11 INFO TaskSetManager: Starting task 0.0 in stage 274.0 (TID 447) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 17:22:11 INFO BlockManagerInfo: Added broadcast_408_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:22:11 INFO BlockManagerInfo: Added broadcast_408_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:22:11 INFO TaskSetManager: Finished task 1.0 in stage 274.0 (TID 446) in 73 ms on 172.18.0.13 (executor 0) (1/2)
26/01/04 17:22:11 INFO TaskSetManager: Finished task 0.0 in stage 274.0 (TID 447) in 132 ms on 172.18.0.14 (executor 1) (2/2)
26/01/04 17:22:11 INFO TaskSchedulerImpl: Removed TaskSet 274.0, whose tasks have all completed, from pool 
26/01/04 17:22:11 INFO DAGScheduler: ResultStage 274 (start at NativeMethodAccessorImpl.java:0) finished in 0.155 s
26/01/04 17:22:11 INFO DAGScheduler: Job 273 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:22:11 INFO TaskSchedulerImpl: Killing all running tasks in stage 274: Stage finished
26/01/04 17:22:11 INFO DAGScheduler: Job 273 finished: start at NativeMethodAccessorImpl.java:0, took 0.163924 s
26/01/04 17:22:11 INFO MemoryStore: Block broadcast_409_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.3 MiB)
26/01/04 17:22:11 INFO BlockManagerInfo: Added broadcast_409_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:22:11 INFO SparkContext: Created broadcast 409 from start at NativeMethodAccessorImpl.java:0
26/01/04 17:22:11 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 135, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@1046a2c8]. The input RDD has 1 partitions.
26/01/04 17:22:11 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:22:11 INFO DAGScheduler: Got job 274 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/01/04 17:22:11 INFO DAGScheduler: Final stage: ResultStage 275 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:22:11 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:22:11 INFO DAGScheduler: Missing parents: List()
26/01/04 17:22:11 INFO DAGScheduler: Submitting ResultStage 275 (MapPartitionsRDD[1377] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:22:11 INFO MemoryStore: Block broadcast_410 stored as values in memory (estimated size 52.9 KiB, free 434.3 MiB)
26/01/04 17:22:11 INFO MemoryStore: Block broadcast_410_piece0 stored as bytes in memory (estimated size 20.2 KiB, free 434.3 MiB)
26/01/04 17:22:11 INFO BlockManagerInfo: Added broadcast_410_piece0 in memory on spark-master:33535 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:22:11 INFO SparkContext: Created broadcast 410 from broadcast at DAGScheduler.scala:1585
26/01/04 17:22:11 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 275 (MapPartitionsRDD[1377] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/01/04 17:22:11 INFO TaskSchedulerImpl: Adding task set 275.0 with 1 tasks resource profile 0
26/01/04 17:22:11 INFO TaskSetManager: Starting task 0.0 in stage 275.0 (TID 448) (172.18.0.13, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:22:11 INFO BlockManagerInfo: Added broadcast_410_piece0 in memory on 172.18.0.13:44401 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:22:11 INFO BlockManagerInfo: Added broadcast_409_piece0 in memory on 172.18.0.13:44401 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:22:12 INFO TaskSetManager: Finished task 0.0 in stage 275.0 (TID 448) in 590 ms on 172.18.0.13 (executor 0) (1/1)
26/01/04 17:22:12 INFO TaskSchedulerImpl: Removed TaskSet 275.0, whose tasks have all completed, from pool 
26/01/04 17:22:12 INFO DAGScheduler: ResultStage 275 (start at NativeMethodAccessorImpl.java:0) finished in 0.600 s
26/01/04 17:22:12 INFO DAGScheduler: Job 274 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:22:12 INFO TaskSchedulerImpl: Killing all running tasks in stage 275: Stage finished
26/01/04 17:22:12 INFO DAGScheduler: Job 274 finished: start at NativeMethodAccessorImpl.java:0, took 0.602712 s
26/01/04 17:22:12 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 135, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@1046a2c8] is committing.
26/01/04 17:22:12 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 135, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@1046a2c8] committed.
26/01/04 17:22:12 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/135 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.135.cd92ba58-d46e-4151-923f-115be5b29737.tmp
26/01/04 17:22:12 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.135.cd92ba58-d46e-4151-923f-115be5b29737.tmp to file:/tmp/spark-checkpoint-enrichment/commits/135
26/01/04 17:22:12 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T17:22:11.466Z",
  "batchId" : 135,
  "numInputRows" : 30,
  "inputRowsPerSecond" : 2307.6923076923076,
  "processedRowsPerSecond" : 26.642984014209596,
  "durationMs" : {
    "addBatch" : 870,
    "commitOffsets" : 129,
    "getBatch" : 0,
    "latestOffset" : 2,
    "queryPlanning" : 27,
    "triggerExecution" : 1126,
    "walCommit" : 97
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "0" : 3309
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "0" : 3339
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "0" : 3339
      }
    },
    "numInputRows" : 30,
    "inputRowsPerSecond" : 2307.6923076923076,
    "processedRowsPerSecond" : 26.642984014209596,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 31
  }
}
26/01/04 17:22:22 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/136 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.136.50920f3e-f7b4-4c73-ae39-f7bbe8ae1a37.tmp
26/01/04 17:22:22 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.136.50920f3e-f7b4-4c73-ae39-f7bbe8ae1a37.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/136
26/01/04 17:22:22 INFO MicroBatchExecution: Committed offsets for batch 136. Metadata OffsetSeqMetadata(0,1767547342474,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 17:22:22 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:22:22 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:22:22 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:22:22 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:22:22 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:22:22 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:22:22 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#110827 - airline_prefix.nullCount#110826) > 0)
26/01/04 17:22:22 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#110862 - min_flight_num.nullCount#110861) > 0)
26/01/04 17:22:22 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#110857 - max_flight_num.nullCount#110856) > 0)
26/01/04 17:22:22 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:22:22 INFO DAGScheduler: Got job 275 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 17:22:22 INFO DAGScheduler: Final stage: ResultStage 276 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:22:22 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:22:22 INFO DAGScheduler: Missing parents: List()
26/01/04 17:22:22 INFO DAGScheduler: Submitting ResultStage 276 (MapPartitionsRDD[1382] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:22:22 INFO MemoryStore: Block broadcast_411 stored as values in memory (estimated size 38.5 KiB, free 434.2 MiB)
26/01/04 17:22:22 INFO MemoryStore: Block broadcast_411_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.2 MiB)
26/01/04 17:22:22 INFO BlockManagerInfo: Added broadcast_411_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:22:22 INFO SparkContext: Created broadcast 411 from broadcast at DAGScheduler.scala:1585
26/01/04 17:22:22 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 276 (MapPartitionsRDD[1382] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 17:22:22 INFO TaskSchedulerImpl: Adding task set 276.0 with 2 tasks resource profile 0
26/01/04 17:22:22 INFO TaskSetManager: Starting task 0.0 in stage 276.0 (TID 449) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 17:22:22 INFO TaskSetManager: Starting task 1.0 in stage 276.0 (TID 450) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 17:22:22 INFO BlockManagerInfo: Added broadcast_411_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:22:22 INFO TaskSetManager: Finished task 1.0 in stage 276.0 (TID 450) in 30 ms on 172.18.0.13 (executor 0) (1/2)
26/01/04 17:22:22 INFO BlockManagerInfo: Added broadcast_411_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:22:22 INFO TaskSetManager: Finished task 0.0 in stage 276.0 (TID 449) in 55 ms on 172.18.0.14 (executor 1) (2/2)
26/01/04 17:22:22 INFO TaskSchedulerImpl: Removed TaskSet 276.0, whose tasks have all completed, from pool 
26/01/04 17:22:22 INFO DAGScheduler: ResultStage 276 (start at NativeMethodAccessorImpl.java:0) finished in 0.062 s
26/01/04 17:22:22 INFO DAGScheduler: Job 275 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:22:22 INFO TaskSchedulerImpl: Killing all running tasks in stage 276: Stage finished
26/01/04 17:22:22 INFO DAGScheduler: Job 275 finished: start at NativeMethodAccessorImpl.java:0, took 0.066641 s
26/01/04 17:22:22 INFO MemoryStore: Block broadcast_412_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.2 MiB)
26/01/04 17:22:22 INFO BlockManagerInfo: Added broadcast_412_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:22:22 INFO SparkContext: Created broadcast 412 from start at NativeMethodAccessorImpl.java:0
26/01/04 17:22:22 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 136, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@3c403dee]. The input RDD has 1 partitions.
26/01/04 17:22:22 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:22:22 INFO DAGScheduler: Got job 276 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/01/04 17:22:22 INFO DAGScheduler: Final stage: ResultStage 277 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:22:22 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:22:22 INFO DAGScheduler: Missing parents: List()
26/01/04 17:22:22 INFO DAGScheduler: Submitting ResultStage 277 (MapPartitionsRDD[1387] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:22:22 INFO MemoryStore: Block broadcast_413 stored as values in memory (estimated size 52.9 KiB, free 434.2 MiB)
26/01/04 17:22:22 INFO MemoryStore: Block broadcast_413_piece0 stored as bytes in memory (estimated size 20.2 KiB, free 434.1 MiB)
26/01/04 17:22:22 INFO BlockManagerInfo: Added broadcast_413_piece0 in memory on spark-master:33535 (size: 20.2 KiB, free: 434.3 MiB)
26/01/04 17:22:22 INFO SparkContext: Created broadcast 413 from broadcast at DAGScheduler.scala:1585
26/01/04 17:22:22 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 277 (MapPartitionsRDD[1387] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/01/04 17:22:22 INFO TaskSchedulerImpl: Adding task set 277.0 with 1 tasks resource profile 0
26/01/04 17:22:22 INFO TaskSetManager: Starting task 0.0 in stage 277.0 (TID 451) (172.18.0.13, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:22:22 INFO BlockManagerInfo: Added broadcast_413_piece0 in memory on 172.18.0.13:44401 (size: 20.2 KiB, free: 434.3 MiB)
26/01/04 17:22:22 INFO BlockManagerInfo: Added broadcast_412_piece0 in memory on 172.18.0.13:44401 (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:22:23 INFO TaskSetManager: Finished task 0.0 in stage 277.0 (TID 451) in 577 ms on 172.18.0.13 (executor 0) (1/1)
26/01/04 17:22:23 INFO TaskSchedulerImpl: Removed TaskSet 277.0, whose tasks have all completed, from pool 
26/01/04 17:22:23 INFO DAGScheduler: ResultStage 277 (start at NativeMethodAccessorImpl.java:0) finished in 0.586 s
26/01/04 17:22:23 INFO DAGScheduler: Job 276 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:22:23 INFO TaskSchedulerImpl: Killing all running tasks in stage 277: Stage finished
26/01/04 17:22:23 INFO DAGScheduler: Job 276 finished: start at NativeMethodAccessorImpl.java:0, took 0.589480 s
26/01/04 17:22:23 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 136, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@3c403dee] is committing.
26/01/04 17:22:23 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 136, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@3c403dee] committed.
26/01/04 17:22:23 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/136 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.136.65e783da-5cad-4e5d-9884-5c68c741ce3d.tmp
26/01/04 17:22:23 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.136.65e783da-5cad-4e5d-9884-5c68c741ce3d.tmp to file:/tmp/spark-checkpoint-enrichment/commits/136
26/01/04 17:22:23 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T17:22:22.471Z",
  "batchId" : 136,
  "numInputRows" : 30,
  "inputRowsPerSecond" : 2500.0,
  "processedRowsPerSecond" : 31.746031746031747,
  "durationMs" : {
    "addBatch" : 755,
    "commitOffsets" : 82,
    "getBatch" : 0,
    "latestOffset" : 3,
    "queryPlanning" : 30,
    "triggerExecution" : 945,
    "walCommit" : 74
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "0" : 3339
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "0" : 3369
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "0" : 3369
      }
    },
    "numInputRows" : 30,
    "inputRowsPerSecond" : 2500.0,
    "processedRowsPerSecond" : 31.746031746031747,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 31
  }
}
26/01/04 17:22:26 INFO BlockManagerInfo: Removed broadcast_413_piece0 on spark-master:33535 in memory (size: 20.2 KiB, free: 434.3 MiB)
26/01/04 17:22:26 INFO BlockManagerInfo: Removed broadcast_413_piece0 on 172.18.0.13:44401 in memory (size: 20.2 KiB, free: 434.3 MiB)
26/01/04 17:22:26 INFO BlockManagerInfo: Removed broadcast_411_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:22:26 INFO BlockManagerInfo: Removed broadcast_411_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:22:26 INFO BlockManagerInfo: Removed broadcast_411_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:22:26 INFO BlockManagerInfo: Removed broadcast_408_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:22:26 INFO BlockManagerInfo: Removed broadcast_408_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:22:26 INFO BlockManagerInfo: Removed broadcast_408_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:22:26 INFO BlockManagerInfo: Removed broadcast_409_piece0 on spark-master:33535 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:22:26 INFO BlockManagerInfo: Removed broadcast_409_piece0 on 172.18.0.13:44401 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:22:26 INFO BlockManagerInfo: Removed broadcast_410_piece0 on spark-master:33535 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:22:26 INFO BlockManagerInfo: Removed broadcast_410_piece0 on 172.18.0.13:44401 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:22:33 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/01/04 17:22:33 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/137 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.137.416035d7-d490-4d35-be8c-64c78c341c27.tmp
26/01/04 17:22:33 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.137.416035d7-d490-4d35-be8c-64c78c341c27.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/137
26/01/04 17:22:33 INFO MicroBatchExecution: Committed offsets for batch 137. Metadata OffsetSeqMetadata(0,1767547353499,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 17:22:33 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:22:33 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:22:33 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:22:33 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:22:33 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:22:33 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:22:33 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#111631 - airline_prefix.nullCount#111630) > 0)
26/01/04 17:22:33 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#111666 - min_flight_num.nullCount#111665) > 0)
26/01/04 17:22:33 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#111661 - max_flight_num.nullCount#111660) > 0)
26/01/04 17:22:33 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:22:33 INFO DAGScheduler: Got job 277 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 17:22:33 INFO DAGScheduler: Final stage: ResultStage 278 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:22:33 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:22:33 INFO DAGScheduler: Missing parents: List()
26/01/04 17:22:33 INFO DAGScheduler: Submitting ResultStage 278 (MapPartitionsRDD[1392] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:22:33 INFO MemoryStore: Block broadcast_414 stored as values in memory (estimated size 38.5 KiB, free 434.4 MiB)
26/01/04 17:22:33 INFO MemoryStore: Block broadcast_414_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.3 MiB)
26/01/04 17:22:33 INFO BlockManagerInfo: Added broadcast_414_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:22:33 INFO BlockManagerInfo: Removed broadcast_412_piece0 on spark-master:33535 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:22:33 INFO SparkContext: Created broadcast 414 from broadcast at DAGScheduler.scala:1585
26/01/04 17:22:33 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 278 (MapPartitionsRDD[1392] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 17:22:33 INFO TaskSchedulerImpl: Adding task set 278.0 with 2 tasks resource profile 0
26/01/04 17:22:33 INFO BlockManagerInfo: Removed broadcast_412_piece0 on 172.18.0.13:44401 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:22:33 INFO TaskSetManager: Starting task 0.0 in stage 278.0 (TID 452) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 17:22:33 INFO TaskSetManager: Starting task 1.0 in stage 278.0 (TID 453) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 17:22:33 INFO BlockManagerInfo: Added broadcast_414_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:22:33 INFO BlockManagerInfo: Added broadcast_414_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:22:33 INFO TaskSetManager: Finished task 1.0 in stage 278.0 (TID 453) in 30 ms on 172.18.0.13 (executor 0) (1/2)
26/01/04 17:22:33 INFO TaskSetManager: Finished task 0.0 in stage 278.0 (TID 452) in 71 ms on 172.18.0.14 (executor 1) (2/2)
26/01/04 17:22:33 INFO TaskSchedulerImpl: Removed TaskSet 278.0, whose tasks have all completed, from pool 
26/01/04 17:22:33 INFO DAGScheduler: ResultStage 278 (start at NativeMethodAccessorImpl.java:0) finished in 0.083 s
26/01/04 17:22:33 INFO DAGScheduler: Job 277 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:22:33 INFO TaskSchedulerImpl: Killing all running tasks in stage 278: Stage finished
26/01/04 17:22:33 INFO DAGScheduler: Job 277 finished: start at NativeMethodAccessorImpl.java:0, took 0.085043 s
26/01/04 17:22:33 INFO MemoryStore: Block broadcast_415_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.3 MiB)
26/01/04 17:22:33 INFO BlockManagerInfo: Added broadcast_415_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:22:33 INFO SparkContext: Created broadcast 415 from start at NativeMethodAccessorImpl.java:0
26/01/04 17:22:33 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 137, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@34886e07]. The input RDD has 1 partitions.
26/01/04 17:22:33 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:22:33 INFO DAGScheduler: Got job 278 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/01/04 17:22:33 INFO DAGScheduler: Final stage: ResultStage 279 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:22:33 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:22:33 INFO DAGScheduler: Missing parents: List()
26/01/04 17:22:33 INFO DAGScheduler: Submitting ResultStage 279 (MapPartitionsRDD[1397] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:22:33 INFO MemoryStore: Block broadcast_416 stored as values in memory (estimated size 52.9 KiB, free 434.3 MiB)
26/01/04 17:22:33 INFO MemoryStore: Block broadcast_416_piece0 stored as bytes in memory (estimated size 20.2 KiB, free 434.3 MiB)
26/01/04 17:22:33 INFO BlockManagerInfo: Added broadcast_416_piece0 in memory on spark-master:33535 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:22:33 INFO SparkContext: Created broadcast 416 from broadcast at DAGScheduler.scala:1585
26/01/04 17:22:33 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 279 (MapPartitionsRDD[1397] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/01/04 17:22:33 INFO TaskSchedulerImpl: Adding task set 279.0 with 1 tasks resource profile 0
26/01/04 17:22:33 INFO TaskSetManager: Starting task 0.0 in stage 279.0 (TID 454) (172.18.0.13, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:22:33 INFO BlockManagerInfo: Added broadcast_416_piece0 in memory on 172.18.0.13:44401 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:22:33 INFO BlockManagerInfo: Added broadcast_415_piece0 in memory on 172.18.0.13:44401 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:22:34 INFO TaskSetManager: Finished task 0.0 in stage 279.0 (TID 454) in 567 ms on 172.18.0.13 (executor 0) (1/1)
26/01/04 17:22:34 INFO TaskSchedulerImpl: Removed TaskSet 279.0, whose tasks have all completed, from pool 
26/01/04 17:22:34 INFO DAGScheduler: ResultStage 279 (start at NativeMethodAccessorImpl.java:0) finished in 0.576 s
26/01/04 17:22:34 INFO DAGScheduler: Job 278 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:22:34 INFO TaskSchedulerImpl: Killing all running tasks in stage 279: Stage finished
26/01/04 17:22:34 INFO DAGScheduler: Job 278 finished: start at NativeMethodAccessorImpl.java:0, took 0.577970 s
26/01/04 17:22:34 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 137, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@34886e07] is committing.
26/01/04 17:22:34 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 137, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@34886e07] committed.
26/01/04 17:22:34 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/137 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.137.222561b0-a320-4484-8d5c-76d3a28d4986.tmp
26/01/04 17:22:34 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.137.222561b0-a320-4484-8d5c-76d3a28d4986.tmp to file:/tmp/spark-checkpoint-enrichment/commits/137
26/01/04 17:22:34 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T17:22:33.497Z",
  "batchId" : 137,
  "numInputRows" : 30,
  "inputRowsPerSecond" : 2727.2727272727275,
  "processedRowsPerSecond" : 30.706243602865918,
  "durationMs" : {
    "addBatch" : 753,
    "commitOffsets" : 116,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 18,
    "triggerExecution" : 977,
    "walCommit" : 86
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "0" : 3369
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "0" : 3399
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "0" : 3399
      }
    },
    "numInputRows" : 30,
    "inputRowsPerSecond" : 2727.2727272727275,
    "processedRowsPerSecond" : 30.706243602865918,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 31
  }
}
26/01/04 17:22:44 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/01/04 17:22:44 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/138 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.138.3ac06ddc-87a9-4d26-9081-3c405446be8d.tmp
26/01/04 17:22:44 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.138.3ac06ddc-87a9-4d26-9081-3c405446be8d.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/138
26/01/04 17:22:44 INFO MicroBatchExecution: Committed offsets for batch 138. Metadata OffsetSeqMetadata(0,1767547364513,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 17:22:44 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:22:44 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:22:44 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:22:44 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:22:44 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:22:44 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:22:44 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#112435 - airline_prefix.nullCount#112434) > 0)
26/01/04 17:22:44 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#112470 - min_flight_num.nullCount#112469) > 0)
26/01/04 17:22:44 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#112465 - max_flight_num.nullCount#112464) > 0)
26/01/04 17:22:44 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:22:44 INFO DAGScheduler: Got job 279 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 17:22:44 INFO DAGScheduler: Final stage: ResultStage 280 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:22:44 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:22:44 INFO DAGScheduler: Missing parents: List()
26/01/04 17:22:44 INFO DAGScheduler: Submitting ResultStage 280 (MapPartitionsRDD[1402] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:22:44 INFO MemoryStore: Block broadcast_417 stored as values in memory (estimated size 38.5 KiB, free 434.2 MiB)
26/01/04 17:22:44 INFO MemoryStore: Block broadcast_417_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.2 MiB)
26/01/04 17:22:44 INFO BlockManagerInfo: Added broadcast_417_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:22:44 INFO SparkContext: Created broadcast 417 from broadcast at DAGScheduler.scala:1585
26/01/04 17:22:44 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 280 (MapPartitionsRDD[1402] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 17:22:44 INFO TaskSchedulerImpl: Adding task set 280.0 with 2 tasks resource profile 0
26/01/04 17:22:44 INFO TaskSetManager: Starting task 1.0 in stage 280.0 (TID 455) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 17:22:44 INFO TaskSetManager: Starting task 0.0 in stage 280.0 (TID 456) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 17:22:44 INFO BlockManagerInfo: Added broadcast_417_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:22:44 INFO BlockManagerInfo: Added broadcast_417_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:22:44 INFO TaskSetManager: Finished task 1.0 in stage 280.0 (TID 455) in 17 ms on 172.18.0.13 (executor 0) (1/2)
26/01/04 17:22:44 INFO TaskSetManager: Finished task 0.0 in stage 280.0 (TID 456) in 27 ms on 172.18.0.14 (executor 1) (2/2)
26/01/04 17:22:44 INFO TaskSchedulerImpl: Removed TaskSet 280.0, whose tasks have all completed, from pool 
26/01/04 17:22:44 INFO DAGScheduler: ResultStage 280 (start at NativeMethodAccessorImpl.java:0) finished in 0.031 s
26/01/04 17:22:44 INFO DAGScheduler: Job 279 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:22:44 INFO TaskSchedulerImpl: Killing all running tasks in stage 280: Stage finished
26/01/04 17:22:44 INFO DAGScheduler: Job 279 finished: start at NativeMethodAccessorImpl.java:0, took 0.033021 s
26/01/04 17:22:44 INFO MemoryStore: Block broadcast_418_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.2 MiB)
26/01/04 17:22:44 INFO BlockManagerInfo: Added broadcast_418_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:22:44 INFO SparkContext: Created broadcast 418 from start at NativeMethodAccessorImpl.java:0
26/01/04 17:22:44 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 138, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@863ae38]. The input RDD has 1 partitions.
26/01/04 17:22:44 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:22:44 INFO DAGScheduler: Got job 280 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/01/04 17:22:44 INFO DAGScheduler: Final stage: ResultStage 281 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:22:44 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:22:44 INFO DAGScheduler: Missing parents: List()
26/01/04 17:22:44 INFO DAGScheduler: Submitting ResultStage 281 (MapPartitionsRDD[1407] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:22:44 INFO MemoryStore: Block broadcast_419 stored as values in memory (estimated size 52.9 KiB, free 434.2 MiB)
26/01/04 17:22:44 INFO MemoryStore: Block broadcast_419_piece0 stored as bytes in memory (estimated size 20.2 KiB, free 434.1 MiB)
26/01/04 17:22:44 INFO BlockManagerInfo: Added broadcast_419_piece0 in memory on spark-master:33535 (size: 20.2 KiB, free: 434.3 MiB)
26/01/04 17:22:44 INFO SparkContext: Created broadcast 419 from broadcast at DAGScheduler.scala:1585
26/01/04 17:22:44 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 281 (MapPartitionsRDD[1407] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/01/04 17:22:44 INFO TaskSchedulerImpl: Adding task set 281.0 with 1 tasks resource profile 0
26/01/04 17:22:44 INFO TaskSetManager: Starting task 0.0 in stage 281.0 (TID 457) (172.18.0.13, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:22:44 INFO BlockManagerInfo: Added broadcast_419_piece0 in memory on 172.18.0.13:44401 (size: 20.2 KiB, free: 434.3 MiB)
26/01/04 17:22:44 INFO BlockManagerInfo: Added broadcast_418_piece0 in memory on 172.18.0.13:44401 (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:22:45 INFO TaskSetManager: Finished task 0.0 in stage 281.0 (TID 457) in 537 ms on 172.18.0.13 (executor 0) (1/1)
26/01/04 17:22:45 INFO TaskSchedulerImpl: Removed TaskSet 281.0, whose tasks have all completed, from pool 
26/01/04 17:22:45 INFO DAGScheduler: ResultStage 281 (start at NativeMethodAccessorImpl.java:0) finished in 0.541 s
26/01/04 17:22:45 INFO DAGScheduler: Job 280 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:22:45 INFO TaskSchedulerImpl: Killing all running tasks in stage 281: Stage finished
26/01/04 17:22:45 INFO DAGScheduler: Job 280 finished: start at NativeMethodAccessorImpl.java:0, took 0.543285 s
26/01/04 17:22:45 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 138, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@863ae38] is committing.
26/01/04 17:22:45 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 138, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@863ae38] committed.
26/01/04 17:22:45 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/138 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.138.d1f13a05-631e-481f-9f2d-d951b2ffc2e8.tmp
26/01/04 17:22:45 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.138.d1f13a05-631e-481f-9f2d-d951b2ffc2e8.tmp to file:/tmp/spark-checkpoint-enrichment/commits/138
26/01/04 17:22:45 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T17:22:44.512Z",
  "batchId" : 138,
  "numInputRows" : 30,
  "inputRowsPerSecond" : 2500.0,
  "processedRowsPerSecond" : 37.359900373599004,
  "durationMs" : {
    "addBatch" : 630,
    "commitOffsets" : 60,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 21,
    "triggerExecution" : 803,
    "walCommit" : 89
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "0" : 3399
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "0" : 3429
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "0" : 3429
      }
    },
    "numInputRows" : 30,
    "inputRowsPerSecond" : 2500.0,
    "processedRowsPerSecond" : 37.359900373599004,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 31
  }
}
26/01/04 17:22:47 INFO BlockManagerInfo: Removed broadcast_419_piece0 on spark-master:33535 in memory (size: 20.2 KiB, free: 434.3 MiB)
26/01/04 17:22:47 INFO BlockManagerInfo: Removed broadcast_419_piece0 on 172.18.0.13:44401 in memory (size: 20.2 KiB, free: 434.3 MiB)
26/01/04 17:22:47 INFO BlockManagerInfo: Removed broadcast_414_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:22:47 INFO BlockManagerInfo: Removed broadcast_414_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:22:47 INFO BlockManagerInfo: Removed broadcast_414_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:22:47 INFO BlockManagerInfo: Removed broadcast_415_piece0 on spark-master:33535 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:22:47 INFO BlockManagerInfo: Removed broadcast_415_piece0 on 172.18.0.13:44401 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:22:47 INFO BlockManagerInfo: Removed broadcast_416_piece0 on spark-master:33535 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:22:47 INFO BlockManagerInfo: Removed broadcast_416_piece0 on 172.18.0.13:44401 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:22:47 INFO BlockManagerInfo: Removed broadcast_417_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:22:47 INFO BlockManagerInfo: Removed broadcast_417_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:22:47 INFO BlockManagerInfo: Removed broadcast_417_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:22:55 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/01/04 17:22:55 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/139 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.139.ed723639-61af-43f1-8c19-92acc3773815.tmp
26/01/04 17:22:55 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.139.ed723639-61af-43f1-8c19-92acc3773815.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/139
26/01/04 17:22:55 INFO MicroBatchExecution: Committed offsets for batch 139. Metadata OffsetSeqMetadata(0,1767547375523,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 17:22:55 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:22:55 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:22:55 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:22:55 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:22:55 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:22:55 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:22:55 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#113239 - airline_prefix.nullCount#113238) > 0)
26/01/04 17:22:55 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#113274 - min_flight_num.nullCount#113273) > 0)
26/01/04 17:22:55 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#113269 - max_flight_num.nullCount#113268) > 0)
26/01/04 17:22:55 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:22:55 INFO DAGScheduler: Got job 281 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 17:22:55 INFO DAGScheduler: Final stage: ResultStage 282 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:22:55 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:22:55 INFO DAGScheduler: Missing parents: List()
26/01/04 17:22:55 INFO DAGScheduler: Submitting ResultStage 282 (MapPartitionsRDD[1412] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:22:55 INFO MemoryStore: Block broadcast_420 stored as values in memory (estimated size 38.5 KiB, free 434.4 MiB)
26/01/04 17:22:55 INFO MemoryStore: Block broadcast_420_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.3 MiB)
26/01/04 17:22:55 INFO BlockManagerInfo: Removed broadcast_418_piece0 on spark-master:33535 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:22:55 INFO BlockManagerInfo: Added broadcast_420_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:22:55 INFO BlockManagerInfo: Removed broadcast_418_piece0 on 172.18.0.13:44401 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:22:55 INFO SparkContext: Created broadcast 420 from broadcast at DAGScheduler.scala:1585
26/01/04 17:22:55 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 282 (MapPartitionsRDD[1412] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 17:22:55 INFO TaskSchedulerImpl: Adding task set 282.0 with 2 tasks resource profile 0
26/01/04 17:22:55 INFO TaskSetManager: Starting task 0.0 in stage 282.0 (TID 458) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 17:22:55 INFO TaskSetManager: Starting task 1.0 in stage 282.0 (TID 459) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 17:22:55 INFO BlockManagerInfo: Added broadcast_420_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:22:55 INFO BlockManagerInfo: Added broadcast_420_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:22:55 INFO TaskSetManager: Finished task 1.0 in stage 282.0 (TID 459) in 32 ms on 172.18.0.13 (executor 0) (1/2)
26/01/04 17:22:55 INFO TaskSetManager: Finished task 0.0 in stage 282.0 (TID 458) in 74 ms on 172.18.0.14 (executor 1) (2/2)
26/01/04 17:22:55 INFO TaskSchedulerImpl: Removed TaskSet 282.0, whose tasks have all completed, from pool 
26/01/04 17:22:55 INFO DAGScheduler: ResultStage 282 (start at NativeMethodAccessorImpl.java:0) finished in 0.091 s
26/01/04 17:22:55 INFO DAGScheduler: Job 281 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:22:55 INFO TaskSchedulerImpl: Killing all running tasks in stage 282: Stage finished
26/01/04 17:22:55 INFO DAGScheduler: Job 281 finished: start at NativeMethodAccessorImpl.java:0, took 0.091909 s
26/01/04 17:22:55 INFO MemoryStore: Block broadcast_421_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.3 MiB)
26/01/04 17:22:55 INFO BlockManagerInfo: Added broadcast_421_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:22:55 INFO SparkContext: Created broadcast 421 from start at NativeMethodAccessorImpl.java:0
26/01/04 17:22:55 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 139, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@189d797e]. The input RDD has 1 partitions.
26/01/04 17:22:55 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:22:55 INFO DAGScheduler: Got job 282 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/01/04 17:22:55 INFO DAGScheduler: Final stage: ResultStage 283 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:22:55 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:22:55 INFO DAGScheduler: Missing parents: List()
26/01/04 17:22:55 INFO DAGScheduler: Submitting ResultStage 283 (MapPartitionsRDD[1417] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:22:55 INFO MemoryStore: Block broadcast_422 stored as values in memory (estimated size 52.9 KiB, free 434.3 MiB)
26/01/04 17:22:55 INFO MemoryStore: Block broadcast_422_piece0 stored as bytes in memory (estimated size 20.2 KiB, free 434.3 MiB)
26/01/04 17:22:55 INFO BlockManagerInfo: Added broadcast_422_piece0 in memory on spark-master:33535 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:22:55 INFO SparkContext: Created broadcast 422 from broadcast at DAGScheduler.scala:1585
26/01/04 17:22:55 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 283 (MapPartitionsRDD[1417] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/01/04 17:22:55 INFO TaskSchedulerImpl: Adding task set 283.0 with 1 tasks resource profile 0
26/01/04 17:22:55 INFO TaskSetManager: Starting task 0.0 in stage 283.0 (TID 460) (172.18.0.13, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:22:55 INFO BlockManagerInfo: Added broadcast_422_piece0 in memory on 172.18.0.13:44401 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:22:55 INFO BlockManagerInfo: Added broadcast_421_piece0 in memory on 172.18.0.13:44401 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:22:56 INFO TaskSetManager: Finished task 0.0 in stage 283.0 (TID 460) in 545 ms on 172.18.0.13 (executor 0) (1/1)
26/01/04 17:22:56 INFO TaskSchedulerImpl: Removed TaskSet 283.0, whose tasks have all completed, from pool 
26/01/04 17:22:56 INFO DAGScheduler: ResultStage 283 (start at NativeMethodAccessorImpl.java:0) finished in 0.552 s
26/01/04 17:22:56 INFO DAGScheduler: Job 282 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:22:56 INFO TaskSchedulerImpl: Killing all running tasks in stage 283: Stage finished
26/01/04 17:22:56 INFO DAGScheduler: Job 282 finished: start at NativeMethodAccessorImpl.java:0, took 0.554970 s
26/01/04 17:22:56 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 139, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@189d797e] is committing.
26/01/04 17:22:56 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 139, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@189d797e] committed.
26/01/04 17:22:56 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/139 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.139.6c09de49-3050-451e-be29-68381c48ef8a.tmp
26/01/04 17:22:56 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.139.6c09de49-3050-451e-be29-68381c48ef8a.tmp to file:/tmp/spark-checkpoint-enrichment/commits/139
26/01/04 17:22:56 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T17:22:55.521Z",
  "batchId" : 139,
  "numInputRows" : 30,
  "inputRowsPerSecond" : 2142.8571428571427,
  "processedRowsPerSecond" : 33.0760749724366,
  "durationMs" : {
    "addBatch" : 722,
    "commitOffsets" : 77,
    "getBatch" : 0,
    "latestOffset" : 2,
    "queryPlanning" : 25,
    "triggerExecution" : 907,
    "walCommit" : 81
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "0" : 3429
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "0" : 3459
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "0" : 3459
      }
    },
    "numInputRows" : 30,
    "inputRowsPerSecond" : 2142.8571428571427,
    "processedRowsPerSecond" : 33.0760749724366,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 31
  }
}
26/01/04 17:23:06 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/01/04 17:23:06 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/140 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.140.b635d102-1779-48b4-a8c3-f56810a409fb.tmp
26/01/04 17:23:06 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.140.b635d102-1779-48b4-a8c3-f56810a409fb.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/140
26/01/04 17:23:06 INFO MicroBatchExecution: Committed offsets for batch 140. Metadata OffsetSeqMetadata(0,1767547386528,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 17:23:06 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:23:06 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:23:06 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:23:06 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:23:06 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:23:06 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:23:06 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#114043 - airline_prefix.nullCount#114042) > 0)
26/01/04 17:23:06 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#114078 - min_flight_num.nullCount#114077) > 0)
26/01/04 17:23:06 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#114073 - max_flight_num.nullCount#114072) > 0)
26/01/04 17:23:06 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:23:06 INFO DAGScheduler: Got job 283 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 17:23:06 INFO DAGScheduler: Final stage: ResultStage 284 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:23:06 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:23:06 INFO DAGScheduler: Missing parents: List()
26/01/04 17:23:06 INFO DAGScheduler: Submitting ResultStage 284 (MapPartitionsRDD[1422] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:23:06 INFO MemoryStore: Block broadcast_423 stored as values in memory (estimated size 38.5 KiB, free 434.2 MiB)
26/01/04 17:23:06 INFO MemoryStore: Block broadcast_423_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.2 MiB)
26/01/04 17:23:06 INFO BlockManagerInfo: Added broadcast_423_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:23:06 INFO SparkContext: Created broadcast 423 from broadcast at DAGScheduler.scala:1585
26/01/04 17:23:06 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 284 (MapPartitionsRDD[1422] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 17:23:06 INFO TaskSchedulerImpl: Adding task set 284.0 with 2 tasks resource profile 0
26/01/04 17:23:06 INFO TaskSetManager: Starting task 1.0 in stage 284.0 (TID 461) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 17:23:06 INFO TaskSetManager: Starting task 0.0 in stage 284.0 (TID 462) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 17:23:06 INFO BlockManagerInfo: Added broadcast_423_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:23:06 INFO BlockManagerInfo: Added broadcast_423_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:23:06 INFO TaskSetManager: Finished task 1.0 in stage 284.0 (TID 461) in 18 ms on 172.18.0.13 (executor 0) (1/2)
26/01/04 17:23:06 INFO TaskSetManager: Finished task 0.0 in stage 284.0 (TID 462) in 30 ms on 172.18.0.14 (executor 1) (2/2)
26/01/04 17:23:06 INFO TaskSchedulerImpl: Removed TaskSet 284.0, whose tasks have all completed, from pool 
26/01/04 17:23:06 INFO DAGScheduler: ResultStage 284 (start at NativeMethodAccessorImpl.java:0) finished in 0.038 s
26/01/04 17:23:06 INFO DAGScheduler: Job 283 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:23:06 INFO TaskSchedulerImpl: Killing all running tasks in stage 284: Stage finished
26/01/04 17:23:06 INFO DAGScheduler: Job 283 finished: start at NativeMethodAccessorImpl.java:0, took 0.041982 s
26/01/04 17:23:06 INFO MemoryStore: Block broadcast_424_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.2 MiB)
26/01/04 17:23:06 INFO BlockManagerInfo: Added broadcast_424_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:23:06 INFO SparkContext: Created broadcast 424 from start at NativeMethodAccessorImpl.java:0
26/01/04 17:23:06 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 140, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@381ab6b2]. The input RDD has 1 partitions.
26/01/04 17:23:06 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:23:06 INFO DAGScheduler: Got job 284 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/01/04 17:23:06 INFO DAGScheduler: Final stage: ResultStage 285 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:23:06 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:23:06 INFO DAGScheduler: Missing parents: List()
26/01/04 17:23:06 INFO DAGScheduler: Submitting ResultStage 285 (MapPartitionsRDD[1427] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:23:06 INFO MemoryStore: Block broadcast_425 stored as values in memory (estimated size 52.9 KiB, free 434.2 MiB)
26/01/04 17:23:06 INFO MemoryStore: Block broadcast_425_piece0 stored as bytes in memory (estimated size 20.2 KiB, free 434.1 MiB)
26/01/04 17:23:06 INFO BlockManagerInfo: Added broadcast_425_piece0 in memory on spark-master:33535 (size: 20.2 KiB, free: 434.3 MiB)
26/01/04 17:23:06 INFO SparkContext: Created broadcast 425 from broadcast at DAGScheduler.scala:1585
26/01/04 17:23:06 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 285 (MapPartitionsRDD[1427] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/01/04 17:23:06 INFO TaskSchedulerImpl: Adding task set 285.0 with 1 tasks resource profile 0
26/01/04 17:23:06 INFO TaskSetManager: Starting task 0.0 in stage 285.0 (TID 463) (172.18.0.13, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:23:06 INFO BlockManagerInfo: Added broadcast_425_piece0 in memory on 172.18.0.13:44401 (size: 20.2 KiB, free: 434.3 MiB)
26/01/04 17:23:06 INFO BlockManagerInfo: Added broadcast_424_piece0 in memory on 172.18.0.13:44401 (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:23:07 INFO TaskSetManager: Finished task 0.0 in stage 285.0 (TID 463) in 567 ms on 172.18.0.13 (executor 0) (1/1)
26/01/04 17:23:07 INFO TaskSchedulerImpl: Removed TaskSet 285.0, whose tasks have all completed, from pool 
26/01/04 17:23:07 INFO DAGScheduler: ResultStage 285 (start at NativeMethodAccessorImpl.java:0) finished in 0.572 s
26/01/04 17:23:07 INFO DAGScheduler: Job 284 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:23:07 INFO TaskSchedulerImpl: Killing all running tasks in stage 285: Stage finished
26/01/04 17:23:07 INFO DAGScheduler: Job 284 finished: start at NativeMethodAccessorImpl.java:0, took 0.573853 s
26/01/04 17:23:07 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 140, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@381ab6b2] is committing.
26/01/04 17:23:07 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 140, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@381ab6b2] committed.
26/01/04 17:23:07 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/140 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.140.64eddb40-6fc8-491e-bcb3-4d1dd10fe670.tmp
26/01/04 17:23:07 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.140.64eddb40-6fc8-491e-bcb3-4d1dd10fe670.tmp to file:/tmp/spark-checkpoint-enrichment/commits/140
26/01/04 17:23:07 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T17:23:06.524Z",
  "batchId" : 140,
  "numInputRows" : 30,
  "inputRowsPerSecond" : 2727.2727272727275,
  "processedRowsPerSecond" : 31.152647975077883,
  "durationMs" : {
    "addBatch" : 669,
    "commitOffsets" : 176,
    "getBatch" : 0,
    "latestOffset" : 4,
    "queryPlanning" : 22,
    "triggerExecution" : 963,
    "walCommit" : 91
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "0" : 3459
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "0" : 3489
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "0" : 3489
      }
    },
    "numInputRows" : 30,
    "inputRowsPerSecond" : 2727.2727272727275,
    "processedRowsPerSecond" : 31.152647975077883,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 31
  }
}
26/01/04 17:23:09 INFO BlockManagerInfo: Removed broadcast_420_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:23:09 INFO BlockManagerInfo: Removed broadcast_420_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:23:09 INFO BlockManagerInfo: Removed broadcast_420_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:23:10 INFO BlockManagerInfo: Removed broadcast_423_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:23:10 INFO BlockManagerInfo: Removed broadcast_423_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:23:10 INFO BlockManagerInfo: Removed broadcast_423_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:23:10 INFO BlockManagerInfo: Removed broadcast_425_piece0 on spark-master:33535 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:23:10 INFO BlockManagerInfo: Removed broadcast_425_piece0 on 172.18.0.13:44401 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:23:10 INFO BlockManagerInfo: Removed broadcast_422_piece0 on 172.18.0.13:44401 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:23:10 INFO BlockManagerInfo: Removed broadcast_422_piece0 on spark-master:33535 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:23:10 INFO BlockManagerInfo: Removed broadcast_421_piece0 on spark-master:33535 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:23:10 INFO BlockManagerInfo: Removed broadcast_421_piece0 on 172.18.0.13:44401 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:23:17 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/01/04 17:23:17 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/141 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.141.f464acb5-a5f1-4842-95e7-276ac39c6a8e.tmp
26/01/04 17:23:17 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.141.f464acb5-a5f1-4842-95e7-276ac39c6a8e.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/141
26/01/04 17:23:17 INFO MicroBatchExecution: Committed offsets for batch 141. Metadata OffsetSeqMetadata(0,1767547397552,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 17:23:18 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:23:18 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:23:18 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:23:18 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:23:18 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:23:18 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:23:18 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#114847 - airline_prefix.nullCount#114846) > 0)
26/01/04 17:23:18 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#114882 - min_flight_num.nullCount#114881) > 0)
26/01/04 17:23:18 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#114877 - max_flight_num.nullCount#114876) > 0)
26/01/04 17:23:18 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:23:18 INFO DAGScheduler: Got job 285 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 17:23:18 INFO DAGScheduler: Final stage: ResultStage 286 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:23:18 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:23:18 INFO DAGScheduler: Missing parents: List()
26/01/04 17:23:18 INFO DAGScheduler: Submitting ResultStage 286 (MapPartitionsRDD[1432] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:23:18 INFO MemoryStore: Block broadcast_426 stored as values in memory (estimated size 38.5 KiB, free 434.4 MiB)
26/01/04 17:23:18 INFO MemoryStore: Block broadcast_426_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.3 MiB)
26/01/04 17:23:18 INFO BlockManagerInfo: Removed broadcast_424_piece0 on spark-master:33535 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:23:18 INFO BlockManagerInfo: Added broadcast_426_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:23:18 INFO BlockManagerInfo: Removed broadcast_424_piece0 on 172.18.0.13:44401 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:23:18 INFO SparkContext: Created broadcast 426 from broadcast at DAGScheduler.scala:1585
26/01/04 17:23:18 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 286 (MapPartitionsRDD[1432] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 17:23:18 INFO TaskSchedulerImpl: Adding task set 286.0 with 2 tasks resource profile 0
26/01/04 17:23:18 INFO TaskSetManager: Starting task 0.0 in stage 286.0 (TID 464) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 17:23:18 INFO TaskSetManager: Starting task 1.0 in stage 286.0 (TID 465) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 17:23:18 INFO BlockManagerInfo: Added broadcast_426_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:23:18 INFO BlockManagerInfo: Added broadcast_426_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:23:18 INFO TaskSetManager: Finished task 1.0 in stage 286.0 (TID 465) in 78 ms on 172.18.0.13 (executor 0) (1/2)
26/01/04 17:23:18 INFO TaskSetManager: Finished task 0.0 in stage 286.0 (TID 464) in 94 ms on 172.18.0.14 (executor 1) (2/2)
26/01/04 17:23:18 INFO TaskSchedulerImpl: Removed TaskSet 286.0, whose tasks have all completed, from pool 
26/01/04 17:23:18 INFO DAGScheduler: ResultStage 286 (start at NativeMethodAccessorImpl.java:0) finished in 0.149 s
26/01/04 17:23:18 INFO DAGScheduler: Job 285 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:23:18 INFO TaskSchedulerImpl: Killing all running tasks in stage 286: Stage finished
26/01/04 17:23:18 INFO DAGScheduler: Job 285 finished: start at NativeMethodAccessorImpl.java:0, took 0.156313 s
26/01/04 17:23:18 INFO MemoryStore: Block broadcast_427_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.3 MiB)
26/01/04 17:23:18 INFO BlockManagerInfo: Added broadcast_427_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:23:18 INFO SparkContext: Created broadcast 427 from start at NativeMethodAccessorImpl.java:0
26/01/04 17:23:18 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 141, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@692e7729]. The input RDD has 1 partitions.
26/01/04 17:23:18 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:23:18 INFO DAGScheduler: Got job 286 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/01/04 17:23:18 INFO DAGScheduler: Final stage: ResultStage 287 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:23:18 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:23:18 INFO DAGScheduler: Missing parents: List()
26/01/04 17:23:18 INFO DAGScheduler: Submitting ResultStage 287 (MapPartitionsRDD[1437] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:23:18 INFO MemoryStore: Block broadcast_428 stored as values in memory (estimated size 52.9 KiB, free 434.3 MiB)
26/01/04 17:23:18 INFO MemoryStore: Block broadcast_428_piece0 stored as bytes in memory (estimated size 20.2 KiB, free 434.3 MiB)
26/01/04 17:23:18 INFO BlockManagerInfo: Added broadcast_428_piece0 in memory on spark-master:33535 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:23:18 INFO SparkContext: Created broadcast 428 from broadcast at DAGScheduler.scala:1585
26/01/04 17:23:18 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 287 (MapPartitionsRDD[1437] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/01/04 17:23:18 INFO TaskSchedulerImpl: Adding task set 287.0 with 1 tasks resource profile 0
26/01/04 17:23:18 INFO TaskSetManager: Starting task 0.0 in stage 287.0 (TID 466) (172.18.0.13, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:23:18 INFO BlockManagerInfo: Added broadcast_428_piece0 in memory on 172.18.0.13:44401 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:23:18 INFO BlockManagerInfo: Added broadcast_427_piece0 in memory on 172.18.0.13:44401 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:23:19 INFO TaskSetManager: Finished task 0.0 in stage 287.0 (TID 466) in 787 ms on 172.18.0.13 (executor 0) (1/1)
26/01/04 17:23:19 INFO TaskSchedulerImpl: Removed TaskSet 287.0, whose tasks have all completed, from pool 
26/01/04 17:23:19 INFO DAGScheduler: ResultStage 287 (start at NativeMethodAccessorImpl.java:0) finished in 0.803 s
26/01/04 17:23:19 INFO DAGScheduler: Job 286 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:23:19 INFO TaskSchedulerImpl: Killing all running tasks in stage 287: Stage finished
26/01/04 17:23:19 INFO DAGScheduler: Job 286 finished: start at NativeMethodAccessorImpl.java:0, took 0.808827 s
26/01/04 17:23:19 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 141, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@692e7729] is committing.
26/01/04 17:23:19 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 141, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@692e7729] committed.
26/01/04 17:23:19 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/141 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.141.d44dc051-3ff9-43b4-b300-a20e29eed9cf.tmp
26/01/04 17:23:19 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.141.d44dc051-3ff9-43b4-b300-a20e29eed9cf.tmp to file:/tmp/spark-checkpoint-enrichment/commits/141
26/01/04 17:23:19 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T17:23:17.544Z",
  "batchId" : 141,
  "numInputRows" : 30,
  "inputRowsPerSecond" : 2142.8571428571427,
  "processedRowsPerSecond" : 14.360938247965533,
  "durationMs" : {
    "addBatch" : 1217,
    "commitOffsets" : 315,
    "getBatch" : 0,
    "latestOffset" : 8,
    "queryPlanning" : 123,
    "triggerExecution" : 2089,
    "walCommit" : 420
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "0" : 3489
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "0" : 3519
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "0" : 3519
      }
    },
    "numInputRows" : 30,
    "inputRowsPerSecond" : 2142.8571428571427,
    "processedRowsPerSecond" : 14.360938247965533,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 31
  }
}
26/01/04 17:23:28 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/142 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.142.aa867b0e-eb01-4cbb-b546-24e18e229bf7.tmp
26/01/04 17:23:28 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.142.aa867b0e-eb01-4cbb-b546-24e18e229bf7.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/142
26/01/04 17:23:28 INFO MicroBatchExecution: Committed offsets for batch 142. Metadata OffsetSeqMetadata(0,1767547408571,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 17:23:28 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:23:28 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:23:28 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:23:28 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:23:28 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:23:28 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:23:29 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#115651 - airline_prefix.nullCount#115650) > 0)
26/01/04 17:23:29 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#115686 - min_flight_num.nullCount#115685) > 0)
26/01/04 17:23:29 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#115681 - max_flight_num.nullCount#115680) > 0)
26/01/04 17:23:29 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:23:29 INFO DAGScheduler: Got job 287 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 17:23:29 INFO DAGScheduler: Final stage: ResultStage 288 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:23:29 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:23:29 INFO DAGScheduler: Missing parents: List()
26/01/04 17:23:29 INFO DAGScheduler: Submitting ResultStage 288 (MapPartitionsRDD[1442] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:23:29 INFO MemoryStore: Block broadcast_429 stored as values in memory (estimated size 38.5 KiB, free 434.2 MiB)
26/01/04 17:23:29 INFO MemoryStore: Block broadcast_429_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.2 MiB)
26/01/04 17:23:29 INFO BlockManagerInfo: Added broadcast_429_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:23:29 INFO SparkContext: Created broadcast 429 from broadcast at DAGScheduler.scala:1585
26/01/04 17:23:29 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 288 (MapPartitionsRDD[1442] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 17:23:29 INFO TaskSchedulerImpl: Adding task set 288.0 with 2 tasks resource profile 0
26/01/04 17:23:29 INFO TaskSetManager: Starting task 1.0 in stage 288.0 (TID 467) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 17:23:29 INFO TaskSetManager: Starting task 0.0 in stage 288.0 (TID 468) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 17:23:29 INFO BlockManagerInfo: Added broadcast_429_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:23:29 INFO BlockManagerInfo: Added broadcast_429_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:23:29 INFO TaskSetManager: Finished task 1.0 in stage 288.0 (TID 467) in 41 ms on 172.18.0.13 (executor 0) (1/2)
26/01/04 17:23:29 INFO TaskSetManager: Finished task 0.0 in stage 288.0 (TID 468) in 85 ms on 172.18.0.14 (executor 1) (2/2)
26/01/04 17:23:29 INFO TaskSchedulerImpl: Removed TaskSet 288.0, whose tasks have all completed, from pool 
26/01/04 17:23:29 INFO DAGScheduler: ResultStage 288 (start at NativeMethodAccessorImpl.java:0) finished in 0.099 s
26/01/04 17:23:29 INFO DAGScheduler: Job 287 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:23:29 INFO TaskSchedulerImpl: Killing all running tasks in stage 288: Stage finished
26/01/04 17:23:29 INFO DAGScheduler: Job 287 finished: start at NativeMethodAccessorImpl.java:0, took 0.107482 s
26/01/04 17:23:29 INFO MemoryStore: Block broadcast_430_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.2 MiB)
26/01/04 17:23:29 INFO BlockManagerInfo: Added broadcast_430_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:23:29 INFO SparkContext: Created broadcast 430 from start at NativeMethodAccessorImpl.java:0
26/01/04 17:23:29 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 142, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@499375f]. The input RDD has 1 partitions.
26/01/04 17:23:29 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:23:29 INFO DAGScheduler: Got job 288 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/01/04 17:23:29 INFO DAGScheduler: Final stage: ResultStage 289 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:23:29 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:23:29 INFO DAGScheduler: Missing parents: List()
26/01/04 17:23:29 INFO DAGScheduler: Submitting ResultStage 289 (MapPartitionsRDD[1447] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:23:29 INFO MemoryStore: Block broadcast_431 stored as values in memory (estimated size 52.9 KiB, free 434.2 MiB)
26/01/04 17:23:29 INFO MemoryStore: Block broadcast_431_piece0 stored as bytes in memory (estimated size 20.2 KiB, free 434.1 MiB)
26/01/04 17:23:29 INFO BlockManagerInfo: Added broadcast_431_piece0 in memory on spark-master:33535 (size: 20.2 KiB, free: 434.3 MiB)
26/01/04 17:23:29 INFO SparkContext: Created broadcast 431 from broadcast at DAGScheduler.scala:1585
26/01/04 17:23:29 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 289 (MapPartitionsRDD[1447] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/01/04 17:23:29 INFO TaskSchedulerImpl: Adding task set 289.0 with 1 tasks resource profile 0
26/01/04 17:23:29 INFO TaskSetManager: Starting task 0.0 in stage 289.0 (TID 469) (172.18.0.13, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:23:29 INFO BlockManagerInfo: Added broadcast_431_piece0 in memory on 172.18.0.13:44401 (size: 20.2 KiB, free: 434.3 MiB)
26/01/04 17:23:29 INFO BlockManagerInfo: Added broadcast_430_piece0 in memory on 172.18.0.13:44401 (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:23:29 INFO TaskSetManager: Finished task 0.0 in stage 289.0 (TID 469) in 667 ms on 172.18.0.13 (executor 0) (1/1)
26/01/04 17:23:29 INFO TaskSchedulerImpl: Removed TaskSet 289.0, whose tasks have all completed, from pool 
26/01/04 17:23:29 INFO DAGScheduler: ResultStage 289 (start at NativeMethodAccessorImpl.java:0) finished in 0.683 s
26/01/04 17:23:29 INFO DAGScheduler: Job 288 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:23:29 INFO TaskSchedulerImpl: Killing all running tasks in stage 289: Stage finished
26/01/04 17:23:29 INFO DAGScheduler: Job 288 finished: start at NativeMethodAccessorImpl.java:0, took 0.686874 s
26/01/04 17:23:29 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 142, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@499375f] is committing.
26/01/04 17:23:29 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 142, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@499375f] committed.
26/01/04 17:23:29 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/142 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.142.c94317e3-1f25-4d96-b2c8-971900f153e2.tmp
26/01/04 17:23:30 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.142.c94317e3-1f25-4d96-b2c8-971900f153e2.tmp to file:/tmp/spark-checkpoint-enrichment/commits/142
26/01/04 17:23:30 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T17:23:28.567Z",
  "batchId" : 142,
  "numInputRows" : 30,
  "inputRowsPerSecond" : 2142.8571428571427,
  "processedRowsPerSecond" : 20.174848688634835,
  "durationMs" : {
    "addBatch" : 945,
    "commitOffsets" : 202,
    "getBatch" : 0,
    "latestOffset" : 4,
    "queryPlanning" : 83,
    "triggerExecution" : 1487,
    "walCommit" : 251
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "0" : 3519
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "0" : 3549
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "0" : 3549
      }
    },
    "numInputRows" : 30,
    "inputRowsPerSecond" : 2142.8571428571427,
    "processedRowsPerSecond" : 20.174848688634835,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 31
  }
}
26/01/04 17:23:38 INFO BlockManagerInfo: Removed broadcast_428_piece0 on spark-master:33535 in memory (size: 20.2 KiB, free: 434.3 MiB)
26/01/04 17:23:38 INFO BlockManagerInfo: Removed broadcast_428_piece0 on 172.18.0.13:44401 in memory (size: 20.2 KiB, free: 434.3 MiB)
26/01/04 17:23:38 INFO BlockManagerInfo: Removed broadcast_429_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:23:38 INFO BlockManagerInfo: Removed broadcast_429_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:23:38 INFO BlockManagerInfo: Removed broadcast_429_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:23:38 INFO BlockManagerInfo: Removed broadcast_431_piece0 on spark-master:33535 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:23:38 INFO BlockManagerInfo: Removed broadcast_431_piece0 on 172.18.0.13:44401 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:23:38 INFO BlockManagerInfo: Removed broadcast_427_piece0 on spark-master:33535 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:23:38 INFO BlockManagerInfo: Removed broadcast_427_piece0 on 172.18.0.13:44401 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:23:38 INFO BlockManagerInfo: Removed broadcast_426_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:23:38 INFO BlockManagerInfo: Removed broadcast_426_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:23:38 INFO BlockManagerInfo: Removed broadcast_426_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:23:39 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/143 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.143.533793ca-00c0-4866-9b72-5b6439ca1a6a.tmp
26/01/04 17:23:39 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.143.533793ca-00c0-4866-9b72-5b6439ca1a6a.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/143
26/01/04 17:23:39 INFO MicroBatchExecution: Committed offsets for batch 143. Metadata OffsetSeqMetadata(0,1767547419594,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 17:23:39 INFO BlockManagerInfo: Removed broadcast_430_piece0 on spark-master:33535 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:23:39 INFO BlockManagerInfo: Removed broadcast_430_piece0 on 172.18.0.13:44401 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:23:39 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:23:39 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:23:39 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:23:39 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:23:39 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:23:39 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:23:39 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#116455 - airline_prefix.nullCount#116454) > 0)
26/01/04 17:23:39 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#116490 - min_flight_num.nullCount#116489) > 0)
26/01/04 17:23:39 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#116485 - max_flight_num.nullCount#116484) > 0)
26/01/04 17:23:39 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:23:39 INFO DAGScheduler: Got job 289 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 17:23:39 INFO DAGScheduler: Final stage: ResultStage 290 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:23:39 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:23:39 INFO DAGScheduler: Missing parents: List()
26/01/04 17:23:39 INFO DAGScheduler: Submitting ResultStage 290 (MapPartitionsRDD[1452] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:23:39 INFO MemoryStore: Block broadcast_432 stored as values in memory (estimated size 38.5 KiB, free 434.4 MiB)
26/01/04 17:23:40 INFO MemoryStore: Block broadcast_432_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.3 MiB)
26/01/04 17:23:40 INFO BlockManagerInfo: Added broadcast_432_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:23:40 INFO SparkContext: Created broadcast 432 from broadcast at DAGScheduler.scala:1585
26/01/04 17:23:40 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 290 (MapPartitionsRDD[1452] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 17:23:40 INFO TaskSchedulerImpl: Adding task set 290.0 with 2 tasks resource profile 0
26/01/04 17:23:40 INFO TaskSetManager: Starting task 1.0 in stage 290.0 (TID 470) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 17:23:40 INFO TaskSetManager: Starting task 0.0 in stage 290.0 (TID 471) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 17:23:40 INFO BlockManagerInfo: Added broadcast_432_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:23:40 INFO BlockManagerInfo: Added broadcast_432_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:23:40 INFO TaskSetManager: Finished task 1.0 in stage 290.0 (TID 470) in 67 ms on 172.18.0.13 (executor 0) (1/2)
26/01/04 17:23:40 INFO TaskSetManager: Finished task 0.0 in stage 290.0 (TID 471) in 134 ms on 172.18.0.14 (executor 1) (2/2)
26/01/04 17:23:40 INFO TaskSchedulerImpl: Removed TaskSet 290.0, whose tasks have all completed, from pool 
26/01/04 17:23:40 INFO DAGScheduler: ResultStage 290 (start at NativeMethodAccessorImpl.java:0) finished in 0.160 s
26/01/04 17:23:40 INFO DAGScheduler: Job 289 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:23:40 INFO TaskSchedulerImpl: Killing all running tasks in stage 290: Stage finished
26/01/04 17:23:40 INFO DAGScheduler: Job 289 finished: start at NativeMethodAccessorImpl.java:0, took 0.169498 s
26/01/04 17:23:40 INFO MemoryStore: Block broadcast_433_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.3 MiB)
26/01/04 17:23:40 INFO BlockManagerInfo: Added broadcast_433_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:23:40 INFO SparkContext: Created broadcast 433 from start at NativeMethodAccessorImpl.java:0
26/01/04 17:23:40 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 143, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@47f77d27]. The input RDD has 1 partitions.
26/01/04 17:23:40 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:23:40 INFO DAGScheduler: Got job 290 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/01/04 17:23:40 INFO DAGScheduler: Final stage: ResultStage 291 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:23:40 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:23:40 INFO DAGScheduler: Missing parents: List()
26/01/04 17:23:40 INFO DAGScheduler: Submitting ResultStage 291 (MapPartitionsRDD[1457] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:23:40 INFO MemoryStore: Block broadcast_434 stored as values in memory (estimated size 52.9 KiB, free 434.3 MiB)
26/01/04 17:23:40 INFO MemoryStore: Block broadcast_434_piece0 stored as bytes in memory (estimated size 20.2 KiB, free 434.3 MiB)
26/01/04 17:23:40 INFO BlockManagerInfo: Added broadcast_434_piece0 in memory on spark-master:33535 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:23:40 INFO SparkContext: Created broadcast 434 from broadcast at DAGScheduler.scala:1585
26/01/04 17:23:40 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 291 (MapPartitionsRDD[1457] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/01/04 17:23:40 INFO TaskSchedulerImpl: Adding task set 291.0 with 1 tasks resource profile 0
26/01/04 17:23:40 INFO TaskSetManager: Starting task 0.0 in stage 291.0 (TID 472) (172.18.0.13, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:23:40 INFO BlockManagerInfo: Added broadcast_434_piece0 in memory on 172.18.0.13:44401 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:23:40 INFO BlockManagerInfo: Added broadcast_433_piece0 in memory on 172.18.0.13:44401 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:23:40 INFO TaskSetManager: Finished task 0.0 in stage 291.0 (TID 472) in 671 ms on 172.18.0.13 (executor 0) (1/1)
26/01/04 17:23:40 INFO TaskSchedulerImpl: Removed TaskSet 291.0, whose tasks have all completed, from pool 
26/01/04 17:23:40 INFO DAGScheduler: ResultStage 291 (start at NativeMethodAccessorImpl.java:0) finished in 0.681 s
26/01/04 17:23:40 INFO DAGScheduler: Job 290 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:23:40 INFO TaskSchedulerImpl: Killing all running tasks in stage 291: Stage finished
26/01/04 17:23:40 INFO DAGScheduler: Job 290 finished: start at NativeMethodAccessorImpl.java:0, took 0.685651 s
26/01/04 17:23:40 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 143, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@47f77d27] is committing.
26/01/04 17:23:40 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 143, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@47f77d27] committed.
26/01/04 17:23:40 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/143 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.143.20908d5c-1b63-45ec-a677-738e330864ec.tmp
26/01/04 17:23:41 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.143.20908d5c-1b63-45ec-a677-738e330864ec.tmp to file:/tmp/spark-checkpoint-enrichment/commits/143
26/01/04 17:23:41 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T17:23:39.589Z",
  "batchId" : 143,
  "numInputRows" : 30,
  "inputRowsPerSecond" : 2142.8571428571427,
  "processedRowsPerSecond" : 20.949720670391063,
  "durationMs" : {
    "addBatch" : 1043,
    "commitOffsets" : 156,
    "getBatch" : 0,
    "latestOffset" : 5,
    "queryPlanning" : 81,
    "triggerExecution" : 1432,
    "walCommit" : 145
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "0" : 3549
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "0" : 3579
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "0" : 3579
      }
    },
    "numInputRows" : 30,
    "inputRowsPerSecond" : 2142.8571428571427,
    "processedRowsPerSecond" : 20.949720670391063,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 31
  }
}
26/01/04 17:23:50 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/144 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.144.a3fffcb6-7def-40eb-928d-5778cbaa4de6.tmp
26/01/04 17:23:50 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.144.a3fffcb6-7def-40eb-928d-5778cbaa4de6.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/144
26/01/04 17:23:50 INFO MicroBatchExecution: Committed offsets for batch 144. Metadata OffsetSeqMetadata(0,1767547430614,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 17:23:50 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:23:50 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:23:50 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:23:50 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:23:50 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:23:50 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:23:50 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#117259 - airline_prefix.nullCount#117258) > 0)
26/01/04 17:23:50 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#117294 - min_flight_num.nullCount#117293) > 0)
26/01/04 17:23:50 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#117289 - max_flight_num.nullCount#117288) > 0)
26/01/04 17:23:51 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:23:51 INFO DAGScheduler: Got job 291 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 17:23:51 INFO DAGScheduler: Final stage: ResultStage 292 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:23:51 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:23:51 INFO DAGScheduler: Missing parents: List()
26/01/04 17:23:51 INFO DAGScheduler: Submitting ResultStage 292 (MapPartitionsRDD[1462] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:23:51 INFO MemoryStore: Block broadcast_435 stored as values in memory (estimated size 38.5 KiB, free 434.2 MiB)
26/01/04 17:23:51 INFO MemoryStore: Block broadcast_435_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.2 MiB)
26/01/04 17:23:51 INFO BlockManagerInfo: Added broadcast_435_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:23:51 INFO SparkContext: Created broadcast 435 from broadcast at DAGScheduler.scala:1585
26/01/04 17:23:51 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 292 (MapPartitionsRDD[1462] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 17:23:51 INFO TaskSchedulerImpl: Adding task set 292.0 with 2 tasks resource profile 0
26/01/04 17:23:51 INFO TaskSetManager: Starting task 1.0 in stage 292.0 (TID 473) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 17:23:51 INFO TaskSetManager: Starting task 0.0 in stage 292.0 (TID 474) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 17:23:51 INFO BlockManagerInfo: Added broadcast_435_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:23:51 INFO BlockManagerInfo: Added broadcast_435_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:23:51 INFO TaskSetManager: Finished task 1.0 in stage 292.0 (TID 473) in 52 ms on 172.18.0.13 (executor 0) (1/2)
26/01/04 17:23:51 INFO TaskSetManager: Finished task 0.0 in stage 292.0 (TID 474) in 99 ms on 172.18.0.14 (executor 1) (2/2)
26/01/04 17:23:51 INFO TaskSchedulerImpl: Removed TaskSet 292.0, whose tasks have all completed, from pool 
26/01/04 17:23:51 INFO DAGScheduler: ResultStage 292 (start at NativeMethodAccessorImpl.java:0) finished in 0.116 s
26/01/04 17:23:51 INFO DAGScheduler: Job 291 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:23:51 INFO TaskSchedulerImpl: Killing all running tasks in stage 292: Stage finished
26/01/04 17:23:51 INFO DAGScheduler: Job 291 finished: start at NativeMethodAccessorImpl.java:0, took 0.124183 s
26/01/04 17:23:51 INFO MemoryStore: Block broadcast_436_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.2 MiB)
26/01/04 17:23:51 INFO BlockManagerInfo: Added broadcast_436_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:23:51 INFO SparkContext: Created broadcast 436 from start at NativeMethodAccessorImpl.java:0
26/01/04 17:23:51 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 144, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@123e6237]. The input RDD has 1 partitions.
26/01/04 17:23:51 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:23:51 INFO DAGScheduler: Got job 292 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/01/04 17:23:51 INFO DAGScheduler: Final stage: ResultStage 293 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:23:51 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:23:51 INFO DAGScheduler: Missing parents: List()
26/01/04 17:23:51 INFO DAGScheduler: Submitting ResultStage 293 (MapPartitionsRDD[1467] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:23:51 INFO MemoryStore: Block broadcast_437 stored as values in memory (estimated size 52.9 KiB, free 434.2 MiB)
26/01/04 17:23:51 INFO MemoryStore: Block broadcast_437_piece0 stored as bytes in memory (estimated size 20.2 KiB, free 434.1 MiB)
26/01/04 17:23:51 INFO BlockManagerInfo: Added broadcast_437_piece0 in memory on spark-master:33535 (size: 20.2 KiB, free: 434.3 MiB)
26/01/04 17:23:51 INFO SparkContext: Created broadcast 437 from broadcast at DAGScheduler.scala:1585
26/01/04 17:23:51 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 293 (MapPartitionsRDD[1467] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/01/04 17:23:51 INFO TaskSchedulerImpl: Adding task set 293.0 with 1 tasks resource profile 0
26/01/04 17:23:51 INFO TaskSetManager: Starting task 0.0 in stage 293.0 (TID 475) (172.18.0.13, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:23:51 INFO BlockManagerInfo: Added broadcast_437_piece0 in memory on 172.18.0.13:44401 (size: 20.2 KiB, free: 434.3 MiB)
26/01/04 17:23:51 INFO BlockManagerInfo: Added broadcast_436_piece0 in memory on 172.18.0.13:44401 (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:23:51 INFO TaskSetManager: Finished task 0.0 in stage 293.0 (TID 475) in 617 ms on 172.18.0.13 (executor 0) (1/1)
26/01/04 17:23:51 INFO TaskSchedulerImpl: Removed TaskSet 293.0, whose tasks have all completed, from pool 
26/01/04 17:23:51 INFO DAGScheduler: ResultStage 293 (start at NativeMethodAccessorImpl.java:0) finished in 0.629 s
26/01/04 17:23:51 INFO DAGScheduler: Job 292 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:23:51 INFO TaskSchedulerImpl: Killing all running tasks in stage 293: Stage finished
26/01/04 17:23:51 INFO DAGScheduler: Job 292 finished: start at NativeMethodAccessorImpl.java:0, took 0.635840 s
26/01/04 17:23:51 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 144, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@123e6237] is committing.
26/01/04 17:23:51 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 144, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@123e6237] committed.
26/01/04 17:23:51 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/144 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.144.5c325a35-9f5c-4e7b-a7cf-fe21440814d3.tmp
26/01/04 17:23:51 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.144.5c325a35-9f5c-4e7b-a7cf-fe21440814d3.tmp to file:/tmp/spark-checkpoint-enrichment/commits/144
26/01/04 17:23:51 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T17:23:50.611Z",
  "batchId" : 144,
  "numInputRows" : 30,
  "inputRowsPerSecond" : 2142.8571428571427,
  "processedRowsPerSecond" : 22.813688212927758,
  "durationMs" : {
    "addBatch" : 932,
    "commitOffsets" : 133,
    "getBatch" : 0,
    "latestOffset" : 3,
    "queryPlanning" : 84,
    "triggerExecution" : 1315,
    "walCommit" : 162
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "0" : 3579
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "0" : 3609
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "0" : 3609
      }
    },
    "numInputRows" : 30,
    "inputRowsPerSecond" : 2142.8571428571427,
    "processedRowsPerSecond" : 22.813688212927758,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 31
  }
}
26/01/04 17:23:56 INFO BlockManagerInfo: Removed broadcast_432_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:23:56 INFO BlockManagerInfo: Removed broadcast_432_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:23:56 INFO BlockManagerInfo: Removed broadcast_432_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:23:56 INFO BlockManagerInfo: Removed broadcast_435_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:23:56 INFO BlockManagerInfo: Removed broadcast_435_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:23:56 INFO BlockManagerInfo: Removed broadcast_435_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:23:56 INFO BlockManagerInfo: Removed broadcast_434_piece0 on spark-master:33535 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:23:56 INFO BlockManagerInfo: Removed broadcast_434_piece0 on 172.18.0.13:44401 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:23:56 INFO BlockManagerInfo: Removed broadcast_437_piece0 on spark-master:33535 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:23:56 INFO BlockManagerInfo: Removed broadcast_437_piece0 on 172.18.0.13:44401 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:23:56 INFO BlockManagerInfo: Removed broadcast_433_piece0 on spark-master:33535 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:23:56 INFO BlockManagerInfo: Removed broadcast_433_piece0 on 172.18.0.13:44401 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:24:01 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/145 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.145.57aab9c8-8607-4389-87ed-1b949239cf54.tmp
26/01/04 17:24:01 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.145.57aab9c8-8607-4389-87ed-1b949239cf54.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/145
26/01/04 17:24:01 INFO MicroBatchExecution: Committed offsets for batch 145. Metadata OffsetSeqMetadata(0,1767547441682,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 17:24:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:24:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:24:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:24:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:24:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:24:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:24:02 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#118063 - airline_prefix.nullCount#118062) > 0)
26/01/04 17:24:02 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#118098 - min_flight_num.nullCount#118097) > 0)
26/01/04 17:24:02 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#118093 - max_flight_num.nullCount#118092) > 0)
26/01/04 17:24:02 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:24:02 INFO DAGScheduler: Got job 293 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 17:24:02 INFO DAGScheduler: Final stage: ResultStage 294 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:24:02 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:24:02 INFO DAGScheduler: Missing parents: List()
26/01/04 17:24:02 INFO DAGScheduler: Submitting ResultStage 294 (MapPartitionsRDD[1472] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:24:02 INFO MemoryStore: Block broadcast_438 stored as values in memory (estimated size 38.5 KiB, free 434.4 MiB)
26/01/04 17:24:02 INFO MemoryStore: Block broadcast_438_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.3 MiB)
26/01/04 17:24:02 INFO BlockManagerInfo: Removed broadcast_436_piece0 on spark-master:33535 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:24:02 INFO BlockManagerInfo: Added broadcast_438_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:24:02 INFO BlockManagerInfo: Removed broadcast_436_piece0 on 172.18.0.13:44401 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:24:02 INFO SparkContext: Created broadcast 438 from broadcast at DAGScheduler.scala:1585
26/01/04 17:24:02 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 294 (MapPartitionsRDD[1472] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 17:24:02 INFO TaskSchedulerImpl: Adding task set 294.0 with 2 tasks resource profile 0
26/01/04 17:24:02 INFO TaskSetManager: Starting task 0.0 in stage 294.0 (TID 476) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 17:24:02 INFO TaskSetManager: Starting task 1.0 in stage 294.0 (TID 477) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 17:24:02 INFO BlockManagerInfo: Added broadcast_438_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:24:02 INFO BlockManagerInfo: Added broadcast_438_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:24:02 INFO TaskSetManager: Finished task 1.0 in stage 294.0 (TID 477) in 45 ms on 172.18.0.13 (executor 0) (1/2)
26/01/04 17:24:02 INFO TaskSetManager: Finished task 0.0 in stage 294.0 (TID 476) in 223 ms on 172.18.0.14 (executor 1) (2/2)
26/01/04 17:24:02 INFO TaskSchedulerImpl: Removed TaskSet 294.0, whose tasks have all completed, from pool 
26/01/04 17:24:02 INFO DAGScheduler: ResultStage 294 (start at NativeMethodAccessorImpl.java:0) finished in 0.259 s
26/01/04 17:24:02 INFO DAGScheduler: Job 293 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:24:02 INFO TaskSchedulerImpl: Killing all running tasks in stage 294: Stage finished
26/01/04 17:24:02 INFO DAGScheduler: Job 293 finished: start at NativeMethodAccessorImpl.java:0, took 0.264805 s
26/01/04 17:24:02 INFO MemoryStore: Block broadcast_439_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.3 MiB)
26/01/04 17:24:02 INFO BlockManagerInfo: Added broadcast_439_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:24:02 INFO SparkContext: Created broadcast 439 from start at NativeMethodAccessorImpl.java:0
26/01/04 17:24:02 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 145, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@65221f9e]. The input RDD has 1 partitions.
26/01/04 17:24:02 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:24:02 INFO DAGScheduler: Got job 294 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/01/04 17:24:02 INFO DAGScheduler: Final stage: ResultStage 295 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:24:02 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:24:02 INFO DAGScheduler: Missing parents: List()
26/01/04 17:24:02 INFO DAGScheduler: Submitting ResultStage 295 (MapPartitionsRDD[1477] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:24:02 INFO MemoryStore: Block broadcast_440 stored as values in memory (estimated size 52.9 KiB, free 434.3 MiB)
26/01/04 17:24:02 INFO MemoryStore: Block broadcast_440_piece0 stored as bytes in memory (estimated size 20.2 KiB, free 434.3 MiB)
26/01/04 17:24:02 INFO BlockManagerInfo: Added broadcast_440_piece0 in memory on spark-master:33535 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:24:02 INFO SparkContext: Created broadcast 440 from broadcast at DAGScheduler.scala:1585
26/01/04 17:24:02 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 295 (MapPartitionsRDD[1477] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/01/04 17:24:02 INFO TaskSchedulerImpl: Adding task set 295.0 with 1 tasks resource profile 0
26/01/04 17:24:02 INFO TaskSetManager: Starting task 0.0 in stage 295.0 (TID 478) (172.18.0.13, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:24:02 INFO BlockManagerInfo: Added broadcast_440_piece0 in memory on 172.18.0.13:44401 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:24:02 INFO BlockManagerInfo: Added broadcast_439_piece0 in memory on 172.18.0.13:44401 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:24:03 INFO TaskSetManager: Finished task 0.0 in stage 295.0 (TID 478) in 635 ms on 172.18.0.13 (executor 0) (1/1)
26/01/04 17:24:03 INFO TaskSchedulerImpl: Removed TaskSet 295.0, whose tasks have all completed, from pool 
26/01/04 17:24:03 INFO DAGScheduler: ResultStage 295 (start at NativeMethodAccessorImpl.java:0) finished in 0.661 s
26/01/04 17:24:03 INFO DAGScheduler: Job 294 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:24:03 INFO TaskSchedulerImpl: Killing all running tasks in stage 295: Stage finished
26/01/04 17:24:03 INFO DAGScheduler: Job 294 finished: start at NativeMethodAccessorImpl.java:0, took 0.670662 s
26/01/04 17:24:03 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 145, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@65221f9e] is committing.
26/01/04 17:24:03 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 145, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@65221f9e] committed.
26/01/04 17:24:03 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/145 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.145.d1ba5b14-989c-4d5f-ab0d-ec37c3b20a71.tmp
26/01/04 17:24:03 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.145.d1ba5b14-989c-4d5f-ab0d-ec37c3b20a71.tmp to file:/tmp/spark-checkpoint-enrichment/commits/145
26/01/04 17:24:03 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T17:24:01.647Z",
  "batchId" : 145,
  "numInputRows" : 30,
  "inputRowsPerSecond" : 1000.0,
  "processedRowsPerSecond" : 18.315018315018317,
  "durationMs" : {
    "addBatch" : 1099,
    "commitOffsets" : 137,
    "getBatch" : 0,
    "latestOffset" : 35,
    "queryPlanning" : 55,
    "triggerExecution" : 1638,
    "walCommit" : 310
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "0" : 3609
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "0" : 3639
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "0" : 3639
      }
    },
    "numInputRows" : 30,
    "inputRowsPerSecond" : 1000.0,
    "processedRowsPerSecond" : 18.315018315018317,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 31
  }
}
26/01/04 17:24:12 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/146 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.146.9aadd388-8f34-4669-97d9-547b00a2107d.tmp
26/01/04 17:24:12 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.146.9aadd388-8f34-4669-97d9-547b00a2107d.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/146
26/01/04 17:24:12 INFO MicroBatchExecution: Committed offsets for batch 146. Metadata OffsetSeqMetadata(0,1767547452669,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 17:24:12 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:24:12 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:24:12 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:24:12 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:24:12 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:24:12 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:24:12 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#118867 - airline_prefix.nullCount#118866) > 0)
26/01/04 17:24:12 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#118902 - min_flight_num.nullCount#118901) > 0)
26/01/04 17:24:12 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#118897 - max_flight_num.nullCount#118896) > 0)
26/01/04 17:24:12 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:24:12 INFO DAGScheduler: Got job 295 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 17:24:12 INFO DAGScheduler: Final stage: ResultStage 296 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:24:12 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:24:12 INFO DAGScheduler: Missing parents: List()
26/01/04 17:24:12 INFO DAGScheduler: Submitting ResultStage 296 (MapPartitionsRDD[1482] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:24:12 INFO MemoryStore: Block broadcast_441 stored as values in memory (estimated size 38.5 KiB, free 434.2 MiB)
26/01/04 17:24:13 INFO MemoryStore: Block broadcast_441_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.2 MiB)
26/01/04 17:24:13 INFO BlockManagerInfo: Added broadcast_441_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:24:13 INFO SparkContext: Created broadcast 441 from broadcast at DAGScheduler.scala:1585
26/01/04 17:24:13 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 296 (MapPartitionsRDD[1482] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 17:24:13 INFO TaskSchedulerImpl: Adding task set 296.0 with 2 tasks resource profile 0
26/01/04 17:24:13 INFO TaskSetManager: Starting task 1.0 in stage 296.0 (TID 479) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 17:24:13 INFO TaskSetManager: Starting task 0.0 in stage 296.0 (TID 480) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 17:24:13 INFO BlockManagerInfo: Added broadcast_441_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:24:13 INFO BlockManagerInfo: Added broadcast_441_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:24:13 INFO TaskSetManager: Finished task 1.0 in stage 296.0 (TID 479) in 54 ms on 172.18.0.13 (executor 0) (1/2)
26/01/04 17:24:13 INFO TaskSetManager: Finished task 0.0 in stage 296.0 (TID 480) in 75 ms on 172.18.0.14 (executor 1) (2/2)
26/01/04 17:24:13 INFO TaskSchedulerImpl: Removed TaskSet 296.0, whose tasks have all completed, from pool 
26/01/04 17:24:13 INFO DAGScheduler: ResultStage 296 (start at NativeMethodAccessorImpl.java:0) finished in 0.096 s
26/01/04 17:24:13 INFO DAGScheduler: Job 295 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:24:13 INFO TaskSchedulerImpl: Killing all running tasks in stage 296: Stage finished
26/01/04 17:24:13 INFO DAGScheduler: Job 295 finished: start at NativeMethodAccessorImpl.java:0, took 0.105842 s
26/01/04 17:24:13 INFO MemoryStore: Block broadcast_442_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.2 MiB)
26/01/04 17:24:13 INFO BlockManagerInfo: Added broadcast_442_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:24:13 INFO SparkContext: Created broadcast 442 from start at NativeMethodAccessorImpl.java:0
26/01/04 17:24:13 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 146, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@15801408]. The input RDD has 1 partitions.
26/01/04 17:24:13 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:24:13 INFO DAGScheduler: Got job 296 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/01/04 17:24:13 INFO DAGScheduler: Final stage: ResultStage 297 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:24:13 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:24:13 INFO DAGScheduler: Missing parents: List()
26/01/04 17:24:13 INFO DAGScheduler: Submitting ResultStage 297 (MapPartitionsRDD[1487] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:24:13 INFO MemoryStore: Block broadcast_443 stored as values in memory (estimated size 52.9 KiB, free 434.2 MiB)
26/01/04 17:24:13 INFO MemoryStore: Block broadcast_443_piece0 stored as bytes in memory (estimated size 20.2 KiB, free 434.1 MiB)
26/01/04 17:24:13 INFO BlockManagerInfo: Added broadcast_443_piece0 in memory on spark-master:33535 (size: 20.2 KiB, free: 434.3 MiB)
26/01/04 17:24:13 INFO SparkContext: Created broadcast 443 from broadcast at DAGScheduler.scala:1585
26/01/04 17:24:13 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 297 (MapPartitionsRDD[1487] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/01/04 17:24:13 INFO TaskSchedulerImpl: Adding task set 297.0 with 1 tasks resource profile 0
26/01/04 17:24:13 INFO TaskSetManager: Starting task 0.0 in stage 297.0 (TID 481) (172.18.0.13, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:24:13 INFO BlockManagerInfo: Added broadcast_443_piece0 in memory on 172.18.0.13:44401 (size: 20.2 KiB, free: 434.3 MiB)
26/01/04 17:24:13 INFO BlockManagerInfo: Added broadcast_442_piece0 in memory on 172.18.0.13:44401 (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:24:13 INFO TaskSetManager: Finished task 0.0 in stage 297.0 (TID 481) in 616 ms on 172.18.0.13 (executor 0) (1/1)
26/01/04 17:24:13 INFO TaskSchedulerImpl: Removed TaskSet 297.0, whose tasks have all completed, from pool 
26/01/04 17:24:13 INFO DAGScheduler: ResultStage 297 (start at NativeMethodAccessorImpl.java:0) finished in 0.629 s
26/01/04 17:24:13 INFO DAGScheduler: Job 296 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:24:13 INFO TaskSchedulerImpl: Killing all running tasks in stage 297: Stage finished
26/01/04 17:24:13 INFO DAGScheduler: Job 296 finished: start at NativeMethodAccessorImpl.java:0, took 0.636213 s
26/01/04 17:24:13 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 146, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@15801408] is committing.
26/01/04 17:24:13 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 146, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@15801408] committed.
26/01/04 17:24:13 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/146 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.146.fc70f627-b8ef-4331-9534-22bb62b55fdc.tmp
26/01/04 17:24:13 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.146.fc70f627-b8ef-4331-9534-22bb62b55fdc.tmp to file:/tmp/spark-checkpoint-enrichment/commits/146
26/01/04 17:24:13 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T17:24:12.664Z",
  "batchId" : 146,
  "numInputRows" : 30,
  "inputRowsPerSecond" : 2307.6923076923076,
  "processedRowsPerSecond" : 23.923444976076556,
  "durationMs" : {
    "addBatch" : 884,
    "commitOffsets" : 159,
    "getBatch" : 2,
    "latestOffset" : 5,
    "queryPlanning" : 50,
    "triggerExecution" : 1254,
    "walCommit" : 154
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "0" : 3639
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "0" : 3669
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "0" : 3669
      }
    },
    "numInputRows" : 30,
    "inputRowsPerSecond" : 2307.6923076923076,
    "processedRowsPerSecond" : 23.923444976076556,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 31
  }
}
26/01/04 17:24:19 INFO BlockManagerInfo: Removed broadcast_440_piece0 on spark-master:33535 in memory (size: 20.2 KiB, free: 434.3 MiB)
26/01/04 17:24:19 INFO BlockManagerInfo: Removed broadcast_440_piece0 on 172.18.0.13:44401 in memory (size: 20.2 KiB, free: 434.3 MiB)
26/01/04 17:24:19 INFO BlockManagerInfo: Removed broadcast_439_piece0 on spark-master:33535 in memory (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:24:19 INFO BlockManagerInfo: Removed broadcast_439_piece0 on 172.18.0.13:44401 in memory (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:24:19 INFO BlockManagerInfo: Removed broadcast_441_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:24:19 INFO BlockManagerInfo: Removed broadcast_441_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:24:19 INFO BlockManagerInfo: Removed broadcast_441_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:24:19 INFO BlockManagerInfo: Removed broadcast_438_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:24:19 INFO BlockManagerInfo: Removed broadcast_438_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:24:19 INFO BlockManagerInfo: Removed broadcast_438_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:24:19 INFO BlockManagerInfo: Removed broadcast_443_piece0 on spark-master:33535 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:24:19 INFO BlockManagerInfo: Removed broadcast_443_piece0 on 172.18.0.13:44401 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:24:23 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/147 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.147.eb607879-d994-4abf-b78e-8647215b76f4.tmp
26/01/04 17:24:23 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.147.eb607879-d994-4abf-b78e-8647215b76f4.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/147
26/01/04 17:24:23 INFO MicroBatchExecution: Committed offsets for batch 147. Metadata OffsetSeqMetadata(0,1767547463682,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 17:24:23 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:24:23 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:24:23 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:24:23 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:24:23 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:24:23 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:24:23 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#119671 - airline_prefix.nullCount#119670) > 0)
26/01/04 17:24:23 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#119706 - min_flight_num.nullCount#119705) > 0)
26/01/04 17:24:23 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#119701 - max_flight_num.nullCount#119700) > 0)
26/01/04 17:24:23 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:24:23 INFO DAGScheduler: Got job 297 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 17:24:23 INFO DAGScheduler: Final stage: ResultStage 298 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:24:23 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:24:23 INFO DAGScheduler: Missing parents: List()
26/01/04 17:24:23 INFO DAGScheduler: Submitting ResultStage 298 (MapPartitionsRDD[1492] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:24:23 INFO MemoryStore: Block broadcast_444 stored as values in memory (estimated size 38.5 KiB, free 434.4 MiB)
26/01/04 17:24:24 INFO MemoryStore: Block broadcast_444_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.3 MiB)
26/01/04 17:24:24 INFO BlockManagerInfo: Removed broadcast_442_piece0 on spark-master:33535 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:24:24 INFO BlockManagerInfo: Added broadcast_444_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:24:24 INFO SparkContext: Created broadcast 444 from broadcast at DAGScheduler.scala:1585
26/01/04 17:24:24 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 298 (MapPartitionsRDD[1492] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 17:24:24 INFO TaskSchedulerImpl: Adding task set 298.0 with 2 tasks resource profile 0
26/01/04 17:24:24 INFO TaskSetManager: Starting task 1.0 in stage 298.0 (TID 482) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 17:24:24 INFO TaskSetManager: Starting task 0.0 in stage 298.0 (TID 483) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 17:24:24 INFO BlockManagerInfo: Removed broadcast_442_piece0 on 172.18.0.13:44401 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:24:24 INFO BlockManagerInfo: Added broadcast_444_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:24:24 INFO BlockManagerInfo: Added broadcast_444_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:24:24 INFO TaskSetManager: Finished task 1.0 in stage 298.0 (TID 482) in 52 ms on 172.18.0.13 (executor 0) (1/2)
26/01/04 17:24:24 INFO TaskSetManager: Finished task 0.0 in stage 298.0 (TID 483) in 112 ms on 172.18.0.14 (executor 1) (2/2)
26/01/04 17:24:24 INFO TaskSchedulerImpl: Removed TaskSet 298.0, whose tasks have all completed, from pool 
26/01/04 17:24:24 INFO DAGScheduler: ResultStage 298 (start at NativeMethodAccessorImpl.java:0) finished in 0.148 s
26/01/04 17:24:24 INFO DAGScheduler: Job 297 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:24:24 INFO TaskSchedulerImpl: Killing all running tasks in stage 298: Stage finished
26/01/04 17:24:24 INFO DAGScheduler: Job 297 finished: start at NativeMethodAccessorImpl.java:0, took 0.152008 s
26/01/04 17:24:24 INFO MemoryStore: Block broadcast_445_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.3 MiB)
26/01/04 17:24:24 INFO BlockManagerInfo: Added broadcast_445_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:24:24 INFO SparkContext: Created broadcast 445 from start at NativeMethodAccessorImpl.java:0
26/01/04 17:24:24 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 147, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@4938a93d]. The input RDD has 1 partitions.
26/01/04 17:24:24 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:24:24 INFO DAGScheduler: Got job 298 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/01/04 17:24:24 INFO DAGScheduler: Final stage: ResultStage 299 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:24:24 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:24:24 INFO DAGScheduler: Missing parents: List()
26/01/04 17:24:24 INFO DAGScheduler: Submitting ResultStage 299 (MapPartitionsRDD[1497] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:24:24 INFO MemoryStore: Block broadcast_446 stored as values in memory (estimated size 52.9 KiB, free 434.3 MiB)
26/01/04 17:24:24 INFO MemoryStore: Block broadcast_446_piece0 stored as bytes in memory (estimated size 20.2 KiB, free 434.3 MiB)
26/01/04 17:24:24 INFO BlockManagerInfo: Added broadcast_446_piece0 in memory on spark-master:33535 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:24:24 INFO SparkContext: Created broadcast 446 from broadcast at DAGScheduler.scala:1585
26/01/04 17:24:24 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 299 (MapPartitionsRDD[1497] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/01/04 17:24:24 INFO TaskSchedulerImpl: Adding task set 299.0 with 1 tasks resource profile 0
26/01/04 17:24:24 INFO TaskSetManager: Starting task 0.0 in stage 299.0 (TID 484) (172.18.0.13, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:24:24 INFO BlockManagerInfo: Added broadcast_446_piece0 in memory on 172.18.0.13:44401 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:24:24 INFO BlockManagerInfo: Added broadcast_445_piece0 in memory on 172.18.0.13:44401 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:24:24 INFO TaskSetManager: Finished task 0.0 in stage 299.0 (TID 484) in 756 ms on 172.18.0.13 (executor 0) (1/1)
26/01/04 17:24:24 INFO TaskSchedulerImpl: Removed TaskSet 299.0, whose tasks have all completed, from pool 
26/01/04 17:24:24 INFO DAGScheduler: ResultStage 299 (start at NativeMethodAccessorImpl.java:0) finished in 0.767 s
26/01/04 17:24:24 INFO DAGScheduler: Job 298 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:24:24 INFO TaskSchedulerImpl: Killing all running tasks in stage 299: Stage finished
26/01/04 17:24:24 INFO DAGScheduler: Job 298 finished: start at NativeMethodAccessorImpl.java:0, took 0.772228 s
26/01/04 17:24:24 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 147, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@4938a93d] is committing.
26/01/04 17:24:24 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 147, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@4938a93d] committed.
26/01/04 17:24:24 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/147 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.147.2bae5397-56cc-4e9d-a482-e864aa760075.tmp
26/01/04 17:24:25 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.147.2bae5397-56cc-4e9d-a482-e864aa760075.tmp to file:/tmp/spark-checkpoint-enrichment/commits/147
26/01/04 17:24:25 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T17:24:23.680Z",
  "batchId" : 147,
  "numInputRows" : 30,
  "inputRowsPerSecond" : 2307.6923076923076,
  "processedRowsPerSecond" : 21.50537634408602,
  "durationMs" : {
    "addBatch" : 1049,
    "commitOffsets" : 141,
    "getBatch" : 1,
    "latestOffset" : 2,
    "queryPlanning" : 50,
    "triggerExecution" : 1395,
    "walCommit" : 151
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "0" : 3669
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "0" : 3699
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "0" : 3699
      }
    },
    "numInputRows" : 30,
    "inputRowsPerSecond" : 2307.6923076923076,
    "processedRowsPerSecond" : 21.50537634408602,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 31
  }
}
26/01/04 17:24:34 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/148 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.148.332417a5-dd8c-440a-b57a-adff527ca0fb.tmp
26/01/04 17:24:34 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.148.332417a5-dd8c-440a-b57a-adff527ca0fb.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/148
26/01/04 17:24:34 INFO MicroBatchExecution: Committed offsets for batch 148. Metadata OffsetSeqMetadata(0,1767547474747,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 17:24:35 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:24:35 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:24:35 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:24:35 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:24:35 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:24:35 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:24:35 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#120475 - airline_prefix.nullCount#120474) > 0)
26/01/04 17:24:35 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#120510 - min_flight_num.nullCount#120509) > 0)
26/01/04 17:24:35 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#120505 - max_flight_num.nullCount#120504) > 0)
26/01/04 17:24:35 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:24:35 INFO DAGScheduler: Got job 299 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 17:24:35 INFO DAGScheduler: Final stage: ResultStage 300 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:24:35 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:24:35 INFO DAGScheduler: Missing parents: List()
26/01/04 17:24:35 INFO DAGScheduler: Submitting ResultStage 300 (MapPartitionsRDD[1502] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:24:35 INFO MemoryStore: Block broadcast_447 stored as values in memory (estimated size 38.5 KiB, free 434.2 MiB)
26/01/04 17:24:35 INFO MemoryStore: Block broadcast_447_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.2 MiB)
26/01/04 17:24:35 INFO BlockManagerInfo: Added broadcast_447_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:24:35 INFO SparkContext: Created broadcast 447 from broadcast at DAGScheduler.scala:1585
26/01/04 17:24:35 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 300 (MapPartitionsRDD[1502] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 17:24:35 INFO TaskSchedulerImpl: Adding task set 300.0 with 2 tasks resource profile 0
26/01/04 17:24:35 INFO TaskSetManager: Starting task 1.0 in stage 300.0 (TID 485) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 17:24:35 INFO TaskSetManager: Starting task 0.0 in stage 300.0 (TID 486) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 17:24:35 INFO BlockManagerInfo: Added broadcast_447_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:24:35 INFO BlockManagerInfo: Added broadcast_447_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:24:35 INFO TaskSetManager: Finished task 1.0 in stage 300.0 (TID 485) in 64 ms on 172.18.0.13 (executor 0) (1/2)
26/01/04 17:24:35 INFO TaskSetManager: Finished task 0.0 in stage 300.0 (TID 486) in 263 ms on 172.18.0.14 (executor 1) (2/2)
26/01/04 17:24:35 INFO TaskSchedulerImpl: Removed TaskSet 300.0, whose tasks have all completed, from pool 
26/01/04 17:24:35 INFO DAGScheduler: ResultStage 300 (start at NativeMethodAccessorImpl.java:0) finished in 0.282 s
26/01/04 17:24:35 INFO DAGScheduler: Job 299 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:24:35 INFO TaskSchedulerImpl: Killing all running tasks in stage 300: Stage finished
26/01/04 17:24:35 INFO DAGScheduler: Job 299 finished: start at NativeMethodAccessorImpl.java:0, took 0.293572 s
26/01/04 17:24:35 INFO MemoryStore: Block broadcast_448_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.2 MiB)
26/01/04 17:24:35 INFO BlockManagerInfo: Added broadcast_448_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:24:35 INFO SparkContext: Created broadcast 448 from start at NativeMethodAccessorImpl.java:0
26/01/04 17:24:35 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 148, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@6f7642da]. The input RDD has 1 partitions.
26/01/04 17:24:35 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:24:35 INFO DAGScheduler: Got job 300 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/01/04 17:24:35 INFO DAGScheduler: Final stage: ResultStage 301 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:24:35 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:24:35 INFO DAGScheduler: Missing parents: List()
26/01/04 17:24:35 INFO DAGScheduler: Submitting ResultStage 301 (MapPartitionsRDD[1507] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:24:35 INFO MemoryStore: Block broadcast_449 stored as values in memory (estimated size 52.9 KiB, free 434.2 MiB)
26/01/04 17:24:35 INFO MemoryStore: Block broadcast_449_piece0 stored as bytes in memory (estimated size 20.2 KiB, free 434.1 MiB)
26/01/04 17:24:35 INFO BlockManagerInfo: Added broadcast_449_piece0 in memory on spark-master:33535 (size: 20.2 KiB, free: 434.3 MiB)
26/01/04 17:24:35 INFO SparkContext: Created broadcast 449 from broadcast at DAGScheduler.scala:1585
26/01/04 17:24:35 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 301 (MapPartitionsRDD[1507] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/01/04 17:24:35 INFO TaskSchedulerImpl: Adding task set 301.0 with 1 tasks resource profile 0
26/01/04 17:24:35 INFO TaskSetManager: Starting task 0.0 in stage 301.0 (TID 487) (172.18.0.13, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:24:35 INFO BlockManagerInfo: Added broadcast_449_piece0 in memory on 172.18.0.13:44401 (size: 20.2 KiB, free: 434.3 MiB)
26/01/04 17:24:35 INFO BlockManagerInfo: Added broadcast_448_piece0 in memory on 172.18.0.13:44401 (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:24:36 INFO TaskSetManager: Finished task 0.0 in stage 301.0 (TID 487) in 723 ms on 172.18.0.13 (executor 0) (1/1)
26/01/04 17:24:36 INFO TaskSchedulerImpl: Removed TaskSet 301.0, whose tasks have all completed, from pool 
26/01/04 17:24:36 INFO DAGScheduler: ResultStage 301 (start at NativeMethodAccessorImpl.java:0) finished in 0.735 s
26/01/04 17:24:36 INFO DAGScheduler: Job 300 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:24:36 INFO TaskSchedulerImpl: Killing all running tasks in stage 301: Stage finished
26/01/04 17:24:36 INFO DAGScheduler: Job 300 finished: start at NativeMethodAccessorImpl.java:0, took 0.737904 s
26/01/04 17:24:36 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 148, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@6f7642da] is committing.
26/01/04 17:24:36 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 148, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@6f7642da] committed.
26/01/04 17:24:36 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/148 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.148.fb052b34-0c72-405a-bb43-255a8baa675d.tmp
26/01/04 17:24:36 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.148.fb052b34-0c72-405a-bb43-255a8baa675d.tmp to file:/tmp/spark-checkpoint-enrichment/commits/148
26/01/04 17:24:36 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T17:24:34.744Z",
  "batchId" : 148,
  "numInputRows" : 30,
  "inputRowsPerSecond" : 357.1428571428571,
  "processedRowsPerSecond" : 18.404907975460123,
  "durationMs" : {
    "addBatch" : 1198,
    "commitOffsets" : 131,
    "getBatch" : 0,
    "latestOffset" : 3,
    "queryPlanning" : 53,
    "triggerExecution" : 1630,
    "walCommit" : 242
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "0" : 3699
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "0" : 3729
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "0" : 3729
      }
    },
    "numInputRows" : 30,
    "inputRowsPerSecond" : 357.1428571428571,
    "processedRowsPerSecond" : 18.404907975460123,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 31
  }
}
26/01/04 17:24:43 INFO BlockManagerInfo: Removed broadcast_444_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:24:43 INFO BlockManagerInfo: Removed broadcast_444_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:24:43 INFO BlockManagerInfo: Removed broadcast_444_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:24:43 INFO BlockManagerInfo: Removed broadcast_445_piece0 on 172.18.0.13:44401 in memory (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:24:43 INFO BlockManagerInfo: Removed broadcast_445_piece0 on spark-master:33535 in memory (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:24:43 INFO BlockManagerInfo: Removed broadcast_447_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:24:43 INFO BlockManagerInfo: Removed broadcast_447_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:24:43 INFO BlockManagerInfo: Removed broadcast_447_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:24:43 INFO BlockManagerInfo: Removed broadcast_446_piece0 on spark-master:33535 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:24:43 INFO BlockManagerInfo: Removed broadcast_446_piece0 on 172.18.0.13:44401 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:24:43 INFO BlockManagerInfo: Removed broadcast_449_piece0 on 172.18.0.13:44401 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:24:43 INFO BlockManagerInfo: Removed broadcast_449_piece0 on spark-master:33535 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:24:45 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/149 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.149.2419a24f-738b-4911-a6dc-8e5d821cc607.tmp
26/01/04 17:24:45 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.149.2419a24f-738b-4911-a6dc-8e5d821cc607.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/149
26/01/04 17:24:45 INFO MicroBatchExecution: Committed offsets for batch 149. Metadata OffsetSeqMetadata(0,1767547485763,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 17:24:45 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:24:45 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:24:46 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:24:46 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:24:46 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:24:46 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:24:46 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#121279 - airline_prefix.nullCount#121278) > 0)
26/01/04 17:24:46 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#121314 - min_flight_num.nullCount#121313) > 0)
26/01/04 17:24:46 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#121309 - max_flight_num.nullCount#121308) > 0)
26/01/04 17:24:46 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:24:46 INFO DAGScheduler: Got job 301 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 17:24:46 INFO DAGScheduler: Final stage: ResultStage 302 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:24:46 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:24:46 INFO DAGScheduler: Missing parents: List()
26/01/04 17:24:46 INFO DAGScheduler: Submitting ResultStage 302 (MapPartitionsRDD[1512] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:24:46 INFO MemoryStore: Block broadcast_450 stored as values in memory (estimated size 38.5 KiB, free 434.4 MiB)
26/01/04 17:24:46 INFO MemoryStore: Block broadcast_450_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.3 MiB)
26/01/04 17:24:46 INFO BlockManagerInfo: Removed broadcast_448_piece0 on spark-master:33535 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:24:46 INFO BlockManagerInfo: Added broadcast_450_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:24:46 INFO SparkContext: Created broadcast 450 from broadcast at DAGScheduler.scala:1585
26/01/04 17:24:46 INFO BlockManagerInfo: Removed broadcast_448_piece0 on 172.18.0.13:44401 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:24:46 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 302 (MapPartitionsRDD[1512] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 17:24:46 INFO TaskSchedulerImpl: Adding task set 302.0 with 2 tasks resource profile 0
26/01/04 17:24:46 INFO TaskSetManager: Starting task 0.0 in stage 302.0 (TID 488) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 17:24:46 INFO TaskSetManager: Starting task 1.0 in stage 302.0 (TID 489) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 17:24:46 INFO BlockManagerInfo: Added broadcast_450_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:24:46 INFO BlockManagerInfo: Added broadcast_450_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:24:46 INFO TaskSetManager: Finished task 1.0 in stage 302.0 (TID 489) in 63 ms on 172.18.0.13 (executor 0) (1/2)
26/01/04 17:24:46 INFO TaskSetManager: Finished task 0.0 in stage 302.0 (TID 488) in 79 ms on 172.18.0.14 (executor 1) (2/2)
26/01/04 17:24:46 INFO TaskSchedulerImpl: Removed TaskSet 302.0, whose tasks have all completed, from pool 
26/01/04 17:24:46 INFO DAGScheduler: ResultStage 302 (start at NativeMethodAccessorImpl.java:0) finished in 0.108 s
26/01/04 17:24:46 INFO DAGScheduler: Job 301 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:24:46 INFO TaskSchedulerImpl: Killing all running tasks in stage 302: Stage finished
26/01/04 17:24:46 INFO DAGScheduler: Job 301 finished: start at NativeMethodAccessorImpl.java:0, took 0.113291 s
26/01/04 17:24:46 INFO MemoryStore: Block broadcast_451_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.3 MiB)
26/01/04 17:24:46 INFO BlockManagerInfo: Added broadcast_451_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:24:46 INFO SparkContext: Created broadcast 451 from start at NativeMethodAccessorImpl.java:0
26/01/04 17:24:46 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 149, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@6dbc03ca]. The input RDD has 1 partitions.
26/01/04 17:24:46 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:24:46 INFO DAGScheduler: Got job 302 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/01/04 17:24:46 INFO DAGScheduler: Final stage: ResultStage 303 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:24:46 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:24:46 INFO DAGScheduler: Missing parents: List()
26/01/04 17:24:46 INFO DAGScheduler: Submitting ResultStage 303 (MapPartitionsRDD[1517] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:24:46 INFO MemoryStore: Block broadcast_452 stored as values in memory (estimated size 52.9 KiB, free 434.3 MiB)
26/01/04 17:24:46 INFO MemoryStore: Block broadcast_452_piece0 stored as bytes in memory (estimated size 20.2 KiB, free 434.3 MiB)
26/01/04 17:24:46 INFO BlockManagerInfo: Added broadcast_452_piece0 in memory on spark-master:33535 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:24:46 INFO SparkContext: Created broadcast 452 from broadcast at DAGScheduler.scala:1585
26/01/04 17:24:46 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 303 (MapPartitionsRDD[1517] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/01/04 17:24:46 INFO TaskSchedulerImpl: Adding task set 303.0 with 1 tasks resource profile 0
26/01/04 17:24:46 INFO TaskSetManager: Starting task 0.0 in stage 303.0 (TID 490) (172.18.0.13, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:24:46 INFO BlockManagerInfo: Added broadcast_452_piece0 in memory on 172.18.0.13:44401 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:24:46 INFO BlockManagerInfo: Added broadcast_451_piece0 in memory on 172.18.0.13:44401 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:24:46 INFO TaskSetManager: Finished task 0.0 in stage 303.0 (TID 490) in 617 ms on 172.18.0.13 (executor 0) (1/1)
26/01/04 17:24:46 INFO TaskSchedulerImpl: Removed TaskSet 303.0, whose tasks have all completed, from pool 
26/01/04 17:24:46 INFO DAGScheduler: ResultStage 303 (start at NativeMethodAccessorImpl.java:0) finished in 0.634 s
26/01/04 17:24:46 INFO DAGScheduler: Job 302 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:24:46 INFO TaskSchedulerImpl: Killing all running tasks in stage 303: Stage finished
26/01/04 17:24:46 INFO DAGScheduler: Job 302 finished: start at NativeMethodAccessorImpl.java:0, took 0.638415 s
26/01/04 17:24:46 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 149, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@6dbc03ca] is committing.
26/01/04 17:24:46 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 149, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@6dbc03ca] committed.
26/01/04 17:24:46 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/149 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.149.621ca628-f620-40ec-a80e-86a0caa43575.tmp
26/01/04 17:24:47 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.149.621ca628-f620-40ec-a80e-86a0caa43575.tmp to file:/tmp/spark-checkpoint-enrichment/commits/149
26/01/04 17:24:47 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T17:24:45.752Z",
  "batchId" : 149,
  "numInputRows" : 30,
  "inputRowsPerSecond" : 2307.6923076923076,
  "processedRowsPerSecond" : 21.536252692031585,
  "durationMs" : {
    "addBatch" : 969,
    "commitOffsets" : 196,
    "getBatch" : 0,
    "latestOffset" : 11,
    "queryPlanning" : 59,
    "triggerExecution" : 1393,
    "walCommit" : 156
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "0" : 3729
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "0" : 3759
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "0" : 3759
      }
    },
    "numInputRows" : 30,
    "inputRowsPerSecond" : 2307.6923076923076,
    "processedRowsPerSecond" : 21.536252692031585,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 31
  }
}
26/01/04 17:24:54 WARN KafkaOffsetReaderAdmin: Error in attempt 1 getting Kafka offsets: 
java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
	at java.base/java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:396)
	at java.base/java.util.concurrent.CompletableFuture.get(CompletableFuture.java:2073)
	at org.apache.kafka.common.internals.KafkaFutureImpl.get(KafkaFutureImpl.java:165)
	at org.apache.spark.sql.kafka010.ConsumerStrategy.retrieveAllPartitions(ConsumerStrategy.scala:66)
	at org.apache.spark.sql.kafka010.ConsumerStrategy.retrieveAllPartitions$(ConsumerStrategy.scala:65)
	at org.apache.spark.sql.kafka010.SubscribeStrategy.retrieveAllPartitions(ConsumerStrategy.scala:102)
	at org.apache.spark.sql.kafka010.SubscribeStrategy.assignedTopicPartitions(ConsumerStrategy.scala:113)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.$anonfun$partitionsAssignedToAdmin$1(KafkaOffsetReaderAdmin.scala:499)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.withRetries(KafkaOffsetReaderAdmin.scala:518)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.partitionsAssignedToAdmin(KafkaOffsetReaderAdmin.scala:498)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.fetchLatestOffsets(KafkaOffsetReaderAdmin.scala:297)
	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.latestOffset(KafkaMicroBatchStream.scala:130)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at scala.collection.TraversableLike.map(TraversableLike.scala:286)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:279)
	at scala.collection.AbstractTraversable.map(Traversable.scala:108)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)
	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)
	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)
Caused by: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
26/01/04 17:24:55 INFO AppInfoParser: App info kafka.admin.client for adminclient-2 unregistered
26/01/04 17:24:55 INFO Metrics: Metrics scheduler closed
26/01/04 17:24:55 INFO Metrics: Closing reporter org.apache.kafka.common.metrics.JmxReporter
26/01/04 17:24:55 INFO Metrics: Metrics reporters closed
26/01/04 17:24:55 INFO AdminClientConfig: AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.servers = [kafka:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

26/01/04 17:24:55 INFO AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
26/01/04 17:24:55 INFO AppInfoParser: Kafka version: 3.5.1
26/01/04 17:24:55 INFO AppInfoParser: Kafka commitId: 2c6fb6c54472e90a
26/01/04 17:24:55 INFO AppInfoParser: Kafka startTimeMs: 1767547495741
26/01/04 17:24:55 WARN KafkaOffsetReaderAdmin: Error in attempt 2 getting Kafka offsets: 
java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
	at java.base/java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:396)
	at java.base/java.util.concurrent.CompletableFuture.get(CompletableFuture.java:2073)
	at org.apache.kafka.common.internals.KafkaFutureImpl.get(KafkaFutureImpl.java:165)
	at org.apache.spark.sql.kafka010.ConsumerStrategy.retrieveAllPartitions(ConsumerStrategy.scala:66)
	at org.apache.spark.sql.kafka010.ConsumerStrategy.retrieveAllPartitions$(ConsumerStrategy.scala:65)
	at org.apache.spark.sql.kafka010.SubscribeStrategy.retrieveAllPartitions(ConsumerStrategy.scala:102)
	at org.apache.spark.sql.kafka010.SubscribeStrategy.assignedTopicPartitions(ConsumerStrategy.scala:113)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.$anonfun$partitionsAssignedToAdmin$1(KafkaOffsetReaderAdmin.scala:499)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.withRetries(KafkaOffsetReaderAdmin.scala:518)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.partitionsAssignedToAdmin(KafkaOffsetReaderAdmin.scala:498)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.fetchLatestOffsets(KafkaOffsetReaderAdmin.scala:297)
	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.latestOffset(KafkaMicroBatchStream.scala:130)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at scala.collection.TraversableLike.map(TraversableLike.scala:286)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:279)
	at scala.collection.AbstractTraversable.map(Traversable.scala:108)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)
	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)
	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)
Caused by: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
26/01/04 17:24:56 INFO AppInfoParser: App info kafka.admin.client for adminclient-3 unregistered
26/01/04 17:24:56 INFO Metrics: Metrics scheduler closed
26/01/04 17:24:56 INFO Metrics: Closing reporter org.apache.kafka.common.metrics.JmxReporter
26/01/04 17:24:56 INFO Metrics: Metrics reporters closed
26/01/04 17:24:56 INFO AdminClientConfig: AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.servers = [kafka:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

26/01/04 17:24:56 INFO AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
26/01/04 17:24:56 INFO AppInfoParser: Kafka version: 3.5.1
26/01/04 17:24:56 INFO AppInfoParser: Kafka commitId: 2c6fb6c54472e90a
26/01/04 17:24:56 INFO AppInfoParser: Kafka startTimeMs: 1767547496836
26/01/04 17:24:56 WARN KafkaOffsetReaderAdmin: Found incorrect offsets in some partitions (partition, previous offset, fetched offset): ArrayBuffer((aviation-india-states-0,3759,30))
26/01/04 17:24:56 WARN KafkaOffsetReaderAdmin: Retrying to fetch latest offsets because of incorrect offsets
26/01/04 17:24:57 WARN KafkaOffsetReaderAdmin: Found incorrect offsets in some partitions (partition, previous offset, fetched offset): ArrayBuffer((aviation-india-states-0,3759,30))
26/01/04 17:24:57 WARN KafkaOffsetReaderAdmin: Retrying to fetch latest offsets because of incorrect offsets
26/01/04 17:24:58 WARN KafkaOffsetReaderAdmin: Found incorrect offsets in some partitions (partition, previous offset, fetched offset): ArrayBuffer((aviation-india-states-0,3759,30))
26/01/04 17:24:58 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/150 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.150.0886609a-b2cb-4553-bbd2-dd373099e111.tmp
26/01/04 17:24:59 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.150.0886609a-b2cb-4553-bbd2-dd373099e111.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/150
26/01/04 17:24:59 INFO MicroBatchExecution: Committed offsets for batch 150. Metadata OffsetSeqMetadata(0,1767547498933,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 17:24:59 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:24:59 WARN KafkaMicroBatchStream: Partition aviation-india-states-0's offset was changed from 3759 to 30, some data may have been missed. 
Some data may have been lost because they are not available in Kafka any more; either the
 data was aged out by Kafka or the topic may have been deleted before all the data in the
 topic was processed. If you want your streaming query to fail on such cases, set the source
 option "failOnDataLoss" to "true".
    
26/01/04 17:24:59 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:24:59 WARN KafkaMicroBatchStream: Partition aviation-india-states-0's offset was changed from 3759 to 30, some data may have been missed. 
Some data may have been lost because they are not available in Kafka any more; either the
 data was aged out by Kafka or the topic may have been deleted before all the data in the
 topic was processed. If you want your streaming query to fail on such cases, set the source
 option "failOnDataLoss" to "true".
    
26/01/04 17:24:59 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:24:59 WARN KafkaMicroBatchStream: Partition aviation-india-states-0's offset was changed from 3759 to 30, some data may have been missed. 
Some data may have been lost because they are not available in Kafka any more; either the
 data was aged out by Kafka or the topic may have been deleted before all the data in the
 topic was processed. If you want your streaming query to fail on such cases, set the source
 option "failOnDataLoss" to "true".
    
26/01/04 17:24:59 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:24:59 WARN KafkaMicroBatchStream: Partition aviation-india-states-0's offset was changed from 3759 to 30, some data may have been missed. 
Some data may have been lost because they are not available in Kafka any more; either the
 data was aged out by Kafka or the topic may have been deleted before all the data in the
 topic was processed. If you want your streaming query to fail on such cases, set the source
 option "failOnDataLoss" to "true".
    
26/01/04 17:24:59 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:24:59 WARN KafkaMicroBatchStream: Partition aviation-india-states-0's offset was changed from 3759 to 30, some data may have been missed. 
Some data may have been lost because they are not available in Kafka any more; either the
 data was aged out by Kafka or the topic may have been deleted before all the data in the
 topic was processed. If you want your streaming query to fail on such cases, set the source
 option "failOnDataLoss" to "true".
    
26/01/04 17:24:59 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:24:59 WARN KafkaMicroBatchStream: Partition aviation-india-states-0's offset was changed from 3759 to 30, some data may have been missed. 
Some data may have been lost because they are not available in Kafka any more; either the
 data was aged out by Kafka or the topic may have been deleted before all the data in the
 topic was processed. If you want your streaming query to fail on such cases, set the source
 option "failOnDataLoss" to "true".
    
26/01/04 17:24:59 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#122083 - airline_prefix.nullCount#122082) > 0)
26/01/04 17:24:59 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#122118 - min_flight_num.nullCount#122117) > 0)
26/01/04 17:24:59 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#122113 - max_flight_num.nullCount#122112) > 0)
26/01/04 17:24:59 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:24:59 INFO DAGScheduler: Got job 303 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 17:24:59 INFO DAGScheduler: Final stage: ResultStage 304 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:24:59 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:24:59 INFO DAGScheduler: Missing parents: List()
26/01/04 17:24:59 INFO DAGScheduler: Submitting ResultStage 304 (MapPartitionsRDD[1522] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:24:59 INFO MemoryStore: Block broadcast_453 stored as values in memory (estimated size 38.5 KiB, free 434.2 MiB)
26/01/04 17:24:59 INFO MemoryStore: Block broadcast_453_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.2 MiB)
26/01/04 17:24:59 INFO BlockManagerInfo: Added broadcast_453_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:24:59 INFO SparkContext: Created broadcast 453 from broadcast at DAGScheduler.scala:1585
26/01/04 17:24:59 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 304 (MapPartitionsRDD[1522] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 17:24:59 INFO TaskSchedulerImpl: Adding task set 304.0 with 2 tasks resource profile 0
26/01/04 17:24:59 INFO TaskSetManager: Starting task 0.0 in stage 304.0 (TID 491) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 17:24:59 INFO TaskSetManager: Starting task 1.0 in stage 304.0 (TID 492) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 17:24:59 INFO BlockManagerInfo: Added broadcast_453_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:24:59 INFO BlockManagerInfo: Added broadcast_453_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:24:59 INFO TaskSetManager: Finished task 1.0 in stage 304.0 (TID 492) in 154 ms on 172.18.0.13 (executor 0) (1/2)
26/01/04 17:25:00 INFO TaskSetManager: Finished task 0.0 in stage 304.0 (TID 491) in 204 ms on 172.18.0.14 (executor 1) (2/2)
26/01/04 17:25:00 INFO TaskSchedulerImpl: Removed TaskSet 304.0, whose tasks have all completed, from pool 
26/01/04 17:25:00 INFO DAGScheduler: ResultStage 304 (start at NativeMethodAccessorImpl.java:0) finished in 0.259 s
26/01/04 17:25:00 INFO DAGScheduler: Job 303 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:25:00 INFO TaskSchedulerImpl: Killing all running tasks in stage 304: Stage finished
26/01/04 17:25:00 INFO DAGScheduler: Job 303 finished: start at NativeMethodAccessorImpl.java:0, took 0.273072 s
26/01/04 17:25:00 INFO MemoryStore: Block broadcast_454_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.2 MiB)
26/01/04 17:25:00 INFO BlockManagerInfo: Added broadcast_454_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:25:00 INFO SparkContext: Created broadcast 454 from start at NativeMethodAccessorImpl.java:0
26/01/04 17:25:00 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 150, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@7cec6451]. The input RDD has 1 partitions.
26/01/04 17:25:00 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:25:00 INFO DAGScheduler: Got job 304 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/01/04 17:25:00 INFO DAGScheduler: Final stage: ResultStage 305 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:25:00 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:25:00 INFO DAGScheduler: Missing parents: List()
26/01/04 17:25:00 INFO DAGScheduler: Submitting ResultStage 305 (ParallelCollectionRDD[1528] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:25:00 INFO MemoryStore: Block broadcast_455 stored as values in memory (estimated size 4.4 KiB, free 434.2 MiB)
26/01/04 17:25:00 INFO MemoryStore: Block broadcast_455_piece0 stored as bytes in memory (estimated size 2.6 KiB, free 434.2 MiB)
26/01/04 17:25:00 INFO BlockManagerInfo: Added broadcast_455_piece0 in memory on spark-master:33535 (size: 2.6 KiB, free: 434.3 MiB)
26/01/04 17:25:00 INFO SparkContext: Created broadcast 455 from broadcast at DAGScheduler.scala:1585
26/01/04 17:25:00 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 305 (ParallelCollectionRDD[1528] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/01/04 17:25:00 INFO TaskSchedulerImpl: Adding task set 305.0 with 1 tasks resource profile 0
26/01/04 17:25:00 INFO TaskSetManager: Starting task 0.0 in stage 305.0 (TID 493) (172.18.0.13, executor 0, partition 0, PROCESS_LOCAL, 7640 bytes) 
26/01/04 17:25:00 INFO BlockManagerInfo: Added broadcast_455_piece0 in memory on 172.18.0.13:44401 (size: 2.6 KiB, free: 434.3 MiB)
26/01/04 17:25:00 INFO TaskSetManager: Finished task 0.0 in stage 305.0 (TID 493) in 96 ms on 172.18.0.13 (executor 0) (1/1)
26/01/04 17:25:00 INFO TaskSchedulerImpl: Removed TaskSet 305.0, whose tasks have all completed, from pool 
26/01/04 17:25:00 INFO DAGScheduler: ResultStage 305 (start at NativeMethodAccessorImpl.java:0) finished in 0.131 s
26/01/04 17:25:00 INFO DAGScheduler: Job 304 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:25:00 INFO TaskSchedulerImpl: Killing all running tasks in stage 305: Stage finished
26/01/04 17:25:00 INFO DAGScheduler: Job 304 finished: start at NativeMethodAccessorImpl.java:0, took 0.146942 s
26/01/04 17:25:00 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 150, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@7cec6451] is committing.
26/01/04 17:25:00 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 150, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@7cec6451] committed.
26/01/04 17:25:00 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/150 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.150.86e0b611-c526-4a9a-ad51-b97d6343f80c.tmp
26/01/04 17:25:00 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.150.86e0b611-c526-4a9a-ad51-b97d6343f80c.tmp to file:/tmp/spark-checkpoint-enrichment/commits/150
26/01/04 17:25:00 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T17:24:54.465Z",
  "batchId" : 150,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "addBatch" : 1189,
    "commitOffsets" : 139,
    "getBatch" : 0,
    "latestOffset" : 4468,
    "queryPlanning" : 78,
    "triggerExecution" : 6092,
    "walCommit" : 216
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "0" : 3759
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "0" : 30
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "0" : 30
      }
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 0
  }
}
26/01/04 17:25:07 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/151 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.151.d3edf526-4c94-4fe3-ae8b-3abd972476af.tmp
26/01/04 17:25:08 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.151.d3edf526-4c94-4fe3-ae8b-3abd972476af.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/151
26/01/04 17:25:08 INFO MicroBatchExecution: Committed offsets for batch 151. Metadata OffsetSeqMetadata(0,1767547507908,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 17:25:08 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:25:08 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:25:08 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:25:08 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:25:08 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:25:08 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:25:08 INFO BlockManagerInfo: Removed broadcast_450_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:25:08 INFO BlockManagerInfo: Removed broadcast_450_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:25:08 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#122887 - airline_prefix.nullCount#122886) > 0)
26/01/04 17:25:08 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#122922 - min_flight_num.nullCount#122921) > 0)
26/01/04 17:25:08 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#122917 - max_flight_num.nullCount#122916) > 0)
26/01/04 17:25:08 INFO BlockManagerInfo: Removed broadcast_450_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:25:08 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:25:08 INFO DAGScheduler: Got job 305 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 17:25:08 INFO DAGScheduler: Final stage: ResultStage 306 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:25:08 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:25:08 INFO DAGScheduler: Missing parents: List()
26/01/04 17:25:08 INFO DAGScheduler: Submitting ResultStage 306 (MapPartitionsRDD[1533] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:25:08 INFO MemoryStore: Block broadcast_456 stored as values in memory (estimated size 38.5 KiB, free 434.2 MiB)
26/01/04 17:25:08 INFO MemoryStore: Block broadcast_456_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.2 MiB)
26/01/04 17:25:08 INFO BlockManagerInfo: Added broadcast_456_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:25:08 INFO SparkContext: Created broadcast 456 from broadcast at DAGScheduler.scala:1585
26/01/04 17:25:08 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 306 (MapPartitionsRDD[1533] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 17:25:08 INFO TaskSchedulerImpl: Adding task set 306.0 with 2 tasks resource profile 0
26/01/04 17:25:08 INFO TaskSetManager: Starting task 1.0 in stage 306.0 (TID 494) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 17:25:08 INFO TaskSetManager: Starting task 0.0 in stage 306.0 (TID 495) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 17:25:08 INFO BlockManagerInfo: Removed broadcast_452_piece0 on spark-master:33535 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:25:08 INFO BlockManagerInfo: Removed broadcast_452_piece0 on 172.18.0.13:44401 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:25:08 INFO BlockManagerInfo: Added broadcast_456_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:25:08 INFO BlockManagerInfo: Removed broadcast_454_piece0 on spark-master:33535 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:25:08 INFO BlockManagerInfo: Added broadcast_456_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:25:08 INFO BlockManagerInfo: Removed broadcast_455_piece0 on spark-master:33535 in memory (size: 2.6 KiB, free: 434.4 MiB)
26/01/04 17:25:08 INFO BlockManagerInfo: Removed broadcast_455_piece0 on 172.18.0.13:44401 in memory (size: 2.6 KiB, free: 434.4 MiB)
26/01/04 17:25:08 INFO TaskSetManager: Finished task 0.0 in stage 306.0 (TID 495) in 147 ms on 172.18.0.14 (executor 1) (1/2)
26/01/04 17:25:08 INFO TaskSetManager: Finished task 1.0 in stage 306.0 (TID 494) in 162 ms on 172.18.0.13 (executor 0) (2/2)
26/01/04 17:25:08 INFO TaskSchedulerImpl: Removed TaskSet 306.0, whose tasks have all completed, from pool 
26/01/04 17:25:08 INFO DAGScheduler: ResultStage 306 (start at NativeMethodAccessorImpl.java:0) finished in 0.200 s
26/01/04 17:25:08 INFO DAGScheduler: Job 305 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:25:08 INFO TaskSchedulerImpl: Killing all running tasks in stage 306: Stage finished
26/01/04 17:25:08 INFO DAGScheduler: Job 305 finished: start at NativeMethodAccessorImpl.java:0, took 0.206768 s
26/01/04 17:25:08 INFO MemoryStore: Block broadcast_457_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.3 MiB)
26/01/04 17:25:08 INFO BlockManagerInfo: Added broadcast_457_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:25:08 INFO SparkContext: Created broadcast 457 from start at NativeMethodAccessorImpl.java:0
26/01/04 17:25:08 INFO BlockManagerInfo: Removed broadcast_456_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:25:08 INFO BlockManagerInfo: Removed broadcast_456_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:25:08 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 151, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@44d00032]. The input RDD has 1 partitions.
26/01/04 17:25:08 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:25:08 INFO DAGScheduler: Got job 306 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/01/04 17:25:08 INFO DAGScheduler: Final stage: ResultStage 307 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:25:08 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:25:08 INFO DAGScheduler: Missing parents: List()
26/01/04 17:25:08 INFO DAGScheduler: Submitting ResultStage 307 (MapPartitionsRDD[1538] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:25:08 INFO MemoryStore: Block broadcast_458 stored as values in memory (estimated size 52.9 KiB, free 434.3 MiB)
26/01/04 17:25:08 INFO MemoryStore: Block broadcast_458_piece0 stored as bytes in memory (estimated size 20.2 KiB, free 434.3 MiB)
26/01/04 17:25:08 INFO BlockManagerInfo: Added broadcast_458_piece0 in memory on spark-master:33535 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:25:08 INFO SparkContext: Created broadcast 458 from broadcast at DAGScheduler.scala:1585
26/01/04 17:25:08 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 307 (MapPartitionsRDD[1538] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/01/04 17:25:08 INFO TaskSchedulerImpl: Adding task set 307.0 with 1 tasks resource profile 0
26/01/04 17:25:09 INFO BlockManagerInfo: Removed broadcast_456_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:25:09 INFO TaskSetManager: Starting task 0.0 in stage 307.0 (TID 496) (172.18.0.13, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:25:09 INFO BlockManagerInfo: Removed broadcast_453_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:25:09 INFO BlockManagerInfo: Added broadcast_458_piece0 in memory on 172.18.0.13:44401 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:25:09 INFO BlockManagerInfo: Removed broadcast_453_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:25:09 INFO BlockManagerInfo: Removed broadcast_453_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:25:09 INFO BlockManagerInfo: Added broadcast_457_piece0 in memory on 172.18.0.13:44401 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:25:09 INFO BlockManagerInfo: Removed broadcast_451_piece0 on spark-master:33535 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:25:09 INFO BlockManagerInfo: Removed broadcast_451_piece0 on 172.18.0.13:44401 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:25:10 INFO TaskSetManager: Finished task 0.0 in stage 307.0 (TID 496) in 1142 ms on 172.18.0.13 (executor 0) (1/1)
26/01/04 17:25:10 INFO TaskSchedulerImpl: Removed TaskSet 307.0, whose tasks have all completed, from pool 
26/01/04 17:25:10 INFO DAGScheduler: ResultStage 307 (start at NativeMethodAccessorImpl.java:0) finished in 1.345 s
26/01/04 17:25:10 INFO DAGScheduler: Job 306 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:25:10 INFO TaskSchedulerImpl: Killing all running tasks in stage 307: Stage finished
26/01/04 17:25:10 INFO DAGScheduler: Job 306 finished: start at NativeMethodAccessorImpl.java:0, took 1.354548 s
26/01/04 17:25:10 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 151, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@44d00032] is committing.
26/01/04 17:25:10 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 151, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@44d00032] committed.
26/01/04 17:25:10 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/151 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.151.48ebf132-88fc-47b8-86a2-62562d995610.tmp
26/01/04 17:25:10 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.151.48ebf132-88fc-47b8-86a2-62562d995610.tmp to file:/tmp/spark-checkpoint-enrichment/commits/151
26/01/04 17:25:10 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T17:25:07.905Z",
  "batchId" : 151,
  "numInputRows" : 30,
  "inputRowsPerSecond" : 2000.0,
  "processedRowsPerSecond" : 11.933174224343677,
  "durationMs" : {
    "addBatch" : 1860,
    "commitOffsets" : 248,
    "getBatch" : 0,
    "latestOffset" : 3,
    "queryPlanning" : 50,
    "triggerExecution" : 2514,
    "walCommit" : 351
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "0" : 30
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "0" : 60
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "0" : 60
      }
    },
    "numInputRows" : 30,
    "inputRowsPerSecond" : 2000.0,
    "processedRowsPerSecond" : 11.933174224343677,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 31
  }
}
26/01/04 17:25:18 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/152 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.152.8750d961-b461-46c0-870c-bbac8eaf585d.tmp
26/01/04 17:25:19 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.152.8750d961-b461-46c0-870c-bbac8eaf585d.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/152
26/01/04 17:25:19 INFO MicroBatchExecution: Committed offsets for batch 152. Metadata OffsetSeqMetadata(0,1767547518918,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 17:25:19 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:25:19 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:25:19 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:25:19 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:25:19 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:25:19 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:25:19 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#123691 - airline_prefix.nullCount#123690) > 0)
26/01/04 17:25:19 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#123726 - min_flight_num.nullCount#123725) > 0)
26/01/04 17:25:19 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#123721 - max_flight_num.nullCount#123720) > 0)
26/01/04 17:25:19 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:25:19 INFO DAGScheduler: Got job 307 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 17:25:19 INFO DAGScheduler: Final stage: ResultStage 308 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:25:19 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:25:19 INFO DAGScheduler: Missing parents: List()
26/01/04 17:25:19 INFO DAGScheduler: Submitting ResultStage 308 (MapPartitionsRDD[1543] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:25:19 INFO MemoryStore: Block broadcast_459 stored as values in memory (estimated size 38.5 KiB, free 434.3 MiB)
26/01/04 17:25:19 INFO MemoryStore: Block broadcast_459_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.3 MiB)
26/01/04 17:25:19 INFO BlockManagerInfo: Added broadcast_459_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:25:19 INFO SparkContext: Created broadcast 459 from broadcast at DAGScheduler.scala:1585
26/01/04 17:25:19 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 308 (MapPartitionsRDD[1543] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 17:25:19 INFO TaskSchedulerImpl: Adding task set 308.0 with 2 tasks resource profile 0
26/01/04 17:25:19 INFO TaskSetManager: Starting task 0.0 in stage 308.0 (TID 497) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 17:25:19 INFO TaskSetManager: Starting task 1.0 in stage 308.0 (TID 498) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 17:25:19 INFO BlockManagerInfo: Added broadcast_459_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:25:19 INFO BlockManagerInfo: Added broadcast_459_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:25:19 INFO TaskSetManager: Finished task 1.0 in stage 308.0 (TID 498) in 100 ms on 172.18.0.13 (executor 0) (1/2)
26/01/04 17:25:19 INFO TaskSetManager: Finished task 0.0 in stage 308.0 (TID 497) in 109 ms on 172.18.0.14 (executor 1) (2/2)
26/01/04 17:25:19 INFO TaskSchedulerImpl: Removed TaskSet 308.0, whose tasks have all completed, from pool 
26/01/04 17:25:19 INFO DAGScheduler: ResultStage 308 (start at NativeMethodAccessorImpl.java:0) finished in 0.129 s
26/01/04 17:25:19 INFO DAGScheduler: Job 307 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:25:19 INFO TaskSchedulerImpl: Killing all running tasks in stage 308: Stage finished
26/01/04 17:25:19 INFO DAGScheduler: Job 307 finished: start at NativeMethodAccessorImpl.java:0, took 0.152668 s
26/01/04 17:25:19 INFO MemoryStore: Block broadcast_460_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.3 MiB)
26/01/04 17:25:19 INFO BlockManagerInfo: Added broadcast_460_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:25:19 INFO SparkContext: Created broadcast 460 from start at NativeMethodAccessorImpl.java:0
26/01/04 17:25:19 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 152, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@6949f9d2]. The input RDD has 1 partitions.
26/01/04 17:25:19 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:25:19 INFO DAGScheduler: Got job 308 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/01/04 17:25:19 INFO DAGScheduler: Final stage: ResultStage 309 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:25:19 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:25:19 INFO DAGScheduler: Missing parents: List()
26/01/04 17:25:19 INFO DAGScheduler: Submitting ResultStage 309 (MapPartitionsRDD[1548] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:25:19 INFO MemoryStore: Block broadcast_461 stored as values in memory (estimated size 52.9 KiB, free 434.2 MiB)
26/01/04 17:25:19 INFO MemoryStore: Block broadcast_461_piece0 stored as bytes in memory (estimated size 20.2 KiB, free 434.2 MiB)
26/01/04 17:25:19 INFO BlockManagerInfo: Added broadcast_461_piece0 in memory on spark-master:33535 (size: 20.2 KiB, free: 434.3 MiB)
26/01/04 17:25:19 INFO SparkContext: Created broadcast 461 from broadcast at DAGScheduler.scala:1585
26/01/04 17:25:19 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 309 (MapPartitionsRDD[1548] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/01/04 17:25:19 INFO TaskSchedulerImpl: Adding task set 309.0 with 1 tasks resource profile 0
26/01/04 17:25:19 INFO TaskSetManager: Starting task 0.0 in stage 309.0 (TID 499) (172.18.0.13, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:25:19 INFO BlockManagerInfo: Added broadcast_461_piece0 in memory on 172.18.0.13:44401 (size: 20.2 KiB, free: 434.3 MiB)
26/01/04 17:25:19 INFO BlockManagerInfo: Added broadcast_460_piece0 in memory on 172.18.0.13:44401 (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:25:20 INFO TaskSetManager: Finished task 0.0 in stage 309.0 (TID 499) in 737 ms on 172.18.0.13 (executor 0) (1/1)
26/01/04 17:25:20 INFO TaskSchedulerImpl: Removed TaskSet 309.0, whose tasks have all completed, from pool 
26/01/04 17:25:20 INFO DAGScheduler: ResultStage 309 (start at NativeMethodAccessorImpl.java:0) finished in 0.753 s
26/01/04 17:25:20 INFO DAGScheduler: Job 308 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:25:20 INFO TaskSchedulerImpl: Killing all running tasks in stage 309: Stage finished
26/01/04 17:25:20 INFO DAGScheduler: Job 308 finished: start at NativeMethodAccessorImpl.java:0, took 0.758625 s
26/01/04 17:25:20 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 152, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@6949f9d2] is committing.
26/01/04 17:25:20 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 152, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@6949f9d2] committed.
26/01/04 17:25:20 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/152 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.152.9c37605a-2499-4f0a-81b0-952b04f6bd9e.tmp
26/01/04 17:25:20 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.152.9c37605a-2499-4f0a-81b0-952b04f6bd9e.tmp to file:/tmp/spark-checkpoint-enrichment/commits/152
26/01/04 17:25:20 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T17:25:18.914Z",
  "batchId" : 152,
  "numInputRows" : 30,
  "inputRowsPerSecond" : 1764.705882352941,
  "processedRowsPerSecond" : 21.382751247327157,
  "durationMs" : {
    "addBatch" : 1082,
    "commitOffsets" : 121,
    "getBatch" : 0,
    "latestOffset" : 3,
    "queryPlanning" : 56,
    "triggerExecution" : 1403,
    "walCommit" : 138
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "0" : 60
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "0" : 90
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "0" : 90
      }
    },
    "numInputRows" : 30,
    "inputRowsPerSecond" : 1764.705882352941,
    "processedRowsPerSecond" : 21.382751247327157,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 31
  }
}
26/01/04 17:25:29 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/153 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.153.33911dbd-7aea-4230-89cf-f0495b4496bb.tmp
26/01/04 17:25:30 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.153.33911dbd-7aea-4230-89cf-f0495b4496bb.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/153
26/01/04 17:25:30 INFO MicroBatchExecution: Committed offsets for batch 153. Metadata OffsetSeqMetadata(0,1767547529943,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 17:25:30 INFO BlockManagerInfo: Removed broadcast_458_piece0 on spark-master:33535 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:25:30 INFO BlockManagerInfo: Removed broadcast_458_piece0 on 172.18.0.13:44401 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:25:30 INFO BlockManagerInfo: Removed broadcast_461_piece0 on 172.18.0.13:44401 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:25:30 INFO BlockManagerInfo: Removed broadcast_461_piece0 on spark-master:33535 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:25:30 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:25:30 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:25:30 INFO BlockManagerInfo: Removed broadcast_460_piece0 on spark-master:33535 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:25:30 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:25:30 INFO BlockManagerInfo: Removed broadcast_460_piece0 on 172.18.0.13:44401 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:25:30 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:25:30 INFO BlockManagerInfo: Removed broadcast_457_piece0 on spark-master:33535 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:25:30 INFO BlockManagerInfo: Removed broadcast_457_piece0 on 172.18.0.13:44401 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:25:30 INFO BlockManagerInfo: Removed broadcast_459_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:25:30 INFO BlockManagerInfo: Removed broadcast_459_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:25:30 INFO BlockManagerInfo: Removed broadcast_459_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:25:30 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:25:30 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:25:30 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#124495 - airline_prefix.nullCount#124494) > 0)
26/01/04 17:25:30 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#124530 - min_flight_num.nullCount#124529) > 0)
26/01/04 17:25:30 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#124525 - max_flight_num.nullCount#124524) > 0)
26/01/04 17:25:30 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:25:30 INFO DAGScheduler: Got job 309 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 17:25:30 INFO DAGScheduler: Final stage: ResultStage 310 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:25:30 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:25:30 INFO DAGScheduler: Missing parents: List()
26/01/04 17:25:30 INFO DAGScheduler: Submitting ResultStage 310 (MapPartitionsRDD[1553] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:25:30 INFO MemoryStore: Block broadcast_462 stored as values in memory (estimated size 38.5 KiB, free 434.4 MiB)
26/01/04 17:25:30 INFO MemoryStore: Block broadcast_462_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.3 MiB)
26/01/04 17:25:30 INFO BlockManagerInfo: Added broadcast_462_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:25:30 INFO SparkContext: Created broadcast 462 from broadcast at DAGScheduler.scala:1585
26/01/04 17:25:30 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 310 (MapPartitionsRDD[1553] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 17:25:30 INFO TaskSchedulerImpl: Adding task set 310.0 with 2 tasks resource profile 0
26/01/04 17:25:30 INFO TaskSetManager: Starting task 0.0 in stage 310.0 (TID 500) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 17:25:30 INFO TaskSetManager: Starting task 1.0 in stage 310.0 (TID 501) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 17:25:30 INFO BlockManagerInfo: Added broadcast_462_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:25:30 INFO BlockManagerInfo: Added broadcast_462_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:25:30 INFO TaskSetManager: Finished task 1.0 in stage 310.0 (TID 501) in 76 ms on 172.18.0.13 (executor 0) (1/2)
26/01/04 17:25:30 INFO TaskSetManager: Finished task 0.0 in stage 310.0 (TID 500) in 109 ms on 172.18.0.14 (executor 1) (2/2)
26/01/04 17:25:30 INFO TaskSchedulerImpl: Removed TaskSet 310.0, whose tasks have all completed, from pool 
26/01/04 17:25:30 INFO DAGScheduler: ResultStage 310 (start at NativeMethodAccessorImpl.java:0) finished in 0.154 s
26/01/04 17:25:30 INFO DAGScheduler: Job 309 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:25:30 INFO TaskSchedulerImpl: Killing all running tasks in stage 310: Stage finished
26/01/04 17:25:30 INFO DAGScheduler: Job 309 finished: start at NativeMethodAccessorImpl.java:0, took 0.160728 s
26/01/04 17:25:30 INFO MemoryStore: Block broadcast_463_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.3 MiB)
26/01/04 17:25:30 INFO BlockManagerInfo: Added broadcast_463_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:25:30 INFO SparkContext: Created broadcast 463 from start at NativeMethodAccessorImpl.java:0
26/01/04 17:25:30 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 153, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@3afa1315]. The input RDD has 1 partitions.
26/01/04 17:25:30 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:25:30 INFO DAGScheduler: Got job 310 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/01/04 17:25:30 INFO DAGScheduler: Final stage: ResultStage 311 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:25:30 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:25:30 INFO DAGScheduler: Missing parents: List()
26/01/04 17:25:30 INFO DAGScheduler: Submitting ResultStage 311 (MapPartitionsRDD[1558] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:25:30 INFO MemoryStore: Block broadcast_464 stored as values in memory (estimated size 52.9 KiB, free 434.3 MiB)
26/01/04 17:25:30 INFO MemoryStore: Block broadcast_464_piece0 stored as bytes in memory (estimated size 20.2 KiB, free 434.3 MiB)
26/01/04 17:25:30 INFO BlockManagerInfo: Added broadcast_464_piece0 in memory on spark-master:33535 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:25:30 INFO SparkContext: Created broadcast 464 from broadcast at DAGScheduler.scala:1585
26/01/04 17:25:30 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 311 (MapPartitionsRDD[1558] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/01/04 17:25:30 INFO TaskSchedulerImpl: Adding task set 311.0 with 1 tasks resource profile 0
26/01/04 17:25:30 INFO TaskSetManager: Starting task 0.0 in stage 311.0 (TID 502) (172.18.0.13, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:25:30 INFO BlockManagerInfo: Added broadcast_464_piece0 in memory on 172.18.0.13:44401 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:25:30 INFO BlockManagerInfo: Added broadcast_463_piece0 in memory on 172.18.0.13:44401 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:25:31 INFO TaskSetManager: Finished task 0.0 in stage 311.0 (TID 502) in 747 ms on 172.18.0.13 (executor 0) (1/1)
26/01/04 17:25:31 INFO TaskSchedulerImpl: Removed TaskSet 311.0, whose tasks have all completed, from pool 
26/01/04 17:25:31 INFO DAGScheduler: ResultStage 311 (start at NativeMethodAccessorImpl.java:0) finished in 0.761 s
26/01/04 17:25:31 INFO DAGScheduler: Job 310 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:25:31 INFO TaskSchedulerImpl: Killing all running tasks in stage 311: Stage finished
26/01/04 17:25:31 INFO DAGScheduler: Job 310 finished: start at NativeMethodAccessorImpl.java:0, took 0.767741 s
26/01/04 17:25:31 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 153, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@3afa1315] is committing.
26/01/04 17:25:31 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 153, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@3afa1315] committed.
26/01/04 17:25:31 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/153 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.153.def666d1-d1a9-46fd-82ed-c80f0f7964e9.tmp
26/01/04 17:25:31 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.153.def666d1-d1a9-46fd-82ed-c80f0f7964e9.tmp to file:/tmp/spark-checkpoint-enrichment/commits/153
26/01/04 17:25:31 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T17:25:29.935Z",
  "batchId" : 153,
  "numInputRows" : 30,
  "inputRowsPerSecond" : 1764.705882352941,
  "processedRowsPerSecond" : 19.023462270133166,
  "durationMs" : {
    "addBatch" : 1178,
    "commitOffsets" : 152,
    "getBatch" : 0,
    "latestOffset" : 8,
    "queryPlanning" : 87,
    "triggerExecution" : 1577,
    "walCommit" : 150
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "0" : 90
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "0" : 120
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "0" : 120
      }
    },
    "numInputRows" : 30,
    "inputRowsPerSecond" : 1764.705882352941,
    "processedRowsPerSecond" : 19.023462270133166,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 31
  }
}
26/01/04 17:25:40 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/154 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.154.4a416b57-f016-4544-9d2e-45204b378a4a.tmp
26/01/04 17:25:41 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.154.4a416b57-f016-4544-9d2e-45204b378a4a.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/154
26/01/04 17:25:41 INFO MicroBatchExecution: Committed offsets for batch 154. Metadata OffsetSeqMetadata(0,1767547540962,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 17:25:41 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:25:41 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:25:41 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:25:41 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:25:41 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:25:41 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:25:41 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#125299 - airline_prefix.nullCount#125298) > 0)
26/01/04 17:25:41 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#125334 - min_flight_num.nullCount#125333) > 0)
26/01/04 17:25:41 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#125329 - max_flight_num.nullCount#125328) > 0)
26/01/04 17:25:41 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:25:41 INFO DAGScheduler: Got job 311 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 17:25:41 INFO DAGScheduler: Final stage: ResultStage 312 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:25:41 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:25:41 INFO DAGScheduler: Missing parents: List()
26/01/04 17:25:41 INFO DAGScheduler: Submitting ResultStage 312 (MapPartitionsRDD[1563] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:25:41 INFO MemoryStore: Block broadcast_465 stored as values in memory (estimated size 38.5 KiB, free 434.2 MiB)
26/01/04 17:25:41 INFO MemoryStore: Block broadcast_465_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.2 MiB)
26/01/04 17:25:41 INFO BlockManagerInfo: Added broadcast_465_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:25:41 INFO SparkContext: Created broadcast 465 from broadcast at DAGScheduler.scala:1585
26/01/04 17:25:41 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 312 (MapPartitionsRDD[1563] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 17:25:41 INFO TaskSchedulerImpl: Adding task set 312.0 with 2 tasks resource profile 0
26/01/04 17:25:41 INFO TaskSetManager: Starting task 0.0 in stage 312.0 (TID 503) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 17:25:41 INFO TaskSetManager: Starting task 1.0 in stage 312.0 (TID 504) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 17:25:41 INFO BlockManagerInfo: Added broadcast_465_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:25:41 INFO BlockManagerInfo: Added broadcast_465_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:25:41 INFO TaskSetManager: Finished task 1.0 in stage 312.0 (TID 504) in 78 ms on 172.18.0.13 (executor 0) (1/2)
26/01/04 17:25:41 INFO TaskSetManager: Finished task 0.0 in stage 312.0 (TID 503) in 106 ms on 172.18.0.14 (executor 1) (2/2)
26/01/04 17:25:41 INFO TaskSchedulerImpl: Removed TaskSet 312.0, whose tasks have all completed, from pool 
26/01/04 17:25:41 INFO DAGScheduler: ResultStage 312 (start at NativeMethodAccessorImpl.java:0) finished in 0.120 s
26/01/04 17:25:41 INFO DAGScheduler: Job 311 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:25:41 INFO TaskSchedulerImpl: Killing all running tasks in stage 312: Stage finished
26/01/04 17:25:41 INFO DAGScheduler: Job 311 finished: start at NativeMethodAccessorImpl.java:0, took 0.135420 s
26/01/04 17:25:41 INFO MemoryStore: Block broadcast_466_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.2 MiB)
26/01/04 17:25:41 INFO BlockManagerInfo: Added broadcast_466_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:25:41 INFO SparkContext: Created broadcast 466 from start at NativeMethodAccessorImpl.java:0
26/01/04 17:25:41 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 154, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@1be3f825]. The input RDD has 1 partitions.
26/01/04 17:25:41 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:25:41 INFO DAGScheduler: Got job 312 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/01/04 17:25:41 INFO DAGScheduler: Final stage: ResultStage 313 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:25:41 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:25:41 INFO DAGScheduler: Missing parents: List()
26/01/04 17:25:41 INFO DAGScheduler: Submitting ResultStage 313 (MapPartitionsRDD[1568] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:25:41 INFO MemoryStore: Block broadcast_467 stored as values in memory (estimated size 52.9 KiB, free 434.2 MiB)
26/01/04 17:25:41 INFO MemoryStore: Block broadcast_467_piece0 stored as bytes in memory (estimated size 20.2 KiB, free 434.1 MiB)
26/01/04 17:25:41 INFO BlockManagerInfo: Added broadcast_467_piece0 in memory on spark-master:33535 (size: 20.2 KiB, free: 434.3 MiB)
26/01/04 17:25:41 INFO SparkContext: Created broadcast 467 from broadcast at DAGScheduler.scala:1585
26/01/04 17:25:41 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 313 (MapPartitionsRDD[1568] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/01/04 17:25:41 INFO TaskSchedulerImpl: Adding task set 313.0 with 1 tasks resource profile 0
26/01/04 17:25:41 INFO TaskSetManager: Starting task 0.0 in stage 313.0 (TID 505) (172.18.0.13, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:25:41 INFO BlockManagerInfo: Added broadcast_467_piece0 in memory on 172.18.0.13:44401 (size: 20.2 KiB, free: 434.3 MiB)
26/01/04 17:25:41 INFO BlockManagerInfo: Added broadcast_466_piece0 in memory on 172.18.0.13:44401 (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:25:42 INFO TaskSetManager: Finished task 0.0 in stage 313.0 (TID 505) in 661 ms on 172.18.0.13 (executor 0) (1/1)
26/01/04 17:25:42 INFO TaskSchedulerImpl: Removed TaskSet 313.0, whose tasks have all completed, from pool 
26/01/04 17:25:42 INFO DAGScheduler: ResultStage 313 (start at NativeMethodAccessorImpl.java:0) finished in 0.678 s
26/01/04 17:25:42 INFO DAGScheduler: Job 312 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:25:42 INFO TaskSchedulerImpl: Killing all running tasks in stage 313: Stage finished
26/01/04 17:25:42 INFO DAGScheduler: Job 312 finished: start at NativeMethodAccessorImpl.java:0, took 0.685796 s
26/01/04 17:25:42 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 154, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@1be3f825] is committing.
26/01/04 17:25:42 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 154, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@1be3f825] committed.
26/01/04 17:25:42 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/154 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.154.9c47853a-448e-4b17-a468-61d0e85c8880.tmp
26/01/04 17:25:42 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.154.9c47853a-448e-4b17-a468-61d0e85c8880.tmp to file:/tmp/spark-checkpoint-enrichment/commits/154
26/01/04 17:25:42 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T17:25:40.957Z",
  "batchId" : 154,
  "numInputRows" : 30,
  "inputRowsPerSecond" : 1764.705882352941,
  "processedRowsPerSecond" : 18.203883495145632,
  "durationMs" : {
    "addBatch" : 1158,
    "commitOffsets" : 165,
    "getBatch" : 0,
    "latestOffset" : 5,
    "queryPlanning" : 117,
    "triggerExecution" : 1648,
    "walCommit" : 187
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "0" : 120
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "0" : 150
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "0" : 150
      }
    },
    "numInputRows" : 30,
    "inputRowsPerSecond" : 1764.705882352941,
    "processedRowsPerSecond" : 18.203883495145632,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 31
  }
}
26/01/04 17:25:49 INFO BlockManagerInfo: Removed broadcast_462_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:25:49 INFO BlockManagerInfo: Removed broadcast_462_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:25:49 INFO BlockManagerInfo: Removed broadcast_462_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:25:49 INFO BlockManagerInfo: Removed broadcast_464_piece0 on spark-master:33535 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:25:49 INFO BlockManagerInfo: Removed broadcast_464_piece0 on 172.18.0.13:44401 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:25:49 INFO BlockManagerInfo: Removed broadcast_465_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:25:49 INFO BlockManagerInfo: Removed broadcast_465_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:25:49 INFO BlockManagerInfo: Removed broadcast_465_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:25:49 INFO BlockManagerInfo: Removed broadcast_467_piece0 on spark-master:33535 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:25:49 INFO BlockManagerInfo: Removed broadcast_467_piece0 on 172.18.0.13:44401 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:25:49 INFO BlockManagerInfo: Removed broadcast_463_piece0 on spark-master:33535 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:25:49 INFO BlockManagerInfo: Removed broadcast_463_piece0 on 172.18.0.13:44401 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:25:52 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/155 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.155.f01011f0-c607-49aa-bfe8-30a8c9291ad5.tmp
26/01/04 17:25:52 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.155.f01011f0-c607-49aa-bfe8-30a8c9291ad5.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/155
26/01/04 17:25:52 INFO MicroBatchExecution: Committed offsets for batch 155. Metadata OffsetSeqMetadata(0,1767547551987,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 17:25:52 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:25:52 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:25:52 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:25:52 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:25:52 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:25:52 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:25:52 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#126103 - airline_prefix.nullCount#126102) > 0)
26/01/04 17:25:52 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#126138 - min_flight_num.nullCount#126137) > 0)
26/01/04 17:25:52 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#126133 - max_flight_num.nullCount#126132) > 0)
26/01/04 17:25:52 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:25:52 INFO DAGScheduler: Got job 313 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 17:25:52 INFO DAGScheduler: Final stage: ResultStage 314 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:25:52 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:25:52 INFO DAGScheduler: Missing parents: List()
26/01/04 17:25:52 INFO DAGScheduler: Submitting ResultStage 314 (MapPartitionsRDD[1573] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:25:52 INFO MemoryStore: Block broadcast_468 stored as values in memory (estimated size 38.5 KiB, free 434.4 MiB)
26/01/04 17:25:52 INFO MemoryStore: Block broadcast_468_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.3 MiB)
26/01/04 17:25:52 INFO BlockManagerInfo: Added broadcast_468_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:25:52 INFO SparkContext: Created broadcast 468 from broadcast at DAGScheduler.scala:1585
26/01/04 17:25:52 INFO BlockManagerInfo: Removed broadcast_466_piece0 on spark-master:33535 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:25:52 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 314 (MapPartitionsRDD[1573] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 17:25:52 INFO TaskSchedulerImpl: Adding task set 314.0 with 2 tasks resource profile 0
26/01/04 17:25:52 INFO TaskSetManager: Starting task 0.0 in stage 314.0 (TID 506) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 17:25:52 INFO TaskSetManager: Starting task 1.0 in stage 314.0 (TID 507) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 17:25:52 INFO BlockManagerInfo: Removed broadcast_466_piece0 on 172.18.0.13:44401 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:25:52 INFO BlockManagerInfo: Added broadcast_468_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:25:52 INFO TaskSetManager: Finished task 1.0 in stage 314.0 (TID 507) in 61 ms on 172.18.0.13 (executor 0) (1/2)
26/01/04 17:25:52 INFO BlockManagerInfo: Added broadcast_468_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:25:52 INFO TaskSetManager: Finished task 0.0 in stage 314.0 (TID 506) in 207 ms on 172.18.0.14 (executor 1) (2/2)
26/01/04 17:25:52 INFO TaskSchedulerImpl: Removed TaskSet 314.0, whose tasks have all completed, from pool 
26/01/04 17:25:52 INFO DAGScheduler: ResultStage 314 (start at NativeMethodAccessorImpl.java:0) finished in 0.245 s
26/01/04 17:25:52 INFO DAGScheduler: Job 313 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:25:52 INFO TaskSchedulerImpl: Killing all running tasks in stage 314: Stage finished
26/01/04 17:25:52 INFO DAGScheduler: Job 313 finished: start at NativeMethodAccessorImpl.java:0, took 0.251070 s
26/01/04 17:25:52 INFO MemoryStore: Block broadcast_469_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.3 MiB)
26/01/04 17:25:52 INFO BlockManagerInfo: Added broadcast_469_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:25:52 INFO SparkContext: Created broadcast 469 from start at NativeMethodAccessorImpl.java:0
26/01/04 17:25:52 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 155, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@28cb2846]. The input RDD has 1 partitions.
26/01/04 17:25:52 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:25:52 INFO DAGScheduler: Got job 314 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/01/04 17:25:52 INFO DAGScheduler: Final stage: ResultStage 315 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:25:52 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:25:52 INFO DAGScheduler: Missing parents: List()
26/01/04 17:25:52 INFO DAGScheduler: Submitting ResultStage 315 (MapPartitionsRDD[1578] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:25:52 INFO MemoryStore: Block broadcast_470 stored as values in memory (estimated size 52.9 KiB, free 434.3 MiB)
26/01/04 17:25:52 INFO MemoryStore: Block broadcast_470_piece0 stored as bytes in memory (estimated size 20.2 KiB, free 434.3 MiB)
26/01/04 17:25:52 INFO BlockManagerInfo: Added broadcast_470_piece0 in memory on spark-master:33535 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:25:52 INFO SparkContext: Created broadcast 470 from broadcast at DAGScheduler.scala:1585
26/01/04 17:25:52 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 315 (MapPartitionsRDD[1578] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/01/04 17:25:52 INFO TaskSchedulerImpl: Adding task set 315.0 with 1 tasks resource profile 0
26/01/04 17:25:52 INFO TaskSetManager: Starting task 0.0 in stage 315.0 (TID 508) (172.18.0.13, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:25:52 INFO BlockManagerInfo: Added broadcast_470_piece0 in memory on 172.18.0.13:44401 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:25:52 INFO BlockManagerInfo: Added broadcast_469_piece0 in memory on 172.18.0.13:44401 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:25:53 INFO TaskSetManager: Finished task 0.0 in stage 315.0 (TID 508) in 664 ms on 172.18.0.13 (executor 0) (1/1)
26/01/04 17:25:53 INFO TaskSchedulerImpl: Removed TaskSet 315.0, whose tasks have all completed, from pool 
26/01/04 17:25:53 INFO DAGScheduler: ResultStage 315 (start at NativeMethodAccessorImpl.java:0) finished in 0.676 s
26/01/04 17:25:53 INFO DAGScheduler: Job 314 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:25:53 INFO TaskSchedulerImpl: Killing all running tasks in stage 315: Stage finished
26/01/04 17:25:53 INFO DAGScheduler: Job 314 finished: start at NativeMethodAccessorImpl.java:0, took 0.683162 s
26/01/04 17:25:53 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 155, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@28cb2846] is committing.
26/01/04 17:25:53 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 155, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@28cb2846] committed.
26/01/04 17:25:53 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/155 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.155.f13902be-380c-4264-8d64-35fa2805ff6e.tmp
26/01/04 17:25:53 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.155.f13902be-380c-4264-8d64-35fa2805ff6e.tmp to file:/tmp/spark-checkpoint-enrichment/commits/155
26/01/04 17:25:53 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T17:25:51.983Z",
  "batchId" : 155,
  "numInputRows" : 30,
  "inputRowsPerSecond" : 1578.9473684210527,
  "processedRowsPerSecond" : 19.243104554201413,
  "durationMs" : {
    "addBatch" : 1137,
    "commitOffsets" : 176,
    "getBatch" : 0,
    "latestOffset" : 4,
    "queryPlanning" : 71,
    "triggerExecution" : 1559,
    "walCommit" : 170
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "0" : 150
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "0" : 180
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "0" : 180
      }
    },
    "numInputRows" : 30,
    "inputRowsPerSecond" : 1578.9473684210527,
    "processedRowsPerSecond" : 19.243104554201413,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 31
  }
}
26/01/04 17:26:03 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/156 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.156.68f59520-19e1-4898-9fbe-83141388e466.tmp
26/01/04 17:26:03 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.156.68f59520-19e1-4898-9fbe-83141388e466.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/156
26/01/04 17:26:03 INFO MicroBatchExecution: Committed offsets for batch 156. Metadata OffsetSeqMetadata(0,1767547563012,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 17:26:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:26:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:26:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:26:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:26:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:26:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:26:03 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#126907 - airline_prefix.nullCount#126906) > 0)
26/01/04 17:26:03 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#126942 - min_flight_num.nullCount#126941) > 0)
26/01/04 17:26:03 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#126937 - max_flight_num.nullCount#126936) > 0)
26/01/04 17:26:03 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:26:03 INFO DAGScheduler: Got job 315 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 17:26:03 INFO DAGScheduler: Final stage: ResultStage 316 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:26:03 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:26:03 INFO DAGScheduler: Missing parents: List()
26/01/04 17:26:03 INFO DAGScheduler: Submitting ResultStage 316 (MapPartitionsRDD[1583] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:26:03 INFO MemoryStore: Block broadcast_471 stored as values in memory (estimated size 38.5 KiB, free 434.2 MiB)
26/01/04 17:26:03 INFO MemoryStore: Block broadcast_471_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.2 MiB)
26/01/04 17:26:03 INFO BlockManagerInfo: Added broadcast_471_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:26:03 INFO SparkContext: Created broadcast 471 from broadcast at DAGScheduler.scala:1585
26/01/04 17:26:03 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 316 (MapPartitionsRDD[1583] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 17:26:03 INFO TaskSchedulerImpl: Adding task set 316.0 with 2 tasks resource profile 0
26/01/04 17:26:03 INFO TaskSetManager: Starting task 1.0 in stage 316.0 (TID 509) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 17:26:03 INFO TaskSetManager: Starting task 0.0 in stage 316.0 (TID 510) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 17:26:03 INFO BlockManagerInfo: Added broadcast_471_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:26:03 INFO BlockManagerInfo: Added broadcast_471_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:26:03 INFO TaskSetManager: Finished task 1.0 in stage 316.0 (TID 509) in 48 ms on 172.18.0.13 (executor 0) (1/2)
26/01/04 17:26:03 INFO TaskSetManager: Finished task 0.0 in stage 316.0 (TID 510) in 94 ms on 172.18.0.14 (executor 1) (2/2)
26/01/04 17:26:03 INFO TaskSchedulerImpl: Removed TaskSet 316.0, whose tasks have all completed, from pool 
26/01/04 17:26:03 INFO DAGScheduler: ResultStage 316 (start at NativeMethodAccessorImpl.java:0) finished in 0.112 s
26/01/04 17:26:03 INFO DAGScheduler: Job 315 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:26:03 INFO TaskSchedulerImpl: Killing all running tasks in stage 316: Stage finished
26/01/04 17:26:03 INFO DAGScheduler: Job 315 finished: start at NativeMethodAccessorImpl.java:0, took 0.117891 s
26/01/04 17:26:03 INFO MemoryStore: Block broadcast_472_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.2 MiB)
26/01/04 17:26:03 INFO BlockManagerInfo: Added broadcast_472_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:26:03 INFO SparkContext: Created broadcast 472 from start at NativeMethodAccessorImpl.java:0
26/01/04 17:26:03 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 156, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@bf8e567]. The input RDD has 1 partitions.
26/01/04 17:26:03 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:26:03 INFO DAGScheduler: Got job 316 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/01/04 17:26:03 INFO DAGScheduler: Final stage: ResultStage 317 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:26:03 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:26:03 INFO DAGScheduler: Missing parents: List()
26/01/04 17:26:03 INFO DAGScheduler: Submitting ResultStage 317 (MapPartitionsRDD[1588] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:26:03 INFO MemoryStore: Block broadcast_473 stored as values in memory (estimated size 52.9 KiB, free 434.2 MiB)
26/01/04 17:26:03 INFO MemoryStore: Block broadcast_473_piece0 stored as bytes in memory (estimated size 20.2 KiB, free 434.1 MiB)
26/01/04 17:26:03 INFO BlockManagerInfo: Added broadcast_473_piece0 in memory on spark-master:33535 (size: 20.2 KiB, free: 434.3 MiB)
26/01/04 17:26:03 INFO SparkContext: Created broadcast 473 from broadcast at DAGScheduler.scala:1585
26/01/04 17:26:03 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 317 (MapPartitionsRDD[1588] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/01/04 17:26:03 INFO TaskSchedulerImpl: Adding task set 317.0 with 1 tasks resource profile 0
26/01/04 17:26:03 INFO TaskSetManager: Starting task 0.0 in stage 317.0 (TID 511) (172.18.0.13, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:26:03 INFO BlockManagerInfo: Added broadcast_473_piece0 in memory on 172.18.0.13:44401 (size: 20.2 KiB, free: 434.3 MiB)
26/01/04 17:26:03 INFO BlockManagerInfo: Added broadcast_472_piece0 in memory on 172.18.0.13:44401 (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:26:04 INFO TaskSetManager: Finished task 0.0 in stage 317.0 (TID 511) in 735 ms on 172.18.0.13 (executor 0) (1/1)
26/01/04 17:26:04 INFO TaskSchedulerImpl: Removed TaskSet 317.0, whose tasks have all completed, from pool 
26/01/04 17:26:04 INFO DAGScheduler: ResultStage 317 (start at NativeMethodAccessorImpl.java:0) finished in 0.757 s
26/01/04 17:26:04 INFO DAGScheduler: Job 316 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:26:04 INFO TaskSchedulerImpl: Killing all running tasks in stage 317: Stage finished
26/01/04 17:26:04 INFO DAGScheduler: Job 316 finished: start at NativeMethodAccessorImpl.java:0, took 0.761754 s
26/01/04 17:26:04 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 156, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@bf8e567] is committing.
26/01/04 17:26:04 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 156, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@bf8e567] committed.
26/01/04 17:26:04 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/156 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.156.fd7c6eb9-0b8b-4920-82b9-92494392c5cf.tmp
26/01/04 17:26:04 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.156.fd7c6eb9-0b8b-4920-82b9-92494392c5cf.tmp to file:/tmp/spark-checkpoint-enrichment/commits/156
26/01/04 17:26:04 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T17:26:03.001Z",
  "batchId" : 156,
  "numInputRows" : 30,
  "inputRowsPerSecond" : 1764.705882352941,
  "processedRowsPerSecond" : 19.53125,
  "durationMs" : {
    "addBatch" : 1111,
    "commitOffsets" : 189,
    "getBatch" : 1,
    "latestOffset" : 11,
    "queryPlanning" : 73,
    "triggerExecution" : 1536,
    "walCommit" : 148
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "0" : 180
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "0" : 210
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "0" : 210
      }
    },
    "numInputRows" : 30,
    "inputRowsPerSecond" : 1764.705882352941,
    "processedRowsPerSecond" : 19.53125,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 31
  }
}
26/01/04 17:26:13 INFO BlockManagerInfo: Removed broadcast_469_piece0 on 172.18.0.13:44401 in memory (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:26:13 INFO BlockManagerInfo: Removed broadcast_469_piece0 on spark-master:33535 in memory (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:26:13 INFO BlockManagerInfo: Removed broadcast_473_piece0 on spark-master:33535 in memory (size: 20.2 KiB, free: 434.3 MiB)
26/01/04 17:26:13 INFO BlockManagerInfo: Removed broadcast_473_piece0 on 172.18.0.13:44401 in memory (size: 20.2 KiB, free: 434.3 MiB)
26/01/04 17:26:13 INFO BlockManagerInfo: Removed broadcast_468_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:26:13 INFO BlockManagerInfo: Removed broadcast_468_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:26:13 INFO BlockManagerInfo: Removed broadcast_468_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:26:13 INFO BlockManagerInfo: Removed broadcast_471_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:26:13 INFO BlockManagerInfo: Removed broadcast_471_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:26:13 INFO BlockManagerInfo: Removed broadcast_471_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:26:13 INFO BlockManagerInfo: Removed broadcast_470_piece0 on 172.18.0.13:44401 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:26:13 INFO BlockManagerInfo: Removed broadcast_470_piece0 on spark-master:33535 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:26:14 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/157 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.157.89a1f072-c1bf-4b1b-baed-a5449f3f576d.tmp
26/01/04 17:26:14 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.157.89a1f072-c1bf-4b1b-baed-a5449f3f576d.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/157
26/01/04 17:26:14 INFO MicroBatchExecution: Committed offsets for batch 157. Metadata OffsetSeqMetadata(0,1767547574045,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 17:26:14 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:26:14 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:26:14 INFO BlockManagerInfo: Removed broadcast_472_piece0 on spark-master:33535 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:26:14 INFO BlockManagerInfo: Removed broadcast_472_piece0 on 172.18.0.13:44401 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:26:14 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:26:14 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:26:14 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:26:14 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:26:14 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#127711 - airline_prefix.nullCount#127710) > 0)
26/01/04 17:26:14 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#127746 - min_flight_num.nullCount#127745) > 0)
26/01/04 17:26:14 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#127741 - max_flight_num.nullCount#127740) > 0)
26/01/04 17:26:14 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:26:14 INFO DAGScheduler: Got job 317 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 17:26:14 INFO DAGScheduler: Final stage: ResultStage 318 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:26:14 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:26:14 INFO DAGScheduler: Missing parents: List()
26/01/04 17:26:14 INFO DAGScheduler: Submitting ResultStage 318 (MapPartitionsRDD[1593] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:26:14 INFO MemoryStore: Block broadcast_474 stored as values in memory (estimated size 38.5 KiB, free 434.4 MiB)
26/01/04 17:26:14 INFO MemoryStore: Block broadcast_474_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.3 MiB)
26/01/04 17:26:14 INFO BlockManagerInfo: Added broadcast_474_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:26:14 INFO SparkContext: Created broadcast 474 from broadcast at DAGScheduler.scala:1585
26/01/04 17:26:14 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 318 (MapPartitionsRDD[1593] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 17:26:14 INFO TaskSchedulerImpl: Adding task set 318.0 with 2 tasks resource profile 0
26/01/04 17:26:14 INFO TaskSetManager: Starting task 0.0 in stage 318.0 (TID 512) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 17:26:14 INFO TaskSetManager: Starting task 1.0 in stage 318.0 (TID 513) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 17:26:14 INFO BlockManagerInfo: Added broadcast_474_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:26:14 INFO BlockManagerInfo: Added broadcast_474_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:26:14 INFO TaskSetManager: Finished task 1.0 in stage 318.0 (TID 513) in 92 ms on 172.18.0.13 (executor 0) (1/2)
26/01/04 17:26:14 INFO TaskSetManager: Finished task 0.0 in stage 318.0 (TID 512) in 260 ms on 172.18.0.14 (executor 1) (2/2)
26/01/04 17:26:14 INFO TaskSchedulerImpl: Removed TaskSet 318.0, whose tasks have all completed, from pool 
26/01/04 17:26:14 INFO DAGScheduler: ResultStage 318 (start at NativeMethodAccessorImpl.java:0) finished in 0.289 s
26/01/04 17:26:14 INFO DAGScheduler: Job 317 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:26:14 INFO TaskSchedulerImpl: Killing all running tasks in stage 318: Stage finished
26/01/04 17:26:14 INFO DAGScheduler: Job 317 finished: start at NativeMethodAccessorImpl.java:0, took 0.292923 s
26/01/04 17:26:14 INFO MemoryStore: Block broadcast_475_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.3 MiB)
26/01/04 17:26:14 INFO BlockManagerInfo: Added broadcast_475_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:26:14 INFO SparkContext: Created broadcast 475 from start at NativeMethodAccessorImpl.java:0
26/01/04 17:26:14 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 157, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@456c21cc]. The input RDD has 1 partitions.
26/01/04 17:26:14 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:26:14 INFO DAGScheduler: Got job 318 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/01/04 17:26:14 INFO DAGScheduler: Final stage: ResultStage 319 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:26:14 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:26:14 INFO DAGScheduler: Missing parents: List()
26/01/04 17:26:14 INFO DAGScheduler: Submitting ResultStage 319 (MapPartitionsRDD[1598] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:26:14 INFO MemoryStore: Block broadcast_476 stored as values in memory (estimated size 52.9 KiB, free 434.3 MiB)
26/01/04 17:26:14 INFO MemoryStore: Block broadcast_476_piece0 stored as bytes in memory (estimated size 20.2 KiB, free 434.3 MiB)
26/01/04 17:26:14 INFO BlockManagerInfo: Added broadcast_476_piece0 in memory on spark-master:33535 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:26:14 INFO SparkContext: Created broadcast 476 from broadcast at DAGScheduler.scala:1585
26/01/04 17:26:14 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 319 (MapPartitionsRDD[1598] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/01/04 17:26:14 INFO TaskSchedulerImpl: Adding task set 319.0 with 1 tasks resource profile 0
26/01/04 17:26:14 INFO TaskSetManager: Starting task 0.0 in stage 319.0 (TID 514) (172.18.0.13, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:26:14 INFO BlockManagerInfo: Added broadcast_476_piece0 in memory on 172.18.0.13:44401 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:26:15 INFO BlockManagerInfo: Added broadcast_475_piece0 in memory on 172.18.0.13:44401 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:26:15 INFO TaskSetManager: Finished task 0.0 in stage 319.0 (TID 514) in 693 ms on 172.18.0.13 (executor 0) (1/1)
26/01/04 17:26:15 INFO TaskSchedulerImpl: Removed TaskSet 319.0, whose tasks have all completed, from pool 
26/01/04 17:26:15 INFO DAGScheduler: ResultStage 319 (start at NativeMethodAccessorImpl.java:0) finished in 0.712 s
26/01/04 17:26:15 INFO DAGScheduler: Job 318 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:26:15 INFO TaskSchedulerImpl: Killing all running tasks in stage 319: Stage finished
26/01/04 17:26:15 INFO DAGScheduler: Job 318 finished: start at NativeMethodAccessorImpl.java:0, took 0.719523 s
26/01/04 17:26:15 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 157, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@456c21cc] is committing.
26/01/04 17:26:15 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 157, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@456c21cc] committed.
26/01/04 17:26:15 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/157 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.157.1f84a251-2161-4dea-90b7-948414534f32.tmp
26/01/04 17:26:15 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.157.1f84a251-2161-4dea-90b7-948414534f32.tmp to file:/tmp/spark-checkpoint-enrichment/commits/157
26/01/04 17:26:15 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T17:26:14.042Z",
  "batchId" : 157,
  "numInputRows" : 30,
  "inputRowsPerSecond" : 2000.0,
  "processedRowsPerSecond" : 16.547159404302263,
  "durationMs" : {
    "addBatch" : 1221,
    "commitOffsets" : 223,
    "getBatch" : 1,
    "latestOffset" : 3,
    "queryPlanning" : 57,
    "triggerExecution" : 1813,
    "walCommit" : 307
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "0" : 210
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "0" : 240
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "0" : 240
      }
    },
    "numInputRows" : 30,
    "inputRowsPerSecond" : 2000.0,
    "processedRowsPerSecond" : 16.547159404302263,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 31
  }
}
26/01/04 17:26:25 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/158 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.158.08822349-dddb-44ed-8b77-d1822066196b.tmp
26/01/04 17:26:25 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.158.08822349-dddb-44ed-8b77-d1822066196b.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/158
26/01/04 17:26:25 INFO MicroBatchExecution: Committed offsets for batch 158. Metadata OffsetSeqMetadata(0,1767547585060,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 17:26:25 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:26:25 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:26:25 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:26:25 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:26:25 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:26:25 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:26:25 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#128515 - airline_prefix.nullCount#128514) > 0)
26/01/04 17:26:25 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#128550 - min_flight_num.nullCount#128549) > 0)
26/01/04 17:26:25 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#128545 - max_flight_num.nullCount#128544) > 0)
26/01/04 17:26:25 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:26:25 INFO DAGScheduler: Got job 319 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 17:26:25 INFO DAGScheduler: Final stage: ResultStage 320 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:26:25 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:26:25 INFO DAGScheduler: Missing parents: List()
26/01/04 17:26:25 INFO DAGScheduler: Submitting ResultStage 320 (MapPartitionsRDD[1603] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:26:25 INFO MemoryStore: Block broadcast_477 stored as values in memory (estimated size 38.5 KiB, free 434.2 MiB)
26/01/04 17:26:25 INFO MemoryStore: Block broadcast_477_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.2 MiB)
26/01/04 17:26:25 INFO BlockManagerInfo: Added broadcast_477_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:26:25 INFO SparkContext: Created broadcast 477 from broadcast at DAGScheduler.scala:1585
26/01/04 17:26:25 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 320 (MapPartitionsRDD[1603] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 17:26:25 INFO TaskSchedulerImpl: Adding task set 320.0 with 2 tasks resource profile 0
26/01/04 17:26:25 INFO TaskSetManager: Starting task 1.0 in stage 320.0 (TID 515) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 17:26:25 INFO TaskSetManager: Starting task 0.0 in stage 320.0 (TID 516) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 17:26:25 INFO BlockManagerInfo: Added broadcast_477_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:26:25 INFO BlockManagerInfo: Added broadcast_477_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:26:25 INFO TaskSetManager: Finished task 1.0 in stage 320.0 (TID 515) in 45 ms on 172.18.0.13 (executor 0) (1/2)
26/01/04 17:26:25 INFO TaskSetManager: Finished task 0.0 in stage 320.0 (TID 516) in 83 ms on 172.18.0.14 (executor 1) (2/2)
26/01/04 17:26:25 INFO TaskSchedulerImpl: Removed TaskSet 320.0, whose tasks have all completed, from pool 
26/01/04 17:26:25 INFO DAGScheduler: ResultStage 320 (start at NativeMethodAccessorImpl.java:0) finished in 0.102 s
26/01/04 17:26:25 INFO DAGScheduler: Job 319 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:26:25 INFO TaskSchedulerImpl: Killing all running tasks in stage 320: Stage finished
26/01/04 17:26:25 INFO DAGScheduler: Job 319 finished: start at NativeMethodAccessorImpl.java:0, took 0.108540 s
26/01/04 17:26:25 INFO MemoryStore: Block broadcast_478_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.2 MiB)
26/01/04 17:26:25 INFO BlockManagerInfo: Added broadcast_478_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:26:25 INFO SparkContext: Created broadcast 478 from start at NativeMethodAccessorImpl.java:0
26/01/04 17:26:25 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 158, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@2212880c]. The input RDD has 1 partitions.
26/01/04 17:26:25 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:26:25 INFO DAGScheduler: Got job 320 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/01/04 17:26:25 INFO DAGScheduler: Final stage: ResultStage 321 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:26:25 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:26:25 INFO DAGScheduler: Missing parents: List()
26/01/04 17:26:25 INFO DAGScheduler: Submitting ResultStage 321 (MapPartitionsRDD[1608] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:26:25 INFO MemoryStore: Block broadcast_479 stored as values in memory (estimated size 52.9 KiB, free 434.2 MiB)
26/01/04 17:26:25 INFO MemoryStore: Block broadcast_479_piece0 stored as bytes in memory (estimated size 20.2 KiB, free 434.1 MiB)
26/01/04 17:26:25 INFO BlockManagerInfo: Added broadcast_479_piece0 in memory on spark-master:33535 (size: 20.2 KiB, free: 434.3 MiB)
26/01/04 17:26:25 INFO SparkContext: Created broadcast 479 from broadcast at DAGScheduler.scala:1585
26/01/04 17:26:25 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 321 (MapPartitionsRDD[1608] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/01/04 17:26:25 INFO TaskSchedulerImpl: Adding task set 321.0 with 1 tasks resource profile 0
26/01/04 17:26:25 INFO TaskSetManager: Starting task 0.0 in stage 321.0 (TID 517) (172.18.0.13, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:26:25 INFO BlockManagerInfo: Added broadcast_479_piece0 in memory on 172.18.0.13:44401 (size: 20.2 KiB, free: 434.3 MiB)
26/01/04 17:26:25 INFO BlockManagerInfo: Added broadcast_478_piece0 in memory on 172.18.0.13:44401 (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:26:26 INFO TaskSetManager: Finished task 0.0 in stage 321.0 (TID 517) in 666 ms on 172.18.0.13 (executor 0) (1/1)
26/01/04 17:26:26 INFO TaskSchedulerImpl: Removed TaskSet 321.0, whose tasks have all completed, from pool 
26/01/04 17:26:26 INFO DAGScheduler: ResultStage 321 (start at NativeMethodAccessorImpl.java:0) finished in 0.676 s
26/01/04 17:26:26 INFO DAGScheduler: Job 320 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:26:26 INFO TaskSchedulerImpl: Killing all running tasks in stage 321: Stage finished
26/01/04 17:26:26 INFO DAGScheduler: Job 320 finished: start at NativeMethodAccessorImpl.java:0, took 0.682645 s
26/01/04 17:26:26 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 158, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@2212880c] is committing.
26/01/04 17:26:26 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 158, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@2212880c] committed.
26/01/04 17:26:26 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/158 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.158.13f8243c-fe56-47fa-ad52-3cbc292d4d69.tmp
26/01/04 17:26:26 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.158.13f8243c-fe56-47fa-ad52-3cbc292d4d69.tmp to file:/tmp/spark-checkpoint-enrichment/commits/158
26/01/04 17:26:26 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T17:26:25.052Z",
  "batchId" : 158,
  "numInputRows" : 30,
  "inputRowsPerSecond" : 2307.6923076923076,
  "processedRowsPerSecond" : 23.80952380952381,
  "durationMs" : {
    "addBatch" : 933,
    "commitOffsets" : 143,
    "getBatch" : 0,
    "latestOffset" : 8,
    "queryPlanning" : 40,
    "triggerExecution" : 1260,
    "walCommit" : 132
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "0" : 240
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "0" : 270
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "0" : 270
      }
    },
    "numInputRows" : 30,
    "inputRowsPerSecond" : 2307.6923076923076,
    "processedRowsPerSecond" : 23.80952380952381,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 31
  }
}
26/01/04 17:26:32 INFO BlockManagerInfo: Removed broadcast_479_piece0 on spark-master:33535 in memory (size: 20.2 KiB, free: 434.3 MiB)
26/01/04 17:26:32 INFO BlockManagerInfo: Removed broadcast_479_piece0 on 172.18.0.13:44401 in memory (size: 20.2 KiB, free: 434.3 MiB)
26/01/04 17:26:32 INFO BlockManagerInfo: Removed broadcast_474_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:26:32 INFO BlockManagerInfo: Removed broadcast_474_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:26:32 INFO BlockManagerInfo: Removed broadcast_474_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:26:32 INFO BlockManagerInfo: Removed broadcast_476_piece0 on spark-master:33535 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:26:32 INFO BlockManagerInfo: Removed broadcast_476_piece0 on 172.18.0.13:44401 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:26:32 INFO BlockManagerInfo: Removed broadcast_477_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:26:32 INFO BlockManagerInfo: Removed broadcast_477_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:26:32 INFO BlockManagerInfo: Removed broadcast_477_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:26:32 INFO BlockManagerInfo: Removed broadcast_475_piece0 on spark-master:33535 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:26:32 INFO BlockManagerInfo: Removed broadcast_475_piece0 on 172.18.0.13:44401 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:26:36 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/159 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.159.29a2f2ae-b3a1-49c5-a701-95c378e5bc7e.tmp
26/01/04 17:26:36 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.159.29a2f2ae-b3a1-49c5-a701-95c378e5bc7e.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/159
26/01/04 17:26:36 INFO MicroBatchExecution: Committed offsets for batch 159. Metadata OffsetSeqMetadata(0,1767547596080,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 17:26:36 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:26:36 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:26:36 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:26:36 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:26:36 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:26:36 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:26:36 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#129319 - airline_prefix.nullCount#129318) > 0)
26/01/04 17:26:36 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#129354 - min_flight_num.nullCount#129353) > 0)
26/01/04 17:26:36 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#129349 - max_flight_num.nullCount#129348) > 0)
26/01/04 17:26:36 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:26:36 INFO DAGScheduler: Got job 321 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 17:26:36 INFO DAGScheduler: Final stage: ResultStage 322 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:26:36 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:26:36 INFO DAGScheduler: Missing parents: List()
26/01/04 17:26:36 INFO DAGScheduler: Submitting ResultStage 322 (MapPartitionsRDD[1613] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:26:36 INFO MemoryStore: Block broadcast_480 stored as values in memory (estimated size 38.5 KiB, free 434.4 MiB)
26/01/04 17:26:36 INFO MemoryStore: Block broadcast_480_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.3 MiB)
26/01/04 17:26:36 INFO BlockManagerInfo: Added broadcast_480_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:26:36 INFO SparkContext: Created broadcast 480 from broadcast at DAGScheduler.scala:1585
26/01/04 17:26:36 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 322 (MapPartitionsRDD[1613] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 17:26:36 INFO TaskSchedulerImpl: Adding task set 322.0 with 2 tasks resource profile 0
26/01/04 17:26:36 INFO BlockManagerInfo: Removed broadcast_478_piece0 on spark-master:33535 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:26:36 INFO TaskSetManager: Starting task 0.0 in stage 322.0 (TID 518) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 17:26:36 INFO TaskSetManager: Starting task 1.0 in stage 322.0 (TID 519) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 17:26:36 INFO BlockManagerInfo: Removed broadcast_478_piece0 on 172.18.0.13:44401 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:26:36 INFO BlockManagerInfo: Added broadcast_480_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:26:36 INFO TaskSetManager: Finished task 1.0 in stage 322.0 (TID 519) in 42 ms on 172.18.0.13 (executor 0) (1/2)
26/01/04 17:26:36 INFO BlockManagerInfo: Added broadcast_480_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:26:36 INFO TaskSetManager: Finished task 0.0 in stage 322.0 (TID 518) in 192 ms on 172.18.0.14 (executor 1) (2/2)
26/01/04 17:26:36 INFO TaskSchedulerImpl: Removed TaskSet 322.0, whose tasks have all completed, from pool 
26/01/04 17:26:36 INFO DAGScheduler: ResultStage 322 (start at NativeMethodAccessorImpl.java:0) finished in 0.221 s
26/01/04 17:26:36 INFO DAGScheduler: Job 321 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:26:36 INFO TaskSchedulerImpl: Killing all running tasks in stage 322: Stage finished
26/01/04 17:26:36 INFO DAGScheduler: Job 321 finished: start at NativeMethodAccessorImpl.java:0, took 0.225734 s
26/01/04 17:26:36 INFO MemoryStore: Block broadcast_481_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.3 MiB)
26/01/04 17:26:36 INFO BlockManagerInfo: Added broadcast_481_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:26:36 INFO SparkContext: Created broadcast 481 from start at NativeMethodAccessorImpl.java:0
26/01/04 17:26:36 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 159, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@7b2daaf8]. The input RDD has 1 partitions.
26/01/04 17:26:36 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:26:36 INFO DAGScheduler: Got job 322 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/01/04 17:26:36 INFO DAGScheduler: Final stage: ResultStage 323 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:26:36 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:26:36 INFO DAGScheduler: Missing parents: List()
26/01/04 17:26:36 INFO DAGScheduler: Submitting ResultStage 323 (MapPartitionsRDD[1618] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:26:36 INFO MemoryStore: Block broadcast_482 stored as values in memory (estimated size 52.9 KiB, free 434.3 MiB)
26/01/04 17:26:36 INFO MemoryStore: Block broadcast_482_piece0 stored as bytes in memory (estimated size 20.2 KiB, free 434.3 MiB)
26/01/04 17:26:36 INFO BlockManagerInfo: Added broadcast_482_piece0 in memory on spark-master:33535 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:26:36 INFO SparkContext: Created broadcast 482 from broadcast at DAGScheduler.scala:1585
26/01/04 17:26:36 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 323 (MapPartitionsRDD[1618] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/01/04 17:26:36 INFO TaskSchedulerImpl: Adding task set 323.0 with 1 tasks resource profile 0
26/01/04 17:26:36 INFO TaskSetManager: Starting task 0.0 in stage 323.0 (TID 520) (172.18.0.13, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:26:36 INFO BlockManagerInfo: Added broadcast_482_piece0 in memory on 172.18.0.13:44401 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:26:36 INFO BlockManagerInfo: Added broadcast_481_piece0 in memory on 172.18.0.13:44401 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:26:37 INFO TaskSetManager: Finished task 0.0 in stage 323.0 (TID 520) in 663 ms on 172.18.0.13 (executor 0) (1/1)
26/01/04 17:26:37 INFO DAGScheduler: ResultStage 323 (start at NativeMethodAccessorImpl.java:0) finished in 0.673 s
26/01/04 17:26:37 INFO DAGScheduler: Job 322 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:26:37 INFO TaskSchedulerImpl: Removed TaskSet 323.0, whose tasks have all completed, from pool 
26/01/04 17:26:37 INFO TaskSchedulerImpl: Killing all running tasks in stage 323: Stage finished
26/01/04 17:26:37 INFO DAGScheduler: Job 322 finished: start at NativeMethodAccessorImpl.java:0, took 0.677246 s
26/01/04 17:26:37 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 159, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@7b2daaf8] is committing.
26/01/04 17:26:37 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 159, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@7b2daaf8] committed.
26/01/04 17:26:37 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/159 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.159.e1415f4d-a999-4f05-8fa6-2d73a4fa2f2e.tmp
26/01/04 17:26:37 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.159.e1415f4d-a999-4f05-8fa6-2d73a4fa2f2e.tmp to file:/tmp/spark-checkpoint-enrichment/commits/159
26/01/04 17:26:37 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T17:26:36.076Z",
  "batchId" : 159,
  "numInputRows" : 22,
  "inputRowsPerSecond" : 1157.8947368421052,
  "processedRowsPerSecond" : 15.341701534170154,
  "durationMs" : {
    "addBatch" : 1087,
    "commitOffsets" : 143,
    "getBatch" : 0,
    "latestOffset" : 4,
    "queryPlanning" : 52,
    "triggerExecution" : 1434,
    "walCommit" : 148
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "0" : 270
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "0" : 292
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "0" : 292
      }
    },
    "numInputRows" : 22,
    "inputRowsPerSecond" : 1157.8947368421052,
    "processedRowsPerSecond" : 15.341701534170154,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 23
  }
}
26/01/04 17:26:37 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/160 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.160.c4fbe699-6dc0-49fd-8e51-07805188d5a2.tmp
26/01/04 17:26:37 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.160.c4fbe699-6dc0-49fd-8e51-07805188d5a2.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/160
26/01/04 17:26:37 INFO MicroBatchExecution: Committed offsets for batch 160. Metadata OffsetSeqMetadata(0,1767547597517,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 17:26:37 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:26:37 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:26:37 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:26:37 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:26:37 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:26:37 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:26:37 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#130123 - airline_prefix.nullCount#130122) > 0)
26/01/04 17:26:37 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#130158 - min_flight_num.nullCount#130157) > 0)
26/01/04 17:26:37 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#130153 - max_flight_num.nullCount#130152) > 0)
26/01/04 17:26:37 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:26:37 INFO DAGScheduler: Got job 323 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 17:26:37 INFO DAGScheduler: Final stage: ResultStage 324 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:26:37 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:26:37 INFO DAGScheduler: Missing parents: List()
26/01/04 17:26:37 INFO DAGScheduler: Submitting ResultStage 324 (MapPartitionsRDD[1623] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:26:37 INFO MemoryStore: Block broadcast_483 stored as values in memory (estimated size 38.5 KiB, free 434.2 MiB)
26/01/04 17:26:37 INFO MemoryStore: Block broadcast_483_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.2 MiB)
26/01/04 17:26:37 INFO BlockManagerInfo: Added broadcast_483_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:26:37 INFO SparkContext: Created broadcast 483 from broadcast at DAGScheduler.scala:1585
26/01/04 17:26:37 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 324 (MapPartitionsRDD[1623] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 17:26:37 INFO TaskSchedulerImpl: Adding task set 324.0 with 2 tasks resource profile 0
26/01/04 17:26:37 INFO TaskSetManager: Starting task 1.0 in stage 324.0 (TID 521) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 17:26:37 INFO TaskSetManager: Starting task 0.0 in stage 324.0 (TID 522) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 17:26:37 INFO BlockManagerInfo: Added broadcast_483_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:26:37 INFO BlockManagerInfo: Added broadcast_483_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:26:37 INFO TaskSetManager: Finished task 1.0 in stage 324.0 (TID 521) in 71 ms on 172.18.0.13 (executor 0) (1/2)
26/01/04 17:26:37 INFO TaskSetManager: Finished task 0.0 in stage 324.0 (TID 522) in 103 ms on 172.18.0.14 (executor 1) (2/2)
26/01/04 17:26:37 INFO TaskSchedulerImpl: Removed TaskSet 324.0, whose tasks have all completed, from pool 
26/01/04 17:26:37 INFO DAGScheduler: ResultStage 324 (start at NativeMethodAccessorImpl.java:0) finished in 0.120 s
26/01/04 17:26:37 INFO DAGScheduler: Job 323 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:26:37 INFO TaskSchedulerImpl: Killing all running tasks in stage 324: Stage finished
26/01/04 17:26:37 INFO DAGScheduler: Job 323 finished: start at NativeMethodAccessorImpl.java:0, took 0.124702 s
26/01/04 17:26:38 INFO MemoryStore: Block broadcast_484_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.2 MiB)
26/01/04 17:26:38 INFO BlockManagerInfo: Added broadcast_484_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:26:38 INFO SparkContext: Created broadcast 484 from start at NativeMethodAccessorImpl.java:0
26/01/04 17:26:38 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 160, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@2e4c6ee4]. The input RDD has 1 partitions.
26/01/04 17:26:38 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:26:38 INFO DAGScheduler: Got job 324 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/01/04 17:26:38 INFO DAGScheduler: Final stage: ResultStage 325 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:26:38 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:26:38 INFO DAGScheduler: Missing parents: List()
26/01/04 17:26:38 INFO DAGScheduler: Submitting ResultStage 325 (MapPartitionsRDD[1628] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:26:38 INFO MemoryStore: Block broadcast_485 stored as values in memory (estimated size 52.9 KiB, free 434.2 MiB)
26/01/04 17:26:38 INFO MemoryStore: Block broadcast_485_piece0 stored as bytes in memory (estimated size 20.2 KiB, free 434.1 MiB)
26/01/04 17:26:38 INFO BlockManagerInfo: Added broadcast_485_piece0 in memory on spark-master:33535 (size: 20.2 KiB, free: 434.3 MiB)
26/01/04 17:26:38 INFO SparkContext: Created broadcast 485 from broadcast at DAGScheduler.scala:1585
26/01/04 17:26:38 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 325 (MapPartitionsRDD[1628] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/01/04 17:26:38 INFO TaskSchedulerImpl: Adding task set 325.0 with 1 tasks resource profile 0
26/01/04 17:26:38 INFO TaskSetManager: Starting task 0.0 in stage 325.0 (TID 523) (172.18.0.13, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:26:38 INFO BlockManagerInfo: Added broadcast_485_piece0 in memory on 172.18.0.13:44401 (size: 20.2 KiB, free: 434.3 MiB)
26/01/04 17:26:38 INFO BlockManagerInfo: Added broadcast_484_piece0 in memory on 172.18.0.13:44401 (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:26:38 INFO TaskSetManager: Finished task 0.0 in stage 325.0 (TID 523) in 149 ms on 172.18.0.13 (executor 0) (1/1)
26/01/04 17:26:38 INFO TaskSchedulerImpl: Removed TaskSet 325.0, whose tasks have all completed, from pool 
26/01/04 17:26:38 INFO DAGScheduler: ResultStage 325 (start at NativeMethodAccessorImpl.java:0) finished in 0.161 s
26/01/04 17:26:38 INFO DAGScheduler: Job 324 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:26:38 INFO TaskSchedulerImpl: Killing all running tasks in stage 325: Stage finished
26/01/04 17:26:38 INFO DAGScheduler: Job 324 finished: start at NativeMethodAccessorImpl.java:0, took 0.165429 s
26/01/04 17:26:38 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 160, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@2e4c6ee4] is committing.
26/01/04 17:26:38 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 160, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@2e4c6ee4] committed.
26/01/04 17:26:38 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/160 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.160.18df113a-a29e-47e5-abdd-5f1db5c08663.tmp
26/01/04 17:26:38 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.160.18df113a-a29e-47e5-abdd-5f1db5c08663.tmp to file:/tmp/spark-checkpoint-enrichment/commits/160
26/01/04 17:26:38 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T17:26:37.513Z",
  "batchId" : 160,
  "numInputRows" : 8,
  "inputRowsPerSecond" : 5.567153792623521,
  "processedRowsPerSecond" : 9.59232613908873,
  "durationMs" : {
    "addBatch" : 445,
    "commitOffsets" : 161,
    "getBatch" : 0,
    "latestOffset" : 4,
    "queryPlanning" : 63,
    "triggerExecution" : 834,
    "walCommit" : 160
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "0" : 292
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "0" : 300
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "0" : 300
      }
    },
    "numInputRows" : 8,
    "inputRowsPerSecond" : 5.567153792623521,
    "processedRowsPerSecond" : 9.59232613908873,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 8
  }
}
26/01/04 17:26:47 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/161 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.161.7131d8fb-c3c9-4d2f-ac1f-899180c4b849.tmp
26/01/04 17:26:47 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.161.7131d8fb-c3c9-4d2f-ac1f-899180c4b849.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/161
26/01/04 17:26:47 INFO MicroBatchExecution: Committed offsets for batch 161. Metadata OffsetSeqMetadata(0,1767547607115,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 17:26:47 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:26:47 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:26:47 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:26:47 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:26:47 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:26:47 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:26:47 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#130927 - airline_prefix.nullCount#130926) > 0)
26/01/04 17:26:47 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#130962 - min_flight_num.nullCount#130961) > 0)
26/01/04 17:26:47 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#130957 - max_flight_num.nullCount#130956) > 0)
26/01/04 17:26:47 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:26:47 INFO DAGScheduler: Got job 325 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 17:26:47 INFO DAGScheduler: Final stage: ResultStage 326 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:26:47 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:26:47 INFO DAGScheduler: Missing parents: List()
26/01/04 17:26:47 INFO DAGScheduler: Submitting ResultStage 326 (MapPartitionsRDD[1633] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:26:47 INFO MemoryStore: Block broadcast_486 stored as values in memory (estimated size 38.5 KiB, free 434.1 MiB)
26/01/04 17:26:47 INFO MemoryStore: Block broadcast_486_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.1 MiB)
26/01/04 17:26:47 INFO BlockManagerInfo: Removed broadcast_480_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:26:47 INFO BlockManagerInfo: Added broadcast_486_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:26:47 INFO SparkContext: Created broadcast 486 from broadcast at DAGScheduler.scala:1585
26/01/04 17:26:47 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 326 (MapPartitionsRDD[1633] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 17:26:47 INFO TaskSchedulerImpl: Adding task set 326.0 with 2 tasks resource profile 0
26/01/04 17:26:47 INFO BlockManagerInfo: Removed broadcast_480_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:26:47 INFO TaskSetManager: Starting task 0.0 in stage 326.0 (TID 524) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 17:26:47 INFO TaskSetManager: Starting task 1.0 in stage 326.0 (TID 525) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 17:26:47 INFO BlockManagerInfo: Removed broadcast_480_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:26:47 INFO BlockManagerInfo: Removed broadcast_485_piece0 on spark-master:33535 in memory (size: 20.2 KiB, free: 434.3 MiB)
26/01/04 17:26:47 INFO BlockManagerInfo: Added broadcast_486_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:26:47 INFO BlockManagerInfo: Removed broadcast_485_piece0 on 172.18.0.13:44401 in memory (size: 20.2 KiB, free: 434.3 MiB)
26/01/04 17:26:47 INFO BlockManagerInfo: Removed broadcast_482_piece0 on spark-master:33535 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:26:47 INFO BlockManagerInfo: Removed broadcast_482_piece0 on 172.18.0.13:44401 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:26:47 INFO BlockManagerInfo: Added broadcast_486_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:26:47 INFO TaskSetManager: Finished task 1.0 in stage 326.0 (TID 525) in 81 ms on 172.18.0.13 (executor 0) (1/2)
26/01/04 17:26:47 INFO BlockManagerInfo: Removed broadcast_483_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:26:47 INFO BlockManagerInfo: Removed broadcast_483_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:26:47 INFO BlockManagerInfo: Removed broadcast_483_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:26:47 INFO BlockManagerInfo: Removed broadcast_481_piece0 on spark-master:33535 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:26:47 INFO BlockManagerInfo: Removed broadcast_481_piece0 on 172.18.0.13:44401 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:26:47 INFO BlockManagerInfo: Removed broadcast_484_piece0 on spark-master:33535 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:26:47 INFO BlockManagerInfo: Removed broadcast_484_piece0 on 172.18.0.13:44401 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:26:47 INFO TaskSetManager: Finished task 0.0 in stage 326.0 (TID 524) in 205 ms on 172.18.0.14 (executor 1) (2/2)
26/01/04 17:26:47 INFO TaskSchedulerImpl: Removed TaskSet 326.0, whose tasks have all completed, from pool 
26/01/04 17:26:47 INFO DAGScheduler: ResultStage 326 (start at NativeMethodAccessorImpl.java:0) finished in 0.243 s
26/01/04 17:26:47 INFO DAGScheduler: Job 325 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:26:47 INFO TaskSchedulerImpl: Killing all running tasks in stage 326: Stage finished
26/01/04 17:26:47 INFO DAGScheduler: Job 325 finished: start at NativeMethodAccessorImpl.java:0, took 0.254250 s
26/01/04 17:26:47 INFO MemoryStore: Block broadcast_487_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.3 MiB)
26/01/04 17:26:47 INFO BlockManagerInfo: Added broadcast_487_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:26:47 INFO SparkContext: Created broadcast 487 from start at NativeMethodAccessorImpl.java:0
26/01/04 17:26:47 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 161, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@4963e7c6]. The input RDD has 1 partitions.
26/01/04 17:26:47 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:26:47 INFO DAGScheduler: Got job 326 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/01/04 17:26:47 INFO DAGScheduler: Final stage: ResultStage 327 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:26:47 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:26:47 INFO DAGScheduler: Missing parents: List()
26/01/04 17:26:47 INFO DAGScheduler: Submitting ResultStage 327 (MapPartitionsRDD[1638] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:26:47 INFO MemoryStore: Block broadcast_488 stored as values in memory (estimated size 52.9 KiB, free 434.3 MiB)
26/01/04 17:26:47 INFO MemoryStore: Block broadcast_488_piece0 stored as bytes in memory (estimated size 20.2 KiB, free 434.3 MiB)
26/01/04 17:26:47 INFO BlockManagerInfo: Added broadcast_488_piece0 in memory on spark-master:33535 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:26:47 INFO SparkContext: Created broadcast 488 from broadcast at DAGScheduler.scala:1585
26/01/04 17:26:47 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 327 (MapPartitionsRDD[1638] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/01/04 17:26:47 INFO TaskSchedulerImpl: Adding task set 327.0 with 1 tasks resource profile 0
26/01/04 17:26:47 INFO TaskSetManager: Starting task 0.0 in stage 327.0 (TID 526) (172.18.0.13, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:26:48 INFO BlockManagerInfo: Added broadcast_488_piece0 in memory on 172.18.0.13:44401 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:26:48 INFO BlockManagerInfo: Added broadcast_487_piece0 in memory on 172.18.0.13:44401 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:26:48 INFO TaskSetManager: Finished task 0.0 in stage 327.0 (TID 526) in 628 ms on 172.18.0.13 (executor 0) (1/1)
26/01/04 17:26:48 INFO TaskSchedulerImpl: Removed TaskSet 327.0, whose tasks have all completed, from pool 
26/01/04 17:26:48 INFO DAGScheduler: ResultStage 327 (start at NativeMethodAccessorImpl.java:0) finished in 0.657 s
26/01/04 17:26:48 INFO DAGScheduler: Job 326 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:26:48 INFO TaskSchedulerImpl: Killing all running tasks in stage 327: Stage finished
26/01/04 17:26:48 INFO DAGScheduler: Job 326 finished: start at NativeMethodAccessorImpl.java:0, took 0.660040 s
26/01/04 17:26:48 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 161, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@4963e7c6] is committing.
26/01/04 17:26:48 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 161, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@4963e7c6] committed.
26/01/04 17:26:48 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/161 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.161.f311924c-0c11-41c1-9ac4-f458d1db8fd7.tmp
26/01/04 17:26:48 INFO BlockManagerInfo: Removed broadcast_488_piece0 on 172.18.0.13:44401 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:26:48 INFO BlockManagerInfo: Removed broadcast_488_piece0 on spark-master:33535 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:26:48 INFO BlockManagerInfo: Removed broadcast_486_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:26:48 INFO BlockManagerInfo: Removed broadcast_486_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:26:48 INFO BlockManagerInfo: Removed broadcast_486_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:26:48 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.161.f311924c-0c11-41c1-9ac4-f458d1db8fd7.tmp to file:/tmp/spark-checkpoint-enrichment/commits/161
26/01/04 17:26:48 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T17:26:47.112Z",
  "batchId" : 161,
  "numInputRows" : 30,
  "inputRowsPerSecond" : 2000.0,
  "processedRowsPerSecond" : 16.882386043894204,
  "durationMs" : {
    "addBatch" : 1237,
    "commitOffsets" : 282,
    "getBatch" : 0,
    "latestOffset" : 3,
    "queryPlanning" : 66,
    "triggerExecution" : 1777,
    "walCommit" : 184
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "0" : 300
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "0" : 330
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "0" : 330
      }
    },
    "numInputRows" : 30,
    "inputRowsPerSecond" : 2000.0,
    "processedRowsPerSecond" : 16.882386043894204,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 31
  }
}
26/01/04 17:26:58 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/162 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.162.7168fce9-78c8-42cc-a12e-368c3026b5e2.tmp
26/01/04 17:26:58 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.162.7168fce9-78c8-42cc-a12e-368c3026b5e2.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/162
26/01/04 17:26:58 INFO MicroBatchExecution: Committed offsets for batch 162. Metadata OffsetSeqMetadata(0,1767547618257,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 17:26:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:26:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:26:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:26:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:26:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:26:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:26:58 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#131731 - airline_prefix.nullCount#131730) > 0)
26/01/04 17:26:58 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#131766 - min_flight_num.nullCount#131765) > 0)
26/01/04 17:26:58 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#131761 - max_flight_num.nullCount#131760) > 0)
26/01/04 17:26:58 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:26:58 INFO DAGScheduler: Got job 327 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 17:26:58 INFO DAGScheduler: Final stage: ResultStage 328 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:26:58 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:26:58 INFO DAGScheduler: Missing parents: List()
26/01/04 17:26:58 INFO DAGScheduler: Submitting ResultStage 328 (MapPartitionsRDD[1643] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:26:58 INFO MemoryStore: Block broadcast_489 stored as values in memory (estimated size 38.5 KiB, free 434.4 MiB)
26/01/04 17:26:58 INFO MemoryStore: Block broadcast_489_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.3 MiB)
26/01/04 17:26:58 INFO BlockManagerInfo: Added broadcast_489_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:26:58 INFO SparkContext: Created broadcast 489 from broadcast at DAGScheduler.scala:1585
26/01/04 17:26:58 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 328 (MapPartitionsRDD[1643] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 17:26:58 INFO TaskSchedulerImpl: Adding task set 328.0 with 2 tasks resource profile 0
26/01/04 17:26:58 INFO TaskSetManager: Starting task 0.0 in stage 328.0 (TID 527) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 17:26:58 INFO TaskSetManager: Starting task 1.0 in stage 328.0 (TID 528) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 17:26:58 INFO BlockManagerInfo: Added broadcast_489_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:26:58 INFO BlockManagerInfo: Added broadcast_489_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:26:58 INFO TaskSetManager: Finished task 0.0 in stage 328.0 (TID 527) in 187 ms on 172.18.0.14 (executor 1) (1/2)
26/01/04 17:26:59 INFO TaskSetManager: Finished task 1.0 in stage 328.0 (TID 528) in 188 ms on 172.18.0.13 (executor 0) (2/2)
26/01/04 17:26:59 INFO TaskSchedulerImpl: Removed TaskSet 328.0, whose tasks have all completed, from pool 
26/01/04 17:26:59 INFO DAGScheduler: ResultStage 328 (start at NativeMethodAccessorImpl.java:0) finished in 0.226 s
26/01/04 17:26:59 INFO DAGScheduler: Job 327 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:26:59 INFO TaskSchedulerImpl: Killing all running tasks in stage 328: Stage finished
26/01/04 17:26:59 INFO DAGScheduler: Job 327 finished: start at NativeMethodAccessorImpl.java:0, took 0.232543 s
26/01/04 17:26:59 INFO MemoryStore: Block broadcast_490_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.3 MiB)
26/01/04 17:26:59 INFO BlockManagerInfo: Added broadcast_490_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:26:59 INFO SparkContext: Created broadcast 490 from start at NativeMethodAccessorImpl.java:0
26/01/04 17:26:59 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 162, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@16e6ba4e]. The input RDD has 1 partitions.
26/01/04 17:26:59 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:26:59 INFO DAGScheduler: Got job 328 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/01/04 17:26:59 INFO DAGScheduler: Final stage: ResultStage 329 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:26:59 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:26:59 INFO DAGScheduler: Missing parents: List()
26/01/04 17:26:59 INFO DAGScheduler: Submitting ResultStage 329 (MapPartitionsRDD[1648] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:26:59 INFO MemoryStore: Block broadcast_491 stored as values in memory (estimated size 52.9 KiB, free 434.3 MiB)
26/01/04 17:26:59 INFO MemoryStore: Block broadcast_491_piece0 stored as bytes in memory (estimated size 20.2 KiB, free 434.3 MiB)
26/01/04 17:26:59 INFO BlockManagerInfo: Added broadcast_491_piece0 in memory on spark-master:33535 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:26:59 INFO SparkContext: Created broadcast 491 from broadcast at DAGScheduler.scala:1585
26/01/04 17:26:59 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 329 (MapPartitionsRDD[1648] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/01/04 17:26:59 INFO TaskSchedulerImpl: Adding task set 329.0 with 1 tasks resource profile 0
26/01/04 17:26:59 INFO TaskSetManager: Starting task 0.0 in stage 329.0 (TID 529) (172.18.0.13, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:26:59 INFO BlockManagerInfo: Added broadcast_491_piece0 in memory on 172.18.0.13:44401 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:26:59 INFO BlockManagerInfo: Added broadcast_490_piece0 in memory on 172.18.0.13:44401 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:26:59 INFO TaskSetManager: Finished task 0.0 in stage 329.0 (TID 529) in 671 ms on 172.18.0.13 (executor 0) (1/1)
26/01/04 17:26:59 INFO TaskSchedulerImpl: Removed TaskSet 329.0, whose tasks have all completed, from pool 
26/01/04 17:26:59 INFO DAGScheduler: ResultStage 329 (start at NativeMethodAccessorImpl.java:0) finished in 0.691 s
26/01/04 17:26:59 INFO DAGScheduler: Job 328 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:26:59 INFO TaskSchedulerImpl: Killing all running tasks in stage 329: Stage finished
26/01/04 17:26:59 INFO DAGScheduler: Job 328 finished: start at NativeMethodAccessorImpl.java:0, took 0.699025 s
26/01/04 17:26:59 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 162, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@16e6ba4e] is committing.
26/01/04 17:26:59 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 162, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@16e6ba4e] committed.
26/01/04 17:26:59 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/162 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.162.e01e78a5-5ffe-40e5-bd40-d7983ec854a6.tmp
26/01/04 17:26:59 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.162.e01e78a5-5ffe-40e5-bd40-d7983ec854a6.tmp to file:/tmp/spark-checkpoint-enrichment/commits/162
26/01/04 17:26:59 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T17:26:58.193Z",
  "batchId" : 162,
  "numInputRows" : 30,
  "inputRowsPerSecond" : 285.7142857142857,
  "processedRowsPerSecond" : 17.301038062283737,
  "durationMs" : {
    "addBatch" : 1158,
    "commitOffsets" : 173,
    "getBatch" : 1,
    "latestOffset" : 64,
    "queryPlanning" : 85,
    "triggerExecution" : 1734,
    "walCommit" : 252
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "0" : 330
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "0" : 360
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "0" : 360
      }
    },
    "numInputRows" : 30,
    "inputRowsPerSecond" : 285.7142857142857,
    "processedRowsPerSecond" : 17.301038062283737,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 31
  }
}
26/01/04 17:27:09 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/163 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.163.64c3bbdc-2048-4bd4-9453-1ad7a35efc12.tmp
26/01/04 17:27:09 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.163.64c3bbdc-2048-4bd4-9453-1ad7a35efc12.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/163
26/01/04 17:27:09 INFO MicroBatchExecution: Committed offsets for batch 163. Metadata OffsetSeqMetadata(0,1767547629202,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 17:27:09 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:27:09 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:27:09 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:27:09 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:27:09 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:27:09 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:27:09 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#132535 - airline_prefix.nullCount#132534) > 0)
26/01/04 17:27:09 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#132570 - min_flight_num.nullCount#132569) > 0)
26/01/04 17:27:09 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#132565 - max_flight_num.nullCount#132564) > 0)
26/01/04 17:27:09 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:27:09 INFO DAGScheduler: Got job 329 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 17:27:09 INFO DAGScheduler: Final stage: ResultStage 330 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:27:09 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:27:09 INFO DAGScheduler: Missing parents: List()
26/01/04 17:27:09 INFO DAGScheduler: Submitting ResultStage 330 (MapPartitionsRDD[1653] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:27:09 INFO MemoryStore: Block broadcast_492 stored as values in memory (estimated size 38.5 KiB, free 434.2 MiB)
26/01/04 17:27:09 INFO MemoryStore: Block broadcast_492_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.2 MiB)
26/01/04 17:27:09 INFO BlockManagerInfo: Added broadcast_492_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:27:09 INFO SparkContext: Created broadcast 492 from broadcast at DAGScheduler.scala:1585
26/01/04 17:27:09 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 330 (MapPartitionsRDD[1653] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 17:27:09 INFO TaskSchedulerImpl: Adding task set 330.0 with 2 tasks resource profile 0
26/01/04 17:27:09 INFO TaskSetManager: Starting task 1.0 in stage 330.0 (TID 530) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 17:27:09 INFO TaskSetManager: Starting task 0.0 in stage 330.0 (TID 531) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 17:27:09 INFO BlockManagerInfo: Added broadcast_492_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:27:09 INFO BlockManagerInfo: Added broadcast_492_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:27:09 INFO TaskSetManager: Finished task 1.0 in stage 330.0 (TID 530) in 86 ms on 172.18.0.13 (executor 0) (1/2)
26/01/04 17:27:09 INFO TaskSetManager: Finished task 0.0 in stage 330.0 (TID 531) in 107 ms on 172.18.0.14 (executor 1) (2/2)
26/01/04 17:27:09 INFO TaskSchedulerImpl: Removed TaskSet 330.0, whose tasks have all completed, from pool 
26/01/04 17:27:09 INFO DAGScheduler: ResultStage 330 (start at NativeMethodAccessorImpl.java:0) finished in 0.121 s
26/01/04 17:27:09 INFO DAGScheduler: Job 329 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:27:09 INFO TaskSchedulerImpl: Killing all running tasks in stage 330: Stage finished
26/01/04 17:27:09 INFO DAGScheduler: Job 329 finished: start at NativeMethodAccessorImpl.java:0, took 0.129505 s
26/01/04 17:27:09 INFO MemoryStore: Block broadcast_493_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.2 MiB)
26/01/04 17:27:09 INFO BlockManagerInfo: Added broadcast_493_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:27:09 INFO SparkContext: Created broadcast 493 from start at NativeMethodAccessorImpl.java:0
26/01/04 17:27:09 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 163, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@9bc141e]. The input RDD has 1 partitions.
26/01/04 17:27:09 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:27:09 INFO DAGScheduler: Got job 330 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/01/04 17:27:09 INFO DAGScheduler: Final stage: ResultStage 331 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:27:09 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:27:09 INFO DAGScheduler: Missing parents: List()
26/01/04 17:27:09 INFO DAGScheduler: Submitting ResultStage 331 (MapPartitionsRDD[1658] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:27:09 INFO MemoryStore: Block broadcast_494 stored as values in memory (estimated size 52.9 KiB, free 434.2 MiB)
26/01/04 17:27:09 INFO MemoryStore: Block broadcast_494_piece0 stored as bytes in memory (estimated size 20.2 KiB, free 434.1 MiB)
26/01/04 17:27:09 INFO BlockManagerInfo: Added broadcast_494_piece0 in memory on spark-master:33535 (size: 20.2 KiB, free: 434.3 MiB)
26/01/04 17:27:09 INFO SparkContext: Created broadcast 494 from broadcast at DAGScheduler.scala:1585
26/01/04 17:27:09 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 331 (MapPartitionsRDD[1658] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/01/04 17:27:09 INFO TaskSchedulerImpl: Adding task set 331.0 with 1 tasks resource profile 0
26/01/04 17:27:09 INFO TaskSetManager: Starting task 0.0 in stage 331.0 (TID 532) (172.18.0.13, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:27:09 INFO BlockManagerInfo: Added broadcast_494_piece0 in memory on 172.18.0.13:44401 (size: 20.2 KiB, free: 434.3 MiB)
26/01/04 17:27:09 INFO BlockManagerInfo: Added broadcast_493_piece0 in memory on 172.18.0.13:44401 (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:27:10 INFO TaskSetManager: Finished task 0.0 in stage 331.0 (TID 532) in 610 ms on 172.18.0.13 (executor 0) (1/1)
26/01/04 17:27:10 INFO TaskSchedulerImpl: Removed TaskSet 331.0, whose tasks have all completed, from pool 
26/01/04 17:27:10 INFO DAGScheduler: ResultStage 331 (start at NativeMethodAccessorImpl.java:0) finished in 0.624 s
26/01/04 17:27:10 INFO DAGScheduler: Job 330 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:27:10 INFO TaskSchedulerImpl: Killing all running tasks in stage 331: Stage finished
26/01/04 17:27:10 INFO DAGScheduler: Job 330 finished: start at NativeMethodAccessorImpl.java:0, took 0.627700 s
26/01/04 17:27:10 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 163, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@9bc141e] is committing.
26/01/04 17:27:10 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 163, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@9bc141e] committed.
26/01/04 17:27:10 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/163 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.163.a73a9bbc-99b8-4a83-a785-12ed78c53cab.tmp
26/01/04 17:27:10 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.163.a73a9bbc-99b8-4a83-a785-12ed78c53cab.tmp to file:/tmp/spark-checkpoint-enrichment/commits/163
26/01/04 17:27:10 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T17:27:09.199Z",
  "batchId" : 163,
  "numInputRows" : 30,
  "inputRowsPerSecond" : 2307.6923076923076,
  "processedRowsPerSecond" : 21.352313167259787,
  "durationMs" : {
    "addBatch" : 989,
    "commitOffsets" : 167,
    "getBatch" : 0,
    "latestOffset" : 3,
    "queryPlanning" : 69,
    "triggerExecution" : 1405,
    "walCommit" : 162
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "0" : 360
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "0" : 390
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "0" : 390
      }
    },
    "numInputRows" : 30,
    "inputRowsPerSecond" : 2307.6923076923076,
    "processedRowsPerSecond" : 21.352313167259787,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 31
  }
}
26/01/04 17:27:19 INFO BlockManagerInfo: Removed broadcast_489_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:27:19 INFO BlockManagerInfo: Removed broadcast_489_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:27:19 INFO BlockManagerInfo: Removed broadcast_489_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:27:19 INFO BlockManagerInfo: Removed broadcast_492_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:27:19 INFO BlockManagerInfo: Removed broadcast_492_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:27:19 INFO BlockManagerInfo: Removed broadcast_492_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:27:19 INFO BlockManagerInfo: Removed broadcast_490_piece0 on spark-master:33535 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:27:19 INFO BlockManagerInfo: Removed broadcast_490_piece0 on 172.18.0.13:44401 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:27:19 INFO BlockManagerInfo: Removed broadcast_487_piece0 on spark-master:33535 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:27:19 INFO BlockManagerInfo: Removed broadcast_487_piece0 on 172.18.0.13:44401 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:27:19 INFO BlockManagerInfo: Removed broadcast_494_piece0 on spark-master:33535 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:27:19 INFO BlockManagerInfo: Removed broadcast_494_piece0 on 172.18.0.13:44401 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:27:19 INFO BlockManagerInfo: Removed broadcast_491_piece0 on spark-master:33535 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:27:19 INFO BlockManagerInfo: Removed broadcast_491_piece0 on 172.18.0.13:44401 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:27:20 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/164 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.164.0967d6cd-3c9b-400d-8c3b-99495ff935c7.tmp
26/01/04 17:27:20 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.164.0967d6cd-3c9b-400d-8c3b-99495ff935c7.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/164
26/01/04 17:27:20 INFO MicroBatchExecution: Committed offsets for batch 164. Metadata OffsetSeqMetadata(0,1767547640221,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 17:27:20 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:27:20 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:27:20 INFO BlockManagerInfo: Removed broadcast_493_piece0 on spark-master:33535 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:27:20 INFO BlockManagerInfo: Removed broadcast_493_piece0 on 172.18.0.13:44401 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:27:20 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:27:20 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:27:20 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:27:20 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:27:20 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#133339 - airline_prefix.nullCount#133338) > 0)
26/01/04 17:27:20 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#133374 - min_flight_num.nullCount#133373) > 0)
26/01/04 17:27:20 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#133369 - max_flight_num.nullCount#133368) > 0)
26/01/04 17:27:20 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:27:20 INFO DAGScheduler: Got job 331 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 17:27:20 INFO DAGScheduler: Final stage: ResultStage 332 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:27:20 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:27:20 INFO DAGScheduler: Missing parents: List()
26/01/04 17:27:20 INFO DAGScheduler: Submitting ResultStage 332 (MapPartitionsRDD[1663] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:27:20 INFO MemoryStore: Block broadcast_495 stored as values in memory (estimated size 38.5 KiB, free 434.4 MiB)
26/01/04 17:27:20 INFO MemoryStore: Block broadcast_495_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.3 MiB)
26/01/04 17:27:20 INFO BlockManagerInfo: Added broadcast_495_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:27:20 INFO SparkContext: Created broadcast 495 from broadcast at DAGScheduler.scala:1585
26/01/04 17:27:20 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 332 (MapPartitionsRDD[1663] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 17:27:20 INFO TaskSchedulerImpl: Adding task set 332.0 with 2 tasks resource profile 0
26/01/04 17:27:20 INFO TaskSetManager: Starting task 0.0 in stage 332.0 (TID 533) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 17:27:20 INFO TaskSetManager: Starting task 1.0 in stage 332.0 (TID 534) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 17:27:20 INFO BlockManagerInfo: Added broadcast_495_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:27:20 INFO TaskSetManager: Finished task 1.0 in stage 332.0 (TID 534) in 27 ms on 172.18.0.13 (executor 0) (1/2)
26/01/04 17:27:20 INFO BlockManagerInfo: Added broadcast_495_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:27:20 INFO TaskSetManager: Finished task 0.0 in stage 332.0 (TID 533) in 151 ms on 172.18.0.14 (executor 1) (2/2)
26/01/04 17:27:20 INFO TaskSchedulerImpl: Removed TaskSet 332.0, whose tasks have all completed, from pool 
26/01/04 17:27:20 INFO DAGScheduler: ResultStage 332 (start at NativeMethodAccessorImpl.java:0) finished in 0.181 s
26/01/04 17:27:20 INFO DAGScheduler: Job 331 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:27:20 INFO TaskSchedulerImpl: Killing all running tasks in stage 332: Stage finished
26/01/04 17:27:20 INFO DAGScheduler: Job 331 finished: start at NativeMethodAccessorImpl.java:0, took 0.188368 s
26/01/04 17:27:20 INFO MemoryStore: Block broadcast_496_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.3 MiB)
26/01/04 17:27:20 INFO BlockManagerInfo: Added broadcast_496_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:27:20 INFO SparkContext: Created broadcast 496 from start at NativeMethodAccessorImpl.java:0
26/01/04 17:27:20 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 164, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@b18b68d]. The input RDD has 1 partitions.
26/01/04 17:27:20 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:27:20 INFO DAGScheduler: Got job 332 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/01/04 17:27:20 INFO DAGScheduler: Final stage: ResultStage 333 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:27:20 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:27:20 INFO DAGScheduler: Missing parents: List()
26/01/04 17:27:20 INFO DAGScheduler: Submitting ResultStage 333 (MapPartitionsRDD[1668] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:27:20 INFO MemoryStore: Block broadcast_497 stored as values in memory (estimated size 52.9 KiB, free 434.3 MiB)
26/01/04 17:27:20 INFO MemoryStore: Block broadcast_497_piece0 stored as bytes in memory (estimated size 20.2 KiB, free 434.3 MiB)
26/01/04 17:27:20 INFO BlockManagerInfo: Added broadcast_497_piece0 in memory on spark-master:33535 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:27:20 INFO SparkContext: Created broadcast 497 from broadcast at DAGScheduler.scala:1585
26/01/04 17:27:20 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 333 (MapPartitionsRDD[1668] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/01/04 17:27:20 INFO TaskSchedulerImpl: Adding task set 333.0 with 1 tasks resource profile 0
26/01/04 17:27:20 INFO TaskSetManager: Starting task 0.0 in stage 333.0 (TID 535) (172.18.0.13, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:27:20 INFO BlockManagerInfo: Added broadcast_497_piece0 in memory on 172.18.0.13:44401 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:27:20 INFO BlockManagerInfo: Added broadcast_496_piece0 in memory on 172.18.0.13:44401 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:27:21 INFO TaskSetManager: Finished task 0.0 in stage 333.0 (TID 535) in 752 ms on 172.18.0.13 (executor 0) (1/1)
26/01/04 17:27:21 INFO TaskSchedulerImpl: Removed TaskSet 333.0, whose tasks have all completed, from pool 
26/01/04 17:27:21 INFO DAGScheduler: ResultStage 333 (start at NativeMethodAccessorImpl.java:0) finished in 0.818 s
26/01/04 17:27:21 INFO DAGScheduler: Job 332 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:27:21 INFO TaskSchedulerImpl: Killing all running tasks in stage 333: Stage finished
26/01/04 17:27:21 INFO DAGScheduler: Job 332 finished: start at NativeMethodAccessorImpl.java:0, took 0.820888 s
26/01/04 17:27:21 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 164, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@b18b68d] is committing.
26/01/04 17:27:21 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 164, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@b18b68d] committed.
26/01/04 17:27:21 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/164 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.164.ee589033-6b75-4e7b-ba5d-72d68205ffc6.tmp
26/01/04 17:27:21 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.164.ee589033-6b75-4e7b-ba5d-72d68205ffc6.tmp to file:/tmp/spark-checkpoint-enrichment/commits/164
26/01/04 17:27:21 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T17:27:20.219Z",
  "batchId" : 164,
  "numInputRows" : 30,
  "inputRowsPerSecond" : 2307.6923076923076,
  "processedRowsPerSecond" : 17.57469244288225,
  "durationMs" : {
    "addBatch" : 1242,
    "commitOffsets" : 261,
    "getBatch" : 0,
    "latestOffset" : 2,
    "queryPlanning" : 62,
    "triggerExecution" : 1707,
    "walCommit" : 138
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "0" : 390
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "0" : 420
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "0" : 420
      }
    },
    "numInputRows" : 30,
    "inputRowsPerSecond" : 2307.6923076923076,
    "processedRowsPerSecond" : 17.57469244288225,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 31
  }
}
26/01/04 17:27:31 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/165 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.165.f1450ac8-389a-4c39-8af2-7fdca48164a8.tmp
26/01/04 17:27:31 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.165.f1450ac8-389a-4c39-8af2-7fdca48164a8.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/165
26/01/04 17:27:31 INFO MicroBatchExecution: Committed offsets for batch 165. Metadata OffsetSeqMetadata(0,1767547651240,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 17:27:31 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:27:31 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:27:31 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:27:31 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:27:31 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:27:31 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:27:31 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#134143 - airline_prefix.nullCount#134142) > 0)
26/01/04 17:27:31 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#134178 - min_flight_num.nullCount#134177) > 0)
26/01/04 17:27:31 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#134173 - max_flight_num.nullCount#134172) > 0)
26/01/04 17:27:31 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:27:31 INFO DAGScheduler: Got job 333 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 17:27:31 INFO DAGScheduler: Final stage: ResultStage 334 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:27:31 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:27:31 INFO DAGScheduler: Missing parents: List()
26/01/04 17:27:31 INFO DAGScheduler: Submitting ResultStage 334 (MapPartitionsRDD[1673] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:27:31 INFO MemoryStore: Block broadcast_498 stored as values in memory (estimated size 38.5 KiB, free 434.2 MiB)
26/01/04 17:27:31 INFO MemoryStore: Block broadcast_498_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.2 MiB)
26/01/04 17:27:31 INFO BlockManagerInfo: Added broadcast_498_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:27:31 INFO SparkContext: Created broadcast 498 from broadcast at DAGScheduler.scala:1585
26/01/04 17:27:31 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 334 (MapPartitionsRDD[1673] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 17:27:31 INFO TaskSchedulerImpl: Adding task set 334.0 with 2 tasks resource profile 0
26/01/04 17:27:31 INFO TaskSetManager: Starting task 0.0 in stage 334.0 (TID 536) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 17:27:31 INFO TaskSetManager: Starting task 1.0 in stage 334.0 (TID 537) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 17:27:31 INFO BlockManagerInfo: Added broadcast_498_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:27:31 INFO TaskSetManager: Finished task 1.0 in stage 334.0 (TID 537) in 59 ms on 172.18.0.13 (executor 0) (1/2)
26/01/04 17:27:31 INFO BlockManagerInfo: Added broadcast_498_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:27:31 INFO TaskSetManager: Finished task 0.0 in stage 334.0 (TID 536) in 135 ms on 172.18.0.14 (executor 1) (2/2)
26/01/04 17:27:31 INFO TaskSchedulerImpl: Removed TaskSet 334.0, whose tasks have all completed, from pool 
26/01/04 17:27:31 INFO DAGScheduler: ResultStage 334 (start at NativeMethodAccessorImpl.java:0) finished in 0.155 s
26/01/04 17:27:31 INFO DAGScheduler: Job 333 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:27:31 INFO TaskSchedulerImpl: Killing all running tasks in stage 334: Stage finished
26/01/04 17:27:31 INFO DAGScheduler: Job 333 finished: start at NativeMethodAccessorImpl.java:0, took 0.162970 s
26/01/04 17:27:31 INFO MemoryStore: Block broadcast_499_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.2 MiB)
26/01/04 17:27:31 INFO BlockManagerInfo: Added broadcast_499_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:27:31 INFO SparkContext: Created broadcast 499 from start at NativeMethodAccessorImpl.java:0
26/01/04 17:27:31 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 165, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@457ba77c]. The input RDD has 1 partitions.
26/01/04 17:27:31 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:27:31 INFO DAGScheduler: Got job 334 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/01/04 17:27:31 INFO DAGScheduler: Final stage: ResultStage 335 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:27:31 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:27:31 INFO DAGScheduler: Missing parents: List()
26/01/04 17:27:31 INFO DAGScheduler: Submitting ResultStage 335 (MapPartitionsRDD[1678] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:27:31 INFO MemoryStore: Block broadcast_500 stored as values in memory (estimated size 52.9 KiB, free 434.2 MiB)
26/01/04 17:27:31 INFO MemoryStore: Block broadcast_500_piece0 stored as bytes in memory (estimated size 20.2 KiB, free 434.1 MiB)
26/01/04 17:27:31 INFO BlockManagerInfo: Added broadcast_500_piece0 in memory on spark-master:33535 (size: 20.2 KiB, free: 434.3 MiB)
26/01/04 17:27:31 INFO SparkContext: Created broadcast 500 from broadcast at DAGScheduler.scala:1585
26/01/04 17:27:31 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 335 (MapPartitionsRDD[1678] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/01/04 17:27:31 INFO TaskSchedulerImpl: Adding task set 335.0 with 1 tasks resource profile 0
26/01/04 17:27:31 INFO TaskSetManager: Starting task 0.0 in stage 335.0 (TID 538) (172.18.0.13, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:27:31 INFO BlockManagerInfo: Added broadcast_500_piece0 in memory on 172.18.0.13:44401 (size: 20.2 KiB, free: 434.3 MiB)
26/01/04 17:27:31 INFO BlockManagerInfo: Added broadcast_499_piece0 in memory on 172.18.0.13:44401 (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:27:32 INFO TaskSetManager: Finished task 0.0 in stage 335.0 (TID 538) in 658 ms on 172.18.0.13 (executor 0) (1/1)
26/01/04 17:27:32 INFO TaskSchedulerImpl: Removed TaskSet 335.0, whose tasks have all completed, from pool 
26/01/04 17:27:32 INFO DAGScheduler: ResultStage 335 (start at NativeMethodAccessorImpl.java:0) finished in 0.674 s
26/01/04 17:27:32 INFO DAGScheduler: Job 334 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:27:32 INFO TaskSchedulerImpl: Killing all running tasks in stage 335: Stage finished
26/01/04 17:27:32 INFO DAGScheduler: Job 334 finished: start at NativeMethodAccessorImpl.java:0, took 0.677207 s
26/01/04 17:27:32 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 165, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@457ba77c] is committing.
26/01/04 17:27:32 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 165, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@457ba77c] committed.
26/01/04 17:27:32 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/165 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.165.973b83ce-b324-4c99-acad-bd51ed12ffda.tmp
26/01/04 17:27:32 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.165.973b83ce-b324-4c99-acad-bd51ed12ffda.tmp to file:/tmp/spark-checkpoint-enrichment/commits/165
26/01/04 17:27:32 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T17:27:31.233Z",
  "batchId" : 165,
  "numInputRows" : 23,
  "inputRowsPerSecond" : 1277.7777777777778,
  "processedRowsPerSecond" : 15.625,
  "durationMs" : {
    "addBatch" : 1005,
    "commitOffsets" : 229,
    "getBatch" : 0,
    "latestOffset" : 7,
    "queryPlanning" : 72,
    "triggerExecution" : 1472,
    "walCommit" : 155
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "0" : 420
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "0" : 443
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "0" : 443
      }
    },
    "numInputRows" : 23,
    "inputRowsPerSecond" : 1277.7777777777778,
    "processedRowsPerSecond" : 15.625,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 24
  }
}
26/01/04 17:27:32 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/166 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.166.c622bfeb-131a-4567-b185-67592b04811a.tmp
26/01/04 17:27:32 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.166.c622bfeb-131a-4567-b185-67592b04811a.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/166
26/01/04 17:27:32 INFO MicroBatchExecution: Committed offsets for batch 166. Metadata OffsetSeqMetadata(0,1767547652718,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 17:27:32 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:27:32 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:27:33 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:27:33 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:27:33 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:27:33 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:27:33 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#134947 - airline_prefix.nullCount#134946) > 0)
26/01/04 17:27:33 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#134982 - min_flight_num.nullCount#134981) > 0)
26/01/04 17:27:33 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#134977 - max_flight_num.nullCount#134976) > 0)
26/01/04 17:27:33 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:27:33 INFO DAGScheduler: Got job 335 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 17:27:33 INFO DAGScheduler: Final stage: ResultStage 336 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:27:33 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:27:33 INFO DAGScheduler: Missing parents: List()
26/01/04 17:27:33 INFO DAGScheduler: Submitting ResultStage 336 (MapPartitionsRDD[1683] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:27:33 INFO MemoryStore: Block broadcast_501 stored as values in memory (estimated size 38.5 KiB, free 434.1 MiB)
26/01/04 17:27:33 INFO MemoryStore: Block broadcast_501_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.1 MiB)
26/01/04 17:27:33 INFO BlockManagerInfo: Removed broadcast_498_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:27:33 INFO BlockManagerInfo: Added broadcast_501_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:27:33 INFO SparkContext: Created broadcast 501 from broadcast at DAGScheduler.scala:1585
26/01/04 17:27:33 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 336 (MapPartitionsRDD[1683] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 17:27:33 INFO TaskSchedulerImpl: Adding task set 336.0 with 2 tasks resource profile 0
26/01/04 17:27:33 INFO TaskSetManager: Starting task 0.0 in stage 336.0 (TID 539) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 17:27:33 INFO BlockManagerInfo: Removed broadcast_498_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:27:33 INFO TaskSetManager: Starting task 1.0 in stage 336.0 (TID 540) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 17:27:33 INFO BlockManagerInfo: Removed broadcast_498_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:27:33 INFO BlockManagerInfo: Removed broadcast_499_piece0 on spark-master:33535 in memory (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:27:33 INFO BlockManagerInfo: Removed broadcast_499_piece0 on 172.18.0.13:44401 in memory (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:27:33 INFO BlockManagerInfo: Added broadcast_501_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:27:33 INFO BlockManagerInfo: Added broadcast_501_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:27:33 INFO BlockManagerInfo: Removed broadcast_500_piece0 on spark-master:33535 in memory (size: 20.2 KiB, free: 434.3 MiB)
26/01/04 17:27:33 INFO BlockManagerInfo: Removed broadcast_500_piece0 on 172.18.0.13:44401 in memory (size: 20.2 KiB, free: 434.3 MiB)
26/01/04 17:27:33 INFO BlockManagerInfo: Removed broadcast_495_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:27:33 INFO BlockManagerInfo: Removed broadcast_495_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:27:33 INFO TaskSetManager: Finished task 1.0 in stage 336.0 (TID 540) in 111 ms on 172.18.0.13 (executor 0) (1/2)
26/01/04 17:27:33 INFO BlockManagerInfo: Removed broadcast_495_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:27:33 INFO BlockManagerInfo: Removed broadcast_496_piece0 on spark-master:33535 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:27:33 INFO TaskSetManager: Finished task 0.0 in stage 336.0 (TID 539) in 159 ms on 172.18.0.14 (executor 1) (2/2)
26/01/04 17:27:33 INFO TaskSchedulerImpl: Removed TaskSet 336.0, whose tasks have all completed, from pool 
26/01/04 17:27:33 INFO BlockManagerInfo: Removed broadcast_496_piece0 on 172.18.0.13:44401 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:27:33 INFO DAGScheduler: ResultStage 336 (start at NativeMethodAccessorImpl.java:0) finished in 0.212 s
26/01/04 17:27:33 INFO DAGScheduler: Job 335 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:27:33 INFO TaskSchedulerImpl: Killing all running tasks in stage 336: Stage finished
26/01/04 17:27:33 INFO DAGScheduler: Job 335 finished: start at NativeMethodAccessorImpl.java:0, took 0.226304 s
26/01/04 17:27:33 INFO MemoryStore: Block broadcast_502_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.3 MiB)
26/01/04 17:27:33 INFO BlockManagerInfo: Added broadcast_502_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:27:33 INFO SparkContext: Created broadcast 502 from start at NativeMethodAccessorImpl.java:0
26/01/04 17:27:33 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 166, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@6e145587]. The input RDD has 1 partitions.
26/01/04 17:27:33 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:27:33 INFO DAGScheduler: Got job 336 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/01/04 17:27:33 INFO DAGScheduler: Final stage: ResultStage 337 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:27:33 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:27:33 INFO DAGScheduler: Missing parents: List()
26/01/04 17:27:33 INFO BlockManagerInfo: Removed broadcast_497_piece0 on 172.18.0.13:44401 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:27:33 INFO DAGScheduler: Submitting ResultStage 337 (MapPartitionsRDD[1688] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:27:33 INFO BlockManagerInfo: Removed broadcast_497_piece0 on spark-master:33535 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:27:33 INFO MemoryStore: Block broadcast_503 stored as values in memory (estimated size 52.9 KiB, free 434.2 MiB)
26/01/04 17:27:33 INFO MemoryStore: Block broadcast_503_piece0 stored as bytes in memory (estimated size 20.1 KiB, free 434.3 MiB)
26/01/04 17:27:33 INFO BlockManagerInfo: Added broadcast_503_piece0 in memory on spark-master:33535 (size: 20.1 KiB, free: 434.4 MiB)
26/01/04 17:27:33 INFO SparkContext: Created broadcast 503 from broadcast at DAGScheduler.scala:1585
26/01/04 17:27:33 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 337 (MapPartitionsRDD[1688] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/01/04 17:27:33 INFO TaskSchedulerImpl: Adding task set 337.0 with 1 tasks resource profile 0
26/01/04 17:27:33 INFO TaskSetManager: Starting task 0.0 in stage 337.0 (TID 541) (172.18.0.13, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:27:33 INFO BlockManagerInfo: Added broadcast_503_piece0 in memory on 172.18.0.13:44401 (size: 20.1 KiB, free: 434.4 MiB)
26/01/04 17:27:33 INFO BlockManagerInfo: Added broadcast_502_piece0 in memory on 172.18.0.13:44401 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:27:33 INFO TaskSetManager: Finished task 0.0 in stage 337.0 (TID 541) in 221 ms on 172.18.0.13 (executor 0) (1/1)
26/01/04 17:27:33 INFO TaskSchedulerImpl: Removed TaskSet 337.0, whose tasks have all completed, from pool 
26/01/04 17:27:33 INFO DAGScheduler: ResultStage 337 (start at NativeMethodAccessorImpl.java:0) finished in 0.237 s
26/01/04 17:27:33 INFO DAGScheduler: Job 336 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:27:33 INFO TaskSchedulerImpl: Killing all running tasks in stage 337: Stage finished
26/01/04 17:27:33 INFO DAGScheduler: Job 336 finished: start at NativeMethodAccessorImpl.java:0, took 0.243187 s
26/01/04 17:27:33 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 166, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@6e145587] is committing.
26/01/04 17:27:33 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 166, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@6e145587] committed.
26/01/04 17:27:33 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/166 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.166.8428c534-1459-4aec-b2df-cb90656f720d.tmp
26/01/04 17:27:33 INFO BlockManagerInfo: Removed broadcast_501_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:27:33 INFO BlockManagerInfo: Removed broadcast_501_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:27:33 INFO BlockManagerInfo: Removed broadcast_501_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:27:33 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.166.8428c534-1459-4aec-b2df-cb90656f720d.tmp to file:/tmp/spark-checkpoint-enrichment/commits/166
26/01/04 17:27:33 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T17:27:32.708Z",
  "batchId" : 166,
  "numInputRows" : 7,
  "inputRowsPerSecond" : 4.745762711864407,
  "processedRowsPerSecond" : 5.613472333600641,
  "durationMs" : {
    "addBatch" : 694,
    "commitOffsets" : 266,
    "getBatch" : 2,
    "latestOffset" : 10,
    "queryPlanning" : 69,
    "triggerExecution" : 1247,
    "walCommit" : 204
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "0" : 443
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "0" : 450
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "0" : 450
      }
    },
    "numInputRows" : 7,
    "inputRowsPerSecond" : 4.745762711864407,
    "processedRowsPerSecond" : 5.613472333600641,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 7
  }
}
26/01/04 17:27:33 INFO BlockManagerInfo: Removed broadcast_503_piece0 on spark-master:33535 in memory (size: 20.1 KiB, free: 434.4 MiB)
26/01/04 17:27:33 INFO BlockManagerInfo: Removed broadcast_503_piece0 on 172.18.0.13:44401 in memory (size: 20.1 KiB, free: 434.4 MiB)
26/01/04 17:27:42 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/167 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.167.a27a8edb-5e15-4472-83da-d6ea33abd887.tmp
26/01/04 17:27:42 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.167.a27a8edb-5e15-4472-83da-d6ea33abd887.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/167
26/01/04 17:27:42 INFO MicroBatchExecution: Committed offsets for batch 167. Metadata OffsetSeqMetadata(0,1767547662280,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 17:27:42 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:27:42 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:27:42 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:27:42 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:27:42 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:27:42 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:27:42 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#135751 - airline_prefix.nullCount#135750) > 0)
26/01/04 17:27:42 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#135786 - min_flight_num.nullCount#135785) > 0)
26/01/04 17:27:42 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#135781 - max_flight_num.nullCount#135780) > 0)
26/01/04 17:27:42 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:27:42 INFO DAGScheduler: Got job 337 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 17:27:42 INFO DAGScheduler: Final stage: ResultStage 338 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:27:42 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:27:42 INFO DAGScheduler: Missing parents: List()
26/01/04 17:27:42 INFO DAGScheduler: Submitting ResultStage 338 (MapPartitionsRDD[1693] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:27:42 INFO MemoryStore: Block broadcast_504 stored as values in memory (estimated size 38.5 KiB, free 434.4 MiB)
26/01/04 17:27:42 INFO MemoryStore: Block broadcast_504_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.3 MiB)
26/01/04 17:27:42 INFO BlockManagerInfo: Added broadcast_504_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:27:42 INFO SparkContext: Created broadcast 504 from broadcast at DAGScheduler.scala:1585
26/01/04 17:27:42 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 338 (MapPartitionsRDD[1693] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 17:27:42 INFO TaskSchedulerImpl: Adding task set 338.0 with 2 tasks resource profile 0
26/01/04 17:27:42 INFO TaskSetManager: Starting task 1.0 in stage 338.0 (TID 542) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 17:27:42 INFO TaskSetManager: Starting task 0.0 in stage 338.0 (TID 543) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 17:27:42 INFO BlockManagerInfo: Added broadcast_504_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:27:42 INFO BlockManagerInfo: Added broadcast_504_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:27:42 INFO TaskSetManager: Finished task 1.0 in stage 338.0 (TID 542) in 58 ms on 172.18.0.13 (executor 0) (1/2)
26/01/04 17:27:42 INFO TaskSetManager: Finished task 0.0 in stage 338.0 (TID 543) in 89 ms on 172.18.0.14 (executor 1) (2/2)
26/01/04 17:27:42 INFO TaskSchedulerImpl: Removed TaskSet 338.0, whose tasks have all completed, from pool 
26/01/04 17:27:42 INFO DAGScheduler: ResultStage 338 (start at NativeMethodAccessorImpl.java:0) finished in 0.157 s
26/01/04 17:27:42 INFO DAGScheduler: Job 337 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:27:42 INFO TaskSchedulerImpl: Killing all running tasks in stage 338: Stage finished
26/01/04 17:27:42 INFO DAGScheduler: Job 337 finished: start at NativeMethodAccessorImpl.java:0, took 0.215208 s
26/01/04 17:27:42 INFO BlockManagerInfo: Removed broadcast_502_piece0 on spark-master:33535 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:27:42 INFO BlockManagerInfo: Removed broadcast_502_piece0 on 172.18.0.13:44401 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:27:42 INFO MemoryStore: Block broadcast_505_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.3 MiB)
26/01/04 17:27:42 INFO BlockManagerInfo: Added broadcast_505_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:27:42 INFO SparkContext: Created broadcast 505 from start at NativeMethodAccessorImpl.java:0
26/01/04 17:27:42 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 167, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@35682740]. The input RDD has 1 partitions.
26/01/04 17:27:42 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:27:42 INFO DAGScheduler: Got job 338 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/01/04 17:27:42 INFO DAGScheduler: Final stage: ResultStage 339 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:27:42 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:27:42 INFO DAGScheduler: Missing parents: List()
26/01/04 17:27:42 INFO DAGScheduler: Submitting ResultStage 339 (MapPartitionsRDD[1698] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:27:42 INFO MemoryStore: Block broadcast_506 stored as values in memory (estimated size 52.9 KiB, free 434.3 MiB)
26/01/04 17:27:42 INFO MemoryStore: Block broadcast_506_piece0 stored as bytes in memory (estimated size 20.2 KiB, free 434.3 MiB)
26/01/04 17:27:42 INFO BlockManagerInfo: Added broadcast_506_piece0 in memory on spark-master:33535 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:27:42 INFO SparkContext: Created broadcast 506 from broadcast at DAGScheduler.scala:1585
26/01/04 17:27:42 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 339 (MapPartitionsRDD[1698] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/01/04 17:27:42 INFO TaskSchedulerImpl: Adding task set 339.0 with 1 tasks resource profile 0
26/01/04 17:27:42 INFO TaskSetManager: Starting task 0.0 in stage 339.0 (TID 544) (172.18.0.13, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:27:42 INFO BlockManagerInfo: Added broadcast_506_piece0 in memory on 172.18.0.13:44401 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:27:42 INFO BlockManagerInfo: Added broadcast_505_piece0 in memory on 172.18.0.13:44401 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:27:43 INFO TaskSetManager: Finished task 0.0 in stage 339.0 (TID 544) in 617 ms on 172.18.0.13 (executor 0) (1/1)
26/01/04 17:27:43 INFO TaskSchedulerImpl: Removed TaskSet 339.0, whose tasks have all completed, from pool 
26/01/04 17:27:43 INFO DAGScheduler: ResultStage 339 (start at NativeMethodAccessorImpl.java:0) finished in 0.634 s
26/01/04 17:27:43 INFO DAGScheduler: Job 338 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:27:43 INFO TaskSchedulerImpl: Killing all running tasks in stage 339: Stage finished
26/01/04 17:27:43 INFO DAGScheduler: Job 338 finished: start at NativeMethodAccessorImpl.java:0, took 0.641351 s
26/01/04 17:27:43 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 167, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@35682740] is committing.
26/01/04 17:27:43 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 167, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@35682740] committed.
26/01/04 17:27:43 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/167 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.167.09ec8586-c54a-4a1e-9139-246d355f3684.tmp
26/01/04 17:27:43 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.167.09ec8586-c54a-4a1e-9139-246d355f3684.tmp to file:/tmp/spark-checkpoint-enrichment/commits/167
26/01/04 17:27:43 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T17:27:42.273Z",
  "batchId" : 167,
  "numInputRows" : 30,
  "inputRowsPerSecond" : 1764.705882352941,
  "processedRowsPerSecond" : 21.03786816269285,
  "durationMs" : {
    "addBatch" : 1027,
    "commitOffsets" : 170,
    "getBatch" : 0,
    "latestOffset" : 6,
    "queryPlanning" : 51,
    "triggerExecution" : 1426,
    "walCommit" : 168
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "0" : 450
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "0" : 480
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "0" : 480
      }
    },
    "numInputRows" : 30,
    "inputRowsPerSecond" : 1764.705882352941,
    "processedRowsPerSecond" : 21.03786816269285,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 31
  }
}
26/01/04 17:27:53 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/168 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.168.7f668e10-df58-4400-900f-1097626ab044.tmp
26/01/04 17:27:53 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.168.7f668e10-df58-4400-900f-1097626ab044.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/168
26/01/04 17:27:53 INFO MicroBatchExecution: Committed offsets for batch 168. Metadata OffsetSeqMetadata(0,1767547673291,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 17:27:53 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:27:53 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:27:53 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:27:53 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:27:53 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:27:53 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:27:53 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#136555 - airline_prefix.nullCount#136554) > 0)
26/01/04 17:27:53 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#136590 - min_flight_num.nullCount#136589) > 0)
26/01/04 17:27:53 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#136585 - max_flight_num.nullCount#136584) > 0)
26/01/04 17:27:53 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:27:53 INFO DAGScheduler: Got job 339 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 17:27:53 INFO DAGScheduler: Final stage: ResultStage 340 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:27:53 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:27:53 INFO DAGScheduler: Missing parents: List()
26/01/04 17:27:53 INFO DAGScheduler: Submitting ResultStage 340 (MapPartitionsRDD[1703] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:27:53 INFO MemoryStore: Block broadcast_507 stored as values in memory (estimated size 38.5 KiB, free 434.2 MiB)
26/01/04 17:27:53 INFO MemoryStore: Block broadcast_507_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.2 MiB)
26/01/04 17:27:53 INFO BlockManagerInfo: Added broadcast_507_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:27:53 INFO BlockManagerInfo: Removed broadcast_504_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:27:53 INFO SparkContext: Created broadcast 507 from broadcast at DAGScheduler.scala:1585
26/01/04 17:27:53 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 340 (MapPartitionsRDD[1703] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 17:27:53 INFO TaskSchedulerImpl: Adding task set 340.0 with 2 tasks resource profile 0
26/01/04 17:27:53 INFO TaskSetManager: Starting task 1.0 in stage 340.0 (TID 545) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 17:27:53 INFO TaskSetManager: Starting task 0.0 in stage 340.0 (TID 546) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 17:27:53 INFO BlockManagerInfo: Removed broadcast_504_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:27:53 INFO BlockManagerInfo: Removed broadcast_504_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:27:53 INFO BlockManagerInfo: Removed broadcast_505_piece0 on 172.18.0.13:44401 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:27:53 INFO BlockManagerInfo: Added broadcast_507_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:27:53 INFO BlockManagerInfo: Removed broadcast_505_piece0 on spark-master:33535 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:27:53 INFO BlockManagerInfo: Added broadcast_507_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:27:53 INFO TaskSetManager: Finished task 1.0 in stage 340.0 (TID 545) in 86 ms on 172.18.0.13 (executor 0) (1/2)
26/01/04 17:27:53 INFO BlockManagerInfo: Removed broadcast_506_piece0 on spark-master:33535 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:27:53 INFO BlockManagerInfo: Removed broadcast_506_piece0 on 172.18.0.13:44401 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:27:53 INFO TaskSetManager: Finished task 0.0 in stage 340.0 (TID 546) in 188 ms on 172.18.0.14 (executor 1) (2/2)
26/01/04 17:27:53 INFO TaskSchedulerImpl: Removed TaskSet 340.0, whose tasks have all completed, from pool 
26/01/04 17:27:53 INFO DAGScheduler: ResultStage 340 (start at NativeMethodAccessorImpl.java:0) finished in 0.235 s
26/01/04 17:27:53 INFO DAGScheduler: Job 339 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:27:53 INFO TaskSchedulerImpl: Killing all running tasks in stage 340: Stage finished
26/01/04 17:27:53 INFO DAGScheduler: Job 339 finished: start at NativeMethodAccessorImpl.java:0, took 0.243411 s
26/01/04 17:27:53 INFO MemoryStore: Block broadcast_508_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.3 MiB)
26/01/04 17:27:53 INFO BlockManagerInfo: Added broadcast_508_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:27:53 INFO SparkContext: Created broadcast 508 from start at NativeMethodAccessorImpl.java:0
26/01/04 17:27:53 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 168, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@534d8b58]. The input RDD has 1 partitions.
26/01/04 17:27:53 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:27:53 INFO DAGScheduler: Got job 340 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/01/04 17:27:53 INFO DAGScheduler: Final stage: ResultStage 341 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:27:53 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:27:53 INFO DAGScheduler: Missing parents: List()
26/01/04 17:27:53 INFO DAGScheduler: Submitting ResultStage 341 (MapPartitionsRDD[1708] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:27:53 INFO MemoryStore: Block broadcast_509 stored as values in memory (estimated size 52.9 KiB, free 434.3 MiB)
26/01/04 17:27:53 INFO MemoryStore: Block broadcast_509_piece0 stored as bytes in memory (estimated size 20.2 KiB, free 434.3 MiB)
26/01/04 17:27:53 INFO BlockManagerInfo: Added broadcast_509_piece0 in memory on spark-master:33535 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:27:53 INFO BlockManagerInfo: Removed broadcast_507_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:27:53 INFO SparkContext: Created broadcast 509 from broadcast at DAGScheduler.scala:1585
26/01/04 17:27:53 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 341 (MapPartitionsRDD[1708] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/01/04 17:27:53 INFO TaskSchedulerImpl: Adding task set 341.0 with 1 tasks resource profile 0
26/01/04 17:27:53 INFO BlockManagerInfo: Removed broadcast_507_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:27:53 INFO BlockManagerInfo: Removed broadcast_507_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:27:53 INFO TaskSetManager: Starting task 0.0 in stage 341.0 (TID 547) (172.18.0.13, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:27:54 INFO BlockManagerInfo: Added broadcast_509_piece0 in memory on 172.18.0.13:44401 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:27:54 INFO BlockManagerInfo: Added broadcast_508_piece0 in memory on 172.18.0.13:44401 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:27:54 INFO TaskSetManager: Finished task 0.0 in stage 341.0 (TID 547) in 791 ms on 172.18.0.13 (executor 0) (1/1)
26/01/04 17:27:54 INFO DAGScheduler: ResultStage 341 (start at NativeMethodAccessorImpl.java:0) finished in 0.823 s
26/01/04 17:27:54 INFO DAGScheduler: Job 340 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:27:54 INFO TaskSchedulerImpl: Removed TaskSet 341.0, whose tasks have all completed, from pool 
26/01/04 17:27:54 INFO TaskSchedulerImpl: Killing all running tasks in stage 341: Stage finished
26/01/04 17:27:54 INFO DAGScheduler: Job 340 finished: start at NativeMethodAccessorImpl.java:0, took 0.831678 s
26/01/04 17:27:54 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 168, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@534d8b58] is committing.
26/01/04 17:27:54 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 168, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@534d8b58] committed.
26/01/04 17:27:54 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/168 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.168.8a436db8-4a37-46b1-860d-723c78b00429.tmp
26/01/04 17:27:54 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.168.8a436db8-4a37-46b1-860d-723c78b00429.tmp to file:/tmp/spark-checkpoint-enrichment/commits/168
26/01/04 17:27:54 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T17:27:53.287Z",
  "batchId" : 168,
  "numInputRows" : 30,
  "inputRowsPerSecond" : 2000.0,
  "processedRowsPerSecond" : 18.237082066869302,
  "durationMs" : {
    "addBatch" : 1237,
    "commitOffsets" : 146,
    "getBatch" : 0,
    "latestOffset" : 4,
    "queryPlanning" : 59,
    "triggerExecution" : 1645,
    "walCommit" : 196
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "0" : 480
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "0" : 510
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "0" : 510
      }
    },
    "numInputRows" : 30,
    "inputRowsPerSecond" : 2000.0,
    "processedRowsPerSecond" : 18.237082066869302,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 31
  }
}
26/01/04 17:28:04 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/169 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.169.1aea4c6e-4815-4165-bea0-fd266822904d.tmp
26/01/04 17:28:04 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.169.1aea4c6e-4815-4165-bea0-fd266822904d.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/169
26/01/04 17:28:04 INFO MicroBatchExecution: Committed offsets for batch 169. Metadata OffsetSeqMetadata(0,1767547684307,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 17:28:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:28:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:28:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:28:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:28:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:28:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:28:04 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#137359 - airline_prefix.nullCount#137358) > 0)
26/01/04 17:28:04 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#137394 - min_flight_num.nullCount#137393) > 0)
26/01/04 17:28:04 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#137389 - max_flight_num.nullCount#137388) > 0)
26/01/04 17:28:04 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:28:04 INFO DAGScheduler: Got job 341 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 17:28:04 INFO DAGScheduler: Final stage: ResultStage 342 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:28:04 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:28:04 INFO DAGScheduler: Missing parents: List()
26/01/04 17:28:04 INFO DAGScheduler: Submitting ResultStage 342 (MapPartitionsRDD[1713] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:28:04 INFO MemoryStore: Block broadcast_510 stored as values in memory (estimated size 38.5 KiB, free 434.3 MiB)
26/01/04 17:28:04 INFO MemoryStore: Block broadcast_510_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.3 MiB)
26/01/04 17:28:04 INFO BlockManagerInfo: Added broadcast_510_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:28:04 INFO BlockManagerInfo: Removed broadcast_509_piece0 on spark-master:33535 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:28:04 INFO BlockManagerInfo: Removed broadcast_509_piece0 on 172.18.0.13:44401 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:28:04 INFO SparkContext: Created broadcast 510 from broadcast at DAGScheduler.scala:1585
26/01/04 17:28:04 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 342 (MapPartitionsRDD[1713] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 17:28:04 INFO TaskSchedulerImpl: Adding task set 342.0 with 2 tasks resource profile 0
26/01/04 17:28:04 INFO TaskSetManager: Starting task 1.0 in stage 342.0 (TID 548) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 17:28:04 INFO TaskSetManager: Starting task 0.0 in stage 342.0 (TID 549) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 17:28:04 INFO BlockManagerInfo: Removed broadcast_508_piece0 on spark-master:33535 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:28:04 INFO BlockManagerInfo: Added broadcast_510_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:28:04 INFO BlockManagerInfo: Removed broadcast_508_piece0 on 172.18.0.13:44401 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:28:04 INFO BlockManagerInfo: Added broadcast_510_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:28:04 INFO TaskSetManager: Finished task 0.0 in stage 342.0 (TID 549) in 105 ms on 172.18.0.14 (executor 1) (1/2)
26/01/04 17:28:04 INFO TaskSetManager: Finished task 1.0 in stage 342.0 (TID 548) in 124 ms on 172.18.0.13 (executor 0) (2/2)
26/01/04 17:28:04 INFO TaskSchedulerImpl: Removed TaskSet 342.0, whose tasks have all completed, from pool 
26/01/04 17:28:04 INFO DAGScheduler: ResultStage 342 (start at NativeMethodAccessorImpl.java:0) finished in 0.227 s
26/01/04 17:28:04 INFO DAGScheduler: Job 341 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:28:04 INFO TaskSchedulerImpl: Killing all running tasks in stage 342: Stage finished
26/01/04 17:28:04 INFO DAGScheduler: Job 341 finished: start at NativeMethodAccessorImpl.java:0, took 0.233823 s
26/01/04 17:28:04 INFO MemoryStore: Block broadcast_511_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.3 MiB)
26/01/04 17:28:04 INFO BlockManagerInfo: Added broadcast_511_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:28:04 INFO SparkContext: Created broadcast 511 from start at NativeMethodAccessorImpl.java:0
26/01/04 17:28:04 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 169, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@15d2539b]. The input RDD has 1 partitions.
26/01/04 17:28:04 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:28:04 INFO DAGScheduler: Got job 342 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/01/04 17:28:04 INFO DAGScheduler: Final stage: ResultStage 343 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:28:05 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:28:05 INFO DAGScheduler: Missing parents: List()
26/01/04 17:28:05 INFO DAGScheduler: Submitting ResultStage 343 (MapPartitionsRDD[1718] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:28:05 INFO MemoryStore: Block broadcast_512 stored as values in memory (estimated size 52.9 KiB, free 434.3 MiB)
26/01/04 17:28:05 INFO MemoryStore: Block broadcast_512_piece0 stored as bytes in memory (estimated size 20.2 KiB, free 434.3 MiB)
26/01/04 17:28:05 INFO BlockManagerInfo: Added broadcast_512_piece0 in memory on spark-master:33535 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:28:05 INFO SparkContext: Created broadcast 512 from broadcast at DAGScheduler.scala:1585
26/01/04 17:28:05 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 343 (MapPartitionsRDD[1718] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/01/04 17:28:05 INFO TaskSchedulerImpl: Adding task set 343.0 with 1 tasks resource profile 0
26/01/04 17:28:05 INFO TaskSetManager: Starting task 0.0 in stage 343.0 (TID 550) (172.18.0.13, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:28:05 INFO BlockManagerInfo: Added broadcast_512_piece0 in memory on 172.18.0.13:44401 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:28:05 INFO BlockManagerInfo: Added broadcast_511_piece0 in memory on 172.18.0.13:44401 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:28:05 INFO TaskSetManager: Finished task 0.0 in stage 343.0 (TID 550) in 654 ms on 172.18.0.13 (executor 0) (1/1)
26/01/04 17:28:05 INFO TaskSchedulerImpl: Removed TaskSet 343.0, whose tasks have all completed, from pool 
26/01/04 17:28:05 INFO DAGScheduler: ResultStage 343 (start at NativeMethodAccessorImpl.java:0) finished in 0.679 s
26/01/04 17:28:05 INFO DAGScheduler: Job 342 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:28:05 INFO TaskSchedulerImpl: Killing all running tasks in stage 343: Stage finished
26/01/04 17:28:05 INFO DAGScheduler: Job 342 finished: start at NativeMethodAccessorImpl.java:0, took 0.730729 s
26/01/04 17:28:05 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 169, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@15d2539b] is committing.
26/01/04 17:28:05 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 169, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@15d2539b] committed.
26/01/04 17:28:05 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/169 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.169.f7770f67-d44c-46a3-b6f7-0d653c7b31e8.tmp
26/01/04 17:28:05 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.169.f7770f67-d44c-46a3-b6f7-0d653c7b31e8.tmp to file:/tmp/spark-checkpoint-enrichment/commits/169
26/01/04 17:28:05 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T17:28:04.299Z",
  "batchId" : 169,
  "numInputRows" : 30,
  "inputRowsPerSecond" : 1250.0,
  "processedRowsPerSecond" : 18.51851851851852,
  "durationMs" : {
    "addBatch" : 1148,
    "commitOffsets" : 191,
    "getBatch" : 0,
    "latestOffset" : 7,
    "queryPlanning" : 56,
    "triggerExecution" : 1620,
    "walCommit" : 215
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "0" : 510
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "0" : 540
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "0" : 540
      }
    },
    "numInputRows" : 30,
    "inputRowsPerSecond" : 1250.0,
    "processedRowsPerSecond" : 18.51851851851852,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 31
  }
}
26/01/04 17:28:15 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/170 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.170.573db6b0-f4ee-4e61-9403-2557fa9bdd1b.tmp
26/01/04 17:28:15 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.170.573db6b0-f4ee-4e61-9403-2557fa9bdd1b.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/170
26/01/04 17:28:15 INFO MicroBatchExecution: Committed offsets for batch 170. Metadata OffsetSeqMetadata(0,1767547695348,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 17:28:15 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:28:15 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:28:15 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:28:15 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:28:15 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:28:15 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:28:15 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#138163 - airline_prefix.nullCount#138162) > 0)
26/01/04 17:28:15 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#138198 - min_flight_num.nullCount#138197) > 0)
26/01/04 17:28:15 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#138193 - max_flight_num.nullCount#138192) > 0)
26/01/04 17:28:15 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:28:15 INFO DAGScheduler: Got job 343 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 17:28:15 INFO DAGScheduler: Final stage: ResultStage 344 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:28:15 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:28:15 INFO DAGScheduler: Missing parents: List()
26/01/04 17:28:15 INFO DAGScheduler: Submitting ResultStage 344 (MapPartitionsRDD[1723] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:28:15 INFO MemoryStore: Block broadcast_513 stored as values in memory (estimated size 38.5 KiB, free 434.2 MiB)
26/01/04 17:28:15 INFO MemoryStore: Block broadcast_513_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.2 MiB)
26/01/04 17:28:15 INFO BlockManagerInfo: Removed broadcast_511_piece0 on spark-master:33535 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:28:15 INFO BlockManagerInfo: Added broadcast_513_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:28:15 INFO SparkContext: Created broadcast 513 from broadcast at DAGScheduler.scala:1585
26/01/04 17:28:15 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 344 (MapPartitionsRDD[1723] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 17:28:15 INFO TaskSchedulerImpl: Adding task set 344.0 with 2 tasks resource profile 0
26/01/04 17:28:15 INFO TaskSetManager: Starting task 0.0 in stage 344.0 (TID 551) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 17:28:15 INFO TaskSetManager: Starting task 1.0 in stage 344.0 (TID 552) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 17:28:15 INFO BlockManagerInfo: Removed broadcast_511_piece0 on 172.18.0.13:44401 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:28:15 INFO BlockManagerInfo: Removed broadcast_510_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:28:15 INFO BlockManagerInfo: Removed broadcast_510_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:28:15 INFO BlockManagerInfo: Removed broadcast_510_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:28:15 INFO BlockManagerInfo: Added broadcast_513_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:28:15 INFO BlockManagerInfo: Removed broadcast_512_piece0 on spark-master:33535 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:28:15 INFO BlockManagerInfo: Removed broadcast_512_piece0 on 172.18.0.13:44401 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:28:15 INFO BlockManagerInfo: Added broadcast_513_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:28:15 INFO TaskSetManager: Finished task 1.0 in stage 344.0 (TID 552) in 116 ms on 172.18.0.13 (executor 0) (1/2)
26/01/04 17:28:16 INFO TaskSetManager: Finished task 0.0 in stage 344.0 (TID 551) in 189 ms on 172.18.0.14 (executor 1) (2/2)
26/01/04 17:28:16 INFO TaskSchedulerImpl: Removed TaskSet 344.0, whose tasks have all completed, from pool 
26/01/04 17:28:16 INFO DAGScheduler: ResultStage 344 (start at NativeMethodAccessorImpl.java:0) finished in 0.231 s
26/01/04 17:28:16 INFO DAGScheduler: Job 343 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:28:16 INFO TaskSchedulerImpl: Killing all running tasks in stage 344: Stage finished
26/01/04 17:28:16 INFO DAGScheduler: Job 343 finished: start at NativeMethodAccessorImpl.java:0, took 0.247442 s
26/01/04 17:28:16 INFO MemoryStore: Block broadcast_514_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.3 MiB)
26/01/04 17:28:16 INFO BlockManagerInfo: Added broadcast_514_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:28:16 INFO SparkContext: Created broadcast 514 from start at NativeMethodAccessorImpl.java:0
26/01/04 17:28:16 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 170, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@1a519db1]. The input RDD has 1 partitions.
26/01/04 17:28:16 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:28:16 INFO DAGScheduler: Got job 344 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/01/04 17:28:16 INFO DAGScheduler: Final stage: ResultStage 345 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:28:16 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:28:16 INFO DAGScheduler: Missing parents: List()
26/01/04 17:28:16 INFO DAGScheduler: Submitting ResultStage 345 (MapPartitionsRDD[1728] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:28:16 INFO MemoryStore: Block broadcast_515 stored as values in memory (estimated size 52.9 KiB, free 434.3 MiB)
26/01/04 17:28:16 INFO MemoryStore: Block broadcast_515_piece0 stored as bytes in memory (estimated size 20.2 KiB, free 434.3 MiB)
26/01/04 17:28:16 INFO BlockManagerInfo: Added broadcast_515_piece0 in memory on spark-master:33535 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:28:16 INFO BlockManagerInfo: Removed broadcast_513_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:28:16 INFO SparkContext: Created broadcast 515 from broadcast at DAGScheduler.scala:1585
26/01/04 17:28:16 INFO BlockManagerInfo: Removed broadcast_513_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:28:16 INFO BlockManagerInfo: Removed broadcast_513_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:28:16 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 345 (MapPartitionsRDD[1728] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/01/04 17:28:16 INFO TaskSchedulerImpl: Adding task set 345.0 with 1 tasks resource profile 0
26/01/04 17:28:16 INFO TaskSetManager: Starting task 0.0 in stage 345.0 (TID 553) (172.18.0.13, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:28:16 INFO BlockManagerInfo: Added broadcast_515_piece0 in memory on 172.18.0.13:44401 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:28:16 INFO BlockManagerInfo: Added broadcast_514_piece0 in memory on 172.18.0.13:44401 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:28:16 INFO TaskSetManager: Finished task 0.0 in stage 345.0 (TID 553) in 683 ms on 172.18.0.13 (executor 0) (1/1)
26/01/04 17:28:16 INFO TaskSchedulerImpl: Removed TaskSet 345.0, whose tasks have all completed, from pool 
26/01/04 17:28:16 INFO DAGScheduler: ResultStage 345 (start at NativeMethodAccessorImpl.java:0) finished in 0.727 s
26/01/04 17:28:16 INFO DAGScheduler: Job 344 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:28:16 INFO TaskSchedulerImpl: Killing all running tasks in stage 345: Stage finished
26/01/04 17:28:16 INFO DAGScheduler: Job 344 finished: start at NativeMethodAccessorImpl.java:0, took 0.737470 s
26/01/04 17:28:16 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 170, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@1a519db1] is committing.
26/01/04 17:28:16 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 170, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@1a519db1] committed.
26/01/04 17:28:16 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/170 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.170.da9ba8bb-0958-4113-93c8-fbf304469d11.tmp
26/01/04 17:28:17 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.170.da9ba8bb-0958-4113-93c8-fbf304469d11.tmp to file:/tmp/spark-checkpoint-enrichment/commits/170
26/01/04 17:28:17 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T17:28:15.344Z",
  "batchId" : 170,
  "numInputRows" : 30,
  "inputRowsPerSecond" : 1875.0,
  "processedRowsPerSecond" : 17.81472684085511,
  "durationMs" : {
    "addBatch" : 1187,
    "commitOffsets" : 215,
    "getBatch" : 0,
    "latestOffset" : 4,
    "queryPlanning" : 111,
    "triggerExecution" : 1684,
    "walCommit" : 164
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "0" : 540
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "0" : 570
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "0" : 570
      }
    },
    "numInputRows" : 30,
    "inputRowsPerSecond" : 1875.0,
    "processedRowsPerSecond" : 17.81472684085511,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 31
  }
}
26/01/04 17:28:26 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/171 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.171.c5a33c0f-047c-461b-9c56-c66bafaec26b.tmp
26/01/04 17:28:26 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.171.c5a33c0f-047c-461b-9c56-c66bafaec26b.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/171
26/01/04 17:28:26 INFO MicroBatchExecution: Committed offsets for batch 171. Metadata OffsetSeqMetadata(0,1767547706364,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 17:28:26 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:28:26 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:28:26 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:28:26 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:28:26 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:28:26 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:28:26 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#138967 - airline_prefix.nullCount#138966) > 0)
26/01/04 17:28:26 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#139002 - min_flight_num.nullCount#139001) > 0)
26/01/04 17:28:26 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#138997 - max_flight_num.nullCount#138996) > 0)
26/01/04 17:28:26 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:28:26 INFO DAGScheduler: Got job 345 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 17:28:26 INFO DAGScheduler: Final stage: ResultStage 346 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:28:26 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:28:26 INFO DAGScheduler: Missing parents: List()
26/01/04 17:28:26 INFO DAGScheduler: Submitting ResultStage 346 (MapPartitionsRDD[1733] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:28:26 INFO MemoryStore: Block broadcast_516 stored as values in memory (estimated size 38.5 KiB, free 434.3 MiB)
26/01/04 17:28:26 INFO MemoryStore: Block broadcast_516_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.3 MiB)
26/01/04 17:28:26 INFO BlockManagerInfo: Removed broadcast_515_piece0 on spark-master:33535 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:28:26 INFO BlockManagerInfo: Added broadcast_516_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:28:26 INFO SparkContext: Created broadcast 516 from broadcast at DAGScheduler.scala:1585
26/01/04 17:28:26 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 346 (MapPartitionsRDD[1733] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 17:28:26 INFO TaskSchedulerImpl: Adding task set 346.0 with 2 tasks resource profile 0
26/01/04 17:28:26 INFO TaskSetManager: Starting task 1.0 in stage 346.0 (TID 554) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 17:28:26 INFO TaskSetManager: Starting task 0.0 in stage 346.0 (TID 555) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 17:28:26 INFO BlockManagerInfo: Removed broadcast_515_piece0 on 172.18.0.13:44401 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:28:26 INFO BlockManagerInfo: Added broadcast_516_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:28:26 INFO BlockManagerInfo: Removed broadcast_514_piece0 on spark-master:33535 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:28:26 INFO BlockManagerInfo: Removed broadcast_514_piece0 on 172.18.0.13:44401 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:28:26 INFO TaskSetManager: Finished task 1.0 in stage 346.0 (TID 554) in 71 ms on 172.18.0.13 (executor 0) (1/2)
26/01/04 17:28:26 INFO BlockManagerInfo: Added broadcast_516_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:28:26 INFO TaskSetManager: Finished task 0.0 in stage 346.0 (TID 555) in 170 ms on 172.18.0.14 (executor 1) (2/2)
26/01/04 17:28:26 INFO TaskSchedulerImpl: Removed TaskSet 346.0, whose tasks have all completed, from pool 
26/01/04 17:28:26 INFO DAGScheduler: ResultStage 346 (start at NativeMethodAccessorImpl.java:0) finished in 0.212 s
26/01/04 17:28:26 INFO DAGScheduler: Job 345 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:28:26 INFO TaskSchedulerImpl: Killing all running tasks in stage 346: Stage finished
26/01/04 17:28:26 INFO DAGScheduler: Job 345 finished: start at NativeMethodAccessorImpl.java:0, took 0.223459 s
26/01/04 17:28:26 INFO MemoryStore: Block broadcast_517_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.3 MiB)
26/01/04 17:28:26 INFO BlockManagerInfo: Added broadcast_517_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:28:26 INFO SparkContext: Created broadcast 517 from start at NativeMethodAccessorImpl.java:0
26/01/04 17:28:26 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 171, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@2ce4a478]. The input RDD has 1 partitions.
26/01/04 17:28:26 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:28:26 INFO DAGScheduler: Got job 346 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/01/04 17:28:26 INFO DAGScheduler: Final stage: ResultStage 347 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:28:26 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:28:26 INFO DAGScheduler: Missing parents: List()
26/01/04 17:28:26 INFO DAGScheduler: Submitting ResultStage 347 (MapPartitionsRDD[1738] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:28:26 INFO MemoryStore: Block broadcast_518 stored as values in memory (estimated size 52.9 KiB, free 434.3 MiB)
26/01/04 17:28:26 INFO MemoryStore: Block broadcast_518_piece0 stored as bytes in memory (estimated size 20.2 KiB, free 434.3 MiB)
26/01/04 17:28:26 INFO BlockManagerInfo: Added broadcast_518_piece0 in memory on spark-master:33535 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:28:26 INFO SparkContext: Created broadcast 518 from broadcast at DAGScheduler.scala:1585
26/01/04 17:28:26 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 347 (MapPartitionsRDD[1738] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/01/04 17:28:26 INFO TaskSchedulerImpl: Adding task set 347.0 with 1 tasks resource profile 0
26/01/04 17:28:26 INFO TaskSetManager: Starting task 0.0 in stage 347.0 (TID 556) (172.18.0.13, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:28:27 INFO BlockManagerInfo: Added broadcast_518_piece0 in memory on 172.18.0.13:44401 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:28:27 INFO BlockManagerInfo: Added broadcast_517_piece0 in memory on 172.18.0.13:44401 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:28:27 INFO TaskSetManager: Finished task 0.0 in stage 347.0 (TID 556) in 698 ms on 172.18.0.13 (executor 0) (1/1)
26/01/04 17:28:27 INFO TaskSchedulerImpl: Removed TaskSet 347.0, whose tasks have all completed, from pool 
26/01/04 17:28:27 INFO DAGScheduler: ResultStage 347 (start at NativeMethodAccessorImpl.java:0) finished in 0.714 s
26/01/04 17:28:27 INFO DAGScheduler: Job 346 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:28:27 INFO TaskSchedulerImpl: Killing all running tasks in stage 347: Stage finished
26/01/04 17:28:27 INFO DAGScheduler: Job 346 finished: start at NativeMethodAccessorImpl.java:0, took 0.722385 s
26/01/04 17:28:27 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 171, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@2ce4a478] is committing.
26/01/04 17:28:27 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 171, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@2ce4a478] committed.
26/01/04 17:28:27 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/171 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.171.3319a213-9b5c-4955-9c08-9cd394993a32.tmp
26/01/04 17:28:28 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.171.3319a213-9b5c-4955-9c08-9cd394993a32.tmp to file:/tmp/spark-checkpoint-enrichment/commits/171
26/01/04 17:28:28 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T17:28:26.357Z",
  "batchId" : 171,
  "numInputRows" : 30,
  "inputRowsPerSecond" : 1666.6666666666667,
  "processedRowsPerSecond" : 17.28110599078341,
  "durationMs" : {
    "addBatch" : 1099,
    "commitOffsets" : 402,
    "getBatch" : 0,
    "latestOffset" : 7,
    "queryPlanning" : 55,
    "triggerExecution" : 1736,
    "walCommit" : 171
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "0" : 570
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "0" : 600
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "0" : 600
      }
    },
    "numInputRows" : 30,
    "inputRowsPerSecond" : 1666.6666666666667,
    "processedRowsPerSecond" : 17.28110599078341,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 31
  }
}
26/01/04 17:28:37 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/172 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.172.f2cd1dd7-406f-494c-be6e-e1c9abe94825.tmp
26/01/04 17:28:37 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.172.f2cd1dd7-406f-494c-be6e-e1c9abe94825.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/172
26/01/04 17:28:37 INFO MicroBatchExecution: Committed offsets for batch 172. Metadata OffsetSeqMetadata(0,1767547717386,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 17:28:37 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:28:37 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:28:37 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:28:37 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:28:37 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:28:37 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:28:37 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#139771 - airline_prefix.nullCount#139770) > 0)
26/01/04 17:28:37 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#139806 - min_flight_num.nullCount#139805) > 0)
26/01/04 17:28:37 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#139801 - max_flight_num.nullCount#139800) > 0)
26/01/04 17:28:37 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:28:37 INFO DAGScheduler: Got job 347 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 17:28:37 INFO DAGScheduler: Final stage: ResultStage 348 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:28:37 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:28:37 INFO DAGScheduler: Missing parents: List()
26/01/04 17:28:37 INFO DAGScheduler: Submitting ResultStage 348 (MapPartitionsRDD[1743] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:28:37 INFO MemoryStore: Block broadcast_519 stored as values in memory (estimated size 38.5 KiB, free 434.2 MiB)
26/01/04 17:28:37 INFO MemoryStore: Block broadcast_519_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.2 MiB)
26/01/04 17:28:37 INFO BlockManagerInfo: Removed broadcast_518_piece0 on spark-master:33535 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:28:37 INFO BlockManagerInfo: Added broadcast_519_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:28:37 INFO SparkContext: Created broadcast 519 from broadcast at DAGScheduler.scala:1585
26/01/04 17:28:37 INFO BlockManagerInfo: Removed broadcast_518_piece0 on 172.18.0.13:44401 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:28:37 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 348 (MapPartitionsRDD[1743] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 17:28:37 INFO TaskSchedulerImpl: Adding task set 348.0 with 2 tasks resource profile 0
26/01/04 17:28:37 INFO TaskSetManager: Starting task 1.0 in stage 348.0 (TID 557) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 17:28:37 INFO TaskSetManager: Starting task 0.0 in stage 348.0 (TID 558) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 17:28:38 INFO BlockManagerInfo: Added broadcast_519_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:28:38 INFO BlockManagerInfo: Removed broadcast_516_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:28:38 INFO BlockManagerInfo: Removed broadcast_516_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:28:38 INFO BlockManagerInfo: Added broadcast_519_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:28:38 INFO BlockManagerInfo: Removed broadcast_516_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:28:38 INFO TaskSetManager: Finished task 0.0 in stage 348.0 (TID 558) in 144 ms on 172.18.0.14 (executor 1) (1/2)
26/01/04 17:28:38 INFO TaskSetManager: Finished task 1.0 in stage 348.0 (TID 557) in 152 ms on 172.18.0.13 (executor 0) (2/2)
26/01/04 17:28:38 INFO TaskSchedulerImpl: Removed TaskSet 348.0, whose tasks have all completed, from pool 
26/01/04 17:28:38 INFO DAGScheduler: ResultStage 348 (start at NativeMethodAccessorImpl.java:0) finished in 0.211 s
26/01/04 17:28:38 INFO DAGScheduler: Job 347 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:28:38 INFO TaskSchedulerImpl: Killing all running tasks in stage 348: Stage finished
26/01/04 17:28:38 INFO DAGScheduler: Job 347 finished: start at NativeMethodAccessorImpl.java:0, took 0.215844 s
26/01/04 17:28:38 INFO MemoryStore: Block broadcast_520_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.3 MiB)
26/01/04 17:28:38 INFO BlockManagerInfo: Added broadcast_520_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:28:38 INFO SparkContext: Created broadcast 520 from start at NativeMethodAccessorImpl.java:0
26/01/04 17:28:38 INFO BlockManagerInfo: Removed broadcast_517_piece0 on spark-master:33535 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:28:38 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 172, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@29c329e9]. The input RDD has 1 partitions.
26/01/04 17:28:38 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:28:38 INFO DAGScheduler: Got job 348 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/01/04 17:28:38 INFO DAGScheduler: Final stage: ResultStage 349 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:28:38 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:28:38 INFO DAGScheduler: Missing parents: List()
26/01/04 17:28:38 INFO DAGScheduler: Submitting ResultStage 349 (MapPartitionsRDD[1748] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:28:38 INFO BlockManagerInfo: Removed broadcast_517_piece0 on 172.18.0.13:44401 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:28:38 INFO MemoryStore: Block broadcast_521 stored as values in memory (estimated size 52.9 KiB, free 434.3 MiB)
26/01/04 17:28:38 INFO MemoryStore: Block broadcast_521_piece0 stored as bytes in memory (estimated size 20.2 KiB, free 434.3 MiB)
26/01/04 17:28:38 INFO BlockManagerInfo: Added broadcast_521_piece0 in memory on spark-master:33535 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:28:38 INFO SparkContext: Created broadcast 521 from broadcast at DAGScheduler.scala:1585
26/01/04 17:28:38 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 349 (MapPartitionsRDD[1748] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/01/04 17:28:38 INFO TaskSchedulerImpl: Adding task set 349.0 with 1 tasks resource profile 0
26/01/04 17:28:38 INFO TaskSetManager: Starting task 0.0 in stage 349.0 (TID 559) (172.18.0.13, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:28:38 INFO BlockManagerInfo: Added broadcast_521_piece0 in memory on 172.18.0.13:44401 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:28:38 INFO BlockManagerInfo: Added broadcast_520_piece0 in memory on 172.18.0.13:44401 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:28:39 INFO TaskSetManager: Finished task 0.0 in stage 349.0 (TID 559) in 817 ms on 172.18.0.13 (executor 0) (1/1)
26/01/04 17:28:39 INFO TaskSchedulerImpl: Removed TaskSet 349.0, whose tasks have all completed, from pool 
26/01/04 17:28:39 INFO DAGScheduler: ResultStage 349 (start at NativeMethodAccessorImpl.java:0) finished in 0.924 s
26/01/04 17:28:39 INFO DAGScheduler: Job 348 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:28:39 INFO TaskSchedulerImpl: Killing all running tasks in stage 349: Stage finished
26/01/04 17:28:39 INFO DAGScheduler: Job 348 finished: start at NativeMethodAccessorImpl.java:0, took 0.940368 s
26/01/04 17:28:39 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 172, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@29c329e9] is committing.
26/01/04 17:28:39 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 172, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@29c329e9] committed.
26/01/04 17:28:39 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/172 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.172.b8826221-2898-4a39-abcf-db8b5f439757.tmp
26/01/04 17:28:39 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.172.b8826221-2898-4a39-abcf-db8b5f439757.tmp to file:/tmp/spark-checkpoint-enrichment/commits/172
26/01/04 17:28:39 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T17:28:37.376Z",
  "batchId" : 172,
  "numInputRows" : 27,
  "inputRowsPerSecond" : 729.7297297297298,
  "processedRowsPerSecond" : 13.38621715418939,
  "durationMs" : {
    "addBatch" : 1314,
    "commitOffsets" : 301,
    "getBatch" : 1,
    "latestOffset" : 9,
    "queryPlanning" : 48,
    "triggerExecution" : 2017,
    "walCommit" : 342
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "0" : 600
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "0" : 627
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "0" : 627
      }
    },
    "numInputRows" : 27,
    "inputRowsPerSecond" : 729.7297297297298,
    "processedRowsPerSecond" : 13.38621715418939,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 28
  }
}
26/01/04 17:28:39 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/173 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.173.034907ca-4e0c-4fe8-ab36-d438ac91f3c0.tmp
26/01/04 17:28:39 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.173.034907ca-4e0c-4fe8-ab36-d438ac91f3c0.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/173
26/01/04 17:28:39 INFO MicroBatchExecution: Committed offsets for batch 173. Metadata OffsetSeqMetadata(0,1767547719406,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 17:28:39 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:28:39 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:28:39 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:28:39 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:28:39 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:28:39 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:28:39 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#140575 - airline_prefix.nullCount#140574) > 0)
26/01/04 17:28:39 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#140610 - min_flight_num.nullCount#140609) > 0)
26/01/04 17:28:39 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#140605 - max_flight_num.nullCount#140604) > 0)
26/01/04 17:28:39 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:28:39 INFO DAGScheduler: Got job 349 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 17:28:39 INFO DAGScheduler: Final stage: ResultStage 350 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:28:39 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:28:39 INFO DAGScheduler: Missing parents: List()
26/01/04 17:28:39 INFO DAGScheduler: Submitting ResultStage 350 (MapPartitionsRDD[1753] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:28:39 INFO MemoryStore: Block broadcast_522 stored as values in memory (estimated size 38.5 KiB, free 434.2 MiB)
26/01/04 17:28:39 INFO MemoryStore: Block broadcast_522_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.2 MiB)
26/01/04 17:28:39 INFO BlockManagerInfo: Removed broadcast_521_piece0 on spark-master:33535 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:28:39 INFO BlockManagerInfo: Added broadcast_522_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:28:39 INFO SparkContext: Created broadcast 522 from broadcast at DAGScheduler.scala:1585
26/01/04 17:28:39 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 350 (MapPartitionsRDD[1753] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 17:28:39 INFO TaskSchedulerImpl: Adding task set 350.0 with 2 tasks resource profile 0
26/01/04 17:28:39 INFO TaskSetManager: Starting task 0.0 in stage 350.0 (TID 560) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 17:28:39 INFO TaskSetManager: Starting task 1.0 in stage 350.0 (TID 561) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 17:28:40 INFO BlockManagerInfo: Removed broadcast_521_piece0 on 172.18.0.13:44401 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:28:40 INFO BlockManagerInfo: Added broadcast_522_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:28:40 INFO BlockManagerInfo: Removed broadcast_519_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:28:40 INFO BlockManagerInfo: Removed broadcast_519_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:28:40 INFO BlockManagerInfo: Removed broadcast_519_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:28:40 INFO BlockManagerInfo: Added broadcast_522_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:28:40 INFO TaskSetManager: Finished task 1.0 in stage 350.0 (TID 561) in 263 ms on 172.18.0.13 (executor 0) (1/2)
26/01/04 17:28:40 INFO BlockManagerInfo: Removed broadcast_520_piece0 on spark-master:33535 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:28:40 INFO BlockManagerInfo: Removed broadcast_520_piece0 on 172.18.0.13:44401 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:28:40 INFO TaskSetManager: Finished task 0.0 in stage 350.0 (TID 560) in 386 ms on 172.18.0.14 (executor 1) (2/2)
26/01/04 17:28:40 INFO TaskSchedulerImpl: Removed TaskSet 350.0, whose tasks have all completed, from pool 
26/01/04 17:28:40 INFO DAGScheduler: ResultStage 350 (start at NativeMethodAccessorImpl.java:0) finished in 0.462 s
26/01/04 17:28:40 INFO DAGScheduler: Job 349 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:28:40 INFO TaskSchedulerImpl: Killing all running tasks in stage 350: Stage finished
26/01/04 17:28:40 INFO DAGScheduler: Job 349 finished: start at NativeMethodAccessorImpl.java:0, took 0.468847 s
26/01/04 17:28:40 INFO BlockManagerInfo: Removed broadcast_522_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:28:40 INFO MemoryStore: Block broadcast_523_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.4 MiB)
26/01/04 17:28:40 INFO BlockManagerInfo: Added broadcast_523_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:28:40 INFO SparkContext: Created broadcast 523 from start at NativeMethodAccessorImpl.java:0
26/01/04 17:28:40 INFO BlockManagerInfo: Removed broadcast_522_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:28:40 INFO BlockManagerInfo: Removed broadcast_522_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:28:40 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 173, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@5ed3a305]. The input RDD has 1 partitions.
26/01/04 17:28:40 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:28:40 INFO DAGScheduler: Got job 350 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/01/04 17:28:40 INFO DAGScheduler: Final stage: ResultStage 351 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:28:40 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:28:40 INFO DAGScheduler: Missing parents: List()
26/01/04 17:28:40 INFO DAGScheduler: Submitting ResultStage 351 (MapPartitionsRDD[1758] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:28:40 INFO MemoryStore: Block broadcast_524 stored as values in memory (estimated size 52.9 KiB, free 434.3 MiB)
26/01/04 17:28:40 INFO MemoryStore: Block broadcast_524_piece0 stored as bytes in memory (estimated size 20.1 KiB, free 434.3 MiB)
26/01/04 17:28:40 INFO BlockManagerInfo: Added broadcast_524_piece0 in memory on spark-master:33535 (size: 20.1 KiB, free: 434.4 MiB)
26/01/04 17:28:40 INFO SparkContext: Created broadcast 524 from broadcast at DAGScheduler.scala:1585
26/01/04 17:28:40 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 351 (MapPartitionsRDD[1758] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/01/04 17:28:40 INFO TaskSchedulerImpl: Adding task set 351.0 with 1 tasks resource profile 0
26/01/04 17:28:40 INFO TaskSetManager: Starting task 0.0 in stage 351.0 (TID 562) (172.18.0.13, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:28:40 INFO BlockManagerInfo: Added broadcast_524_piece0 in memory on 172.18.0.13:44401 (size: 20.1 KiB, free: 434.4 MiB)
26/01/04 17:28:40 INFO BlockManagerInfo: Added broadcast_523_piece0 in memory on 172.18.0.13:44401 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:28:40 INFO TaskSetManager: Finished task 0.0 in stage 351.0 (TID 562) in 320 ms on 172.18.0.13 (executor 0) (1/1)
26/01/04 17:28:40 INFO TaskSchedulerImpl: Removed TaskSet 351.0, whose tasks have all completed, from pool 
26/01/04 17:28:40 INFO DAGScheduler: ResultStage 351 (start at NativeMethodAccessorImpl.java:0) finished in 0.342 s
26/01/04 17:28:40 INFO DAGScheduler: Job 350 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:28:40 INFO TaskSchedulerImpl: Killing all running tasks in stage 351: Stage finished
26/01/04 17:28:40 INFO DAGScheduler: Job 350 finished: start at NativeMethodAccessorImpl.java:0, took 0.347220 s
26/01/04 17:28:40 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 173, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@5ed3a305] is committing.
26/01/04 17:28:40 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 173, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@5ed3a305] committed.
26/01/04 17:28:40 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/173 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.173.b0036c38-f8a4-438d-9aad-97156f7db9f0.tmp
26/01/04 17:28:40 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.173.b0036c38-f8a4-438d-9aad-97156f7db9f0.tmp to file:/tmp/spark-checkpoint-enrichment/commits/173
26/01/04 17:28:40 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T17:28:39.395Z",
  "batchId" : 173,
  "numInputRows" : 3,
  "inputRowsPerSecond" : 1.4858841010401187,
  "processedRowsPerSecond" : 2.018842530282638,
  "durationMs" : {
    "addBatch" : 1060,
    "commitOffsets" : 157,
    "getBatch" : 1,
    "latestOffset" : 11,
    "queryPlanning" : 71,
    "triggerExecution" : 1486,
    "walCommit" : 183
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "0" : 627
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "0" : 630
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "0" : 630
      }
    },
    "numInputRows" : 3,
    "inputRowsPerSecond" : 1.4858841010401187,
    "processedRowsPerSecond" : 2.018842530282638,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 3
  }
}
26/01/04 17:28:48 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/174 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.174.20fd8ab4-0fac-41f9-ac25-04646aa2f1b5.tmp
26/01/04 17:28:48 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.174.20fd8ab4-0fac-41f9-ac25-04646aa2f1b5.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/174
26/01/04 17:28:48 INFO MicroBatchExecution: Committed offsets for batch 174. Metadata OffsetSeqMetadata(0,1767547728430,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 17:28:48 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:28:48 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:28:48 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:28:48 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:28:48 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:28:48 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:28:48 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#141379 - airline_prefix.nullCount#141378) > 0)
26/01/04 17:28:48 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#141414 - min_flight_num.nullCount#141413) > 0)
26/01/04 17:28:48 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#141409 - max_flight_num.nullCount#141408) > 0)
26/01/04 17:28:48 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:28:48 INFO DAGScheduler: Got job 351 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 17:28:48 INFO DAGScheduler: Final stage: ResultStage 352 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:28:48 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:28:48 INFO DAGScheduler: Missing parents: List()
26/01/04 17:28:48 INFO DAGScheduler: Submitting ResultStage 352 (MapPartitionsRDD[1763] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:28:48 INFO MemoryStore: Block broadcast_525 stored as values in memory (estimated size 38.5 KiB, free 434.3 MiB)
26/01/04 17:28:48 INFO MemoryStore: Block broadcast_525_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.3 MiB)
26/01/04 17:28:48 INFO BlockManagerInfo: Removed broadcast_524_piece0 on spark-master:33535 in memory (size: 20.1 KiB, free: 434.4 MiB)
26/01/04 17:28:48 INFO BlockManagerInfo: Added broadcast_525_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:28:48 INFO BlockManagerInfo: Removed broadcast_524_piece0 on 172.18.0.13:44401 in memory (size: 20.1 KiB, free: 434.4 MiB)
26/01/04 17:28:48 INFO SparkContext: Created broadcast 525 from broadcast at DAGScheduler.scala:1585
26/01/04 17:28:48 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 352 (MapPartitionsRDD[1763] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 17:28:48 INFO TaskSchedulerImpl: Adding task set 352.0 with 2 tasks resource profile 0
26/01/04 17:28:48 INFO TaskSetManager: Starting task 0.0 in stage 352.0 (TID 563) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 17:28:48 INFO TaskSetManager: Starting task 1.0 in stage 352.0 (TID 564) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 17:28:48 INFO BlockManagerInfo: Added broadcast_525_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:28:48 INFO BlockManagerInfo: Added broadcast_525_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:28:48 INFO TaskSetManager: Finished task 1.0 in stage 352.0 (TID 564) in 56 ms on 172.18.0.13 (executor 0) (1/2)
26/01/04 17:28:48 INFO BlockManagerInfo: Removed broadcast_523_piece0 on spark-master:33535 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:28:48 INFO BlockManagerInfo: Removed broadcast_523_piece0 on 172.18.0.13:44401 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:28:49 INFO TaskSetManager: Finished task 0.0 in stage 352.0 (TID 563) in 187 ms on 172.18.0.14 (executor 1) (2/2)
26/01/04 17:28:49 INFO TaskSchedulerImpl: Removed TaskSet 352.0, whose tasks have all completed, from pool 
26/01/04 17:28:49 INFO DAGScheduler: ResultStage 352 (start at NativeMethodAccessorImpl.java:0) finished in 0.255 s
26/01/04 17:28:49 INFO DAGScheduler: Job 351 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:28:49 INFO TaskSchedulerImpl: Killing all running tasks in stage 352: Stage finished
26/01/04 17:28:49 INFO DAGScheduler: Job 351 finished: start at NativeMethodAccessorImpl.java:0, took 0.262990 s
26/01/04 17:28:49 INFO MemoryStore: Block broadcast_526_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.3 MiB)
26/01/04 17:28:49 INFO BlockManagerInfo: Added broadcast_526_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:28:49 INFO SparkContext: Created broadcast 526 from start at NativeMethodAccessorImpl.java:0
26/01/04 17:28:49 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 174, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@1ee26637]. The input RDD has 1 partitions.
26/01/04 17:28:49 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:28:49 INFO DAGScheduler: Got job 352 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/01/04 17:28:49 INFO DAGScheduler: Final stage: ResultStage 353 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:28:49 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:28:49 INFO DAGScheduler: Missing parents: List()
26/01/04 17:28:49 INFO DAGScheduler: Submitting ResultStage 353 (MapPartitionsRDD[1768] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:28:49 INFO MemoryStore: Block broadcast_527 stored as values in memory (estimated size 52.9 KiB, free 434.3 MiB)
26/01/04 17:28:49 INFO MemoryStore: Block broadcast_527_piece0 stored as bytes in memory (estimated size 20.2 KiB, free 434.3 MiB)
26/01/04 17:28:49 INFO BlockManagerInfo: Added broadcast_527_piece0 in memory on spark-master:33535 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:28:49 INFO BlockManagerInfo: Removed broadcast_525_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:28:49 INFO BlockManagerInfo: Removed broadcast_525_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:28:49 INFO SparkContext: Created broadcast 527 from broadcast at DAGScheduler.scala:1585
26/01/04 17:28:49 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 353 (MapPartitionsRDD[1768] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/01/04 17:28:49 INFO TaskSchedulerImpl: Adding task set 353.0 with 1 tasks resource profile 0
26/01/04 17:28:49 INFO TaskSetManager: Starting task 0.0 in stage 353.0 (TID 565) (172.18.0.13, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:28:49 INFO BlockManagerInfo: Removed broadcast_525_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:28:49 INFO BlockManagerInfo: Added broadcast_527_piece0 in memory on 172.18.0.13:44401 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:28:49 INFO BlockManagerInfo: Added broadcast_526_piece0 in memory on 172.18.0.13:44401 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:28:49 INFO TaskSetManager: Finished task 0.0 in stage 353.0 (TID 565) in 623 ms on 172.18.0.13 (executor 0) (1/1)
26/01/04 17:28:49 INFO TaskSchedulerImpl: Removed TaskSet 353.0, whose tasks have all completed, from pool 
26/01/04 17:28:49 INFO DAGScheduler: ResultStage 353 (start at NativeMethodAccessorImpl.java:0) finished in 0.672 s
26/01/04 17:28:49 INFO DAGScheduler: Job 352 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:28:49 INFO TaskSchedulerImpl: Killing all running tasks in stage 353: Stage finished
26/01/04 17:28:49 INFO DAGScheduler: Job 352 finished: start at NativeMethodAccessorImpl.java:0, took 0.681601 s
26/01/04 17:28:49 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 174, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@1ee26637] is committing.
26/01/04 17:28:49 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 174, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@1ee26637] committed.
26/01/04 17:28:49 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/174 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.174.341d4ae3-0b14-4efb-8eb9-d19d1e130b81.tmp
26/01/04 17:28:49 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.174.341d4ae3-0b14-4efb-8eb9-d19d1e130b81.tmp to file:/tmp/spark-checkpoint-enrichment/commits/174
26/01/04 17:28:49 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T17:28:48.420Z",
  "batchId" : 174,
  "numInputRows" : 18,
  "inputRowsPerSecond" : 857.1428571428571,
  "processedRowsPerSecond" : 11.726384364820847,
  "durationMs" : {
    "addBatch" : 1122,
    "commitOffsets" : 155,
    "getBatch" : 0,
    "latestOffset" : 10,
    "queryPlanning" : 40,
    "triggerExecution" : 1535,
    "walCommit" : 207
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "0" : 630
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "0" : 648
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "0" : 648
      }
    },
    "numInputRows" : 18,
    "inputRowsPerSecond" : 857.1428571428571,
    "processedRowsPerSecond" : 11.726384364820847,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 19
  }
}
26/01/04 17:28:50 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/175 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.175.c42c819b-63fb-43f7-aca3-9eebf70c6edc.tmp
26/01/04 17:28:50 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.175.c42c819b-63fb-43f7-aca3-9eebf70c6edc.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/175
26/01/04 17:28:50 INFO MicroBatchExecution: Committed offsets for batch 175. Metadata OffsetSeqMetadata(0,1767547729961,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 17:28:50 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:28:50 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:28:50 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:28:50 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:28:50 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:28:50 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:28:50 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#142183 - airline_prefix.nullCount#142182) > 0)
26/01/04 17:28:50 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#142218 - min_flight_num.nullCount#142217) > 0)
26/01/04 17:28:50 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#142213 - max_flight_num.nullCount#142212) > 0)
26/01/04 17:28:50 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:28:50 INFO DAGScheduler: Got job 353 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 17:28:50 INFO DAGScheduler: Final stage: ResultStage 354 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:28:50 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:28:50 INFO DAGScheduler: Missing parents: List()
26/01/04 17:28:50 INFO DAGScheduler: Submitting ResultStage 354 (MapPartitionsRDD[1773] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:28:50 INFO MemoryStore: Block broadcast_528 stored as values in memory (estimated size 38.5 KiB, free 434.3 MiB)
26/01/04 17:28:50 INFO MemoryStore: Block broadcast_528_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.3 MiB)
26/01/04 17:28:50 INFO BlockManagerInfo: Removed broadcast_527_piece0 on spark-master:33535 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:28:50 INFO BlockManagerInfo: Added broadcast_528_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:28:50 INFO BlockManagerInfo: Removed broadcast_527_piece0 on 172.18.0.13:44401 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:28:50 INFO SparkContext: Created broadcast 528 from broadcast at DAGScheduler.scala:1585
26/01/04 17:28:50 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 354 (MapPartitionsRDD[1773] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 17:28:50 INFO TaskSchedulerImpl: Adding task set 354.0 with 2 tasks resource profile 0
26/01/04 17:28:50 INFO TaskSetManager: Starting task 1.0 in stage 354.0 (TID 566) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 17:28:50 INFO TaskSetManager: Starting task 0.0 in stage 354.0 (TID 567) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 17:28:50 INFO BlockManagerInfo: Removed broadcast_526_piece0 on spark-master:33535 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:28:50 INFO BlockManagerInfo: Removed broadcast_526_piece0 on 172.18.0.13:44401 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:28:50 INFO BlockManagerInfo: Added broadcast_528_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:28:50 INFO TaskSetManager: Finished task 1.0 in stage 354.0 (TID 566) in 98 ms on 172.18.0.13 (executor 0) (1/2)
26/01/04 17:28:50 INFO BlockManagerInfo: Added broadcast_528_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:28:50 INFO TaskSetManager: Finished task 0.0 in stage 354.0 (TID 567) in 261 ms on 172.18.0.14 (executor 1) (2/2)
26/01/04 17:28:50 INFO TaskSchedulerImpl: Removed TaskSet 354.0, whose tasks have all completed, from pool 
26/01/04 17:28:50 INFO DAGScheduler: ResultStage 354 (start at NativeMethodAccessorImpl.java:0) finished in 0.321 s
26/01/04 17:28:50 INFO DAGScheduler: Job 353 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:28:50 INFO TaskSchedulerImpl: Killing all running tasks in stage 354: Stage finished
26/01/04 17:28:50 INFO DAGScheduler: Job 353 finished: start at NativeMethodAccessorImpl.java:0, took 0.332756 s
26/01/04 17:28:50 INFO MemoryStore: Block broadcast_529_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.3 MiB)
26/01/04 17:28:50 INFO BlockManagerInfo: Added broadcast_529_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:28:50 INFO BlockManagerInfo: Removed broadcast_528_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:28:50 INFO SparkContext: Created broadcast 529 from start at NativeMethodAccessorImpl.java:0
26/01/04 17:28:50 INFO BlockManagerInfo: Removed broadcast_528_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:28:50 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 175, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@53a1a2b1]. The input RDD has 1 partitions.
26/01/04 17:28:50 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:28:50 INFO DAGScheduler: Got job 354 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/01/04 17:28:50 INFO DAGScheduler: Final stage: ResultStage 355 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:28:50 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:28:50 INFO DAGScheduler: Missing parents: List()
26/01/04 17:28:50 INFO BlockManagerInfo: Removed broadcast_528_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:28:50 INFO DAGScheduler: Submitting ResultStage 355 (MapPartitionsRDD[1778] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:28:50 INFO MemoryStore: Block broadcast_530 stored as values in memory (estimated size 52.9 KiB, free 434.3 MiB)
26/01/04 17:28:50 INFO MemoryStore: Block broadcast_530_piece0 stored as bytes in memory (estimated size 20.2 KiB, free 434.3 MiB)
26/01/04 17:28:50 INFO BlockManagerInfo: Added broadcast_530_piece0 in memory on spark-master:33535 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:28:50 INFO SparkContext: Created broadcast 530 from broadcast at DAGScheduler.scala:1585
26/01/04 17:28:50 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 355 (MapPartitionsRDD[1778] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/01/04 17:28:50 INFO TaskSchedulerImpl: Adding task set 355.0 with 1 tasks resource profile 0
26/01/04 17:28:50 INFO TaskSetManager: Starting task 0.0 in stage 355.0 (TID 568) (172.18.0.13, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:28:51 INFO BlockManagerInfo: Added broadcast_530_piece0 in memory on 172.18.0.13:44401 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:28:51 INFO BlockManagerInfo: Added broadcast_529_piece0 in memory on 172.18.0.13:44401 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:28:51 INFO TaskSetManager: Finished task 0.0 in stage 355.0 (TID 568) in 203 ms on 172.18.0.13 (executor 0) (1/1)
26/01/04 17:28:51 INFO DAGScheduler: ResultStage 355 (start at NativeMethodAccessorImpl.java:0) finished in 0.241 s
26/01/04 17:28:51 INFO DAGScheduler: Job 354 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:28:51 INFO TaskSchedulerImpl: Removed TaskSet 355.0, whose tasks have all completed, from pool 
26/01/04 17:28:51 INFO TaskSchedulerImpl: Killing all running tasks in stage 355: Stage finished
26/01/04 17:28:51 INFO DAGScheduler: Job 354 finished: start at NativeMethodAccessorImpl.java:0, took 0.259640 s
26/01/04 17:28:51 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 175, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@53a1a2b1] is committing.
26/01/04 17:28:51 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 175, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@53a1a2b1] committed.
26/01/04 17:28:51 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/175 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.175.8a2648c6-c8a7-41aa-b2f5-09a5cc9ee03e.tmp
26/01/04 17:28:51 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.175.8a2648c6-c8a7-41aa-b2f5-09a5cc9ee03e.tmp to file:/tmp/spark-checkpoint-enrichment/commits/175
26/01/04 17:28:51 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T17:28:49.956Z",
  "batchId" : 175,
  "numInputRows" : 12,
  "inputRowsPerSecond" : 7.8125,
  "processedRowsPerSecond" : 9.06344410876133,
  "durationMs" : {
    "addBatch" : 803,
    "commitOffsets" : 192,
    "getBatch" : 0,
    "latestOffset" : 5,
    "queryPlanning" : 79,
    "triggerExecution" : 1324,
    "walCommit" : 244
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "0" : 648
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "0" : 660
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "0" : 660
      }
    },
    "numInputRows" : 12,
    "inputRowsPerSecond" : 7.8125,
    "processedRowsPerSecond" : 9.06344410876133,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 12
  }
}
26/01/04 17:28:59 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/176 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.176.0ad50b5e-2c69-4ff2-a7ce-93c4fe647048.tmp
26/01/04 17:28:59 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.176.0ad50b5e-2c69-4ff2-a7ce-93c4fe647048.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/176
26/01/04 17:28:59 INFO MicroBatchExecution: Committed offsets for batch 176. Metadata OffsetSeqMetadata(0,1767547739488,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 17:28:59 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:28:59 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:28:59 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:28:59 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:28:59 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:28:59 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:28:59 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#142987 - airline_prefix.nullCount#142986) > 0)
26/01/04 17:28:59 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#143022 - min_flight_num.nullCount#143021) > 0)
26/01/04 17:28:59 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#143017 - max_flight_num.nullCount#143016) > 0)
26/01/04 17:28:59 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:28:59 INFO DAGScheduler: Got job 355 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 17:28:59 INFO DAGScheduler: Final stage: ResultStage 356 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:28:59 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:28:59 INFO DAGScheduler: Missing parents: List()
26/01/04 17:28:59 INFO DAGScheduler: Submitting ResultStage 356 (MapPartitionsRDD[1783] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:28:59 INFO MemoryStore: Block broadcast_531 stored as values in memory (estimated size 38.5 KiB, free 434.3 MiB)
26/01/04 17:28:59 INFO MemoryStore: Block broadcast_531_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.3 MiB)
26/01/04 17:28:59 INFO BlockManagerInfo: Added broadcast_531_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:28:59 INFO SparkContext: Created broadcast 531 from broadcast at DAGScheduler.scala:1585
26/01/04 17:28:59 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 356 (MapPartitionsRDD[1783] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 17:28:59 INFO TaskSchedulerImpl: Adding task set 356.0 with 2 tasks resource profile 0
26/01/04 17:28:59 INFO TaskSetManager: Starting task 0.0 in stage 356.0 (TID 569) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 17:28:59 INFO TaskSetManager: Starting task 1.0 in stage 356.0 (TID 570) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 17:28:59 INFO BlockManagerInfo: Added broadcast_531_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:28:59 INFO BlockManagerInfo: Removed broadcast_530_piece0 on spark-master:33535 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:29:00 INFO BlockManagerInfo: Removed broadcast_530_piece0 on 172.18.0.13:44401 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:29:00 INFO BlockManagerInfo: Added broadcast_531_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:29:00 INFO TaskSetManager: Finished task 0.0 in stage 356.0 (TID 569) in 177 ms on 172.18.0.14 (executor 1) (1/2)
26/01/04 17:29:00 INFO TaskSetManager: Finished task 1.0 in stage 356.0 (TID 570) in 184 ms on 172.18.0.13 (executor 0) (2/2)
26/01/04 17:29:00 INFO TaskSchedulerImpl: Removed TaskSet 356.0, whose tasks have all completed, from pool 
26/01/04 17:29:00 INFO DAGScheduler: ResultStage 356 (start at NativeMethodAccessorImpl.java:0) finished in 0.245 s
26/01/04 17:29:00 INFO DAGScheduler: Job 355 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:29:00 INFO TaskSchedulerImpl: Killing all running tasks in stage 356: Stage finished
26/01/04 17:29:00 INFO DAGScheduler: Job 355 finished: start at NativeMethodAccessorImpl.java:0, took 0.249981 s
26/01/04 17:29:00 INFO MemoryStore: Block broadcast_532_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.3 MiB)
26/01/04 17:29:00 INFO BlockManagerInfo: Added broadcast_532_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:29:00 INFO SparkContext: Created broadcast 532 from start at NativeMethodAccessorImpl.java:0
26/01/04 17:29:00 INFO BlockManagerInfo: Removed broadcast_529_piece0 on spark-master:33535 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:29:00 INFO BlockManagerInfo: Removed broadcast_529_piece0 on 172.18.0.13:44401 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:29:00 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 176, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@1b45a71]. The input RDD has 1 partitions.
26/01/04 17:29:00 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:29:00 INFO DAGScheduler: Got job 356 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/01/04 17:29:00 INFO DAGScheduler: Final stage: ResultStage 357 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:29:00 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:29:00 INFO DAGScheduler: Missing parents: List()
26/01/04 17:29:00 INFO DAGScheduler: Submitting ResultStage 357 (MapPartitionsRDD[1788] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:29:00 INFO MemoryStore: Block broadcast_533 stored as values in memory (estimated size 52.9 KiB, free 434.3 MiB)
26/01/04 17:29:00 INFO MemoryStore: Block broadcast_533_piece0 stored as bytes in memory (estimated size 20.2 KiB, free 434.3 MiB)
26/01/04 17:29:00 INFO BlockManagerInfo: Added broadcast_533_piece0 in memory on spark-master:33535 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:29:00 INFO SparkContext: Created broadcast 533 from broadcast at DAGScheduler.scala:1585
26/01/04 17:29:00 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 357 (MapPartitionsRDD[1788] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/01/04 17:29:00 INFO TaskSchedulerImpl: Adding task set 357.0 with 1 tasks resource profile 0
26/01/04 17:29:00 INFO TaskSetManager: Starting task 0.0 in stage 357.0 (TID 571) (172.18.0.13, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:29:00 INFO BlockManagerInfo: Added broadcast_533_piece0 in memory on 172.18.0.13:44401 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:29:00 INFO BlockManagerInfo: Added broadcast_532_piece0 in memory on 172.18.0.13:44401 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:29:00 INFO TaskSetManager: Finished task 0.0 in stage 357.0 (TID 571) in 655 ms on 172.18.0.13 (executor 0) (1/1)
26/01/04 17:29:00 INFO TaskSchedulerImpl: Removed TaskSet 357.0, whose tasks have all completed, from pool 
26/01/04 17:29:00 INFO DAGScheduler: ResultStage 357 (start at NativeMethodAccessorImpl.java:0) finished in 0.672 s
26/01/04 17:29:00 INFO DAGScheduler: Job 356 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:29:00 INFO TaskSchedulerImpl: Killing all running tasks in stage 357: Stage finished
26/01/04 17:29:00 INFO DAGScheduler: Job 356 finished: start at NativeMethodAccessorImpl.java:0, took 0.678387 s
26/01/04 17:29:00 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 176, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@1b45a71] is committing.
26/01/04 17:29:00 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 176, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@1b45a71] committed.
26/01/04 17:29:00 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/176 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.176.5eb068b8-1004-4994-be9b-fe7d7ee11f1e.tmp
26/01/04 17:29:01 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.176.5eb068b8-1004-4994-be9b-fe7d7ee11f1e.tmp to file:/tmp/spark-checkpoint-enrichment/commits/176
26/01/04 17:29:01 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T17:28:59.485Z",
  "batchId" : 176,
  "numInputRows" : 30,
  "inputRowsPerSecond" : 2142.8571428571427,
  "processedRowsPerSecond" : 18.76172607879925,
  "durationMs" : {
    "addBatch" : 1143,
    "commitOffsets" : 238,
    "getBatch" : 0,
    "latestOffset" : 3,
    "queryPlanning" : 44,
    "triggerExecution" : 1599,
    "walCommit" : 170
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "0" : 660
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "0" : 690
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "0" : 690
      }
    },
    "numInputRows" : 30,
    "inputRowsPerSecond" : 2142.8571428571427,
    "processedRowsPerSecond" : 18.76172607879925,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 31
  }
}
26/01/04 17:29:10 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/177 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.177.c8857bc8-db76-4712-a8a0-e2240a3b0d3a.tmp
26/01/04 17:29:10 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.177.c8857bc8-db76-4712-a8a0-e2240a3b0d3a.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/177
26/01/04 17:29:10 INFO MicroBatchExecution: Committed offsets for batch 177. Metadata OffsetSeqMetadata(0,1767547750526,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 17:29:10 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:29:10 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:29:10 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:29:10 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:29:10 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:29:10 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:29:10 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#143791 - airline_prefix.nullCount#143790) > 0)
26/01/04 17:29:10 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#143826 - min_flight_num.nullCount#143825) > 0)
26/01/04 17:29:10 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#143821 - max_flight_num.nullCount#143820) > 0)
26/01/04 17:29:10 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:29:10 INFO DAGScheduler: Got job 357 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 17:29:10 INFO DAGScheduler: Final stage: ResultStage 358 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:29:10 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:29:10 INFO DAGScheduler: Missing parents: List()
26/01/04 17:29:10 INFO DAGScheduler: Submitting ResultStage 358 (MapPartitionsRDD[1793] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:29:10 INFO MemoryStore: Block broadcast_534 stored as values in memory (estimated size 38.5 KiB, free 434.2 MiB)
26/01/04 17:29:11 INFO MemoryStore: Block broadcast_534_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.2 MiB)
26/01/04 17:29:11 INFO BlockManagerInfo: Added broadcast_534_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:29:11 INFO SparkContext: Created broadcast 534 from broadcast at DAGScheduler.scala:1585
26/01/04 17:29:11 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 358 (MapPartitionsRDD[1793] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 17:29:11 INFO TaskSchedulerImpl: Adding task set 358.0 with 2 tasks resource profile 0
26/01/04 17:29:11 INFO BlockManagerInfo: Removed broadcast_533_piece0 on spark-master:33535 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:29:11 INFO TaskSetManager: Starting task 0.0 in stage 358.0 (TID 572) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 17:29:11 INFO TaskSetManager: Starting task 1.0 in stage 358.0 (TID 573) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 17:29:11 INFO BlockManagerInfo: Removed broadcast_533_piece0 on 172.18.0.13:44401 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:29:11 INFO BlockManagerInfo: Added broadcast_534_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:29:11 INFO TaskSetManager: Finished task 1.0 in stage 358.0 (TID 573) in 211 ms on 172.18.0.13 (executor 0) (1/2)
26/01/04 17:29:11 INFO BlockManagerInfo: Removed broadcast_532_piece0 on 172.18.0.13:44401 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:29:11 INFO BlockManagerInfo: Removed broadcast_532_piece0 on spark-master:33535 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:29:11 INFO BlockManagerInfo: Removed broadcast_531_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:29:11 INFO BlockManagerInfo: Removed broadcast_531_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:29:11 INFO BlockManagerInfo: Removed broadcast_531_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:29:11 INFO BlockManagerInfo: Added broadcast_534_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:29:11 INFO TaskSetManager: Finished task 0.0 in stage 358.0 (TID 572) in 403 ms on 172.18.0.14 (executor 1) (2/2)
26/01/04 17:29:11 INFO TaskSchedulerImpl: Removed TaskSet 358.0, whose tasks have all completed, from pool 
26/01/04 17:29:11 INFO DAGScheduler: ResultStage 358 (start at NativeMethodAccessorImpl.java:0) finished in 0.469 s
26/01/04 17:29:11 INFO DAGScheduler: Job 357 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:29:11 INFO TaskSchedulerImpl: Killing all running tasks in stage 358: Stage finished
26/01/04 17:29:11 INFO DAGScheduler: Job 357 finished: start at NativeMethodAccessorImpl.java:0, took 0.477278 s
26/01/04 17:29:11 INFO MemoryStore: Block broadcast_535_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.3 MiB)
26/01/04 17:29:11 INFO BlockManagerInfo: Added broadcast_535_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:29:11 INFO SparkContext: Created broadcast 535 from start at NativeMethodAccessorImpl.java:0
26/01/04 17:29:11 INFO BlockManagerInfo: Removed broadcast_534_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:29:11 INFO BlockManagerInfo: Removed broadcast_534_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:29:11 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 177, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@707a1c56]. The input RDD has 1 partitions.
26/01/04 17:29:11 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:29:11 INFO DAGScheduler: Got job 358 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/01/04 17:29:11 INFO DAGScheduler: Final stage: ResultStage 359 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:29:11 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:29:11 INFO DAGScheduler: Missing parents: List()
26/01/04 17:29:11 INFO DAGScheduler: Submitting ResultStage 359 (MapPartitionsRDD[1798] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:29:11 INFO MemoryStore: Block broadcast_536 stored as values in memory (estimated size 52.9 KiB, free 434.3 MiB)
26/01/04 17:29:11 INFO MemoryStore: Block broadcast_536_piece0 stored as bytes in memory (estimated size 20.2 KiB, free 434.3 MiB)
26/01/04 17:29:11 INFO BlockManagerInfo: Added broadcast_536_piece0 in memory on spark-master:33535 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:29:11 INFO SparkContext: Created broadcast 536 from broadcast at DAGScheduler.scala:1585
26/01/04 17:29:11 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 359 (MapPartitionsRDD[1798] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/01/04 17:29:11 INFO TaskSchedulerImpl: Adding task set 359.0 with 1 tasks resource profile 0
26/01/04 17:29:11 INFO TaskSetManager: Starting task 0.0 in stage 359.0 (TID 574) (172.18.0.13, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:29:11 INFO BlockManagerInfo: Added broadcast_536_piece0 in memory on 172.18.0.13:44401 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:29:11 INFO BlockManagerInfo: Added broadcast_535_piece0 in memory on 172.18.0.13:44401 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:29:11 INFO BlockManagerInfo: Removed broadcast_534_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:29:12 INFO TaskSetManager: Finished task 0.0 in stage 359.0 (TID 574) in 741 ms on 172.18.0.13 (executor 0) (1/1)
26/01/04 17:29:12 INFO TaskSchedulerImpl: Removed TaskSet 359.0, whose tasks have all completed, from pool 
26/01/04 17:29:12 INFO DAGScheduler: ResultStage 359 (start at NativeMethodAccessorImpl.java:0) finished in 0.765 s
26/01/04 17:29:12 INFO DAGScheduler: Job 358 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:29:12 INFO TaskSchedulerImpl: Killing all running tasks in stage 359: Stage finished
26/01/04 17:29:12 INFO DAGScheduler: Job 358 finished: start at NativeMethodAccessorImpl.java:0, took 0.777560 s
26/01/04 17:29:12 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 177, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@707a1c56] is committing.
26/01/04 17:29:12 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 177, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@707a1c56] committed.
26/01/04 17:29:12 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/177 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.177.b64ebdf3-0918-4bff-a445-7055b7560614.tmp
26/01/04 17:29:12 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.177.b64ebdf3-0918-4bff-a445-7055b7560614.tmp to file:/tmp/spark-checkpoint-enrichment/commits/177
26/01/04 17:29:12 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T17:29:10.521Z",
  "batchId" : 177,
  "numInputRows" : 30,
  "inputRowsPerSecond" : 1875.0,
  "processedRowsPerSecond" : 13.901760889712698,
  "durationMs" : {
    "addBatch" : 1463,
    "commitOffsets" : 403,
    "getBatch" : 0,
    "latestOffset" : 5,
    "queryPlanning" : 45,
    "triggerExecution" : 2158,
    "walCommit" : 238
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "0" : 690
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "0" : 720
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "0" : 720
      }
    },
    "numInputRows" : 30,
    "inputRowsPerSecond" : 1875.0,
    "processedRowsPerSecond" : 13.901760889712698,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 31
  }
}
26/01/04 17:29:21 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/178 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.178.f605468f-0c76-4c47-9849-0d8b950df969.tmp
26/01/04 17:29:21 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.178.f605468f-0c76-4c47-9849-0d8b950df969.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/178
26/01/04 17:29:21 INFO MicroBatchExecution: Committed offsets for batch 178. Metadata OffsetSeqMetadata(0,1767547761542,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 17:29:21 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:29:21 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:29:21 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:29:21 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:29:22 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:29:22 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:29:22 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#144595 - airline_prefix.nullCount#144594) > 0)
26/01/04 17:29:22 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#144630 - min_flight_num.nullCount#144629) > 0)
26/01/04 17:29:22 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#144625 - max_flight_num.nullCount#144624) > 0)
26/01/04 17:29:22 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:29:22 INFO DAGScheduler: Got job 359 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 17:29:22 INFO DAGScheduler: Final stage: ResultStage 360 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:29:22 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:29:22 INFO DAGScheduler: Missing parents: List()
26/01/04 17:29:22 INFO DAGScheduler: Submitting ResultStage 360 (MapPartitionsRDD[1803] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:29:22 INFO MemoryStore: Block broadcast_537 stored as values in memory (estimated size 38.5 KiB, free 434.3 MiB)
26/01/04 17:29:22 INFO MemoryStore: Block broadcast_537_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.3 MiB)
26/01/04 17:29:22 INFO BlockManagerInfo: Removed broadcast_535_piece0 on spark-master:33535 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:29:22 INFO BlockManagerInfo: Removed broadcast_535_piece0 on 172.18.0.13:44401 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:29:22 INFO BlockManagerInfo: Added broadcast_537_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:29:22 INFO SparkContext: Created broadcast 537 from broadcast at DAGScheduler.scala:1585
26/01/04 17:29:22 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 360 (MapPartitionsRDD[1803] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 17:29:22 INFO TaskSchedulerImpl: Adding task set 360.0 with 2 tasks resource profile 0
26/01/04 17:29:22 INFO TaskSetManager: Starting task 1.0 in stage 360.0 (TID 575) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 17:29:22 INFO TaskSetManager: Starting task 0.0 in stage 360.0 (TID 576) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 17:29:22 INFO BlockManagerInfo: Removed broadcast_536_piece0 on spark-master:33535 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:29:22 INFO BlockManagerInfo: Added broadcast_537_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:29:22 INFO BlockManagerInfo: Removed broadcast_536_piece0 on 172.18.0.13:44401 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:29:22 INFO BlockManagerInfo: Added broadcast_537_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:29:22 INFO TaskSetManager: Finished task 0.0 in stage 360.0 (TID 576) in 421 ms on 172.18.0.14 (executor 1) (1/2)
26/01/04 17:29:22 INFO TaskSetManager: Finished task 1.0 in stage 360.0 (TID 575) in 429 ms on 172.18.0.13 (executor 0) (2/2)
26/01/04 17:29:22 INFO TaskSchedulerImpl: Removed TaskSet 360.0, whose tasks have all completed, from pool 
26/01/04 17:29:22 INFO DAGScheduler: ResultStage 360 (start at NativeMethodAccessorImpl.java:0) finished in 0.485 s
26/01/04 17:29:22 INFO DAGScheduler: Job 359 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:29:22 INFO TaskSchedulerImpl: Killing all running tasks in stage 360: Stage finished
26/01/04 17:29:22 INFO DAGScheduler: Job 359 finished: start at NativeMethodAccessorImpl.java:0, took 0.489355 s
26/01/04 17:29:22 INFO MemoryStore: Block broadcast_538_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.3 MiB)
26/01/04 17:29:22 INFO BlockManagerInfo: Added broadcast_538_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:29:22 INFO SparkContext: Created broadcast 538 from start at NativeMethodAccessorImpl.java:0
26/01/04 17:29:22 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 178, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@31aab178]. The input RDD has 1 partitions.
26/01/04 17:29:22 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:29:22 INFO DAGScheduler: Got job 360 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/01/04 17:29:22 INFO DAGScheduler: Final stage: ResultStage 361 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:29:22 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:29:22 INFO DAGScheduler: Missing parents: List()
26/01/04 17:29:22 INFO DAGScheduler: Submitting ResultStage 361 (MapPartitionsRDD[1808] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:29:22 INFO MemoryStore: Block broadcast_539 stored as values in memory (estimated size 52.9 KiB, free 434.3 MiB)
26/01/04 17:29:22 INFO MemoryStore: Block broadcast_539_piece0 stored as bytes in memory (estimated size 20.2 KiB, free 434.3 MiB)
26/01/04 17:29:22 INFO BlockManagerInfo: Added broadcast_539_piece0 in memory on spark-master:33535 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:29:22 INFO SparkContext: Created broadcast 539 from broadcast at DAGScheduler.scala:1585
26/01/04 17:29:22 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 361 (MapPartitionsRDD[1808] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/01/04 17:29:22 INFO TaskSchedulerImpl: Adding task set 361.0 with 1 tasks resource profile 0
26/01/04 17:29:22 INFO TaskSetManager: Starting task 0.0 in stage 361.0 (TID 577) (172.18.0.13, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:29:22 INFO BlockManagerInfo: Added broadcast_539_piece0 in memory on 172.18.0.13:44401 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:29:22 INFO BlockManagerInfo: Added broadcast_538_piece0 in memory on 172.18.0.13:44401 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:29:23 INFO TaskSetManager: Finished task 0.0 in stage 361.0 (TID 577) in 730 ms on 172.18.0.13 (executor 0) (1/1)
26/01/04 17:29:23 INFO TaskSchedulerImpl: Removed TaskSet 361.0, whose tasks have all completed, from pool 
26/01/04 17:29:23 INFO DAGScheduler: ResultStage 361 (start at NativeMethodAccessorImpl.java:0) finished in 0.756 s
26/01/04 17:29:23 INFO DAGScheduler: Job 360 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:29:23 INFO TaskSchedulerImpl: Killing all running tasks in stage 361: Stage finished
26/01/04 17:29:23 INFO DAGScheduler: Job 360 finished: start at NativeMethodAccessorImpl.java:0, took 0.763361 s
26/01/04 17:29:23 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 178, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@31aab178] is committing.
26/01/04 17:29:23 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 178, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@31aab178] committed.
26/01/04 17:29:23 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/178 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.178.4291f7ef-426c-4f83-abef-2b78ef651c68.tmp
26/01/04 17:29:23 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.178.4291f7ef-426c-4f83-abef-2b78ef651c68.tmp to file:/tmp/spark-checkpoint-enrichment/commits/178
26/01/04 17:29:23 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T17:29:21.537Z",
  "batchId" : 178,
  "numInputRows" : 30,
  "inputRowsPerSecond" : 2000.0,
  "processedRowsPerSecond" : 14.570179698882951,
  "durationMs" : {
    "addBatch" : 1502,
    "commitOffsets" : 189,
    "getBatch" : 0,
    "latestOffset" : 5,
    "queryPlanning" : 96,
    "triggerExecution" : 2059,
    "walCommit" : 265
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "0" : 720
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "0" : 750
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "0" : 750
      }
    },
    "numInputRows" : 30,
    "inputRowsPerSecond" : 2000.0,
    "processedRowsPerSecond" : 14.570179698882951,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 31
  }
}
26/01/04 17:29:32 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/179 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.179.c3ffcab9-0eae-461e-9975-a72f87ba0341.tmp
26/01/04 17:29:32 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.179.c3ffcab9-0eae-461e-9975-a72f87ba0341.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/179
26/01/04 17:29:32 INFO MicroBatchExecution: Committed offsets for batch 179. Metadata OffsetSeqMetadata(0,1767547772602,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 17:29:32 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:29:32 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:29:33 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:29:33 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:29:33 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:29:33 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:29:33 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#145399 - airline_prefix.nullCount#145398) > 0)
26/01/04 17:29:33 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#145434 - min_flight_num.nullCount#145433) > 0)
26/01/04 17:29:33 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#145429 - max_flight_num.nullCount#145428) > 0)
26/01/04 17:29:33 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:29:33 INFO DAGScheduler: Got job 361 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 17:29:33 INFO DAGScheduler: Final stage: ResultStage 362 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:29:33 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:29:33 INFO DAGScheduler: Missing parents: List()
26/01/04 17:29:33 INFO DAGScheduler: Submitting ResultStage 362 (MapPartitionsRDD[1813] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:29:33 INFO MemoryStore: Block broadcast_540 stored as values in memory (estimated size 38.5 KiB, free 434.2 MiB)
26/01/04 17:29:33 INFO MemoryStore: Block broadcast_540_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.2 MiB)
26/01/04 17:29:33 INFO BlockManagerInfo: Removed broadcast_538_piece0 on 172.18.0.13:44401 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:29:33 INFO BlockManagerInfo: Added broadcast_540_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:29:33 INFO SparkContext: Created broadcast 540 from broadcast at DAGScheduler.scala:1585
26/01/04 17:29:33 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 362 (MapPartitionsRDD[1813] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 17:29:33 INFO TaskSchedulerImpl: Adding task set 362.0 with 2 tasks resource profile 0
26/01/04 17:29:33 INFO TaskSetManager: Starting task 1.0 in stage 362.0 (TID 578) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 17:29:33 INFO TaskSetManager: Starting task 0.0 in stage 362.0 (TID 579) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 17:29:33 INFO BlockManagerInfo: Removed broadcast_538_piece0 on spark-master:33535 in memory (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:29:33 INFO BlockManagerInfo: Added broadcast_540_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:29:33 INFO BlockManagerInfo: Added broadcast_540_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:29:33 INFO BlockManagerInfo: Removed broadcast_537_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:29:33 INFO BlockManagerInfo: Removed broadcast_537_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:29:33 INFO BlockManagerInfo: Removed broadcast_537_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:29:33 INFO TaskSetManager: Finished task 1.0 in stage 362.0 (TID 578) in 101 ms on 172.18.0.13 (executor 0) (1/2)
26/01/04 17:29:33 INFO TaskSetManager: Finished task 0.0 in stage 362.0 (TID 579) in 287 ms on 172.18.0.14 (executor 1) (2/2)
26/01/04 17:29:33 INFO TaskSchedulerImpl: Removed TaskSet 362.0, whose tasks have all completed, from pool 
26/01/04 17:29:33 INFO BlockManagerInfo: Removed broadcast_539_piece0 on 172.18.0.13:44401 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:29:33 INFO DAGScheduler: ResultStage 362 (start at NativeMethodAccessorImpl.java:0) finished in 0.438 s
26/01/04 17:29:33 INFO DAGScheduler: Job 361 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:29:33 INFO TaskSchedulerImpl: Killing all running tasks in stage 362: Stage finished
26/01/04 17:29:33 INFO BlockManagerInfo: Removed broadcast_539_piece0 on spark-master:33535 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:29:33 INFO DAGScheduler: Job 361 finished: start at NativeMethodAccessorImpl.java:0, took 0.462340 s
26/01/04 17:29:33 INFO MemoryStore: Block broadcast_541_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.3 MiB)
26/01/04 17:29:33 INFO BlockManagerInfo: Added broadcast_541_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:29:33 INFO SparkContext: Created broadcast 541 from start at NativeMethodAccessorImpl.java:0
26/01/04 17:29:33 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 179, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@60beee1c]. The input RDD has 1 partitions.
26/01/04 17:29:33 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:29:33 INFO DAGScheduler: Got job 362 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/01/04 17:29:33 INFO DAGScheduler: Final stage: ResultStage 363 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:29:33 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:29:33 INFO DAGScheduler: Missing parents: List()
26/01/04 17:29:33 INFO DAGScheduler: Submitting ResultStage 363 (MapPartitionsRDD[1818] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:29:33 INFO MemoryStore: Block broadcast_542 stored as values in memory (estimated size 52.9 KiB, free 434.3 MiB)
26/01/04 17:29:33 INFO MemoryStore: Block broadcast_542_piece0 stored as bytes in memory (estimated size 20.2 KiB, free 434.3 MiB)
26/01/04 17:29:33 INFO BlockManagerInfo: Added broadcast_542_piece0 in memory on spark-master:33535 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:29:33 INFO SparkContext: Created broadcast 542 from broadcast at DAGScheduler.scala:1585
26/01/04 17:29:33 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 363 (MapPartitionsRDD[1818] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/01/04 17:29:33 INFO TaskSchedulerImpl: Adding task set 363.0 with 1 tasks resource profile 0
26/01/04 17:29:33 INFO TaskSetManager: Starting task 0.0 in stage 363.0 (TID 580) (172.18.0.13, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:29:33 INFO BlockManagerInfo: Removed broadcast_540_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:29:33 INFO BlockManagerInfo: Removed broadcast_540_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:29:33 INFO BlockManagerInfo: Removed broadcast_540_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:29:33 INFO BlockManagerInfo: Added broadcast_542_piece0 in memory on 172.18.0.13:44401 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:29:33 INFO BlockManagerInfo: Added broadcast_541_piece0 in memory on 172.18.0.13:44401 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:29:34 INFO TaskSetManager: Finished task 0.0 in stage 363.0 (TID 580) in 805 ms on 172.18.0.13 (executor 0) (1/1)
26/01/04 17:29:34 INFO DAGScheduler: ResultStage 363 (start at NativeMethodAccessorImpl.java:0) finished in 0.837 s
26/01/04 17:29:34 INFO DAGScheduler: Job 362 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:29:34 INFO TaskSchedulerImpl: Removed TaskSet 363.0, whose tasks have all completed, from pool 
26/01/04 17:29:34 INFO TaskSchedulerImpl: Killing all running tasks in stage 363: Stage finished
26/01/04 17:29:34 INFO DAGScheduler: Job 362 finished: start at NativeMethodAccessorImpl.java:0, took 0.856597 s
26/01/04 17:29:34 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 179, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@60beee1c] is committing.
26/01/04 17:29:34 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 179, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@60beee1c] committed.
26/01/04 17:29:34 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/179 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.179.ff1d5e5b-cbee-475e-9b19-5f30a6fdc7a1.tmp
26/01/04 17:29:34 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.179.ff1d5e5b-cbee-475e-9b19-5f30a6fdc7a1.tmp to file:/tmp/spark-checkpoint-enrichment/commits/179
26/01/04 17:29:34 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T17:29:32.591Z",
  "batchId" : 179,
  "numInputRows" : 30,
  "inputRowsPerSecond" : 1250.0,
  "processedRowsPerSecond" : 13.1521262604121,
  "durationMs" : {
    "addBatch" : 1763,
    "commitOffsets" : 174,
    "getBatch" : 0,
    "latestOffset" : 11,
    "queryPlanning" : 87,
    "triggerExecution" : 2281,
    "walCommit" : 243
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "0" : 750
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "0" : 780
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "0" : 780
      }
    },
    "numInputRows" : 30,
    "inputRowsPerSecond" : 1250.0,
    "processedRowsPerSecond" : 13.1521262604121,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 31
  }
}
26/01/04 17:29:43 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/180 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.180.be1543b4-06db-4929-ae50-3fbd351e17b4.tmp
26/01/04 17:29:43 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.180.be1543b4-06db-4929-ae50-3fbd351e17b4.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/180
26/01/04 17:29:43 INFO MicroBatchExecution: Committed offsets for batch 180. Metadata OffsetSeqMetadata(0,1767547783620,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 17:29:43 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:29:43 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:29:43 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:29:43 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:29:43 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:29:43 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:29:43 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#146203 - airline_prefix.nullCount#146202) > 0)
26/01/04 17:29:43 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#146238 - min_flight_num.nullCount#146237) > 0)
26/01/04 17:29:43 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#146233 - max_flight_num.nullCount#146232) > 0)
26/01/04 17:29:44 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:29:44 INFO DAGScheduler: Got job 363 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 17:29:44 INFO DAGScheduler: Final stage: ResultStage 364 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:29:44 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:29:44 INFO DAGScheduler: Missing parents: List()
26/01/04 17:29:44 INFO DAGScheduler: Submitting ResultStage 364 (MapPartitionsRDD[1823] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:29:44 INFO MemoryStore: Block broadcast_543 stored as values in memory (estimated size 38.5 KiB, free 434.3 MiB)
26/01/04 17:29:44 INFO MemoryStore: Block broadcast_543_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.3 MiB)
26/01/04 17:29:44 INFO BlockManagerInfo: Added broadcast_543_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:29:44 INFO BlockManagerInfo: Removed broadcast_542_piece0 on spark-master:33535 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:29:44 INFO BlockManagerInfo: Removed broadcast_542_piece0 on 172.18.0.13:44401 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:29:44 INFO SparkContext: Created broadcast 543 from broadcast at DAGScheduler.scala:1585
26/01/04 17:29:44 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 364 (MapPartitionsRDD[1823] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 17:29:44 INFO TaskSchedulerImpl: Adding task set 364.0 with 2 tasks resource profile 0
26/01/04 17:29:44 INFO TaskSetManager: Starting task 0.0 in stage 364.0 (TID 581) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 17:29:44 INFO TaskSetManager: Starting task 1.0 in stage 364.0 (TID 582) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 17:29:44 INFO BlockManagerInfo: Removed broadcast_541_piece0 on spark-master:33535 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:29:44 INFO BlockManagerInfo: Removed broadcast_541_piece0 on 172.18.0.13:44401 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:29:44 INFO BlockManagerInfo: Added broadcast_543_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:29:44 INFO BlockManagerInfo: Added broadcast_543_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:29:44 INFO TaskSetManager: Finished task 1.0 in stage 364.0 (TID 582) in 69 ms on 172.18.0.13 (executor 0) (1/2)
26/01/04 17:29:44 INFO TaskSetManager: Finished task 0.0 in stage 364.0 (TID 581) in 124 ms on 172.18.0.14 (executor 1) (2/2)
26/01/04 17:29:44 INFO TaskSchedulerImpl: Removed TaskSet 364.0, whose tasks have all completed, from pool 
26/01/04 17:29:44 INFO DAGScheduler: ResultStage 364 (start at NativeMethodAccessorImpl.java:0) finished in 0.287 s
26/01/04 17:29:44 INFO DAGScheduler: Job 363 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:29:44 INFO TaskSchedulerImpl: Killing all running tasks in stage 364: Stage finished
26/01/04 17:29:44 INFO DAGScheduler: Job 363 finished: start at NativeMethodAccessorImpl.java:0, took 0.295736 s
26/01/04 17:29:44 INFO MemoryStore: Block broadcast_544_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.3 MiB)
26/01/04 17:29:44 INFO BlockManagerInfo: Added broadcast_544_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:29:44 INFO SparkContext: Created broadcast 544 from start at NativeMethodAccessorImpl.java:0
26/01/04 17:29:44 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 180, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@2ecb0fb9]. The input RDD has 1 partitions.
26/01/04 17:29:44 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:29:44 INFO DAGScheduler: Got job 364 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/01/04 17:29:44 INFO DAGScheduler: Final stage: ResultStage 365 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:29:44 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:29:44 INFO DAGScheduler: Missing parents: List()
26/01/04 17:29:44 INFO DAGScheduler: Submitting ResultStage 365 (MapPartitionsRDD[1828] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:29:44 INFO MemoryStore: Block broadcast_545 stored as values in memory (estimated size 52.9 KiB, free 434.3 MiB)
26/01/04 17:29:44 INFO MemoryStore: Block broadcast_545_piece0 stored as bytes in memory (estimated size 20.2 KiB, free 434.3 MiB)
26/01/04 17:29:44 INFO BlockManagerInfo: Added broadcast_545_piece0 in memory on spark-master:33535 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:29:44 INFO BlockManagerInfo: Removed broadcast_543_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:29:44 INFO SparkContext: Created broadcast 545 from broadcast at DAGScheduler.scala:1585
26/01/04 17:29:44 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 365 (MapPartitionsRDD[1828] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/01/04 17:29:44 INFO TaskSchedulerImpl: Adding task set 365.0 with 1 tasks resource profile 0
26/01/04 17:29:44 INFO TaskSetManager: Starting task 0.0 in stage 365.0 (TID 583) (172.18.0.13, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:29:44 INFO BlockManagerInfo: Removed broadcast_543_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:29:44 INFO BlockManagerInfo: Removed broadcast_543_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:29:44 INFO BlockManagerInfo: Added broadcast_545_piece0 in memory on 172.18.0.13:44401 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:29:44 INFO BlockManagerInfo: Added broadcast_544_piece0 in memory on 172.18.0.13:44401 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:29:45 INFO TaskSetManager: Finished task 0.0 in stage 365.0 (TID 583) in 974 ms on 172.18.0.13 (executor 0) (1/1)
26/01/04 17:29:45 INFO TaskSchedulerImpl: Removed TaskSet 365.0, whose tasks have all completed, from pool 
26/01/04 17:29:45 INFO DAGScheduler: ResultStage 365 (start at NativeMethodAccessorImpl.java:0) finished in 1.080 s
26/01/04 17:29:45 INFO DAGScheduler: Job 364 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:29:45 INFO TaskSchedulerImpl: Killing all running tasks in stage 365: Stage finished
26/01/04 17:29:45 INFO DAGScheduler: Job 364 finished: start at NativeMethodAccessorImpl.java:0, took 1.085472 s
26/01/04 17:29:45 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 180, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@2ecb0fb9] is committing.
26/01/04 17:29:45 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 180, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@2ecb0fb9] committed.
26/01/04 17:29:45 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/180 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.180.d0ed27ff-24c1-492c-88d1-28587812da26.tmp
26/01/04 17:29:45 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.180.d0ed27ff-24c1-492c-88d1-28587812da26.tmp to file:/tmp/spark-checkpoint-enrichment/commits/180
26/01/04 17:29:45 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T17:29:43.612Z",
  "batchId" : 180,
  "numInputRows" : 30,
  "inputRowsPerSecond" : 2307.6923076923076,
  "processedRowsPerSecond" : 14.051522248243561,
  "durationMs" : {
    "addBatch" : 1605,
    "commitOffsets" : 265,
    "getBatch" : 0,
    "latestOffset" : 8,
    "queryPlanning" : 60,
    "triggerExecution" : 2135,
    "walCommit" : 196
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "0" : 780
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "0" : 810
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "0" : 810
      }
    },
    "numInputRows" : 30,
    "inputRowsPerSecond" : 2307.6923076923076,
    "processedRowsPerSecond" : 14.051522248243561,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 31
  }
}
26/01/04 17:29:54 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/181 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.181.66a09c5e-13b3-4397-98ef-d412ab30c315.tmp
26/01/04 17:29:54 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.181.66a09c5e-13b3-4397-98ef-d412ab30c315.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/181
26/01/04 17:29:54 INFO MicroBatchExecution: Committed offsets for batch 181. Metadata OffsetSeqMetadata(0,1767547794647,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 17:29:54 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:29:54 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:29:54 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:29:54 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:29:55 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:29:55 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:29:55 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#147007 - airline_prefix.nullCount#147006) > 0)
26/01/04 17:29:55 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#147042 - min_flight_num.nullCount#147041) > 0)
26/01/04 17:29:55 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#147037 - max_flight_num.nullCount#147036) > 0)
26/01/04 17:29:55 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:29:55 INFO DAGScheduler: Got job 365 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 17:29:55 INFO DAGScheduler: Final stage: ResultStage 366 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:29:55 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:29:55 INFO DAGScheduler: Missing parents: List()
26/01/04 17:29:55 INFO DAGScheduler: Submitting ResultStage 366 (MapPartitionsRDD[1833] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:29:55 INFO MemoryStore: Block broadcast_546 stored as values in memory (estimated size 38.5 KiB, free 434.3 MiB)
26/01/04 17:29:55 INFO MemoryStore: Block broadcast_546_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.3 MiB)
26/01/04 17:29:55 INFO BlockManagerInfo: Removed broadcast_545_piece0 on spark-master:33535 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:29:55 INFO BlockManagerInfo: Added broadcast_546_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:29:55 INFO BlockManagerInfo: Removed broadcast_545_piece0 on 172.18.0.13:44401 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:29:55 INFO SparkContext: Created broadcast 546 from broadcast at DAGScheduler.scala:1585
26/01/04 17:29:55 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 366 (MapPartitionsRDD[1833] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 17:29:55 INFO TaskSchedulerImpl: Adding task set 366.0 with 2 tasks resource profile 0
26/01/04 17:29:55 INFO TaskSetManager: Starting task 0.0 in stage 366.0 (TID 584) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 17:29:55 INFO TaskSetManager: Starting task 1.0 in stage 366.0 (TID 585) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 17:29:55 INFO BlockManagerInfo: Removed broadcast_544_piece0 on spark-master:33535 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:29:55 INFO BlockManagerInfo: Removed broadcast_544_piece0 on 172.18.0.13:44401 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:29:55 INFO BlockManagerInfo: Added broadcast_546_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:29:55 INFO BlockManagerInfo: Added broadcast_546_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:29:55 INFO TaskSetManager: Finished task 1.0 in stage 366.0 (TID 585) in 70 ms on 172.18.0.13 (executor 0) (1/2)
26/01/04 17:29:55 INFO TaskSetManager: Finished task 0.0 in stage 366.0 (TID 584) in 112 ms on 172.18.0.14 (executor 1) (2/2)
26/01/04 17:29:55 INFO TaskSchedulerImpl: Removed TaskSet 366.0, whose tasks have all completed, from pool 
26/01/04 17:29:55 INFO DAGScheduler: ResultStage 366 (start at NativeMethodAccessorImpl.java:0) finished in 0.162 s
26/01/04 17:29:55 INFO DAGScheduler: Job 365 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:29:55 INFO TaskSchedulerImpl: Killing all running tasks in stage 366: Stage finished
26/01/04 17:29:55 INFO DAGScheduler: Job 365 finished: start at NativeMethodAccessorImpl.java:0, took 0.168756 s
26/01/04 17:29:55 INFO MemoryStore: Block broadcast_547_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.3 MiB)
26/01/04 17:29:55 INFO BlockManagerInfo: Added broadcast_547_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:29:55 INFO SparkContext: Created broadcast 547 from start at NativeMethodAccessorImpl.java:0
26/01/04 17:29:55 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 181, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@12c63a20]. The input RDD has 1 partitions.
26/01/04 17:29:55 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:29:55 INFO DAGScheduler: Got job 366 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/01/04 17:29:55 INFO DAGScheduler: Final stage: ResultStage 367 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:29:55 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:29:55 INFO DAGScheduler: Missing parents: List()
26/01/04 17:29:55 INFO DAGScheduler: Submitting ResultStage 367 (MapPartitionsRDD[1838] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:29:55 INFO MemoryStore: Block broadcast_548 stored as values in memory (estimated size 52.9 KiB, free 434.3 MiB)
26/01/04 17:29:55 INFO MemoryStore: Block broadcast_548_piece0 stored as bytes in memory (estimated size 20.2 KiB, free 434.3 MiB)
26/01/04 17:29:55 INFO BlockManagerInfo: Added broadcast_548_piece0 in memory on spark-master:33535 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:29:55 INFO SparkContext: Created broadcast 548 from broadcast at DAGScheduler.scala:1585
26/01/04 17:29:55 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 367 (MapPartitionsRDD[1838] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/01/04 17:29:55 INFO TaskSchedulerImpl: Adding task set 367.0 with 1 tasks resource profile 0
26/01/04 17:29:55 INFO TaskSetManager: Starting task 0.0 in stage 367.0 (TID 586) (172.18.0.13, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:29:55 INFO BlockManagerInfo: Added broadcast_548_piece0 in memory on 172.18.0.13:44401 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:29:55 INFO BlockManagerInfo: Added broadcast_547_piece0 in memory on 172.18.0.13:44401 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:29:55 INFO TaskSetManager: Finished task 0.0 in stage 367.0 (TID 586) in 721 ms on 172.18.0.13 (executor 0) (1/1)
26/01/04 17:29:55 INFO TaskSchedulerImpl: Removed TaskSet 367.0, whose tasks have all completed, from pool 
26/01/04 17:29:55 INFO DAGScheduler: ResultStage 367 (start at NativeMethodAccessorImpl.java:0) finished in 0.731 s
26/01/04 17:29:55 INFO DAGScheduler: Job 366 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:29:55 INFO TaskSchedulerImpl: Killing all running tasks in stage 367: Stage finished
26/01/04 17:29:55 INFO DAGScheduler: Job 366 finished: start at NativeMethodAccessorImpl.java:0, took 0.735861 s
26/01/04 17:29:55 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 181, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@12c63a20] is committing.
26/01/04 17:29:55 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 181, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@12c63a20] committed.
26/01/04 17:29:56 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/181 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.181.39e7e938-bd4f-498d-8170-ae4c4ea632a3.tmp
26/01/04 17:29:56 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.181.39e7e938-bd4f-498d-8170-ae4c4ea632a3.tmp to file:/tmp/spark-checkpoint-enrichment/commits/181
26/01/04 17:29:56 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T17:29:54.642Z",
  "batchId" : 181,
  "numInputRows" : 30,
  "inputRowsPerSecond" : 1666.6666666666667,
  "processedRowsPerSecond" : 20.283975659229206,
  "durationMs" : {
    "addBatch" : 1038,
    "commitOffsets" : 143,
    "getBatch" : 1,
    "latestOffset" : 5,
    "queryPlanning" : 70,
    "triggerExecution" : 1479,
    "walCommit" : 220
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "0" : 810
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "0" : 840
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "0" : 840
      }
    },
    "numInputRows" : 30,
    "inputRowsPerSecond" : 1666.6666666666667,
    "processedRowsPerSecond" : 20.283975659229206,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 31
  }
}
26/01/04 17:29:56 INFO NetworkClient: [AdminClient clientId=adminclient-4] Node -1 disconnected.
26/01/04 17:30:05 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/182 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.182.da62e1f6-670c-49da-a909-40fa82a8ae04.tmp
26/01/04 17:30:06 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.182.da62e1f6-670c-49da-a909-40fa82a8ae04.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/182
26/01/04 17:30:06 INFO MicroBatchExecution: Committed offsets for batch 182. Metadata OffsetSeqMetadata(0,1767547805668,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 17:30:06 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:30:06 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:30:06 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:30:06 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:30:06 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:30:06 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:30:06 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#147811 - airline_prefix.nullCount#147810) > 0)
26/01/04 17:30:06 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#147846 - min_flight_num.nullCount#147845) > 0)
26/01/04 17:30:06 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#147841 - max_flight_num.nullCount#147840) > 0)
26/01/04 17:30:06 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:30:06 INFO DAGScheduler: Got job 367 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 17:30:06 INFO DAGScheduler: Final stage: ResultStage 368 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:30:06 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:30:06 INFO DAGScheduler: Missing parents: List()
26/01/04 17:30:06 INFO DAGScheduler: Submitting ResultStage 368 (MapPartitionsRDD[1843] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:30:06 INFO MemoryStore: Block broadcast_549 stored as values in memory (estimated size 38.5 KiB, free 434.2 MiB)
26/01/04 17:30:06 INFO MemoryStore: Block broadcast_549_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.2 MiB)
26/01/04 17:30:06 INFO BlockManagerInfo: Added broadcast_549_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:30:06 INFO SparkContext: Created broadcast 549 from broadcast at DAGScheduler.scala:1585
26/01/04 17:30:06 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 368 (MapPartitionsRDD[1843] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 17:30:06 INFO TaskSchedulerImpl: Adding task set 368.0 with 2 tasks resource profile 0
26/01/04 17:30:06 INFO BlockManagerInfo: Removed broadcast_548_piece0 on spark-master:33535 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:30:06 INFO TaskSetManager: Starting task 1.0 in stage 368.0 (TID 587) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 17:30:06 INFO BlockManagerInfo: Removed broadcast_548_piece0 on 172.18.0.13:44401 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:30:06 INFO TaskSetManager: Starting task 0.0 in stage 368.0 (TID 588) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 17:30:06 INFO BlockManagerInfo: Removed broadcast_546_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:30:06 INFO BlockManagerInfo: Removed broadcast_546_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:30:06 INFO BlockManagerInfo: Removed broadcast_546_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:30:06 INFO BlockManagerInfo: Added broadcast_549_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:30:06 INFO BlockManagerInfo: Added broadcast_549_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:30:06 INFO BlockManagerInfo: Removed broadcast_547_piece0 on spark-master:33535 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:30:06 INFO BlockManagerInfo: Removed broadcast_547_piece0 on 172.18.0.13:44401 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:30:06 INFO TaskSetManager: Finished task 0.0 in stage 368.0 (TID 588) in 117 ms on 172.18.0.14 (executor 1) (1/2)
26/01/04 17:30:06 INFO TaskSetManager: Finished task 1.0 in stage 368.0 (TID 587) in 143 ms on 172.18.0.13 (executor 0) (2/2)
26/01/04 17:30:06 INFO TaskSchedulerImpl: Removed TaskSet 368.0, whose tasks have all completed, from pool 
26/01/04 17:30:06 INFO DAGScheduler: ResultStage 368 (start at NativeMethodAccessorImpl.java:0) finished in 0.194 s
26/01/04 17:30:06 INFO DAGScheduler: Job 367 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:30:06 INFO TaskSchedulerImpl: Killing all running tasks in stage 368: Stage finished
26/01/04 17:30:06 INFO DAGScheduler: Job 367 finished: start at NativeMethodAccessorImpl.java:0, took 0.203258 s
26/01/04 17:30:06 INFO MemoryStore: Block broadcast_550_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.3 MiB)
26/01/04 17:30:06 INFO BlockManagerInfo: Added broadcast_550_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:30:06 INFO SparkContext: Created broadcast 550 from start at NativeMethodAccessorImpl.java:0
26/01/04 17:30:06 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 182, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@3983b696]. The input RDD has 1 partitions.
26/01/04 17:30:06 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:30:06 INFO DAGScheduler: Got job 368 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/01/04 17:30:06 INFO DAGScheduler: Final stage: ResultStage 369 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:30:06 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:30:06 INFO DAGScheduler: Missing parents: List()
26/01/04 17:30:06 INFO DAGScheduler: Submitting ResultStage 369 (MapPartitionsRDD[1848] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:30:06 INFO MemoryStore: Block broadcast_551 stored as values in memory (estimated size 52.9 KiB, free 434.3 MiB)
26/01/04 17:30:06 INFO MemoryStore: Block broadcast_551_piece0 stored as bytes in memory (estimated size 20.2 KiB, free 434.3 MiB)
26/01/04 17:30:06 INFO BlockManagerInfo: Added broadcast_551_piece0 in memory on spark-master:33535 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:30:06 INFO SparkContext: Created broadcast 551 from broadcast at DAGScheduler.scala:1585
26/01/04 17:30:06 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 369 (MapPartitionsRDD[1848] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/01/04 17:30:06 INFO TaskSchedulerImpl: Adding task set 369.0 with 1 tasks resource profile 0
26/01/04 17:30:06 INFO TaskSetManager: Starting task 0.0 in stage 369.0 (TID 589) (172.18.0.13, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:30:06 INFO BlockManagerInfo: Added broadcast_551_piece0 in memory on 172.18.0.13:44401 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:30:06 INFO BlockManagerInfo: Added broadcast_550_piece0 in memory on 172.18.0.13:44401 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:30:07 INFO TaskSetManager: Finished task 0.0 in stage 369.0 (TID 589) in 772 ms on 172.18.0.13 (executor 0) (1/1)
26/01/04 17:30:07 INFO TaskSchedulerImpl: Removed TaskSet 369.0, whose tasks have all completed, from pool 
26/01/04 17:30:07 INFO DAGScheduler: ResultStage 369 (start at NativeMethodAccessorImpl.java:0) finished in 0.794 s
26/01/04 17:30:07 INFO DAGScheduler: Job 368 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:30:07 INFO TaskSchedulerImpl: Killing all running tasks in stage 369: Stage finished
26/01/04 17:30:07 INFO DAGScheduler: Job 368 finished: start at NativeMethodAccessorImpl.java:0, took 0.801502 s
26/01/04 17:30:07 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 182, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@3983b696] is committing.
26/01/04 17:30:07 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 182, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@3983b696] committed.
26/01/04 17:30:07 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/182 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.182.3832cee3-d433-4189-a1cb-10823eba9a6c.tmp
26/01/04 17:30:07 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.182.3832cee3-d433-4189-a1cb-10823eba9a6c.tmp to file:/tmp/spark-checkpoint-enrichment/commits/182
26/01/04 17:30:07 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T17:30:05.664Z",
  "batchId" : 182,
  "numInputRows" : 30,
  "inputRowsPerSecond" : 2000.0,
  "processedRowsPerSecond" : 15.657620041753654,
  "durationMs" : {
    "addBatch" : 1236,
    "commitOffsets" : 272,
    "getBatch" : 1,
    "latestOffset" : 4,
    "queryPlanning" : 59,
    "triggerExecution" : 1916,
    "walCommit" : 342
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "0" : 840
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "0" : 870
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "0" : 870
      }
    },
    "numInputRows" : 30,
    "inputRowsPerSecond" : 2000.0,
    "processedRowsPerSecond" : 15.657620041753654,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 31
  }
}
26/01/04 17:30:16 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/183 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.183.ddbd8b0a-338c-4ef5-b08a-cc4abf2b71b2.tmp
26/01/04 17:30:16 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.183.ddbd8b0a-338c-4ef5-b08a-cc4abf2b71b2.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/183
26/01/04 17:30:16 INFO MicroBatchExecution: Committed offsets for batch 183. Metadata OffsetSeqMetadata(0,1767547816709,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 17:30:16 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:30:16 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:30:17 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:30:17 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:30:17 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:30:17 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:30:17 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#148615 - airline_prefix.nullCount#148614) > 0)
26/01/04 17:30:17 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#148650 - min_flight_num.nullCount#148649) > 0)
26/01/04 17:30:17 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#148645 - max_flight_num.nullCount#148644) > 0)
26/01/04 17:30:17 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:30:17 INFO DAGScheduler: Got job 369 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 17:30:17 INFO DAGScheduler: Final stage: ResultStage 370 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:30:17 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:30:17 INFO DAGScheduler: Missing parents: List()
26/01/04 17:30:17 INFO DAGScheduler: Submitting ResultStage 370 (MapPartitionsRDD[1853] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:30:17 INFO MemoryStore: Block broadcast_552 stored as values in memory (estimated size 38.5 KiB, free 434.2 MiB)
26/01/04 17:30:17 INFO MemoryStore: Block broadcast_552_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.2 MiB)
26/01/04 17:30:17 INFO BlockManagerInfo: Added broadcast_552_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:30:17 INFO SparkContext: Created broadcast 552 from broadcast at DAGScheduler.scala:1585
26/01/04 17:30:17 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 370 (MapPartitionsRDD[1853] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 17:30:17 INFO TaskSchedulerImpl: Adding task set 370.0 with 2 tasks resource profile 0
26/01/04 17:30:17 INFO TaskSetManager: Starting task 1.0 in stage 370.0 (TID 590) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 17:30:17 INFO TaskSetManager: Starting task 0.0 in stage 370.0 (TID 591) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 17:30:17 INFO BlockManagerInfo: Removed broadcast_550_piece0 on 172.18.0.13:44401 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:30:17 INFO BlockManagerInfo: Removed broadcast_550_piece0 on spark-master:33535 in memory (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:30:17 INFO BlockManagerInfo: Added broadcast_552_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:30:17 INFO BlockManagerInfo: Removed broadcast_551_piece0 on spark-master:33535 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:30:17 INFO BlockManagerInfo: Removed broadcast_551_piece0 on 172.18.0.13:44401 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:30:17 INFO TaskSetManager: Finished task 1.0 in stage 370.0 (TID 590) in 148 ms on 172.18.0.13 (executor 0) (1/2)
26/01/04 17:30:17 INFO BlockManagerInfo: Added broadcast_552_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:30:17 INFO BlockManagerInfo: Removed broadcast_549_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:30:17 INFO BlockManagerInfo: Removed broadcast_549_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:30:17 INFO BlockManagerInfo: Removed broadcast_549_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:30:17 INFO TaskSetManager: Finished task 0.0 in stage 370.0 (TID 591) in 232 ms on 172.18.0.14 (executor 1) (2/2)
26/01/04 17:30:17 INFO TaskSchedulerImpl: Removed TaskSet 370.0, whose tasks have all completed, from pool 
26/01/04 17:30:17 INFO DAGScheduler: ResultStage 370 (start at NativeMethodAccessorImpl.java:0) finished in 0.284 s
26/01/04 17:30:17 INFO DAGScheduler: Job 369 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:30:17 INFO TaskSchedulerImpl: Killing all running tasks in stage 370: Stage finished
26/01/04 17:30:17 INFO DAGScheduler: Job 369 finished: start at NativeMethodAccessorImpl.java:0, took 0.305761 s
26/01/04 17:30:17 INFO MemoryStore: Block broadcast_553_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.3 MiB)
26/01/04 17:30:17 INFO BlockManagerInfo: Added broadcast_553_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:30:17 INFO SparkContext: Created broadcast 553 from start at NativeMethodAccessorImpl.java:0
26/01/04 17:30:17 INFO BlockManagerInfo: Removed broadcast_552_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:30:17 INFO BlockManagerInfo: Removed broadcast_552_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:30:17 INFO BlockManagerInfo: Removed broadcast_552_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:30:17 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 183, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@3f3eee9d]. The input RDD has 1 partitions.
26/01/04 17:30:17 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:30:17 INFO DAGScheduler: Got job 370 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/01/04 17:30:17 INFO DAGScheduler: Final stage: ResultStage 371 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:30:17 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:30:17 INFO DAGScheduler: Missing parents: List()
26/01/04 17:30:17 INFO DAGScheduler: Submitting ResultStage 371 (MapPartitionsRDD[1858] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:30:17 INFO MemoryStore: Block broadcast_554 stored as values in memory (estimated size 52.9 KiB, free 434.3 MiB)
26/01/04 17:30:17 INFO MemoryStore: Block broadcast_554_piece0 stored as bytes in memory (estimated size 20.2 KiB, free 434.3 MiB)
26/01/04 17:30:17 INFO BlockManagerInfo: Added broadcast_554_piece0 in memory on spark-master:33535 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:30:17 INFO SparkContext: Created broadcast 554 from broadcast at DAGScheduler.scala:1585
26/01/04 17:30:17 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 371 (MapPartitionsRDD[1858] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/01/04 17:30:17 INFO TaskSchedulerImpl: Adding task set 371.0 with 1 tasks resource profile 0
26/01/04 17:30:17 INFO TaskSetManager: Starting task 0.0 in stage 371.0 (TID 592) (172.18.0.13, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:30:17 INFO BlockManagerInfo: Added broadcast_554_piece0 in memory on 172.18.0.13:44401 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:30:17 INFO BlockManagerInfo: Added broadcast_553_piece0 in memory on 172.18.0.13:44401 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:30:18 INFO TaskSetManager: Finished task 0.0 in stage 371.0 (TID 592) in 722 ms on 172.18.0.13 (executor 0) (1/1)
26/01/04 17:30:18 INFO TaskSchedulerImpl: Removed TaskSet 371.0, whose tasks have all completed, from pool 
26/01/04 17:30:18 INFO DAGScheduler: ResultStage 371 (start at NativeMethodAccessorImpl.java:0) finished in 0.742 s
26/01/04 17:30:18 INFO DAGScheduler: Job 370 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:30:18 INFO TaskSchedulerImpl: Killing all running tasks in stage 371: Stage finished
26/01/04 17:30:18 INFO DAGScheduler: Job 370 finished: start at NativeMethodAccessorImpl.java:0, took 0.748898 s
26/01/04 17:30:18 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 183, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@3f3eee9d] is committing.
26/01/04 17:30:18 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 183, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@3f3eee9d] committed.
26/01/04 17:30:18 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/183 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.183.01eef3e8-2454-45ea-9280-6cf3dd246f02.tmp
26/01/04 17:30:18 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.183.01eef3e8-2454-45ea-9280-6cf3dd246f02.tmp to file:/tmp/spark-checkpoint-enrichment/commits/183
26/01/04 17:30:18 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T17:30:16.705Z",
  "batchId" : 183,
  "numInputRows" : 30,
  "inputRowsPerSecond" : 625.0,
  "processedRowsPerSecond" : 16.722408026755854,
  "durationMs" : {
    "addBatch" : 1283,
    "commitOffsets" : 238,
    "getBatch" : 0,
    "latestOffset" : 4,
    "queryPlanning" : 64,
    "triggerExecution" : 1794,
    "walCommit" : 202
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "0" : 870
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "0" : 900
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "0" : 900
      }
    },
    "numInputRows" : 30,
    "inputRowsPerSecond" : 625.0,
    "processedRowsPerSecond" : 16.722408026755854,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 31
  }
}
26/01/04 17:30:27 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/184 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.184.23edde45-53c3-486f-b897-b9d5c4fa6ea6.tmp
26/01/04 17:30:27 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.184.23edde45-53c3-486f-b897-b9d5c4fa6ea6.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/184
26/01/04 17:30:27 INFO MicroBatchExecution: Committed offsets for batch 184. Metadata OffsetSeqMetadata(0,1767547827722,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 17:30:27 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:30:27 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:30:27 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:30:27 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:30:27 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:30:27 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:30:28 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#149419 - airline_prefix.nullCount#149418) > 0)
26/01/04 17:30:28 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#149454 - min_flight_num.nullCount#149453) > 0)
26/01/04 17:30:28 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#149449 - max_flight_num.nullCount#149448) > 0)
26/01/04 17:30:28 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:30:28 INFO DAGScheduler: Got job 371 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 17:30:28 INFO DAGScheduler: Final stage: ResultStage 372 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:30:28 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:30:28 INFO DAGScheduler: Missing parents: List()
26/01/04 17:30:28 INFO DAGScheduler: Submitting ResultStage 372 (MapPartitionsRDD[1863] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:30:28 INFO MemoryStore: Block broadcast_555 stored as values in memory (estimated size 38.5 KiB, free 434.3 MiB)
26/01/04 17:30:28 INFO MemoryStore: Block broadcast_555_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.3 MiB)
26/01/04 17:30:28 INFO BlockManagerInfo: Added broadcast_555_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:30:28 INFO SparkContext: Created broadcast 555 from broadcast at DAGScheduler.scala:1585
26/01/04 17:30:28 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 372 (MapPartitionsRDD[1863] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 17:30:28 INFO TaskSchedulerImpl: Adding task set 372.0 with 2 tasks resource profile 0
26/01/04 17:30:28 INFO TaskSetManager: Starting task 0.0 in stage 372.0 (TID 593) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 17:30:28 INFO TaskSetManager: Starting task 1.0 in stage 372.0 (TID 594) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 17:30:28 INFO BlockManagerInfo: Removed broadcast_553_piece0 on spark-master:33535 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:30:28 INFO BlockManagerInfo: Removed broadcast_553_piece0 on 172.18.0.13:44401 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:30:28 INFO BlockManagerInfo: Added broadcast_555_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:30:28 INFO BlockManagerInfo: Added broadcast_555_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:30:28 INFO BlockManagerInfo: Removed broadcast_554_piece0 on spark-master:33535 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:30:28 INFO BlockManagerInfo: Removed broadcast_554_piece0 on 172.18.0.13:44401 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:30:28 INFO TaskSetManager: Finished task 1.0 in stage 372.0 (TID 594) in 94 ms on 172.18.0.13 (executor 0) (1/2)
26/01/04 17:30:28 INFO TaskSetManager: Finished task 0.0 in stage 372.0 (TID 593) in 109 ms on 172.18.0.14 (executor 1) (2/2)
26/01/04 17:30:28 INFO TaskSchedulerImpl: Removed TaskSet 372.0, whose tasks have all completed, from pool 
26/01/04 17:30:28 INFO DAGScheduler: ResultStage 372 (start at NativeMethodAccessorImpl.java:0) finished in 0.174 s
26/01/04 17:30:28 INFO DAGScheduler: Job 371 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:30:28 INFO TaskSchedulerImpl: Killing all running tasks in stage 372: Stage finished
26/01/04 17:30:28 INFO DAGScheduler: Job 371 finished: start at NativeMethodAccessorImpl.java:0, took 0.185172 s
26/01/04 17:30:28 INFO MemoryStore: Block broadcast_556_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.3 MiB)
26/01/04 17:30:28 INFO BlockManagerInfo: Added broadcast_556_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:30:28 INFO SparkContext: Created broadcast 556 from start at NativeMethodAccessorImpl.java:0
26/01/04 17:30:28 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 184, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@2873a216]. The input RDD has 1 partitions.
26/01/04 17:30:28 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:30:28 INFO DAGScheduler: Got job 372 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/01/04 17:30:28 INFO DAGScheduler: Final stage: ResultStage 373 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:30:28 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:30:28 INFO DAGScheduler: Missing parents: List()
26/01/04 17:30:28 INFO DAGScheduler: Submitting ResultStage 373 (MapPartitionsRDD[1868] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:30:28 INFO MemoryStore: Block broadcast_557 stored as values in memory (estimated size 52.9 KiB, free 434.3 MiB)
26/01/04 17:30:28 INFO MemoryStore: Block broadcast_557_piece0 stored as bytes in memory (estimated size 20.2 KiB, free 434.3 MiB)
26/01/04 17:30:28 INFO BlockManagerInfo: Added broadcast_557_piece0 in memory on spark-master:33535 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:30:28 INFO SparkContext: Created broadcast 557 from broadcast at DAGScheduler.scala:1585
26/01/04 17:30:28 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 373 (MapPartitionsRDD[1868] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/01/04 17:30:28 INFO TaskSchedulerImpl: Adding task set 373.0 with 1 tasks resource profile 0
26/01/04 17:30:28 INFO TaskSetManager: Starting task 0.0 in stage 373.0 (TID 595) (172.18.0.13, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:30:28 INFO BlockManagerInfo: Added broadcast_557_piece0 in memory on 172.18.0.13:44401 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:30:28 INFO BlockManagerInfo: Added broadcast_556_piece0 in memory on 172.18.0.13:44401 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:30:29 INFO TaskSetManager: Finished task 0.0 in stage 373.0 (TID 595) in 729 ms on 172.18.0.13 (executor 0) (1/1)
26/01/04 17:30:29 INFO TaskSchedulerImpl: Removed TaskSet 373.0, whose tasks have all completed, from pool 
26/01/04 17:30:29 INFO DAGScheduler: ResultStage 373 (start at NativeMethodAccessorImpl.java:0) finished in 0.743 s
26/01/04 17:30:29 INFO DAGScheduler: Job 372 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:30:29 INFO TaskSchedulerImpl: Killing all running tasks in stage 373: Stage finished
26/01/04 17:30:29 INFO DAGScheduler: Job 372 finished: start at NativeMethodAccessorImpl.java:0, took 0.749493 s
26/01/04 17:30:29 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 184, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@2873a216] is committing.
26/01/04 17:30:29 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 184, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@2873a216] committed.
26/01/04 17:30:29 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/184 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.184.9df3095c-bfba-4729-b2d7-cdcf7b86dfa3.tmp
26/01/04 17:30:29 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.184.9df3095c-bfba-4729-b2d7-cdcf7b86dfa3.tmp to file:/tmp/spark-checkpoint-enrichment/commits/184
26/01/04 17:30:29 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T17:30:27.719Z",
  "batchId" : 184,
  "numInputRows" : 30,
  "inputRowsPerSecond" : 2000.0,
  "processedRowsPerSecond" : 20.59025394646534,
  "durationMs" : {
    "addBatch" : 1125,
    "commitOffsets" : 148,
    "getBatch" : 0,
    "latestOffset" : 3,
    "queryPlanning" : 51,
    "triggerExecution" : 1457,
    "walCommit" : 130
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "0" : 900
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "0" : 930
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "0" : 930
      }
    },
    "numInputRows" : 30,
    "inputRowsPerSecond" : 2000.0,
    "processedRowsPerSecond" : 20.59025394646534,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 31
  }
}
26/01/04 17:30:38 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/185 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.185.90a71c0b-4cdb-4b5c-849e-363fe42e3892.tmp
26/01/04 17:30:38 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.185.90a71c0b-4cdb-4b5c-849e-363fe42e3892.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/185
26/01/04 17:30:38 INFO MicroBatchExecution: Committed offsets for batch 185. Metadata OffsetSeqMetadata(0,1767547838754,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 17:30:39 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:30:39 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:30:39 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:30:39 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:30:39 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:30:39 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:30:39 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#150223 - airline_prefix.nullCount#150222) > 0)
26/01/04 17:30:39 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#150258 - min_flight_num.nullCount#150257) > 0)
26/01/04 17:30:39 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#150253 - max_flight_num.nullCount#150252) > 0)
26/01/04 17:30:39 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:30:39 INFO DAGScheduler: Got job 373 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 17:30:39 INFO DAGScheduler: Final stage: ResultStage 374 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:30:39 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:30:39 INFO DAGScheduler: Missing parents: List()
26/01/04 17:30:39 INFO DAGScheduler: Submitting ResultStage 374 (MapPartitionsRDD[1873] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:30:39 INFO MemoryStore: Block broadcast_558 stored as values in memory (estimated size 38.5 KiB, free 434.2 MiB)
26/01/04 17:30:39 INFO MemoryStore: Block broadcast_558_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.2 MiB)
26/01/04 17:30:39 INFO BlockManagerInfo: Added broadcast_558_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:30:39 INFO SparkContext: Created broadcast 558 from broadcast at DAGScheduler.scala:1585
26/01/04 17:30:39 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 374 (MapPartitionsRDD[1873] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 17:30:39 INFO TaskSchedulerImpl: Adding task set 374.0 with 2 tasks resource profile 0
26/01/04 17:30:39 INFO BlockManagerInfo: Removed broadcast_555_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:30:39 INFO TaskSetManager: Starting task 1.0 in stage 374.0 (TID 596) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 17:30:39 INFO TaskSetManager: Starting task 0.0 in stage 374.0 (TID 597) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 17:30:39 INFO BlockManagerInfo: Removed broadcast_555_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:30:39 INFO BlockManagerInfo: Removed broadcast_555_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:30:39 INFO BlockManagerInfo: Added broadcast_558_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:30:39 INFO BlockManagerInfo: Removed broadcast_557_piece0 on 172.18.0.13:44401 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:30:39 INFO BlockManagerInfo: Removed broadcast_557_piece0 on spark-master:33535 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:30:39 INFO BlockManagerInfo: Added broadcast_558_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:30:39 INFO BlockManagerInfo: Removed broadcast_556_piece0 on spark-master:33535 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:30:39 INFO BlockManagerInfo: Removed broadcast_556_piece0 on 172.18.0.13:44401 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:30:39 INFO TaskSetManager: Finished task 0.0 in stage 374.0 (TID 597) in 264 ms on 172.18.0.14 (executor 1) (1/2)
26/01/04 17:30:39 INFO TaskSetManager: Finished task 1.0 in stage 374.0 (TID 596) in 281 ms on 172.18.0.13 (executor 0) (2/2)
26/01/04 17:30:39 INFO TaskSchedulerImpl: Removed TaskSet 374.0, whose tasks have all completed, from pool 
26/01/04 17:30:39 INFO DAGScheduler: ResultStage 374 (start at NativeMethodAccessorImpl.java:0) finished in 0.371 s
26/01/04 17:30:39 INFO DAGScheduler: Job 373 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:30:39 INFO TaskSchedulerImpl: Killing all running tasks in stage 374: Stage finished
26/01/04 17:30:39 INFO DAGScheduler: Job 373 finished: start at NativeMethodAccessorImpl.java:0, took 0.381277 s
26/01/04 17:30:39 INFO MemoryStore: Block broadcast_559_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.3 MiB)
26/01/04 17:30:39 INFO BlockManagerInfo: Added broadcast_559_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:30:39 INFO SparkContext: Created broadcast 559 from start at NativeMethodAccessorImpl.java:0
26/01/04 17:30:39 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 185, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@61b124da]. The input RDD has 1 partitions.
26/01/04 17:30:39 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:30:39 INFO DAGScheduler: Got job 374 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/01/04 17:30:39 INFO DAGScheduler: Final stage: ResultStage 375 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:30:39 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:30:39 INFO DAGScheduler: Missing parents: List()
26/01/04 17:30:39 INFO DAGScheduler: Submitting ResultStage 375 (MapPartitionsRDD[1878] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:30:39 INFO MemoryStore: Block broadcast_560 stored as values in memory (estimated size 52.9 KiB, free 434.3 MiB)
26/01/04 17:30:39 INFO MemoryStore: Block broadcast_560_piece0 stored as bytes in memory (estimated size 20.2 KiB, free 434.3 MiB)
26/01/04 17:30:39 INFO BlockManagerInfo: Added broadcast_560_piece0 in memory on spark-master:33535 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:30:39 INFO SparkContext: Created broadcast 560 from broadcast at DAGScheduler.scala:1585
26/01/04 17:30:39 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 375 (MapPartitionsRDD[1878] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/01/04 17:30:39 INFO TaskSchedulerImpl: Adding task set 375.0 with 1 tasks resource profile 0
26/01/04 17:30:39 INFO TaskSetManager: Starting task 0.0 in stage 375.0 (TID 598) (172.18.0.13, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:30:39 INFO BlockManagerInfo: Added broadcast_560_piece0 in memory on 172.18.0.13:44401 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:30:40 INFO BlockManagerInfo: Added broadcast_559_piece0 in memory on 172.18.0.13:44401 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:30:40 INFO TaskSetManager: Finished task 0.0 in stage 375.0 (TID 598) in 846 ms on 172.18.0.13 (executor 0) (1/1)
26/01/04 17:30:40 INFO TaskSchedulerImpl: Removed TaskSet 375.0, whose tasks have all completed, from pool 
26/01/04 17:30:40 INFO DAGScheduler: ResultStage 375 (start at NativeMethodAccessorImpl.java:0) finished in 0.883 s
26/01/04 17:30:40 INFO DAGScheduler: Job 374 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:30:40 INFO TaskSchedulerImpl: Killing all running tasks in stage 375: Stage finished
26/01/04 17:30:40 INFO DAGScheduler: Job 374 finished: start at NativeMethodAccessorImpl.java:0, took 0.893445 s
26/01/04 17:30:40 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 185, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@61b124da] is committing.
26/01/04 17:30:40 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 185, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@61b124da] committed.
26/01/04 17:30:40 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/185 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.185.b20ad81c-10e0-4516-b3bc-3d1b410162a2.tmp
26/01/04 17:30:41 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.185.b20ad81c-10e0-4516-b3bc-3d1b410162a2.tmp to file:/tmp/spark-checkpoint-enrichment/commits/185
26/01/04 17:30:41 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T17:30:38.749Z",
  "batchId" : 185,
  "numInputRows" : 30,
  "inputRowsPerSecond" : 2307.6923076923076,
  "processedRowsPerSecond" : 12.69035532994924,
  "durationMs" : {
    "addBatch" : 1676,
    "commitOffsets" : 392,
    "getBatch" : 0,
    "latestOffset" : 5,
    "queryPlanning" : 50,
    "triggerExecution" : 2364,
    "walCommit" : 241
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "0" : 930
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "0" : 960
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "0" : 960
      }
    },
    "numInputRows" : 30,
    "inputRowsPerSecond" : 2307.6923076923076,
    "processedRowsPerSecond" : 12.69035532994924,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 31
  }
}
26/01/04 17:30:49 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/186 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.186.21fc417c-26d3-4a9a-aa83-175961af2a62.tmp
26/01/04 17:30:49 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.186.21fc417c-26d3-4a9a-aa83-175961af2a62.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/186
26/01/04 17:30:49 INFO MicroBatchExecution: Committed offsets for batch 186. Metadata OffsetSeqMetadata(0,1767547849830,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 17:30:50 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:30:50 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:30:50 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:30:50 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:30:50 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:30:50 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:30:50 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#151027 - airline_prefix.nullCount#151026) > 0)
26/01/04 17:30:50 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#151062 - min_flight_num.nullCount#151061) > 0)
26/01/04 17:30:50 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#151057 - max_flight_num.nullCount#151056) > 0)
26/01/04 17:30:50 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:30:50 INFO DAGScheduler: Got job 375 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 17:30:50 INFO DAGScheduler: Final stage: ResultStage 376 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:30:50 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:30:50 INFO DAGScheduler: Missing parents: List()
26/01/04 17:30:50 INFO DAGScheduler: Submitting ResultStage 376 (MapPartitionsRDD[1883] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:30:50 INFO MemoryStore: Block broadcast_561 stored as values in memory (estimated size 38.5 KiB, free 434.2 MiB)
26/01/04 17:30:50 INFO MemoryStore: Block broadcast_561_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.2 MiB)
26/01/04 17:30:50 INFO BlockManagerInfo: Removed broadcast_559_piece0 on spark-master:33535 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:30:50 INFO BlockManagerInfo: Added broadcast_561_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:30:50 INFO SparkContext: Created broadcast 561 from broadcast at DAGScheduler.scala:1585
26/01/04 17:30:50 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 376 (MapPartitionsRDD[1883] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 17:30:50 INFO TaskSchedulerImpl: Adding task set 376.0 with 2 tasks resource profile 0
26/01/04 17:30:50 INFO BlockManagerInfo: Removed broadcast_559_piece0 on 172.18.0.13:44401 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:30:50 INFO TaskSetManager: Starting task 0.0 in stage 376.0 (TID 599) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 17:30:50 INFO TaskSetManager: Starting task 1.0 in stage 376.0 (TID 600) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 17:30:50 INFO BlockManagerInfo: Added broadcast_561_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:30:50 INFO BlockManagerInfo: Added broadcast_561_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:30:50 INFO TaskSetManager: Finished task 1.0 in stage 376.0 (TID 600) in 76 ms on 172.18.0.13 (executor 0) (1/2)
26/01/04 17:30:50 INFO TaskSetManager: Finished task 0.0 in stage 376.0 (TID 599) in 118 ms on 172.18.0.14 (executor 1) (2/2)
26/01/04 17:30:50 INFO DAGScheduler: ResultStage 376 (start at NativeMethodAccessorImpl.java:0) finished in 0.159 s
26/01/04 17:30:50 INFO DAGScheduler: Job 375 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:30:50 INFO TaskSchedulerImpl: Removed TaskSet 376.0, whose tasks have all completed, from pool 
26/01/04 17:30:50 INFO TaskSchedulerImpl: Killing all running tasks in stage 376: Stage finished
26/01/04 17:30:50 INFO DAGScheduler: Job 375 finished: start at NativeMethodAccessorImpl.java:0, took 0.184403 s
26/01/04 17:30:50 INFO BlockManagerInfo: Removed broadcast_558_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:30:50 INFO MemoryStore: Block broadcast_562_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.3 MiB)
26/01/04 17:30:50 INFO BlockManagerInfo: Removed broadcast_558_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:30:50 INFO BlockManagerInfo: Added broadcast_562_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:30:50 INFO SparkContext: Created broadcast 562 from start at NativeMethodAccessorImpl.java:0
26/01/04 17:30:50 INFO BlockManagerInfo: Removed broadcast_558_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:30:50 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 186, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@21485f4a]. The input RDD has 1 partitions.
26/01/04 17:30:50 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:30:50 INFO DAGScheduler: Got job 376 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/01/04 17:30:50 INFO DAGScheduler: Final stage: ResultStage 377 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:30:50 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:30:50 INFO DAGScheduler: Missing parents: List()
26/01/04 17:30:50 INFO DAGScheduler: Submitting ResultStage 377 (MapPartitionsRDD[1888] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:30:50 INFO MemoryStore: Block broadcast_563 stored as values in memory (estimated size 52.9 KiB, free 434.2 MiB)
26/01/04 17:30:50 INFO MemoryStore: Block broadcast_563_piece0 stored as bytes in memory (estimated size 20.1 KiB, free 434.2 MiB)
26/01/04 17:30:50 INFO BlockManagerInfo: Added broadcast_563_piece0 in memory on spark-master:33535 (size: 20.1 KiB, free: 434.3 MiB)
26/01/04 17:30:50 INFO SparkContext: Created broadcast 563 from broadcast at DAGScheduler.scala:1585
26/01/04 17:30:50 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 377 (MapPartitionsRDD[1888] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/01/04 17:30:50 INFO TaskSchedulerImpl: Adding task set 377.0 with 1 tasks resource profile 0
26/01/04 17:30:50 INFO TaskSetManager: Starting task 0.0 in stage 377.0 (TID 601) (172.18.0.13, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:30:50 INFO BlockManagerInfo: Removed broadcast_560_piece0 on spark-master:33535 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:30:50 INFO BlockManagerInfo: Removed broadcast_560_piece0 on 172.18.0.13:44401 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:30:50 INFO BlockManagerInfo: Added broadcast_563_piece0 in memory on 172.18.0.13:44401 (size: 20.1 KiB, free: 434.4 MiB)
26/01/04 17:30:50 INFO BlockManagerInfo: Added broadcast_562_piece0 in memory on 172.18.0.13:44401 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:30:51 INFO TaskSetManager: Finished task 0.0 in stage 377.0 (TID 601) in 770 ms on 172.18.0.13 (executor 0) (1/1)
26/01/04 17:30:51 INFO TaskSchedulerImpl: Removed TaskSet 377.0, whose tasks have all completed, from pool 
26/01/04 17:30:51 INFO DAGScheduler: ResultStage 377 (start at NativeMethodAccessorImpl.java:0) finished in 0.915 s
26/01/04 17:30:51 INFO DAGScheduler: Job 376 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:30:51 INFO TaskSchedulerImpl: Killing all running tasks in stage 377: Stage finished
26/01/04 17:30:51 INFO DAGScheduler: Job 376 finished: start at NativeMethodAccessorImpl.java:0, took 0.927924 s
26/01/04 17:30:51 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 186, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@21485f4a] is committing.
26/01/04 17:30:51 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 186, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@21485f4a] committed.
26/01/04 17:30:51 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/186 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.186.6cff0091-06aa-4c8b-a228-ec96e563c3ba.tmp
26/01/04 17:30:51 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.186.6cff0091-06aa-4c8b-a228-ec96e563c3ba.tmp to file:/tmp/spark-checkpoint-enrichment/commits/186
26/01/04 17:30:51 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T17:30:49.805Z",
  "batchId" : 186,
  "numInputRows" : 27,
  "inputRowsPerSecond" : 2076.923076923077,
  "processedRowsPerSecond" : 13.910355486862441,
  "durationMs" : {
    "addBatch" : 1317,
    "commitOffsets" : 353,
    "getBatch" : 0,
    "latestOffset" : 25,
    "queryPlanning" : 92,
    "triggerExecution" : 1941,
    "walCommit" : 153
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "0" : 960
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "0" : 987
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "0" : 987
      }
    },
    "numInputRows" : 27,
    "inputRowsPerSecond" : 2076.923076923077,
    "processedRowsPerSecond" : 13.910355486862441,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 28
  }
}
26/01/04 17:30:51 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/187 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.187.f8d1166c-a65f-4d17-8413-c33ccac92b4e.tmp
26/01/04 17:30:51 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.187.f8d1166c-a65f-4d17-8413-c33ccac92b4e.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/187
26/01/04 17:30:51 INFO MicroBatchExecution: Committed offsets for batch 187. Metadata OffsetSeqMetadata(0,1767547851751,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 17:30:52 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:30:52 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:30:52 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:30:52 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:30:52 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:30:52 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:30:52 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#151831 - airline_prefix.nullCount#151830) > 0)
26/01/04 17:30:52 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#151866 - min_flight_num.nullCount#151865) > 0)
26/01/04 17:30:52 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#151861 - max_flight_num.nullCount#151860) > 0)
26/01/04 17:30:52 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:30:52 INFO DAGScheduler: Got job 377 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 17:30:52 INFO DAGScheduler: Final stage: ResultStage 378 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:30:52 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:30:52 INFO DAGScheduler: Missing parents: List()
26/01/04 17:30:52 INFO DAGScheduler: Submitting ResultStage 378 (MapPartitionsRDD[1893] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:30:52 INFO MemoryStore: Block broadcast_564 stored as values in memory (estimated size 38.5 KiB, free 434.2 MiB)
26/01/04 17:30:52 INFO MemoryStore: Block broadcast_564_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.2 MiB)
26/01/04 17:30:52 INFO BlockManagerInfo: Added broadcast_564_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:30:52 INFO SparkContext: Created broadcast 564 from broadcast at DAGScheduler.scala:1585
26/01/04 17:30:52 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 378 (MapPartitionsRDD[1893] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 17:30:52 INFO TaskSchedulerImpl: Adding task set 378.0 with 2 tasks resource profile 0
26/01/04 17:30:52 INFO TaskSetManager: Starting task 0.0 in stage 378.0 (TID 602) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 17:30:52 INFO TaskSetManager: Starting task 1.0 in stage 378.0 (TID 603) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 17:30:52 INFO BlockManagerInfo: Removed broadcast_562_piece0 on 172.18.0.13:44401 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:30:52 INFO BlockManagerInfo: Removed broadcast_562_piece0 on spark-master:33535 in memory (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:30:52 INFO BlockManagerInfo: Removed broadcast_563_piece0 on spark-master:33535 in memory (size: 20.1 KiB, free: 434.4 MiB)
26/01/04 17:30:52 INFO BlockManagerInfo: Added broadcast_564_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:30:52 INFO BlockManagerInfo: Removed broadcast_563_piece0 on 172.18.0.13:44401 in memory (size: 20.1 KiB, free: 434.4 MiB)
26/01/04 17:30:52 INFO TaskSetManager: Finished task 1.0 in stage 378.0 (TID 603) in 140 ms on 172.18.0.13 (executor 0) (1/2)
26/01/04 17:30:52 INFO BlockManagerInfo: Removed broadcast_561_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:30:52 INFO BlockManagerInfo: Removed broadcast_561_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:30:52 INFO BlockManagerInfo: Removed broadcast_561_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:30:52 INFO BlockManagerInfo: Added broadcast_564_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:30:52 INFO TaskSetManager: Finished task 0.0 in stage 378.0 (TID 602) in 569 ms on 172.18.0.14 (executor 1) (2/2)
26/01/04 17:30:52 INFO TaskSchedulerImpl: Removed TaskSet 378.0, whose tasks have all completed, from pool 
26/01/04 17:30:52 INFO DAGScheduler: ResultStage 378 (start at NativeMethodAccessorImpl.java:0) finished in 0.619 s
26/01/04 17:30:52 INFO DAGScheduler: Job 377 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:30:52 INFO TaskSchedulerImpl: Killing all running tasks in stage 378: Stage finished
26/01/04 17:30:52 INFO DAGScheduler: Job 377 finished: start at NativeMethodAccessorImpl.java:0, took 0.646017 s
26/01/04 17:30:52 INFO MemoryStore: Block broadcast_565_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.3 MiB)
26/01/04 17:30:52 INFO BlockManagerInfo: Added broadcast_565_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:30:52 INFO SparkContext: Created broadcast 565 from start at NativeMethodAccessorImpl.java:0
26/01/04 17:30:52 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 187, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@60c90f38]. The input RDD has 1 partitions.
26/01/04 17:30:52 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:30:52 INFO DAGScheduler: Got job 378 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/01/04 17:30:52 INFO DAGScheduler: Final stage: ResultStage 379 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:30:52 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:30:52 INFO DAGScheduler: Missing parents: List()
26/01/04 17:30:52 INFO DAGScheduler: Submitting ResultStage 379 (MapPartitionsRDD[1898] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:30:52 INFO MemoryStore: Block broadcast_566 stored as values in memory (estimated size 52.9 KiB, free 434.3 MiB)
26/01/04 17:30:52 INFO MemoryStore: Block broadcast_566_piece0 stored as bytes in memory (estimated size 20.2 KiB, free 434.3 MiB)
26/01/04 17:30:52 INFO BlockManagerInfo: Added broadcast_566_piece0 in memory on spark-master:33535 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:30:52 INFO SparkContext: Created broadcast 566 from broadcast at DAGScheduler.scala:1585
26/01/04 17:30:52 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 379 (MapPartitionsRDD[1898] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/01/04 17:30:52 INFO TaskSchedulerImpl: Adding task set 379.0 with 1 tasks resource profile 0
26/01/04 17:30:52 INFO TaskSetManager: Starting task 0.0 in stage 379.0 (TID 604) (172.18.0.13, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:30:52 INFO BlockManagerInfo: Added broadcast_566_piece0 in memory on 172.18.0.13:44401 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:30:52 INFO BlockManagerInfo: Added broadcast_565_piece0 in memory on 172.18.0.13:44401 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:30:53 INFO TaskSetManager: Finished task 0.0 in stage 379.0 (TID 604) in 87 ms on 172.18.0.13 (executor 0) (1/1)
26/01/04 17:30:53 INFO TaskSchedulerImpl: Removed TaskSet 379.0, whose tasks have all completed, from pool 
26/01/04 17:30:53 INFO DAGScheduler: ResultStage 379 (start at NativeMethodAccessorImpl.java:0) finished in 0.097 s
26/01/04 17:30:53 INFO DAGScheduler: Job 378 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:30:53 INFO TaskSchedulerImpl: Killing all running tasks in stage 379: Stage finished
26/01/04 17:30:53 INFO DAGScheduler: Job 378 finished: start at NativeMethodAccessorImpl.java:0, took 0.099633 s
26/01/04 17:30:53 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 187, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@60c90f38] is committing.
26/01/04 17:30:53 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 187, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@60c90f38] committed.
26/01/04 17:30:53 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/187 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.187.f09d74f5-848d-428a-9d60-5d0dcdd4bab5.tmp
26/01/04 17:30:53 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.187.f09d74f5-848d-428a-9d60-5d0dcdd4bab5.tmp to file:/tmp/spark-checkpoint-enrichment/commits/187
26/01/04 17:30:53 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T17:30:51.748Z",
  "batchId" : 187,
  "numInputRows" : 3,
  "inputRowsPerSecond" : 1.5440041173443129,
  "processedRowsPerSecond" : 1.757469244288225,
  "durationMs" : {
    "addBatch" : 885,
    "commitOffsets" : 411,
    "getBatch" : 1,
    "latestOffset" : 3,
    "queryPlanning" : 156,
    "triggerExecution" : 1707,
    "walCommit" : 249
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "0" : 987
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "0" : 990
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "0" : 990
      }
    },
    "numInputRows" : 3,
    "inputRowsPerSecond" : 1.5440041173443129,
    "processedRowsPerSecond" : 1.757469244288225,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 3
  }
}
26/01/04 17:31:00 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/188 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.188.3f4dd42e-5502-4379-81d4-88e2eed0f740.tmp
26/01/04 17:31:01 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.188.3f4dd42e-5502-4379-81d4-88e2eed0f740.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/188
26/01/04 17:31:01 INFO MicroBatchExecution: Committed offsets for batch 188. Metadata OffsetSeqMetadata(0,1767547860886,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 17:31:01 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:31:01 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:31:01 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:31:01 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:31:01 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:31:01 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:31:01 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#152635 - airline_prefix.nullCount#152634) > 0)
26/01/04 17:31:01 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#152670 - min_flight_num.nullCount#152669) > 0)
26/01/04 17:31:01 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#152665 - max_flight_num.nullCount#152664) > 0)
26/01/04 17:31:01 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:31:01 INFO DAGScheduler: Got job 379 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 17:31:01 INFO DAGScheduler: Final stage: ResultStage 380 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:31:01 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:31:01 INFO DAGScheduler: Missing parents: List()
26/01/04 17:31:01 INFO DAGScheduler: Submitting ResultStage 380 (MapPartitionsRDD[1903] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:31:01 INFO MemoryStore: Block broadcast_567 stored as values in memory (estimated size 38.5 KiB, free 434.2 MiB)
26/01/04 17:31:01 INFO MemoryStore: Block broadcast_567_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.2 MiB)
26/01/04 17:31:01 INFO BlockManagerInfo: Added broadcast_567_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:31:01 INFO SparkContext: Created broadcast 567 from broadcast at DAGScheduler.scala:1585
26/01/04 17:31:01 INFO BlockManagerInfo: Removed broadcast_566_piece0 on spark-master:33535 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:31:01 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 380 (MapPartitionsRDD[1903] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 17:31:01 INFO TaskSchedulerImpl: Adding task set 380.0 with 2 tasks resource profile 0
26/01/04 17:31:01 INFO TaskSetManager: Starting task 0.0 in stage 380.0 (TID 605) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 17:31:01 INFO TaskSetManager: Starting task 1.0 in stage 380.0 (TID 606) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 17:31:01 INFO BlockManagerInfo: Removed broadcast_566_piece0 on 172.18.0.13:44401 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:31:01 INFO BlockManagerInfo: Added broadcast_567_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:31:01 INFO TaskSetManager: Finished task 1.0 in stage 380.0 (TID 606) in 153 ms on 172.18.0.13 (executor 0) (1/2)
26/01/04 17:31:01 INFO BlockManagerInfo: Removed broadcast_565_piece0 on spark-master:33535 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:31:01 INFO BlockManagerInfo: Removed broadcast_565_piece0 on 172.18.0.13:44401 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:31:01 INFO BlockManagerInfo: Removed broadcast_564_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:31:01 INFO BlockManagerInfo: Removed broadcast_564_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:31:01 INFO BlockManagerInfo: Removed broadcast_564_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:31:01 INFO BlockManagerInfo: Added broadcast_567_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:31:01 INFO TaskSetManager: Finished task 0.0 in stage 380.0 (TID 605) in 412 ms on 172.18.0.14 (executor 1) (2/2)
26/01/04 17:31:01 INFO TaskSchedulerImpl: Removed TaskSet 380.0, whose tasks have all completed, from pool 
26/01/04 17:31:01 INFO DAGScheduler: ResultStage 380 (start at NativeMethodAccessorImpl.java:0) finished in 0.479 s
26/01/04 17:31:01 INFO DAGScheduler: Job 379 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:31:01 INFO TaskSchedulerImpl: Killing all running tasks in stage 380: Stage finished
26/01/04 17:31:01 INFO DAGScheduler: Job 379 finished: start at NativeMethodAccessorImpl.java:0, took 0.483571 s
26/01/04 17:31:01 INFO BlockManagerInfo: Removed broadcast_567_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:31:01 INFO MemoryStore: Block broadcast_568_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.4 MiB)
26/01/04 17:31:01 INFO BlockManagerInfo: Added broadcast_568_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:31:01 INFO SparkContext: Created broadcast 568 from start at NativeMethodAccessorImpl.java:0
26/01/04 17:31:01 INFO BlockManagerInfo: Removed broadcast_567_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:31:01 INFO BlockManagerInfo: Removed broadcast_567_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:31:01 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 188, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@330460a2]. The input RDD has 1 partitions.
26/01/04 17:31:01 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:31:01 INFO DAGScheduler: Got job 380 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/01/04 17:31:01 INFO DAGScheduler: Final stage: ResultStage 381 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:31:01 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:31:01 INFO DAGScheduler: Missing parents: List()
26/01/04 17:31:01 INFO DAGScheduler: Submitting ResultStage 381 (MapPartitionsRDD[1908] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:31:01 INFO MemoryStore: Block broadcast_569 stored as values in memory (estimated size 52.9 KiB, free 434.3 MiB)
26/01/04 17:31:01 INFO MemoryStore: Block broadcast_569_piece0 stored as bytes in memory (estimated size 20.1 KiB, free 434.3 MiB)
26/01/04 17:31:01 INFO BlockManagerInfo: Added broadcast_569_piece0 in memory on spark-master:33535 (size: 20.1 KiB, free: 434.4 MiB)
26/01/04 17:31:01 INFO SparkContext: Created broadcast 569 from broadcast at DAGScheduler.scala:1585
26/01/04 17:31:01 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 381 (MapPartitionsRDD[1908] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/01/04 17:31:01 INFO TaskSchedulerImpl: Adding task set 381.0 with 1 tasks resource profile 0
26/01/04 17:31:01 INFO TaskSetManager: Starting task 0.0 in stage 381.0 (TID 607) (172.18.0.13, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:31:01 INFO BlockManagerInfo: Added broadcast_569_piece0 in memory on 172.18.0.13:44401 (size: 20.1 KiB, free: 434.4 MiB)
26/01/04 17:31:01 INFO BlockManagerInfo: Added broadcast_568_piece0 in memory on 172.18.0.13:44401 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:31:02 INFO TaskSetManager: Finished task 0.0 in stage 381.0 (TID 607) in 792 ms on 172.18.0.13 (executor 0) (1/1)
26/01/04 17:31:02 INFO DAGScheduler: ResultStage 381 (start at NativeMethodAccessorImpl.java:0) finished in 0.808 s
26/01/04 17:31:02 INFO DAGScheduler: Job 380 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:31:02 INFO TaskSchedulerImpl: Removed TaskSet 381.0, whose tasks have all completed, from pool 
26/01/04 17:31:02 INFO TaskSchedulerImpl: Killing all running tasks in stage 381: Stage finished
26/01/04 17:31:02 INFO DAGScheduler: Job 380 finished: start at NativeMethodAccessorImpl.java:0, took 0.813645 s
26/01/04 17:31:02 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 188, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@330460a2] is committing.
26/01/04 17:31:02 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 188, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@330460a2] committed.
26/01/04 17:31:02 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/188 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.188.8e4e5a9d-b8e1-4fd7-a93b-9340816c4687.tmp
26/01/04 17:31:02 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.188.8e4e5a9d-b8e1-4fd7-a93b-9340816c4687.tmp to file:/tmp/spark-checkpoint-enrichment/commits/188
26/01/04 17:31:02 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T17:31:00.841Z",
  "batchId" : 188,
  "numInputRows" : 30,
  "inputRowsPerSecond" : 2142.8571428571427,
  "processedRowsPerSecond" : 15.560165975103734,
  "durationMs" : {
    "addBatch" : 1463,
    "commitOffsets" : 166,
    "getBatch" : 0,
    "latestOffset" : 45,
    "queryPlanning" : 41,
    "triggerExecution" : 1928,
    "walCommit" : 211
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "0" : 990
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "0" : 1020
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "0" : 1020
      }
    },
    "numInputRows" : 30,
    "inputRowsPerSecond" : 2142.8571428571427,
    "processedRowsPerSecond" : 15.560165975103734,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 31
  }
}
26/01/04 17:31:13 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/189 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.189.ebfdd7da-e77f-4f23-9e01-caa2ee21daf7.tmp
26/01/04 17:31:13 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.189.ebfdd7da-e77f-4f23-9e01-caa2ee21daf7.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/189
26/01/04 17:31:13 INFO MicroBatchExecution: Committed offsets for batch 189. Metadata OffsetSeqMetadata(0,1767547872688,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 17:31:13 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:31:13 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:31:13 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:31:13 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:31:13 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:31:13 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:31:13 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#153439 - airline_prefix.nullCount#153438) > 0)
26/01/04 17:31:13 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#153474 - min_flight_num.nullCount#153473) > 0)
26/01/04 17:31:13 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#153469 - max_flight_num.nullCount#153468) > 0)
26/01/04 17:31:13 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:31:13 INFO DAGScheduler: Got job 381 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 17:31:13 INFO DAGScheduler: Final stage: ResultStage 382 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:31:13 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:31:13 INFO DAGScheduler: Missing parents: List()
26/01/04 17:31:13 INFO DAGScheduler: Submitting ResultStage 382 (MapPartitionsRDD[1913] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:31:13 INFO MemoryStore: Block broadcast_570 stored as values in memory (estimated size 38.5 KiB, free 434.3 MiB)
26/01/04 17:31:13 INFO MemoryStore: Block broadcast_570_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.3 MiB)
26/01/04 17:31:13 INFO BlockManagerInfo: Added broadcast_570_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:31:13 INFO SparkContext: Created broadcast 570 from broadcast at DAGScheduler.scala:1585
26/01/04 17:31:13 INFO BlockManagerInfo: Removed broadcast_569_piece0 on 172.18.0.13:44401 in memory (size: 20.1 KiB, free: 434.4 MiB)
26/01/04 17:31:13 INFO BlockManagerInfo: Removed broadcast_569_piece0 on spark-master:33535 in memory (size: 20.1 KiB, free: 434.4 MiB)
26/01/04 17:31:13 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 382 (MapPartitionsRDD[1913] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 17:31:13 INFO TaskSchedulerImpl: Adding task set 382.0 with 2 tasks resource profile 0
26/01/04 17:31:13 INFO BlockManagerInfo: Removed broadcast_568_piece0 on spark-master:33535 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:31:13 INFO TaskSetManager: Starting task 0.0 in stage 382.0 (TID 608) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 17:31:13 INFO TaskSetManager: Starting task 1.0 in stage 382.0 (TID 609) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 17:31:13 INFO BlockManagerInfo: Removed broadcast_568_piece0 on 172.18.0.13:44401 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:31:13 INFO BlockManagerInfo: Added broadcast_570_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:31:13 INFO BlockManagerInfo: Added broadcast_570_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:31:13 INFO TaskSetManager: Finished task 1.0 in stage 382.0 (TID 609) in 183 ms on 172.18.0.13 (executor 0) (1/2)
26/01/04 17:31:14 INFO TaskSetManager: Finished task 0.0 in stage 382.0 (TID 608) in 347 ms on 172.18.0.14 (executor 1) (2/2)
26/01/04 17:31:14 INFO TaskSchedulerImpl: Removed TaskSet 382.0, whose tasks have all completed, from pool 
26/01/04 17:31:14 INFO DAGScheduler: ResultStage 382 (start at NativeMethodAccessorImpl.java:0) finished in 0.468 s
26/01/04 17:31:14 INFO DAGScheduler: Job 381 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:31:14 INFO TaskSchedulerImpl: Killing all running tasks in stage 382: Stage finished
26/01/04 17:31:14 INFO DAGScheduler: Job 381 finished: start at NativeMethodAccessorImpl.java:0, took 0.475925 s
26/01/04 17:31:14 INFO MemoryStore: Block broadcast_571_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.3 MiB)
26/01/04 17:31:14 INFO BlockManagerInfo: Added broadcast_571_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:31:14 INFO SparkContext: Created broadcast 571 from start at NativeMethodAccessorImpl.java:0
26/01/04 17:31:14 INFO BlockManagerInfo: Removed broadcast_570_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:31:14 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 189, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@2a7d4b40]. The input RDD has 1 partitions.
26/01/04 17:31:14 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:31:14 INFO DAGScheduler: Got job 382 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/01/04 17:31:14 INFO DAGScheduler: Final stage: ResultStage 383 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:31:14 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:31:14 INFO DAGScheduler: Missing parents: List()
26/01/04 17:31:14 INFO DAGScheduler: Submitting ResultStage 383 (MapPartitionsRDD[1918] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:31:14 INFO MemoryStore: Block broadcast_572 stored as values in memory (estimated size 52.9 KiB, free 434.3 MiB)
26/01/04 17:31:14 INFO BlockManagerInfo: Removed broadcast_570_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:31:14 INFO MemoryStore: Block broadcast_572_piece0 stored as bytes in memory (estimated size 20.2 KiB, free 434.3 MiB)
26/01/04 17:31:14 INFO BlockManagerInfo: Added broadcast_572_piece0 in memory on spark-master:33535 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:31:14 INFO SparkContext: Created broadcast 572 from broadcast at DAGScheduler.scala:1585
26/01/04 17:31:14 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 383 (MapPartitionsRDD[1918] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/01/04 17:31:14 INFO TaskSchedulerImpl: Adding task set 383.0 with 1 tasks resource profile 0
26/01/04 17:31:14 INFO TaskSetManager: Starting task 0.0 in stage 383.0 (TID 610) (172.18.0.13, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:31:14 INFO BlockManagerInfo: Removed broadcast_570_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:31:14 INFO BlockManagerInfo: Added broadcast_572_piece0 in memory on 172.18.0.13:44401 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:31:14 INFO BlockManagerInfo: Added broadcast_571_piece0 in memory on 172.18.0.13:44401 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:31:15 INFO TaskSetManager: Finished task 0.0 in stage 383.0 (TID 610) in 858 ms on 172.18.0.13 (executor 0) (1/1)
26/01/04 17:31:15 INFO TaskSchedulerImpl: Removed TaskSet 383.0, whose tasks have all completed, from pool 
26/01/04 17:31:15 INFO DAGScheduler: ResultStage 383 (start at NativeMethodAccessorImpl.java:0) finished in 0.872 s
26/01/04 17:31:15 INFO DAGScheduler: Job 382 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:31:15 INFO TaskSchedulerImpl: Killing all running tasks in stage 383: Stage finished
26/01/04 17:31:15 INFO DAGScheduler: Job 382 finished: start at NativeMethodAccessorImpl.java:0, took 0.884053 s
26/01/04 17:31:15 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 189, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@2a7d4b40] is committing.
26/01/04 17:31:15 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 189, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@2a7d4b40] committed.
26/01/04 17:31:15 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/189 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.189.dc225910-984e-4037-8feb-7c0eb8856367.tmp
26/01/04 17:31:15 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.189.dc225910-984e-4037-8feb-7c0eb8856367.tmp to file:/tmp/spark-checkpoint-enrichment/commits/189
26/01/04 17:31:15 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T17:31:12.688Z",
  "batchId" : 189,
  "numInputRows" : 30,
  "inputRowsPerSecond" : 2500.0,
  "processedRowsPerSecond" : 11.009174311926605,
  "durationMs" : {
    "addBatch" : 1628,
    "commitOffsets" : 376,
    "getBatch" : 1,
    "latestOffset" : 0,
    "queryPlanning" : 186,
    "triggerExecution" : 2725,
    "walCommit" : 532
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "0" : 1020
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "0" : 1050
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "0" : 1050
      }
    },
    "numInputRows" : 30,
    "inputRowsPerSecond" : 2500.0,
    "processedRowsPerSecond" : 11.009174311926605,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 31
  }
}
26/01/04 17:31:23 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/190 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.190.736ea781-ad23-4661-8b9b-5d59151aef2b.tmp
26/01/04 17:31:24 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.190.736ea781-ad23-4661-8b9b-5d59151aef2b.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/190
26/01/04 17:31:24 INFO MicroBatchExecution: Committed offsets for batch 190. Metadata OffsetSeqMetadata(0,1767547883725,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 17:31:24 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:31:24 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:31:24 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:31:24 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:31:24 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:31:24 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:31:24 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#154243 - airline_prefix.nullCount#154242) > 0)
26/01/04 17:31:24 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#154278 - min_flight_num.nullCount#154277) > 0)
26/01/04 17:31:24 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#154273 - max_flight_num.nullCount#154272) > 0)
26/01/04 17:31:24 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:31:24 INFO DAGScheduler: Got job 383 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 17:31:24 INFO DAGScheduler: Final stage: ResultStage 384 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:31:24 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:31:24 INFO DAGScheduler: Missing parents: List()
26/01/04 17:31:24 INFO DAGScheduler: Submitting ResultStage 384 (MapPartitionsRDD[1923] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:31:24 INFO MemoryStore: Block broadcast_573 stored as values in memory (estimated size 38.5 KiB, free 434.3 MiB)
26/01/04 17:31:24 INFO MemoryStore: Block broadcast_573_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.3 MiB)
26/01/04 17:31:24 INFO BlockManagerInfo: Added broadcast_573_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:31:24 INFO SparkContext: Created broadcast 573 from broadcast at DAGScheduler.scala:1585
26/01/04 17:31:24 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 384 (MapPartitionsRDD[1923] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 17:31:24 INFO TaskSchedulerImpl: Adding task set 384.0 with 2 tasks resource profile 0
26/01/04 17:31:24 INFO TaskSetManager: Starting task 1.0 in stage 384.0 (TID 611) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 17:31:24 INFO TaskSetManager: Starting task 0.0 in stage 384.0 (TID 612) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 17:31:24 INFO BlockManagerInfo: Removed broadcast_572_piece0 on spark-master:33535 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:31:24 INFO BlockManagerInfo: Removed broadcast_572_piece0 on 172.18.0.13:44401 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:31:24 INFO BlockManagerInfo: Added broadcast_573_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:31:24 INFO BlockManagerInfo: Removed broadcast_571_piece0 on spark-master:33535 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:31:24 INFO BlockManagerInfo: Added broadcast_573_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:31:24 INFO BlockManagerInfo: Removed broadcast_571_piece0 on 172.18.0.13:44401 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:31:24 INFO TaskSetManager: Finished task 1.0 in stage 384.0 (TID 611) in 287 ms on 172.18.0.13 (executor 0) (1/2)
26/01/04 17:31:25 INFO TaskSetManager: Finished task 0.0 in stage 384.0 (TID 612) in 587 ms on 172.18.0.14 (executor 1) (2/2)
26/01/04 17:31:25 INFO TaskSchedulerImpl: Removed TaskSet 384.0, whose tasks have all completed, from pool 
26/01/04 17:31:25 INFO DAGScheduler: ResultStage 384 (start at NativeMethodAccessorImpl.java:0) finished in 0.636 s
26/01/04 17:31:25 INFO DAGScheduler: Job 383 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:31:25 INFO TaskSchedulerImpl: Killing all running tasks in stage 384: Stage finished
26/01/04 17:31:25 INFO DAGScheduler: Job 383 finished: start at NativeMethodAccessorImpl.java:0, took 0.654579 s
26/01/04 17:31:25 INFO MemoryStore: Block broadcast_574_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.3 MiB)
26/01/04 17:31:25 INFO BlockManagerInfo: Added broadcast_574_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:31:25 INFO SparkContext: Created broadcast 574 from start at NativeMethodAccessorImpl.java:0
26/01/04 17:31:25 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 190, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@5629b473]. The input RDD has 1 partitions.
26/01/04 17:31:25 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:31:25 INFO DAGScheduler: Got job 384 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/01/04 17:31:25 INFO DAGScheduler: Final stage: ResultStage 385 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:31:25 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:31:25 INFO DAGScheduler: Missing parents: List()
26/01/04 17:31:25 INFO DAGScheduler: Submitting ResultStage 385 (MapPartitionsRDD[1928] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:31:25 INFO BlockManagerInfo: Removed broadcast_573_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:31:25 INFO MemoryStore: Block broadcast_575 stored as values in memory (estimated size 52.9 KiB, free 434.3 MiB)
26/01/04 17:31:25 INFO MemoryStore: Block broadcast_575_piece0 stored as bytes in memory (estimated size 20.1 KiB, free 434.3 MiB)
26/01/04 17:31:25 INFO BlockManagerInfo: Added broadcast_575_piece0 in memory on spark-master:33535 (size: 20.1 KiB, free: 434.4 MiB)
26/01/04 17:31:25 INFO SparkContext: Created broadcast 575 from broadcast at DAGScheduler.scala:1585
26/01/04 17:31:25 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 385 (MapPartitionsRDD[1928] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/01/04 17:31:25 INFO TaskSchedulerImpl: Adding task set 385.0 with 1 tasks resource profile 0
26/01/04 17:31:25 INFO TaskSetManager: Starting task 0.0 in stage 385.0 (TID 613) (172.18.0.13, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:31:25 INFO BlockManagerInfo: Removed broadcast_573_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:31:25 INFO BlockManagerInfo: Removed broadcast_573_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:31:25 INFO BlockManagerInfo: Added broadcast_575_piece0 in memory on 172.18.0.13:44401 (size: 20.1 KiB, free: 434.4 MiB)
26/01/04 17:31:25 INFO BlockManagerInfo: Added broadcast_574_piece0 in memory on 172.18.0.13:44401 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:31:26 INFO TaskSetManager: Finished task 0.0 in stage 385.0 (TID 613) in 1011 ms on 172.18.0.13 (executor 0) (1/1)
26/01/04 17:31:26 INFO DAGScheduler: ResultStage 385 (start at NativeMethodAccessorImpl.java:0) finished in 1.044 s
26/01/04 17:31:26 INFO DAGScheduler: Job 384 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:31:26 INFO TaskSchedulerImpl: Removed TaskSet 385.0, whose tasks have all completed, from pool 
26/01/04 17:31:26 INFO TaskSchedulerImpl: Killing all running tasks in stage 385: Stage finished
26/01/04 17:31:26 INFO DAGScheduler: Job 384 finished: start at NativeMethodAccessorImpl.java:0, took 1.058682 s
26/01/04 17:31:26 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 190, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@5629b473] is committing.
26/01/04 17:31:26 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 190, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@5629b473] committed.
26/01/04 17:31:26 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/190 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.190.4c4f7dec-7dd0-4f1a-863c-069ed1bd4af0.tmp
26/01/04 17:31:26 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.190.4c4f7dec-7dd0-4f1a-863c-069ed1bd4af0.tmp to file:/tmp/spark-checkpoint-enrichment/commits/190
26/01/04 17:31:26 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T17:31:23.697Z",
  "batchId" : 190,
  "numInputRows" : 30,
  "inputRowsPerSecond" : 1500.0,
  "processedRowsPerSecond" : 11.26126126126126,
  "durationMs" : {
    "addBatch" : 2112,
    "commitOffsets" : 194,
    "getBatch" : 0,
    "latestOffset" : 28,
    "queryPlanning" : 47,
    "triggerExecution" : 2664,
    "walCommit" : 282
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "0" : 1050
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "0" : 1080
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "0" : 1080
      }
    },
    "numInputRows" : 30,
    "inputRowsPerSecond" : 1500.0,
    "processedRowsPerSecond" : 11.26126126126126,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 31
  }
}
26/01/04 17:31:34 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/191 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.191.c7a9a68a-06f1-4d5a-8e5a-49a88a223db8.tmp
26/01/04 17:31:35 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.191.c7a9a68a-06f1-4d5a-8e5a-49a88a223db8.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/191
26/01/04 17:31:35 INFO MicroBatchExecution: Committed offsets for batch 191. Metadata OffsetSeqMetadata(0,1767547894791,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 17:31:35 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:31:35 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:31:35 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:31:35 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:31:35 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:31:35 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:31:35 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#155047 - airline_prefix.nullCount#155046) > 0)
26/01/04 17:31:35 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#155082 - min_flight_num.nullCount#155081) > 0)
26/01/04 17:31:35 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#155077 - max_flight_num.nullCount#155076) > 0)
26/01/04 17:31:35 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:31:35 INFO DAGScheduler: Got job 385 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 17:31:35 INFO DAGScheduler: Final stage: ResultStage 386 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:31:35 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:31:35 INFO DAGScheduler: Missing parents: List()
26/01/04 17:31:35 INFO DAGScheduler: Submitting ResultStage 386 (MapPartitionsRDD[1933] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:31:35 INFO MemoryStore: Block broadcast_576 stored as values in memory (estimated size 38.5 KiB, free 434.3 MiB)
26/01/04 17:31:35 INFO MemoryStore: Block broadcast_576_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.3 MiB)
26/01/04 17:31:35 INFO BlockManagerInfo: Added broadcast_576_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:31:35 INFO SparkContext: Created broadcast 576 from broadcast at DAGScheduler.scala:1585
26/01/04 17:31:35 INFO BlockManagerInfo: Removed broadcast_575_piece0 on spark-master:33535 in memory (size: 20.1 KiB, free: 434.4 MiB)
26/01/04 17:31:35 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 386 (MapPartitionsRDD[1933] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 17:31:35 INFO TaskSchedulerImpl: Adding task set 386.0 with 2 tasks resource profile 0
26/01/04 17:31:35 INFO TaskSetManager: Starting task 0.0 in stage 386.0 (TID 614) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 17:31:35 INFO TaskSetManager: Starting task 1.0 in stage 386.0 (TID 615) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 17:31:35 INFO BlockManagerInfo: Removed broadcast_575_piece0 on 172.18.0.13:44401 in memory (size: 20.1 KiB, free: 434.4 MiB)
26/01/04 17:31:35 INFO BlockManagerInfo: Removed broadcast_574_piece0 on spark-master:33535 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:31:35 INFO BlockManagerInfo: Removed broadcast_574_piece0 on 172.18.0.13:44401 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:31:35 INFO BlockManagerInfo: Added broadcast_576_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:31:35 INFO BlockManagerInfo: Added broadcast_576_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:31:35 INFO TaskSetManager: Finished task 0.0 in stage 386.0 (TID 614) in 199 ms on 172.18.0.14 (executor 1) (1/2)
26/01/04 17:31:35 INFO TaskSetManager: Finished task 1.0 in stage 386.0 (TID 615) in 218 ms on 172.18.0.13 (executor 0) (2/2)
26/01/04 17:31:35 INFO DAGScheduler: ResultStage 386 (start at NativeMethodAccessorImpl.java:0) finished in 0.263 s
26/01/04 17:31:35 INFO TaskSchedulerImpl: Removed TaskSet 386.0, whose tasks have all completed, from pool 
26/01/04 17:31:35 INFO DAGScheduler: Job 385 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:31:35 INFO TaskSchedulerImpl: Killing all running tasks in stage 386: Stage finished
26/01/04 17:31:35 INFO DAGScheduler: Job 385 finished: start at NativeMethodAccessorImpl.java:0, took 0.269672 s
26/01/04 17:31:35 INFO MemoryStore: Block broadcast_577_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.3 MiB)
26/01/04 17:31:35 INFO BlockManagerInfo: Added broadcast_577_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:31:35 INFO SparkContext: Created broadcast 577 from start at NativeMethodAccessorImpl.java:0
26/01/04 17:31:35 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 191, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@4fdeb5f6]. The input RDD has 1 partitions.
26/01/04 17:31:35 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:31:35 INFO DAGScheduler: Got job 386 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/01/04 17:31:35 INFO DAGScheduler: Final stage: ResultStage 387 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:31:35 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:31:35 INFO DAGScheduler: Missing parents: List()
26/01/04 17:31:35 INFO DAGScheduler: Submitting ResultStage 387 (MapPartitionsRDD[1938] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:31:35 INFO MemoryStore: Block broadcast_578 stored as values in memory (estimated size 52.9 KiB, free 434.3 MiB)
26/01/04 17:31:35 INFO MemoryStore: Block broadcast_578_piece0 stored as bytes in memory (estimated size 20.2 KiB, free 434.3 MiB)
26/01/04 17:31:35 INFO BlockManagerInfo: Added broadcast_578_piece0 in memory on spark-master:33535 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:31:35 INFO SparkContext: Created broadcast 578 from broadcast at DAGScheduler.scala:1585
26/01/04 17:31:35 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 387 (MapPartitionsRDD[1938] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/01/04 17:31:35 INFO TaskSchedulerImpl: Adding task set 387.0 with 1 tasks resource profile 0
26/01/04 17:31:35 INFO TaskSetManager: Starting task 0.0 in stage 387.0 (TID 616) (172.18.0.13, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:31:35 INFO BlockManagerInfo: Added broadcast_578_piece0 in memory on 172.18.0.13:44401 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:31:35 INFO BlockManagerInfo: Added broadcast_577_piece0 in memory on 172.18.0.13:44401 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:31:36 INFO TaskSetManager: Finished task 0.0 in stage 387.0 (TID 616) in 826 ms on 172.18.0.13 (executor 0) (1/1)
26/01/04 17:31:36 INFO TaskSchedulerImpl: Removed TaskSet 387.0, whose tasks have all completed, from pool 
26/01/04 17:31:36 INFO DAGScheduler: ResultStage 387 (start at NativeMethodAccessorImpl.java:0) finished in 0.865 s
26/01/04 17:31:36 INFO DAGScheduler: Job 386 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:31:36 INFO TaskSchedulerImpl: Killing all running tasks in stage 387: Stage finished
26/01/04 17:31:36 INFO DAGScheduler: Job 386 finished: start at NativeMethodAccessorImpl.java:0, took 0.874771 s
26/01/04 17:31:36 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 191, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@4fdeb5f6] is committing.
26/01/04 17:31:36 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 191, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@4fdeb5f6] committed.
26/01/04 17:31:36 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/191 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.191.74e42c97-02a6-414a-a0e0-1efe3512e55a.tmp
26/01/04 17:31:36 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.191.74e42c97-02a6-414a-a0e0-1efe3512e55a.tmp to file:/tmp/spark-checkpoint-enrichment/commits/191
26/01/04 17:31:36 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T17:31:34.788Z",
  "batchId" : 191,
  "numInputRows" : 30,
  "inputRowsPerSecond" : 256.4102564102564,
  "processedRowsPerSecond" : 15.353121801432959,
  "durationMs" : {
    "addBatch" : 1497,
    "commitOffsets" : 187,
    "getBatch" : 0,
    "latestOffset" : 3,
    "queryPlanning" : 48,
    "triggerExecution" : 1954,
    "walCommit" : 215
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "0" : 1080
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "0" : 1110
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "0" : 1110
      }
    },
    "numInputRows" : 30,
    "inputRowsPerSecond" : 256.4102564102564,
    "processedRowsPerSecond" : 15.353121801432959,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 31
  }
}
26/01/04 17:31:45 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/192 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.192.20cfede5-e462-49ba-8ef8-ff64081f0cc4.tmp
26/01/04 17:31:46 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.192.20cfede5-e462-49ba-8ef8-ff64081f0cc4.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/192
26/01/04 17:31:46 INFO MicroBatchExecution: Committed offsets for batch 192. Metadata OffsetSeqMetadata(0,1767547905852,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 17:31:46 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:31:46 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:31:46 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:31:46 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:31:46 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:31:46 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:31:46 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#155851 - airline_prefix.nullCount#155850) > 0)
26/01/04 17:31:46 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#155886 - min_flight_num.nullCount#155885) > 0)
26/01/04 17:31:46 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#155881 - max_flight_num.nullCount#155880) > 0)
26/01/04 17:31:46 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:31:46 INFO DAGScheduler: Got job 387 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 17:31:46 INFO DAGScheduler: Final stage: ResultStage 388 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:31:46 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:31:46 INFO DAGScheduler: Missing parents: List()
26/01/04 17:31:46 INFO DAGScheduler: Submitting ResultStage 388 (MapPartitionsRDD[1943] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:31:46 INFO MemoryStore: Block broadcast_579 stored as values in memory (estimated size 38.5 KiB, free 434.2 MiB)
26/01/04 17:31:46 INFO MemoryStore: Block broadcast_579_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.2 MiB)
26/01/04 17:31:46 INFO BlockManagerInfo: Added broadcast_579_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:31:46 INFO SparkContext: Created broadcast 579 from broadcast at DAGScheduler.scala:1585
26/01/04 17:31:46 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 388 (MapPartitionsRDD[1943] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 17:31:46 INFO TaskSchedulerImpl: Adding task set 388.0 with 2 tasks resource profile 0
26/01/04 17:31:46 INFO TaskSetManager: Starting task 1.0 in stage 388.0 (TID 617) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 17:31:46 INFO TaskSetManager: Starting task 0.0 in stage 388.0 (TID 618) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 17:31:46 INFO BlockManagerInfo: Added broadcast_579_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:31:46 INFO BlockManagerInfo: Added broadcast_579_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:31:46 INFO TaskSetManager: Finished task 0.0 in stage 388.0 (TID 618) in 129 ms on 172.18.0.14 (executor 1) (1/2)
26/01/04 17:31:46 INFO TaskSetManager: Finished task 1.0 in stage 388.0 (TID 617) in 170 ms on 172.18.0.13 (executor 0) (2/2)
26/01/04 17:31:46 INFO TaskSchedulerImpl: Removed TaskSet 388.0, whose tasks have all completed, from pool 
26/01/04 17:31:46 INFO DAGScheduler: ResultStage 388 (start at NativeMethodAccessorImpl.java:0) finished in 0.197 s
26/01/04 17:31:46 INFO DAGScheduler: Job 387 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:31:46 INFO TaskSchedulerImpl: Killing all running tasks in stage 388: Stage finished
26/01/04 17:31:46 INFO DAGScheduler: Job 387 finished: start at NativeMethodAccessorImpl.java:0, took 0.204468 s
26/01/04 17:31:46 INFO MemoryStore: Block broadcast_580_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.2 MiB)
26/01/04 17:31:46 INFO BlockManagerInfo: Added broadcast_580_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:31:46 INFO SparkContext: Created broadcast 580 from start at NativeMethodAccessorImpl.java:0
26/01/04 17:31:46 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 192, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@5f98bf6d]. The input RDD has 1 partitions.
26/01/04 17:31:46 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:31:46 INFO DAGScheduler: Got job 388 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/01/04 17:31:46 INFO DAGScheduler: Final stage: ResultStage 389 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:31:46 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:31:46 INFO DAGScheduler: Missing parents: List()
26/01/04 17:31:46 INFO DAGScheduler: Submitting ResultStage 389 (MapPartitionsRDD[1948] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:31:46 INFO MemoryStore: Block broadcast_581 stored as values in memory (estimated size 52.9 KiB, free 434.2 MiB)
26/01/04 17:31:46 INFO MemoryStore: Block broadcast_581_piece0 stored as bytes in memory (estimated size 20.2 KiB, free 434.1 MiB)
26/01/04 17:31:46 INFO BlockManagerInfo: Added broadcast_581_piece0 in memory on spark-master:33535 (size: 20.2 KiB, free: 434.3 MiB)
26/01/04 17:31:46 INFO SparkContext: Created broadcast 581 from broadcast at DAGScheduler.scala:1585
26/01/04 17:31:46 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 389 (MapPartitionsRDD[1948] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/01/04 17:31:46 INFO TaskSchedulerImpl: Adding task set 389.0 with 1 tasks resource profile 0
26/01/04 17:31:46 INFO TaskSetManager: Starting task 0.0 in stage 389.0 (TID 619) (172.18.0.13, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:31:46 INFO BlockManagerInfo: Added broadcast_581_piece0 in memory on 172.18.0.13:44401 (size: 20.2 KiB, free: 434.3 MiB)
26/01/04 17:31:46 INFO BlockManagerInfo: Added broadcast_580_piece0 in memory on 172.18.0.13:44401 (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:31:47 INFO TaskSetManager: Finished task 0.0 in stage 389.0 (TID 619) in 1052 ms on 172.18.0.13 (executor 0) (1/1)
26/01/04 17:31:47 INFO TaskSchedulerImpl: Removed TaskSet 389.0, whose tasks have all completed, from pool 
26/01/04 17:31:47 INFO DAGScheduler: ResultStage 389 (start at NativeMethodAccessorImpl.java:0) finished in 1.120 s
26/01/04 17:31:47 INFO DAGScheduler: Job 388 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:31:47 INFO TaskSchedulerImpl: Killing all running tasks in stage 389: Stage finished
26/01/04 17:31:47 INFO DAGScheduler: Job 388 finished: start at NativeMethodAccessorImpl.java:0, took 1.124923 s
26/01/04 17:31:47 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 192, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@5f98bf6d] is committing.
26/01/04 17:31:47 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 192, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@5f98bf6d] committed.
26/01/04 17:31:47 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/192 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.192.1e9c5fe3-2591-478d-aa20-617ef71d7a89.tmp
26/01/04 17:31:47 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.192.1e9c5fe3-2591-478d-aa20-617ef71d7a89.tmp to file:/tmp/spark-checkpoint-enrichment/commits/192
26/01/04 17:31:47 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T17:31:45.846Z",
  "batchId" : 192,
  "numInputRows" : 30,
  "inputRowsPerSecond" : 750.0,
  "processedRowsPerSecond" : 14.641288433382138,
  "durationMs" : {
    "addBatch" : 1541,
    "commitOffsets" : 249,
    "getBatch" : 0,
    "latestOffset" : 6,
    "queryPlanning" : 42,
    "triggerExecution" : 2049,
    "walCommit" : 208
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "0" : 1110
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "0" : 1140
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "0" : 1140
      }
    },
    "numInputRows" : 30,
    "inputRowsPerSecond" : 750.0,
    "processedRowsPerSecond" : 14.641288433382138,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 31
  }
}
26/01/04 17:31:56 INFO BlockManagerInfo: Removed broadcast_579_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:31:56 INFO BlockManagerInfo: Removed broadcast_579_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:31:56 INFO BlockManagerInfo: Removed broadcast_579_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:31:56 INFO BlockManagerInfo: Removed broadcast_577_piece0 on 172.18.0.13:44401 in memory (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:31:56 INFO BlockManagerInfo: Removed broadcast_577_piece0 on spark-master:33535 in memory (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:31:56 INFO BlockManagerInfo: Removed broadcast_576_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:31:56 INFO BlockManagerInfo: Removed broadcast_576_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:31:56 INFO BlockManagerInfo: Removed broadcast_576_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:31:56 INFO BlockManagerInfo: Removed broadcast_578_piece0 on 172.18.0.13:44401 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:31:56 INFO BlockManagerInfo: Removed broadcast_578_piece0 on spark-master:33535 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:31:56 INFO BlockManagerInfo: Removed broadcast_581_piece0 on spark-master:33535 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:31:56 INFO BlockManagerInfo: Removed broadcast_581_piece0 on 172.18.0.13:44401 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:31:56 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/193 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.193.5d837007-96c1-436d-834f-17936c1609f0.tmp
26/01/04 17:31:57 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.193.5d837007-96c1-436d-834f-17936c1609f0.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/193
26/01/04 17:31:57 INFO MicroBatchExecution: Committed offsets for batch 193. Metadata OffsetSeqMetadata(0,1767547916875,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 17:31:57 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:31:57 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:31:57 INFO BlockManagerInfo: Removed broadcast_580_piece0 on spark-master:33535 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:31:57 INFO BlockManagerInfo: Removed broadcast_580_piece0 on 172.18.0.13:44401 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:31:57 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:31:57 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:31:57 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:31:57 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:31:57 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#156655 - airline_prefix.nullCount#156654) > 0)
26/01/04 17:31:57 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#156690 - min_flight_num.nullCount#156689) > 0)
26/01/04 17:31:57 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#156685 - max_flight_num.nullCount#156684) > 0)
26/01/04 17:31:57 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:31:57 INFO DAGScheduler: Got job 389 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 17:31:57 INFO DAGScheduler: Final stage: ResultStage 390 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:31:57 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:31:57 INFO DAGScheduler: Missing parents: List()
26/01/04 17:31:57 INFO DAGScheduler: Submitting ResultStage 390 (MapPartitionsRDD[1953] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:31:57 INFO MemoryStore: Block broadcast_582 stored as values in memory (estimated size 38.5 KiB, free 434.4 MiB)
26/01/04 17:31:57 INFO MemoryStore: Block broadcast_582_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.3 MiB)
26/01/04 17:31:57 INFO BlockManagerInfo: Added broadcast_582_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:31:57 INFO SparkContext: Created broadcast 582 from broadcast at DAGScheduler.scala:1585
26/01/04 17:31:57 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 390 (MapPartitionsRDD[1953] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 17:31:57 INFO TaskSchedulerImpl: Adding task set 390.0 with 2 tasks resource profile 0
26/01/04 17:31:57 INFO TaskSetManager: Starting task 1.0 in stage 390.0 (TID 620) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 17:31:57 INFO TaskSetManager: Starting task 0.0 in stage 390.0 (TID 621) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 17:31:57 INFO BlockManagerInfo: Added broadcast_582_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:31:57 INFO BlockManagerInfo: Added broadcast_582_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:31:57 INFO TaskSetManager: Finished task 1.0 in stage 390.0 (TID 620) in 39 ms on 172.18.0.13 (executor 0) (1/2)
26/01/04 17:31:57 INFO TaskSetManager: Finished task 0.0 in stage 390.0 (TID 621) in 68 ms on 172.18.0.14 (executor 1) (2/2)
26/01/04 17:31:57 INFO TaskSchedulerImpl: Removed TaskSet 390.0, whose tasks have all completed, from pool 
26/01/04 17:31:57 INFO DAGScheduler: ResultStage 390 (start at NativeMethodAccessorImpl.java:0) finished in 0.084 s
26/01/04 17:31:57 INFO DAGScheduler: Job 389 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:31:57 INFO TaskSchedulerImpl: Killing all running tasks in stage 390: Stage finished
26/01/04 17:31:57 INFO DAGScheduler: Job 389 finished: start at NativeMethodAccessorImpl.java:0, took 0.089776 s
26/01/04 17:31:57 INFO MemoryStore: Block broadcast_583_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.3 MiB)
26/01/04 17:31:57 INFO BlockManagerInfo: Added broadcast_583_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:31:57 INFO SparkContext: Created broadcast 583 from start at NativeMethodAccessorImpl.java:0
26/01/04 17:31:57 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 193, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@1132ef4d]. The input RDD has 1 partitions.
26/01/04 17:31:57 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:31:57 INFO DAGScheduler: Got job 390 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/01/04 17:31:57 INFO DAGScheduler: Final stage: ResultStage 391 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:31:57 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:31:57 INFO DAGScheduler: Missing parents: List()
26/01/04 17:31:57 INFO DAGScheduler: Submitting ResultStage 391 (MapPartitionsRDD[1958] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:31:57 INFO MemoryStore: Block broadcast_584 stored as values in memory (estimated size 52.9 KiB, free 434.3 MiB)
26/01/04 17:31:57 INFO MemoryStore: Block broadcast_584_piece0 stored as bytes in memory (estimated size 20.2 KiB, free 434.3 MiB)
26/01/04 17:31:57 INFO BlockManagerInfo: Added broadcast_584_piece0 in memory on spark-master:33535 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:31:57 INFO SparkContext: Created broadcast 584 from broadcast at DAGScheduler.scala:1585
26/01/04 17:31:57 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 391 (MapPartitionsRDD[1958] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/01/04 17:31:57 INFO TaskSchedulerImpl: Adding task set 391.0 with 1 tasks resource profile 0
26/01/04 17:31:57 INFO TaskSetManager: Starting task 0.0 in stage 391.0 (TID 622) (172.18.0.13, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:31:57 INFO BlockManagerInfo: Added broadcast_584_piece0 in memory on 172.18.0.13:44401 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:31:57 INFO BlockManagerInfo: Added broadcast_583_piece0 in memory on 172.18.0.13:44401 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:31:58 INFO TaskSetManager: Finished task 0.0 in stage 391.0 (TID 622) in 701 ms on 172.18.0.13 (executor 0) (1/1)
26/01/04 17:31:58 INFO TaskSchedulerImpl: Removed TaskSet 391.0, whose tasks have all completed, from pool 
26/01/04 17:31:58 INFO DAGScheduler: ResultStage 391 (start at NativeMethodAccessorImpl.java:0) finished in 0.717 s
26/01/04 17:31:58 INFO DAGScheduler: Job 390 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:31:58 INFO TaskSchedulerImpl: Killing all running tasks in stage 391: Stage finished
26/01/04 17:31:58 INFO DAGScheduler: Job 390 finished: start at NativeMethodAccessorImpl.java:0, took 0.720577 s
26/01/04 17:31:58 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 193, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@1132ef4d] is committing.
26/01/04 17:31:58 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 193, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@1132ef4d] committed.
26/01/04 17:31:58 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/193 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.193.384f5cb7-e1ab-49f3-bc32-bccca69879c0.tmp
26/01/04 17:31:58 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.193.384f5cb7-e1ab-49f3-bc32-bccca69879c0.tmp to file:/tmp/spark-checkpoint-enrichment/commits/193
26/01/04 17:31:58 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T17:31:56.859Z",
  "batchId" : 193,
  "numInputRows" : 30,
  "inputRowsPerSecond" : 2000.0,
  "processedRowsPerSecond" : 18.170805572380374,
  "durationMs" : {
    "addBatch" : 1122,
    "commitOffsets" : 132,
    "getBatch" : 0,
    "latestOffset" : 16,
    "queryPlanning" : 122,
    "triggerExecution" : 1651,
    "walCommit" : 257
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "0" : 1140
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "0" : 1170
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "0" : 1170
      }
    },
    "numInputRows" : 30,
    "inputRowsPerSecond" : 2000.0,
    "processedRowsPerSecond" : 18.170805572380374,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 31
  }
}
26/01/04 17:32:07 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/194 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.194.e0927968-64e9-4b68-8d16-fcf323d4a81b.tmp
26/01/04 17:32:08 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.194.e0927968-64e9-4b68-8d16-fcf323d4a81b.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/194
26/01/04 17:32:08 INFO MicroBatchExecution: Committed offsets for batch 194. Metadata OffsetSeqMetadata(0,1767547927902,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 17:32:08 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:32:08 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:32:08 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:32:08 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:32:08 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:32:08 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:32:08 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#157459 - airline_prefix.nullCount#157458) > 0)
26/01/04 17:32:08 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#157494 - min_flight_num.nullCount#157493) > 0)
26/01/04 17:32:08 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#157489 - max_flight_num.nullCount#157488) > 0)
26/01/04 17:32:08 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:32:08 INFO DAGScheduler: Got job 391 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 17:32:08 INFO DAGScheduler: Final stage: ResultStage 392 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:32:08 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:32:08 INFO DAGScheduler: Missing parents: List()
26/01/04 17:32:08 INFO DAGScheduler: Submitting ResultStage 392 (MapPartitionsRDD[1963] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:32:08 INFO MemoryStore: Block broadcast_585 stored as values in memory (estimated size 38.5 KiB, free 434.2 MiB)
26/01/04 17:32:08 INFO MemoryStore: Block broadcast_585_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.2 MiB)
26/01/04 17:32:08 INFO BlockManagerInfo: Added broadcast_585_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:32:08 INFO SparkContext: Created broadcast 585 from broadcast at DAGScheduler.scala:1585
26/01/04 17:32:08 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 392 (MapPartitionsRDD[1963] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 17:32:08 INFO TaskSchedulerImpl: Adding task set 392.0 with 2 tasks resource profile 0
26/01/04 17:32:08 INFO TaskSetManager: Starting task 1.0 in stage 392.0 (TID 623) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 17:32:08 INFO TaskSetManager: Starting task 0.0 in stage 392.0 (TID 624) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 17:32:08 INFO BlockManagerInfo: Added broadcast_585_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:32:08 INFO BlockManagerInfo: Added broadcast_585_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:32:08 INFO TaskSetManager: Finished task 1.0 in stage 392.0 (TID 623) in 75 ms on 172.18.0.13 (executor 0) (1/2)
26/01/04 17:32:08 INFO TaskSetManager: Finished task 0.0 in stage 392.0 (TID 624) in 126 ms on 172.18.0.14 (executor 1) (2/2)
26/01/04 17:32:08 INFO TaskSchedulerImpl: Removed TaskSet 392.0, whose tasks have all completed, from pool 
26/01/04 17:32:08 INFO DAGScheduler: ResultStage 392 (start at NativeMethodAccessorImpl.java:0) finished in 0.150 s
26/01/04 17:32:08 INFO DAGScheduler: Job 391 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:32:08 INFO TaskSchedulerImpl: Killing all running tasks in stage 392: Stage finished
26/01/04 17:32:08 INFO DAGScheduler: Job 391 finished: start at NativeMethodAccessorImpl.java:0, took 0.178903 s
26/01/04 17:32:08 INFO MemoryStore: Block broadcast_586_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.2 MiB)
26/01/04 17:32:08 INFO BlockManagerInfo: Added broadcast_586_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:32:08 INFO SparkContext: Created broadcast 586 from start at NativeMethodAccessorImpl.java:0
26/01/04 17:32:08 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 194, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@5c89361c]. The input RDD has 1 partitions.
26/01/04 17:32:08 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:32:08 INFO DAGScheduler: Got job 392 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/01/04 17:32:08 INFO DAGScheduler: Final stage: ResultStage 393 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:32:08 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:32:08 INFO DAGScheduler: Missing parents: List()
26/01/04 17:32:08 INFO DAGScheduler: Submitting ResultStage 393 (MapPartitionsRDD[1968] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:32:08 INFO MemoryStore: Block broadcast_587 stored as values in memory (estimated size 52.9 KiB, free 434.2 MiB)
26/01/04 17:32:08 INFO MemoryStore: Block broadcast_587_piece0 stored as bytes in memory (estimated size 20.1 KiB, free 434.1 MiB)
26/01/04 17:32:08 INFO BlockManagerInfo: Added broadcast_587_piece0 in memory on spark-master:33535 (size: 20.1 KiB, free: 434.3 MiB)
26/01/04 17:32:08 INFO BlockManagerInfo: Removed broadcast_584_piece0 on spark-master:33535 in memory (size: 20.2 KiB, free: 434.3 MiB)
26/01/04 17:32:08 INFO BlockManagerInfo: Removed broadcast_584_piece0 on 172.18.0.13:44401 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:32:08 INFO SparkContext: Created broadcast 587 from broadcast at DAGScheduler.scala:1585
26/01/04 17:32:08 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 393 (MapPartitionsRDD[1968] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/01/04 17:32:08 INFO TaskSchedulerImpl: Adding task set 393.0 with 1 tasks resource profile 0
26/01/04 17:32:08 INFO TaskSetManager: Starting task 0.0 in stage 393.0 (TID 625) (172.18.0.13, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:32:08 INFO BlockManagerInfo: Removed broadcast_583_piece0 on spark-master:33535 in memory (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:32:08 INFO BlockManagerInfo: Removed broadcast_583_piece0 on 172.18.0.13:44401 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:32:08 INFO BlockManagerInfo: Removed broadcast_585_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:32:08 INFO BlockManagerInfo: Removed broadcast_585_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:32:08 INFO BlockManagerInfo: Removed broadcast_585_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:32:08 INFO BlockManagerInfo: Removed broadcast_582_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:32:08 INFO BlockManagerInfo: Removed broadcast_582_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:32:08 INFO BlockManagerInfo: Removed broadcast_582_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:32:08 INFO BlockManagerInfo: Added broadcast_587_piece0 in memory on 172.18.0.13:44401 (size: 20.1 KiB, free: 434.4 MiB)
26/01/04 17:32:08 INFO BlockManagerInfo: Added broadcast_586_piece0 in memory on 172.18.0.13:44401 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:32:09 INFO TaskSetManager: Finished task 0.0 in stage 393.0 (TID 625) in 712 ms on 172.18.0.13 (executor 0) (1/1)
26/01/04 17:32:09 INFO TaskSchedulerImpl: Removed TaskSet 393.0, whose tasks have all completed, from pool 
26/01/04 17:32:09 INFO DAGScheduler: ResultStage 393 (start at NativeMethodAccessorImpl.java:0) finished in 0.747 s
26/01/04 17:32:09 INFO DAGScheduler: Job 392 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:32:09 INFO TaskSchedulerImpl: Killing all running tasks in stage 393: Stage finished
26/01/04 17:32:09 INFO DAGScheduler: Job 392 finished: start at NativeMethodAccessorImpl.java:0, took 0.752466 s
26/01/04 17:32:09 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 194, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@5c89361c] is committing.
26/01/04 17:32:09 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 194, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@5c89361c] committed.
26/01/04 17:32:09 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/194 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.194.f87cd448-3344-471e-b5b3-1aa6f214f97c.tmp
26/01/04 17:32:09 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.194.f87cd448-3344-471e-b5b3-1aa6f214f97c.tmp to file:/tmp/spark-checkpoint-enrichment/commits/194
26/01/04 17:32:09 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T17:32:07.890Z",
  "batchId" : 194,
  "numInputRows" : 30,
  "inputRowsPerSecond" : 1764.705882352941,
  "processedRowsPerSecond" : 20.18842530282638,
  "durationMs" : {
    "addBatch" : 1083,
    "commitOffsets" : 159,
    "getBatch" : 0,
    "latestOffset" : 12,
    "queryPlanning" : 95,
    "triggerExecution" : 1486,
    "walCommit" : 136
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "0" : 1170
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "0" : 1200
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "0" : 1200
      }
    },
    "numInputRows" : 30,
    "inputRowsPerSecond" : 1764.705882352941,
    "processedRowsPerSecond" : 20.18842530282638,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 31
  }
}
26/01/04 17:32:18 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/195 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.195.d9685ae1-ec30-402e-b8e6-2b957b46bca8.tmp
26/01/04 17:32:19 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.195.d9685ae1-ec30-402e-b8e6-2b957b46bca8.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/195
26/01/04 17:32:19 INFO MicroBatchExecution: Committed offsets for batch 195. Metadata OffsetSeqMetadata(0,1767547938928,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 17:32:19 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:32:19 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:32:19 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:32:19 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:32:19 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:32:19 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:32:19 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#158263 - airline_prefix.nullCount#158262) > 0)
26/01/04 17:32:19 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#158298 - min_flight_num.nullCount#158297) > 0)
26/01/04 17:32:19 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#158293 - max_flight_num.nullCount#158292) > 0)
26/01/04 17:32:19 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:32:19 INFO DAGScheduler: Got job 393 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 17:32:19 INFO DAGScheduler: Final stage: ResultStage 394 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:32:19 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:32:19 INFO DAGScheduler: Missing parents: List()
26/01/04 17:32:19 INFO DAGScheduler: Submitting ResultStage 394 (MapPartitionsRDD[1973] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:32:19 INFO MemoryStore: Block broadcast_588 stored as values in memory (estimated size 38.5 KiB, free 434.3 MiB)
26/01/04 17:32:19 INFO MemoryStore: Block broadcast_588_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.3 MiB)
26/01/04 17:32:19 INFO BlockManagerInfo: Added broadcast_588_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:32:19 INFO BlockManagerInfo: Removed broadcast_586_piece0 on spark-master:33535 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:32:19 INFO BlockManagerInfo: Removed broadcast_586_piece0 on 172.18.0.13:44401 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:32:19 INFO SparkContext: Created broadcast 588 from broadcast at DAGScheduler.scala:1585
26/01/04 17:32:19 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 394 (MapPartitionsRDD[1973] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 17:32:19 INFO TaskSchedulerImpl: Adding task set 394.0 with 2 tasks resource profile 0
26/01/04 17:32:19 INFO TaskSetManager: Starting task 1.0 in stage 394.0 (TID 626) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 17:32:19 INFO TaskSetManager: Starting task 0.0 in stage 394.0 (TID 627) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 17:32:19 INFO BlockManagerInfo: Removed broadcast_587_piece0 on 172.18.0.13:44401 in memory (size: 20.1 KiB, free: 434.4 MiB)
26/01/04 17:32:19 INFO BlockManagerInfo: Removed broadcast_587_piece0 on spark-master:33535 in memory (size: 20.1 KiB, free: 434.4 MiB)
26/01/04 17:32:19 INFO BlockManagerInfo: Added broadcast_588_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:32:19 INFO TaskSetManager: Finished task 0.0 in stage 394.0 (TID 627) in 78 ms on 172.18.0.14 (executor 1) (1/2)
26/01/04 17:32:19 INFO BlockManagerInfo: Added broadcast_588_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:32:19 INFO TaskSetManager: Finished task 1.0 in stage 394.0 (TID 626) in 144 ms on 172.18.0.13 (executor 0) (2/2)
26/01/04 17:32:19 INFO TaskSchedulerImpl: Removed TaskSet 394.0, whose tasks have all completed, from pool 
26/01/04 17:32:19 INFO DAGScheduler: ResultStage 394 (start at NativeMethodAccessorImpl.java:0) finished in 0.186 s
26/01/04 17:32:19 INFO DAGScheduler: Job 393 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:32:19 INFO TaskSchedulerImpl: Killing all running tasks in stage 394: Stage finished
26/01/04 17:32:19 INFO DAGScheduler: Job 393 finished: start at NativeMethodAccessorImpl.java:0, took 0.188997 s
26/01/04 17:32:19 INFO MemoryStore: Block broadcast_589_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.3 MiB)
26/01/04 17:32:19 INFO BlockManagerInfo: Added broadcast_589_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:32:19 INFO SparkContext: Created broadcast 589 from start at NativeMethodAccessorImpl.java:0
26/01/04 17:32:19 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 195, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@f9a87c9]. The input RDD has 1 partitions.
26/01/04 17:32:19 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:32:19 INFO DAGScheduler: Got job 394 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/01/04 17:32:19 INFO DAGScheduler: Final stage: ResultStage 395 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:32:19 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:32:19 INFO DAGScheduler: Missing parents: List()
26/01/04 17:32:19 INFO DAGScheduler: Submitting ResultStage 395 (MapPartitionsRDD[1978] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:32:19 INFO MemoryStore: Block broadcast_590 stored as values in memory (estimated size 52.9 KiB, free 434.3 MiB)
26/01/04 17:32:19 INFO MemoryStore: Block broadcast_590_piece0 stored as bytes in memory (estimated size 20.2 KiB, free 434.3 MiB)
26/01/04 17:32:19 INFO BlockManagerInfo: Added broadcast_590_piece0 in memory on spark-master:33535 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:32:19 INFO SparkContext: Created broadcast 590 from broadcast at DAGScheduler.scala:1585
26/01/04 17:32:19 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 395 (MapPartitionsRDD[1978] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/01/04 17:32:19 INFO TaskSchedulerImpl: Adding task set 395.0 with 1 tasks resource profile 0
26/01/04 17:32:19 INFO TaskSetManager: Starting task 0.0 in stage 395.0 (TID 628) (172.18.0.13, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:32:19 INFO BlockManagerInfo: Added broadcast_590_piece0 in memory on 172.18.0.13:44401 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:32:19 INFO BlockManagerInfo: Added broadcast_589_piece0 in memory on 172.18.0.13:44401 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:32:20 INFO TaskSetManager: Finished task 0.0 in stage 395.0 (TID 628) in 616 ms on 172.18.0.13 (executor 0) (1/1)
26/01/04 17:32:20 INFO TaskSchedulerImpl: Removed TaskSet 395.0, whose tasks have all completed, from pool 
26/01/04 17:32:20 INFO DAGScheduler: ResultStage 395 (start at NativeMethodAccessorImpl.java:0) finished in 0.625 s
26/01/04 17:32:20 INFO DAGScheduler: Job 394 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:32:20 INFO TaskSchedulerImpl: Killing all running tasks in stage 395: Stage finished
26/01/04 17:32:20 INFO DAGScheduler: Job 394 finished: start at NativeMethodAccessorImpl.java:0, took 0.628619 s
26/01/04 17:32:20 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 195, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@f9a87c9] is committing.
26/01/04 17:32:20 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 195, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@f9a87c9] committed.
26/01/04 17:32:20 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/195 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.195.b2654f80-7d2a-4307-8d78-c9ffa7f64cb6.tmp
26/01/04 17:32:20 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.195.b2654f80-7d2a-4307-8d78-c9ffa7f64cb6.tmp to file:/tmp/spark-checkpoint-enrichment/commits/195
26/01/04 17:32:20 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T17:32:18.924Z",
  "batchId" : 195,
  "numInputRows" : 30,
  "inputRowsPerSecond" : 2000.0,
  "processedRowsPerSecond" : 22.953328232593726,
  "durationMs" : {
    "addBatch" : 980,
    "commitOffsets" : 176,
    "getBatch" : 0,
    "latestOffset" : 4,
    "queryPlanning" : 30,
    "triggerExecution" : 1307,
    "walCommit" : 117
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "0" : 1200
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "0" : 1230
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "0" : 1230
      }
    },
    "numInputRows" : 30,
    "inputRowsPerSecond" : 2000.0,
    "processedRowsPerSecond" : 22.953328232593726,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 31
  }
}
26/01/04 17:32:29 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/196 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.196.02b98827-4175-4354-ac1f-0d8b8a2f4ecb.tmp
26/01/04 17:32:30 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.196.02b98827-4175-4354-ac1f-0d8b8a2f4ecb.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/196
26/01/04 17:32:30 INFO MicroBatchExecution: Committed offsets for batch 196. Metadata OffsetSeqMetadata(0,1767547949956,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 17:32:30 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:32:30 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:32:30 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:32:30 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:32:30 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:32:30 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:32:30 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#159067 - airline_prefix.nullCount#159066) > 0)
26/01/04 17:32:30 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#159102 - min_flight_num.nullCount#159101) > 0)
26/01/04 17:32:30 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#159097 - max_flight_num.nullCount#159096) > 0)
26/01/04 17:32:30 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:32:30 INFO DAGScheduler: Got job 395 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 17:32:30 INFO DAGScheduler: Final stage: ResultStage 396 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:32:30 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:32:30 INFO DAGScheduler: Missing parents: List()
26/01/04 17:32:30 INFO DAGScheduler: Submitting ResultStage 396 (MapPartitionsRDD[1983] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:32:30 INFO MemoryStore: Block broadcast_591 stored as values in memory (estimated size 38.5 KiB, free 434.2 MiB)
26/01/04 17:32:30 INFO MemoryStore: Block broadcast_591_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.2 MiB)
26/01/04 17:32:30 INFO BlockManagerInfo: Added broadcast_591_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:32:30 INFO SparkContext: Created broadcast 591 from broadcast at DAGScheduler.scala:1585
26/01/04 17:32:30 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 396 (MapPartitionsRDD[1983] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 17:32:30 INFO TaskSchedulerImpl: Adding task set 396.0 with 2 tasks resource profile 0
26/01/04 17:32:30 INFO TaskSetManager: Starting task 0.0 in stage 396.0 (TID 629) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 17:32:30 INFO TaskSetManager: Starting task 1.0 in stage 396.0 (TID 630) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 17:32:30 INFO BlockManagerInfo: Added broadcast_591_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:32:30 INFO BlockManagerInfo: Added broadcast_591_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:32:30 INFO TaskSetManager: Finished task 1.0 in stage 396.0 (TID 630) in 44 ms on 172.18.0.13 (executor 0) (1/2)
26/01/04 17:32:30 INFO TaskSetManager: Finished task 0.0 in stage 396.0 (TID 629) in 69 ms on 172.18.0.14 (executor 1) (2/2)
26/01/04 17:32:30 INFO TaskSchedulerImpl: Removed TaskSet 396.0, whose tasks have all completed, from pool 
26/01/04 17:32:30 INFO DAGScheduler: ResultStage 396 (start at NativeMethodAccessorImpl.java:0) finished in 0.079 s
26/01/04 17:32:30 INFO DAGScheduler: Job 395 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:32:30 INFO TaskSchedulerImpl: Killing all running tasks in stage 396: Stage finished
26/01/04 17:32:30 INFO DAGScheduler: Job 395 finished: start at NativeMethodAccessorImpl.java:0, took 0.081409 s
26/01/04 17:32:30 INFO MemoryStore: Block broadcast_592_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.2 MiB)
26/01/04 17:32:30 INFO BlockManagerInfo: Added broadcast_592_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:32:30 INFO SparkContext: Created broadcast 592 from start at NativeMethodAccessorImpl.java:0
26/01/04 17:32:30 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 196, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@6143215c]. The input RDD has 1 partitions.
26/01/04 17:32:30 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:32:30 INFO DAGScheduler: Got job 396 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/01/04 17:32:30 INFO DAGScheduler: Final stage: ResultStage 397 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:32:30 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:32:30 INFO DAGScheduler: Missing parents: List()
26/01/04 17:32:30 INFO DAGScheduler: Submitting ResultStage 397 (MapPartitionsRDD[1988] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:32:30 INFO MemoryStore: Block broadcast_593 stored as values in memory (estimated size 52.9 KiB, free 434.2 MiB)
26/01/04 17:32:30 INFO MemoryStore: Block broadcast_593_piece0 stored as bytes in memory (estimated size 20.2 KiB, free 434.1 MiB)
26/01/04 17:32:30 INFO BlockManagerInfo: Added broadcast_593_piece0 in memory on spark-master:33535 (size: 20.2 KiB, free: 434.3 MiB)
26/01/04 17:32:30 INFO SparkContext: Created broadcast 593 from broadcast at DAGScheduler.scala:1585
26/01/04 17:32:30 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 397 (MapPartitionsRDD[1988] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/01/04 17:32:30 INFO TaskSchedulerImpl: Adding task set 397.0 with 1 tasks resource profile 0
26/01/04 17:32:30 INFO TaskSetManager: Starting task 0.0 in stage 397.0 (TID 631) (172.18.0.13, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:32:30 INFO BlockManagerInfo: Added broadcast_593_piece0 in memory on 172.18.0.13:44401 (size: 20.2 KiB, free: 434.3 MiB)
26/01/04 17:32:30 INFO BlockManagerInfo: Added broadcast_592_piece0 in memory on 172.18.0.13:44401 (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:32:30 INFO TaskSetManager: Finished task 0.0 in stage 397.0 (TID 631) in 587 ms on 172.18.0.13 (executor 0) (1/1)
26/01/04 17:32:30 INFO TaskSchedulerImpl: Removed TaskSet 397.0, whose tasks have all completed, from pool 
26/01/04 17:32:30 INFO DAGScheduler: ResultStage 397 (start at NativeMethodAccessorImpl.java:0) finished in 0.593 s
26/01/04 17:32:30 INFO DAGScheduler: Job 396 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:32:30 INFO TaskSchedulerImpl: Killing all running tasks in stage 397: Stage finished
26/01/04 17:32:30 INFO DAGScheduler: Job 396 finished: start at NativeMethodAccessorImpl.java:0, took 0.595336 s
26/01/04 17:32:30 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 196, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@6143215c] is committing.
26/01/04 17:32:30 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 196, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@6143215c] committed.
26/01/04 17:32:30 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/196 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.196.8a1d1f21-0f62-49aa-8c58-d352aa9966a4.tmp
26/01/04 17:32:30 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.196.8a1d1f21-0f62-49aa-8c58-d352aa9966a4.tmp to file:/tmp/spark-checkpoint-enrichment/commits/196
26/01/04 17:32:30 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T17:32:29.951Z",
  "batchId" : 196,
  "numInputRows" : 30,
  "inputRowsPerSecond" : 2000.0,
  "processedRowsPerSecond" : 30.03003003003003,
  "durationMs" : {
    "addBatch" : 764,
    "commitOffsets" : 91,
    "getBatch" : 0,
    "latestOffset" : 5,
    "queryPlanning" : 35,
    "triggerExecution" : 999,
    "walCommit" : 104
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "0" : 1230
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "0" : 1260
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "0" : 1260
      }
    },
    "numInputRows" : 30,
    "inputRowsPerSecond" : 2000.0,
    "processedRowsPerSecond" : 30.03003003003003,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 31
  }
}
26/01/04 17:32:33 INFO BlockManagerInfo: Removed broadcast_590_piece0 on 172.18.0.13:44401 in memory (size: 20.2 KiB, free: 434.3 MiB)
26/01/04 17:32:33 INFO BlockManagerInfo: Removed broadcast_590_piece0 on spark-master:33535 in memory (size: 20.2 KiB, free: 434.3 MiB)
26/01/04 17:32:33 INFO BlockManagerInfo: Removed broadcast_588_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:32:33 INFO BlockManagerInfo: Removed broadcast_588_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:32:33 INFO BlockManagerInfo: Removed broadcast_588_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:32:33 INFO BlockManagerInfo: Removed broadcast_593_piece0 on 172.18.0.13:44401 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:32:33 INFO BlockManagerInfo: Removed broadcast_593_piece0 on spark-master:33535 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:32:33 INFO BlockManagerInfo: Removed broadcast_589_piece0 on spark-master:33535 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:32:33 INFO BlockManagerInfo: Removed broadcast_589_piece0 on 172.18.0.13:44401 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:32:33 INFO BlockManagerInfo: Removed broadcast_591_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:32:33 INFO BlockManagerInfo: Removed broadcast_591_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:32:33 INFO BlockManagerInfo: Removed broadcast_591_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:32:40 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/01/04 17:32:41 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/197 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.197.7fd44fdd-eac0-432e-b4ae-ee5e0cb7c865.tmp
26/01/04 17:32:41 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.197.7fd44fdd-eac0-432e-b4ae-ee5e0cb7c865.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/197
26/01/04 17:32:41 INFO MicroBatchExecution: Committed offsets for batch 197. Metadata OffsetSeqMetadata(0,1767547960981,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 17:32:41 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:32:41 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:32:41 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:32:41 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:32:41 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:32:41 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:32:41 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#159871 - airline_prefix.nullCount#159870) > 0)
26/01/04 17:32:41 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#159906 - min_flight_num.nullCount#159905) > 0)
26/01/04 17:32:41 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#159901 - max_flight_num.nullCount#159900) > 0)
26/01/04 17:32:41 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:32:41 INFO DAGScheduler: Got job 397 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 17:32:41 INFO DAGScheduler: Final stage: ResultStage 398 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:32:41 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:32:41 INFO DAGScheduler: Missing parents: List()
26/01/04 17:32:41 INFO DAGScheduler: Submitting ResultStage 398 (MapPartitionsRDD[1993] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:32:41 INFO MemoryStore: Block broadcast_594 stored as values in memory (estimated size 38.5 KiB, free 434.4 MiB)
26/01/04 17:32:41 INFO MemoryStore: Block broadcast_594_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.3 MiB)
26/01/04 17:32:41 INFO BlockManagerInfo: Added broadcast_594_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:32:41 INFO SparkContext: Created broadcast 594 from broadcast at DAGScheduler.scala:1585
26/01/04 17:32:41 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 398 (MapPartitionsRDD[1993] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 17:32:41 INFO TaskSchedulerImpl: Adding task set 398.0 with 2 tasks resource profile 0
26/01/04 17:32:41 INFO TaskSetManager: Starting task 0.0 in stage 398.0 (TID 632) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 17:32:41 INFO TaskSetManager: Starting task 1.0 in stage 398.0 (TID 633) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 17:32:41 INFO BlockManagerInfo: Added broadcast_594_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:32:41 INFO BlockManagerInfo: Added broadcast_594_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:32:41 INFO TaskSetManager: Finished task 1.0 in stage 398.0 (TID 633) in 36 ms on 172.18.0.13 (executor 0) (1/2)
26/01/04 17:32:41 INFO TaskSetManager: Finished task 0.0 in stage 398.0 (TID 632) in 86 ms on 172.18.0.14 (executor 1) (2/2)
26/01/04 17:32:41 INFO TaskSchedulerImpl: Removed TaskSet 398.0, whose tasks have all completed, from pool 
26/01/04 17:32:41 INFO DAGScheduler: ResultStage 398 (start at NativeMethodAccessorImpl.java:0) finished in 0.128 s
26/01/04 17:32:41 INFO DAGScheduler: Job 397 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:32:41 INFO TaskSchedulerImpl: Killing all running tasks in stage 398: Stage finished
26/01/04 17:32:41 INFO DAGScheduler: Job 397 finished: start at NativeMethodAccessorImpl.java:0, took 0.130761 s
26/01/04 17:32:41 INFO MemoryStore: Block broadcast_595_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.3 MiB)
26/01/04 17:32:41 INFO BlockManagerInfo: Added broadcast_595_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:32:41 INFO SparkContext: Created broadcast 595 from start at NativeMethodAccessorImpl.java:0
26/01/04 17:32:41 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 197, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@9d07fca]. The input RDD has 1 partitions.
26/01/04 17:32:41 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:32:41 INFO DAGScheduler: Got job 398 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/01/04 17:32:41 INFO DAGScheduler: Final stage: ResultStage 399 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:32:41 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:32:41 INFO DAGScheduler: Missing parents: List()
26/01/04 17:32:41 INFO DAGScheduler: Submitting ResultStage 399 (MapPartitionsRDD[1998] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:32:41 INFO MemoryStore: Block broadcast_596 stored as values in memory (estimated size 52.9 KiB, free 434.3 MiB)
26/01/04 17:32:41 INFO MemoryStore: Block broadcast_596_piece0 stored as bytes in memory (estimated size 20.2 KiB, free 434.3 MiB)
26/01/04 17:32:41 INFO BlockManagerInfo: Added broadcast_596_piece0 in memory on spark-master:33535 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:32:41 INFO SparkContext: Created broadcast 596 from broadcast at DAGScheduler.scala:1585
26/01/04 17:32:41 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 399 (MapPartitionsRDD[1998] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/01/04 17:32:41 INFO TaskSchedulerImpl: Adding task set 399.0 with 1 tasks resource profile 0
26/01/04 17:32:41 INFO TaskSetManager: Starting task 0.0 in stage 399.0 (TID 634) (172.18.0.13, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:32:41 INFO BlockManagerInfo: Added broadcast_596_piece0 in memory on 172.18.0.13:44401 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:32:41 INFO BlockManagerInfo: Added broadcast_595_piece0 in memory on 172.18.0.13:44401 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:32:41 INFO TaskSetManager: Finished task 0.0 in stage 399.0 (TID 634) in 558 ms on 172.18.0.13 (executor 0) (1/1)
26/01/04 17:32:41 INFO TaskSchedulerImpl: Removed TaskSet 399.0, whose tasks have all completed, from pool 
26/01/04 17:32:41 INFO DAGScheduler: ResultStage 399 (start at NativeMethodAccessorImpl.java:0) finished in 0.573 s
26/01/04 17:32:41 INFO DAGScheduler: Job 398 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:32:41 INFO TaskSchedulerImpl: Killing all running tasks in stage 399: Stage finished
26/01/04 17:32:41 INFO DAGScheduler: Job 398 finished: start at NativeMethodAccessorImpl.java:0, took 0.574650 s
26/01/04 17:32:41 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 197, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@9d07fca] is committing.
26/01/04 17:32:41 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 197, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@9d07fca] committed.
26/01/04 17:32:41 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/197 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.197.ae808561-cc63-492c-8576-a615ee4ee090.tmp
26/01/04 17:32:42 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.197.ae808561-cc63-492c-8576-a615ee4ee090.tmp to file:/tmp/spark-checkpoint-enrichment/commits/197
26/01/04 17:32:42 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T17:32:40.977Z",
  "batchId" : 197,
  "numInputRows" : 30,
  "inputRowsPerSecond" : 1428.5714285714284,
  "processedRowsPerSecond" : 28.957528957528957,
  "durationMs" : {
    "addBatch" : 795,
    "commitOffsets" : 94,
    "getBatch" : 0,
    "latestOffset" : 4,
    "queryPlanning" : 31,
    "triggerExecution" : 1036,
    "walCommit" : 111
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "0" : 1260
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "0" : 1290
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "0" : 1290
      }
    },
    "numInputRows" : 30,
    "inputRowsPerSecond" : 1428.5714285714284,
    "processedRowsPerSecond" : 28.957528957528957,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 31
  }
}
26/01/04 17:32:51 INFO BlockManagerInfo: Removed broadcast_592_piece0 on spark-master:33535 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:32:51 INFO BlockManagerInfo: Removed broadcast_592_piece0 on 172.18.0.13:44401 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:32:51 INFO BlockManagerInfo: Removed broadcast_596_piece0 on spark-master:33535 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:32:51 INFO BlockManagerInfo: Removed broadcast_596_piece0 on 172.18.0.13:44401 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:32:51 INFO BlockManagerInfo: Removed broadcast_594_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:32:51 INFO BlockManagerInfo: Removed broadcast_594_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:32:51 INFO BlockManagerInfo: Removed broadcast_594_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:32:52 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/198 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.198.9fd3674f-b924-48ef-b519-5238c3a639f1.tmp
26/01/04 17:32:52 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.198.9fd3674f-b924-48ef-b519-5238c3a639f1.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/198
26/01/04 17:32:52 INFO MicroBatchExecution: Committed offsets for batch 198. Metadata OffsetSeqMetadata(0,1767547971986,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 17:32:52 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:32:52 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:32:52 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:32:52 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:32:52 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:32:52 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:32:52 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#160675 - airline_prefix.nullCount#160674) > 0)
26/01/04 17:32:52 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#160710 - min_flight_num.nullCount#160709) > 0)
26/01/04 17:32:52 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#160705 - max_flight_num.nullCount#160704) > 0)
26/01/04 17:32:52 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:32:52 INFO DAGScheduler: Got job 399 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 17:32:52 INFO DAGScheduler: Final stage: ResultStage 400 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:32:52 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:32:52 INFO DAGScheduler: Missing parents: List()
26/01/04 17:32:52 INFO DAGScheduler: Submitting ResultStage 400 (MapPartitionsRDD[2003] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:32:52 INFO MemoryStore: Block broadcast_597 stored as values in memory (estimated size 38.5 KiB, free 434.4 MiB)
26/01/04 17:32:52 INFO MemoryStore: Block broadcast_597_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.3 MiB)
26/01/04 17:32:52 INFO BlockManagerInfo: Removed broadcast_595_piece0 on spark-master:33535 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:32:52 INFO BlockManagerInfo: Added broadcast_597_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:32:52 INFO SparkContext: Created broadcast 597 from broadcast at DAGScheduler.scala:1585
26/01/04 17:32:52 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 400 (MapPartitionsRDD[2003] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 17:32:52 INFO TaskSchedulerImpl: Adding task set 400.0 with 2 tasks resource profile 0
26/01/04 17:32:52 INFO BlockManagerInfo: Removed broadcast_595_piece0 on 172.18.0.13:44401 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:32:52 INFO TaskSetManager: Starting task 0.0 in stage 400.0 (TID 635) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 17:32:52 INFO TaskSetManager: Starting task 1.0 in stage 400.0 (TID 636) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 17:32:52 INFO BlockManagerInfo: Added broadcast_597_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:32:52 INFO TaskSetManager: Finished task 1.0 in stage 400.0 (TID 636) in 37 ms on 172.18.0.13 (executor 0) (1/2)
26/01/04 17:32:52 INFO BlockManagerInfo: Added broadcast_597_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:32:52 INFO TaskSetManager: Finished task 0.0 in stage 400.0 (TID 635) in 129 ms on 172.18.0.14 (executor 1) (2/2)
26/01/04 17:32:52 INFO TaskSchedulerImpl: Removed TaskSet 400.0, whose tasks have all completed, from pool 
26/01/04 17:32:52 INFO DAGScheduler: ResultStage 400 (start at NativeMethodAccessorImpl.java:0) finished in 0.159 s
26/01/04 17:32:52 INFO DAGScheduler: Job 399 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:32:52 INFO TaskSchedulerImpl: Killing all running tasks in stage 400: Stage finished
26/01/04 17:32:52 INFO DAGScheduler: Job 399 finished: start at NativeMethodAccessorImpl.java:0, took 0.162228 s
26/01/04 17:32:52 INFO MemoryStore: Block broadcast_598_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.3 MiB)
26/01/04 17:32:52 INFO BlockManagerInfo: Added broadcast_598_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:32:52 INFO SparkContext: Created broadcast 598 from start at NativeMethodAccessorImpl.java:0
26/01/04 17:32:52 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 198, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@40a455a6]. The input RDD has 1 partitions.
26/01/04 17:32:52 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:32:52 INFO DAGScheduler: Got job 400 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/01/04 17:32:52 INFO DAGScheduler: Final stage: ResultStage 401 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:32:52 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:32:52 INFO DAGScheduler: Missing parents: List()
26/01/04 17:32:52 INFO DAGScheduler: Submitting ResultStage 401 (MapPartitionsRDD[2008] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:32:52 INFO MemoryStore: Block broadcast_599 stored as values in memory (estimated size 52.9 KiB, free 434.3 MiB)
26/01/04 17:32:52 INFO MemoryStore: Block broadcast_599_piece0 stored as bytes in memory (estimated size 20.2 KiB, free 434.3 MiB)
26/01/04 17:32:52 INFO BlockManagerInfo: Added broadcast_599_piece0 in memory on spark-master:33535 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:32:52 INFO SparkContext: Created broadcast 599 from broadcast at DAGScheduler.scala:1585
26/01/04 17:32:52 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 401 (MapPartitionsRDD[2008] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/01/04 17:32:52 INFO TaskSchedulerImpl: Adding task set 401.0 with 1 tasks resource profile 0
26/01/04 17:32:52 INFO TaskSetManager: Starting task 0.0 in stage 401.0 (TID 637) (172.18.0.13, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:32:52 INFO BlockManagerInfo: Removed broadcast_597_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:32:52 INFO BlockManagerInfo: Removed broadcast_597_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:32:52 INFO BlockManagerInfo: Removed broadcast_597_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:32:52 INFO BlockManagerInfo: Added broadcast_599_piece0 in memory on 172.18.0.13:44401 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:32:52 INFO BlockManagerInfo: Added broadcast_598_piece0 in memory on 172.18.0.13:44401 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:32:53 INFO TaskSetManager: Finished task 0.0 in stage 401.0 (TID 637) in 787 ms on 172.18.0.13 (executor 0) (1/1)
26/01/04 17:32:53 INFO TaskSchedulerImpl: Removed TaskSet 401.0, whose tasks have all completed, from pool 
26/01/04 17:32:53 INFO DAGScheduler: ResultStage 401 (start at NativeMethodAccessorImpl.java:0) finished in 0.808 s
26/01/04 17:32:53 INFO DAGScheduler: Job 400 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:32:53 INFO TaskSchedulerImpl: Killing all running tasks in stage 401: Stage finished
26/01/04 17:32:53 INFO DAGScheduler: Job 400 finished: start at NativeMethodAccessorImpl.java:0, took 0.813752 s
26/01/04 17:32:53 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 198, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@40a455a6] is committing.
26/01/04 17:32:53 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 198, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@40a455a6] committed.
26/01/04 17:32:53 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/198 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.198.399f8aff-139b-48bd-9e0b-fc4a4e6a8199.tmp
26/01/04 17:32:53 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.198.399f8aff-139b-48bd-9e0b-fc4a4e6a8199.tmp to file:/tmp/spark-checkpoint-enrichment/commits/198
26/01/04 17:32:53 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T17:32:51.984Z",
  "batchId" : 198,
  "numInputRows" : 30,
  "inputRowsPerSecond" : 2307.6923076923076,
  "processedRowsPerSecond" : 21.291696238466997,
  "durationMs" : {
    "addBatch" : 1105,
    "commitOffsets" : 173,
    "getBatch" : 0,
    "latestOffset" : 2,
    "queryPlanning" : 27,
    "triggerExecution" : 1409,
    "walCommit" : 100
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "0" : 1290
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "0" : 1320
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "0" : 1320
      }
    },
    "numInputRows" : 30,
    "inputRowsPerSecond" : 2307.6923076923076,
    "processedRowsPerSecond" : 21.291696238466997,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 31
  }
}
26/01/04 17:33:03 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/199 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.199.84168b4a-d797-404c-a5dd-9c6ec8458a0c.tmp
26/01/04 17:33:03 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.199.84168b4a-d797-404c-a5dd-9c6ec8458a0c.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/199
26/01/04 17:33:03 INFO MicroBatchExecution: Committed offsets for batch 199. Metadata OffsetSeqMetadata(0,1767547982990,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 17:33:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:33:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:33:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:33:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:33:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:33:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:33:03 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#161479 - airline_prefix.nullCount#161478) > 0)
26/01/04 17:33:03 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#161514 - min_flight_num.nullCount#161513) > 0)
26/01/04 17:33:03 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#161509 - max_flight_num.nullCount#161508) > 0)
26/01/04 17:33:03 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:33:03 INFO DAGScheduler: Got job 401 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 17:33:03 INFO DAGScheduler: Final stage: ResultStage 402 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:33:03 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:33:03 INFO DAGScheduler: Missing parents: List()
26/01/04 17:33:03 INFO DAGScheduler: Submitting ResultStage 402 (MapPartitionsRDD[2013] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:33:03 INFO MemoryStore: Block broadcast_600 stored as values in memory (estimated size 38.5 KiB, free 434.3 MiB)
26/01/04 17:33:03 INFO MemoryStore: Block broadcast_600_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.3 MiB)
26/01/04 17:33:03 INFO BlockManagerInfo: Added broadcast_600_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:33:03 INFO SparkContext: Created broadcast 600 from broadcast at DAGScheduler.scala:1585
26/01/04 17:33:03 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 402 (MapPartitionsRDD[2013] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 17:33:03 INFO TaskSchedulerImpl: Adding task set 402.0 with 2 tasks resource profile 0
26/01/04 17:33:03 INFO TaskSetManager: Starting task 0.0 in stage 402.0 (TID 638) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 17:33:03 INFO BlockManagerInfo: Removed broadcast_599_piece0 on spark-master:33535 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:33:03 INFO TaskSetManager: Starting task 1.0 in stage 402.0 (TID 639) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 17:33:03 INFO BlockManagerInfo: Removed broadcast_599_piece0 on 172.18.0.13:44401 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:33:03 INFO BlockManagerInfo: Removed broadcast_598_piece0 on spark-master:33535 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:33:03 INFO BlockManagerInfo: Removed broadcast_598_piece0 on 172.18.0.13:44401 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:33:03 INFO BlockManagerInfo: Added broadcast_600_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:33:03 INFO BlockManagerInfo: Added broadcast_600_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:33:03 INFO TaskSetManager: Finished task 1.0 in stage 402.0 (TID 639) in 71 ms on 172.18.0.13 (executor 0) (1/2)
26/01/04 17:33:03 INFO TaskSetManager: Finished task 0.0 in stage 402.0 (TID 638) in 143 ms on 172.18.0.14 (executor 1) (2/2)
26/01/04 17:33:03 INFO TaskSchedulerImpl: Removed TaskSet 402.0, whose tasks have all completed, from pool 
26/01/04 17:33:03 INFO DAGScheduler: ResultStage 402 (start at NativeMethodAccessorImpl.java:0) finished in 0.177 s
26/01/04 17:33:03 INFO DAGScheduler: Job 401 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:33:03 INFO TaskSchedulerImpl: Killing all running tasks in stage 402: Stage finished
26/01/04 17:33:03 INFO DAGScheduler: Job 401 finished: start at NativeMethodAccessorImpl.java:0, took 0.181820 s
26/01/04 17:33:03 INFO MemoryStore: Block broadcast_601_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.3 MiB)
26/01/04 17:33:03 INFO BlockManagerInfo: Added broadcast_601_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:33:03 INFO SparkContext: Created broadcast 601 from start at NativeMethodAccessorImpl.java:0
26/01/04 17:33:03 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 199, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@606a15f1]. The input RDD has 1 partitions.
26/01/04 17:33:03 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:33:03 INFO DAGScheduler: Got job 402 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/01/04 17:33:03 INFO DAGScheduler: Final stage: ResultStage 403 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:33:03 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:33:03 INFO DAGScheduler: Missing parents: List()
26/01/04 17:33:03 INFO DAGScheduler: Submitting ResultStage 403 (MapPartitionsRDD[2018] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:33:03 INFO MemoryStore: Block broadcast_602 stored as values in memory (estimated size 52.9 KiB, free 434.3 MiB)
26/01/04 17:33:03 INFO MemoryStore: Block broadcast_602_piece0 stored as bytes in memory (estimated size 20.2 KiB, free 434.3 MiB)
26/01/04 17:33:03 INFO BlockManagerInfo: Added broadcast_602_piece0 in memory on spark-master:33535 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:33:03 INFO SparkContext: Created broadcast 602 from broadcast at DAGScheduler.scala:1585
26/01/04 17:33:03 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 403 (MapPartitionsRDD[2018] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/01/04 17:33:03 INFO TaskSchedulerImpl: Adding task set 403.0 with 1 tasks resource profile 0
26/01/04 17:33:03 INFO TaskSetManager: Starting task 0.0 in stage 403.0 (TID 640) (172.18.0.13, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:33:03 INFO BlockManagerInfo: Added broadcast_602_piece0 in memory on 172.18.0.13:44401 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:33:03 INFO BlockManagerInfo: Added broadcast_601_piece0 in memory on 172.18.0.13:44401 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:33:04 INFO TaskSetManager: Finished task 0.0 in stage 403.0 (TID 640) in 674 ms on 172.18.0.13 (executor 0) (1/1)
26/01/04 17:33:04 INFO TaskSchedulerImpl: Removed TaskSet 403.0, whose tasks have all completed, from pool 
26/01/04 17:33:04 INFO DAGScheduler: ResultStage 403 (start at NativeMethodAccessorImpl.java:0) finished in 0.685 s
26/01/04 17:33:04 INFO DAGScheduler: Job 402 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:33:04 INFO TaskSchedulerImpl: Killing all running tasks in stage 403: Stage finished
26/01/04 17:33:04 INFO DAGScheduler: Job 402 finished: start at NativeMethodAccessorImpl.java:0, took 0.689002 s
26/01/04 17:33:04 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 199, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@606a15f1] is committing.
26/01/04 17:33:04 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 199, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@606a15f1] committed.
26/01/04 17:33:04 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/199 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.199.f38e130f-f214-44fe-bd48-4b2fa3f90dca.tmp
26/01/04 17:33:04 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.199.f38e130f-f214-44fe-bd48-4b2fa3f90dca.tmp to file:/tmp/spark-checkpoint-enrichment/commits/199
26/01/04 17:33:04 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T17:33:02.987Z",
  "batchId" : 199,
  "numInputRows" : 30,
  "inputRowsPerSecond" : 2307.6923076923076,
  "processedRowsPerSecond" : 22.04261572373255,
  "durationMs" : {
    "addBatch" : 1003,
    "commitOffsets" : 138,
    "getBatch" : 0,
    "latestOffset" : 3,
    "queryPlanning" : 42,
    "triggerExecution" : 1361,
    "walCommit" : 174
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "0" : 1320
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "0" : 1350
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "0" : 1350
      }
    },
    "numInputRows" : 30,
    "inputRowsPerSecond" : 2307.6923076923076,
    "processedRowsPerSecond" : 22.04261572373255,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 31
  }
}
26/01/04 17:33:14 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/200 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.200.8a249338-0a24-4f5c-bb99-fe42ae49160c.tmp
26/01/04 17:33:14 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.200.8a249338-0a24-4f5c-bb99-fe42ae49160c.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/200
26/01/04 17:33:14 INFO MicroBatchExecution: Committed offsets for batch 200. Metadata OffsetSeqMetadata(0,1767547994007,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 17:33:14 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:33:14 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:33:14 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:33:14 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:33:14 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:33:14 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:33:14 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#162283 - airline_prefix.nullCount#162282) > 0)
26/01/04 17:33:14 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#162318 - min_flight_num.nullCount#162317) > 0)
26/01/04 17:33:14 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#162313 - max_flight_num.nullCount#162312) > 0)
26/01/04 17:33:14 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:33:14 INFO DAGScheduler: Got job 403 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 17:33:14 INFO DAGScheduler: Final stage: ResultStage 404 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:33:14 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:33:14 INFO DAGScheduler: Missing parents: List()
26/01/04 17:33:14 INFO DAGScheduler: Submitting ResultStage 404 (MapPartitionsRDD[2023] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:33:14 INFO MemoryStore: Block broadcast_603 stored as values in memory (estimated size 38.5 KiB, free 434.2 MiB)
26/01/04 17:33:14 INFO MemoryStore: Block broadcast_603_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.2 MiB)
26/01/04 17:33:14 INFO BlockManagerInfo: Added broadcast_603_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:33:14 INFO SparkContext: Created broadcast 603 from broadcast at DAGScheduler.scala:1585
26/01/04 17:33:14 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 404 (MapPartitionsRDD[2023] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 17:33:14 INFO TaskSchedulerImpl: Adding task set 404.0 with 2 tasks resource profile 0
26/01/04 17:33:14 INFO TaskSetManager: Starting task 1.0 in stage 404.0 (TID 641) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 17:33:14 INFO TaskSetManager: Starting task 0.0 in stage 404.0 (TID 642) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 17:33:14 INFO BlockManagerInfo: Added broadcast_603_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:33:14 INFO BlockManagerInfo: Added broadcast_603_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:33:14 INFO TaskSetManager: Finished task 1.0 in stage 404.0 (TID 641) in 38 ms on 172.18.0.13 (executor 0) (1/2)
26/01/04 17:33:14 INFO TaskSetManager: Finished task 0.0 in stage 404.0 (TID 642) in 49 ms on 172.18.0.14 (executor 1) (2/2)
26/01/04 17:33:14 INFO TaskSchedulerImpl: Removed TaskSet 404.0, whose tasks have all completed, from pool 
26/01/04 17:33:14 INFO DAGScheduler: ResultStage 404 (start at NativeMethodAccessorImpl.java:0) finished in 0.063 s
26/01/04 17:33:14 INFO DAGScheduler: Job 403 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:33:14 INFO TaskSchedulerImpl: Killing all running tasks in stage 404: Stage finished
26/01/04 17:33:14 INFO DAGScheduler: Job 403 finished: start at NativeMethodAccessorImpl.java:0, took 0.065543 s
26/01/04 17:33:14 INFO MemoryStore: Block broadcast_604_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.2 MiB)
26/01/04 17:33:14 INFO BlockManagerInfo: Added broadcast_604_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:33:14 INFO SparkContext: Created broadcast 604 from start at NativeMethodAccessorImpl.java:0
26/01/04 17:33:14 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 200, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@3e2a2744]. The input RDD has 1 partitions.
26/01/04 17:33:14 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:33:14 INFO DAGScheduler: Got job 404 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/01/04 17:33:14 INFO DAGScheduler: Final stage: ResultStage 405 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:33:14 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:33:14 INFO DAGScheduler: Missing parents: List()
26/01/04 17:33:14 INFO DAGScheduler: Submitting ResultStage 405 (MapPartitionsRDD[2028] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:33:14 INFO MemoryStore: Block broadcast_605 stored as values in memory (estimated size 52.9 KiB, free 434.2 MiB)
26/01/04 17:33:14 INFO MemoryStore: Block broadcast_605_piece0 stored as bytes in memory (estimated size 20.2 KiB, free 434.1 MiB)
26/01/04 17:33:14 INFO BlockManagerInfo: Added broadcast_605_piece0 in memory on spark-master:33535 (size: 20.2 KiB, free: 434.3 MiB)
26/01/04 17:33:14 INFO SparkContext: Created broadcast 605 from broadcast at DAGScheduler.scala:1585
26/01/04 17:33:14 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 405 (MapPartitionsRDD[2028] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/01/04 17:33:14 INFO TaskSchedulerImpl: Adding task set 405.0 with 1 tasks resource profile 0
26/01/04 17:33:14 INFO TaskSetManager: Starting task 0.0 in stage 405.0 (TID 643) (172.18.0.13, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:33:14 INFO BlockManagerInfo: Added broadcast_605_piece0 in memory on 172.18.0.13:44401 (size: 20.2 KiB, free: 434.3 MiB)
26/01/04 17:33:14 INFO BlockManagerInfo: Added broadcast_604_piece0 in memory on 172.18.0.13:44401 (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:33:14 INFO TaskSetManager: Finished task 0.0 in stage 405.0 (TID 643) in 555 ms on 172.18.0.13 (executor 0) (1/1)
26/01/04 17:33:14 INFO TaskSchedulerImpl: Removed TaskSet 405.0, whose tasks have all completed, from pool 
26/01/04 17:33:14 INFO DAGScheduler: ResultStage 405 (start at NativeMethodAccessorImpl.java:0) finished in 0.566 s
26/01/04 17:33:14 INFO DAGScheduler: Job 404 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:33:14 INFO TaskSchedulerImpl: Killing all running tasks in stage 405: Stage finished
26/01/04 17:33:14 INFO DAGScheduler: Job 404 finished: start at NativeMethodAccessorImpl.java:0, took 0.568555 s
26/01/04 17:33:14 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 200, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@3e2a2744] is committing.
26/01/04 17:33:14 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 200, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@3e2a2744] committed.
26/01/04 17:33:14 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/200 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.200.72f52bcd-2e1f-4eb5-bd48-6f3d1e43afc4.tmp
26/01/04 17:33:14 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.200.72f52bcd-2e1f-4eb5-bd48-6f3d1e43afc4.tmp to file:/tmp/spark-checkpoint-enrichment/commits/200
26/01/04 17:33:14 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T17:33:14.005Z",
  "batchId" : 200,
  "numInputRows" : 30,
  "inputRowsPerSecond" : 2000.0,
  "processedRowsPerSecond" : 30.959752321981426,
  "durationMs" : {
    "addBatch" : 752,
    "commitOffsets" : 72,
    "getBatch" : 0,
    "latestOffset" : 2,
    "queryPlanning" : 33,
    "triggerExecution" : 969,
    "walCommit" : 110
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "0" : 1350
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "0" : 1380
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "0" : 1380
      }
    },
    "numInputRows" : 30,
    "inputRowsPerSecond" : 2000.0,
    "processedRowsPerSecond" : 30.959752321981426,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 31
  }
}
26/01/04 17:33:18 INFO BlockManagerInfo: Removed broadcast_602_piece0 on spark-master:33535 in memory (size: 20.2 KiB, free: 434.3 MiB)
26/01/04 17:33:18 INFO BlockManagerInfo: Removed broadcast_602_piece0 on 172.18.0.13:44401 in memory (size: 20.2 KiB, free: 434.3 MiB)
26/01/04 17:33:18 INFO BlockManagerInfo: Removed broadcast_603_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:33:18 INFO BlockManagerInfo: Removed broadcast_603_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:33:18 INFO BlockManagerInfo: Removed broadcast_603_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:33:18 INFO BlockManagerInfo: Removed broadcast_601_piece0 on spark-master:33535 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:33:18 INFO BlockManagerInfo: Removed broadcast_601_piece0 on 172.18.0.13:44401 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:33:18 INFO BlockManagerInfo: Removed broadcast_600_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:33:18 INFO BlockManagerInfo: Removed broadcast_600_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:33:18 INFO BlockManagerInfo: Removed broadcast_600_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:33:18 INFO BlockManagerInfo: Removed broadcast_605_piece0 on spark-master:33535 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:33:18 INFO BlockManagerInfo: Removed broadcast_605_piece0 on 172.18.0.13:44401 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:33:24 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/01/04 17:33:25 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/201 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.201.5abc27c0-2aa6-4fd7-9a63-83609858a8b3.tmp
26/01/04 17:33:25 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.201.5abc27c0-2aa6-4fd7-9a63-83609858a8b3.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/201
26/01/04 17:33:25 INFO MicroBatchExecution: Committed offsets for batch 201. Metadata OffsetSeqMetadata(0,1767548005023,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 17:33:25 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:33:25 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:33:25 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:33:25 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:33:25 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:33:25 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:33:25 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#163087 - airline_prefix.nullCount#163086) > 0)
26/01/04 17:33:25 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#163122 - min_flight_num.nullCount#163121) > 0)
26/01/04 17:33:25 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#163117 - max_flight_num.nullCount#163116) > 0)
26/01/04 17:33:25 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:33:25 INFO DAGScheduler: Got job 405 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 17:33:25 INFO DAGScheduler: Final stage: ResultStage 406 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:33:25 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:33:25 INFO DAGScheduler: Missing parents: List()
26/01/04 17:33:25 INFO DAGScheduler: Submitting ResultStage 406 (MapPartitionsRDD[2033] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:33:25 INFO MemoryStore: Block broadcast_606 stored as values in memory (estimated size 38.5 KiB, free 434.4 MiB)
26/01/04 17:33:25 INFO MemoryStore: Block broadcast_606_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.3 MiB)
26/01/04 17:33:25 INFO BlockManagerInfo: Added broadcast_606_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:33:25 INFO SparkContext: Created broadcast 606 from broadcast at DAGScheduler.scala:1585
26/01/04 17:33:25 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 406 (MapPartitionsRDD[2033] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 17:33:25 INFO TaskSchedulerImpl: Adding task set 406.0 with 2 tasks resource profile 0
26/01/04 17:33:25 INFO TaskSetManager: Starting task 1.0 in stage 406.0 (TID 644) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 17:33:25 INFO TaskSetManager: Starting task 0.0 in stage 406.0 (TID 645) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 17:33:25 INFO BlockManagerInfo: Added broadcast_606_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:33:25 INFO TaskSetManager: Finished task 1.0 in stage 406.0 (TID 644) in 21 ms on 172.18.0.13 (executor 0) (1/2)
26/01/04 17:33:25 INFO BlockManagerInfo: Added broadcast_606_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:33:25 INFO TaskSetManager: Finished task 0.0 in stage 406.0 (TID 645) in 175 ms on 172.18.0.14 (executor 1) (2/2)
26/01/04 17:33:25 INFO TaskSchedulerImpl: Removed TaskSet 406.0, whose tasks have all completed, from pool 
26/01/04 17:33:25 INFO DAGScheduler: ResultStage 406 (start at NativeMethodAccessorImpl.java:0) finished in 0.184 s
26/01/04 17:33:25 INFO DAGScheduler: Job 405 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:33:25 INFO TaskSchedulerImpl: Killing all running tasks in stage 406: Stage finished
26/01/04 17:33:25 INFO DAGScheduler: Job 405 finished: start at NativeMethodAccessorImpl.java:0, took 0.186130 s
26/01/04 17:33:25 INFO MemoryStore: Block broadcast_607_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.3 MiB)
26/01/04 17:33:25 INFO BlockManagerInfo: Added broadcast_607_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:33:25 INFO SparkContext: Created broadcast 607 from start at NativeMethodAccessorImpl.java:0
26/01/04 17:33:25 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 201, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@38c94d6f]. The input RDD has 1 partitions.
26/01/04 17:33:25 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:33:25 INFO DAGScheduler: Got job 406 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/01/04 17:33:25 INFO DAGScheduler: Final stage: ResultStage 407 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:33:25 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:33:25 INFO DAGScheduler: Missing parents: List()
26/01/04 17:33:25 INFO DAGScheduler: Submitting ResultStage 407 (MapPartitionsRDD[2038] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:33:25 INFO MemoryStore: Block broadcast_608 stored as values in memory (estimated size 52.9 KiB, free 434.3 MiB)
26/01/04 17:33:25 INFO MemoryStore: Block broadcast_608_piece0 stored as bytes in memory (estimated size 20.2 KiB, free 434.3 MiB)
26/01/04 17:33:25 INFO BlockManagerInfo: Added broadcast_608_piece0 in memory on spark-master:33535 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:33:25 INFO SparkContext: Created broadcast 608 from broadcast at DAGScheduler.scala:1585
26/01/04 17:33:25 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 407 (MapPartitionsRDD[2038] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/01/04 17:33:25 INFO TaskSchedulerImpl: Adding task set 407.0 with 1 tasks resource profile 0
26/01/04 17:33:25 INFO TaskSetManager: Starting task 0.0 in stage 407.0 (TID 646) (172.18.0.13, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:33:25 INFO BlockManagerInfo: Added broadcast_608_piece0 in memory on 172.18.0.13:44401 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:33:25 INFO BlockManagerInfo: Added broadcast_607_piece0 in memory on 172.18.0.13:44401 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:33:25 INFO TaskSetManager: Finished task 0.0 in stage 407.0 (TID 646) in 546 ms on 172.18.0.13 (executor 0) (1/1)
26/01/04 17:33:25 INFO TaskSchedulerImpl: Removed TaskSet 407.0, whose tasks have all completed, from pool 
26/01/04 17:33:25 INFO DAGScheduler: ResultStage 407 (start at NativeMethodAccessorImpl.java:0) finished in 0.551 s
26/01/04 17:33:25 INFO DAGScheduler: Job 406 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:33:25 INFO TaskSchedulerImpl: Killing all running tasks in stage 407: Stage finished
26/01/04 17:33:25 INFO DAGScheduler: Job 406 finished: start at NativeMethodAccessorImpl.java:0, took 0.552075 s
26/01/04 17:33:25 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 201, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@38c94d6f] is committing.
26/01/04 17:33:25 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 201, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@38c94d6f] committed.
26/01/04 17:33:25 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/201 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.201.5a158cbe-1156-4c3d-8cf1-9c9d4472ffbd.tmp
26/01/04 17:33:26 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.201.5a158cbe-1156-4c3d-8cf1-9c9d4472ffbd.tmp to file:/tmp/spark-checkpoint-enrichment/commits/201
26/01/04 17:33:26 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T17:33:25.022Z",
  "batchId" : 201,
  "numInputRows" : 30,
  "inputRowsPerSecond" : 2307.6923076923076,
  "processedRowsPerSecond" : 30.54989816700611,
  "durationMs" : {
    "addBatch" : 815,
    "commitOffsets" : 64,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 25,
    "triggerExecution" : 982,
    "walCommit" : 77
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "0" : 1380
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "0" : 1410
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "0" : 1410
      }
    },
    "numInputRows" : 30,
    "inputRowsPerSecond" : 2307.6923076923076,
    "processedRowsPerSecond" : 30.54989816700611,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 31
  }
}
26/01/04 17:33:36 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/01/04 17:33:36 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/202 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.202.ee16690a-2102-470a-9157-017b1d8ab11e.tmp
26/01/04 17:33:36 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.202.ee16690a-2102-470a-9157-017b1d8ab11e.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/202
26/01/04 17:33:36 INFO MicroBatchExecution: Committed offsets for batch 202. Metadata OffsetSeqMetadata(0,1767548016038,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 17:33:36 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:33:36 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:33:36 INFO BlockManagerInfo: Removed broadcast_607_piece0 on spark-master:33535 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:33:36 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:33:36 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:33:36 INFO BlockManagerInfo: Removed broadcast_607_piece0 on 172.18.0.13:44401 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:33:36 INFO BlockManagerInfo: Removed broadcast_606_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:33:36 INFO BlockManagerInfo: Removed broadcast_606_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:33:36 INFO BlockManagerInfo: Removed broadcast_606_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:33:36 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:33:36 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:33:36 INFO BlockManagerInfo: Removed broadcast_604_piece0 on spark-master:33535 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:33:36 INFO BlockManagerInfo: Removed broadcast_604_piece0 on 172.18.0.13:44401 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:33:36 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#163891 - airline_prefix.nullCount#163890) > 0)
26/01/04 17:33:36 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#163926 - min_flight_num.nullCount#163925) > 0)
26/01/04 17:33:36 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#163921 - max_flight_num.nullCount#163920) > 0)
26/01/04 17:33:36 INFO BlockManagerInfo: Removed broadcast_608_piece0 on spark-master:33535 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:33:36 INFO BlockManagerInfo: Removed broadcast_608_piece0 on 172.18.0.13:44401 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:33:36 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:33:36 INFO DAGScheduler: Got job 407 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 17:33:36 INFO DAGScheduler: Final stage: ResultStage 408 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:33:36 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:33:36 INFO DAGScheduler: Missing parents: List()
26/01/04 17:33:36 INFO DAGScheduler: Submitting ResultStage 408 (MapPartitionsRDD[2043] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:33:36 INFO MemoryStore: Block broadcast_609 stored as values in memory (estimated size 38.5 KiB, free 434.4 MiB)
26/01/04 17:33:36 INFO MemoryStore: Block broadcast_609_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.3 MiB)
26/01/04 17:33:36 INFO BlockManagerInfo: Added broadcast_609_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:33:36 INFO SparkContext: Created broadcast 609 from broadcast at DAGScheduler.scala:1585
26/01/04 17:33:36 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 408 (MapPartitionsRDD[2043] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 17:33:36 INFO TaskSchedulerImpl: Adding task set 408.0 with 2 tasks resource profile 0
26/01/04 17:33:36 INFO TaskSetManager: Starting task 0.0 in stage 408.0 (TID 647) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 17:33:36 INFO TaskSetManager: Starting task 1.0 in stage 408.0 (TID 648) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 17:33:36 INFO BlockManagerInfo: Added broadcast_609_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:33:36 INFO TaskSetManager: Finished task 1.0 in stage 408.0 (TID 648) in 37 ms on 172.18.0.13 (executor 0) (1/2)
26/01/04 17:33:36 INFO BlockManagerInfo: Added broadcast_609_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:33:36 INFO TaskSetManager: Finished task 0.0 in stage 408.0 (TID 647) in 115 ms on 172.18.0.14 (executor 1) (2/2)
26/01/04 17:33:36 INFO TaskSchedulerImpl: Removed TaskSet 408.0, whose tasks have all completed, from pool 
26/01/04 17:33:36 INFO DAGScheduler: ResultStage 408 (start at NativeMethodAccessorImpl.java:0) finished in 0.129 s
26/01/04 17:33:36 INFO DAGScheduler: Job 407 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:33:36 INFO TaskSchedulerImpl: Killing all running tasks in stage 408: Stage finished
26/01/04 17:33:36 INFO DAGScheduler: Job 407 finished: start at NativeMethodAccessorImpl.java:0, took 0.132384 s
26/01/04 17:33:36 INFO MemoryStore: Block broadcast_610_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.3 MiB)
26/01/04 17:33:36 INFO BlockManagerInfo: Added broadcast_610_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:33:36 INFO SparkContext: Created broadcast 610 from start at NativeMethodAccessorImpl.java:0
26/01/04 17:33:36 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 202, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@649804cf]. The input RDD has 1 partitions.
26/01/04 17:33:36 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:33:36 INFO DAGScheduler: Got job 408 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/01/04 17:33:36 INFO DAGScheduler: Final stage: ResultStage 409 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:33:36 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:33:36 INFO DAGScheduler: Missing parents: List()
26/01/04 17:33:36 INFO DAGScheduler: Submitting ResultStage 409 (MapPartitionsRDD[2048] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:33:36 INFO MemoryStore: Block broadcast_611 stored as values in memory (estimated size 52.9 KiB, free 434.3 MiB)
26/01/04 17:33:36 INFO BlockManagerInfo: Removed broadcast_609_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:33:36 INFO MemoryStore: Block broadcast_611_piece0 stored as bytes in memory (estimated size 20.2 KiB, free 434.3 MiB)
26/01/04 17:33:36 INFO BlockManagerInfo: Added broadcast_611_piece0 in memory on spark-master:33535 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:33:36 INFO BlockManagerInfo: Removed broadcast_609_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:33:36 INFO SparkContext: Created broadcast 611 from broadcast at DAGScheduler.scala:1585
26/01/04 17:33:36 INFO BlockManagerInfo: Removed broadcast_609_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:33:36 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 409 (MapPartitionsRDD[2048] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/01/04 17:33:36 INFO TaskSchedulerImpl: Adding task set 409.0 with 1 tasks resource profile 0
26/01/04 17:33:36 INFO TaskSetManager: Starting task 0.0 in stage 409.0 (TID 649) (172.18.0.13, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:33:36 INFO BlockManagerInfo: Added broadcast_611_piece0 in memory on 172.18.0.13:44401 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:33:36 INFO BlockManagerInfo: Added broadcast_610_piece0 in memory on 172.18.0.13:44401 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:33:36 INFO TaskSetManager: Finished task 0.0 in stage 409.0 (TID 649) in 568 ms on 172.18.0.13 (executor 0) (1/1)
26/01/04 17:33:36 INFO TaskSchedulerImpl: Removed TaskSet 409.0, whose tasks have all completed, from pool 
26/01/04 17:33:36 INFO DAGScheduler: ResultStage 409 (start at NativeMethodAccessorImpl.java:0) finished in 0.589 s
26/01/04 17:33:36 INFO DAGScheduler: Job 408 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:33:36 INFO TaskSchedulerImpl: Killing all running tasks in stage 409: Stage finished
26/01/04 17:33:36 INFO DAGScheduler: Job 408 finished: start at NativeMethodAccessorImpl.java:0, took 0.590588 s
26/01/04 17:33:36 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 202, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@649804cf] is committing.
26/01/04 17:33:36 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 202, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@649804cf] committed.
26/01/04 17:33:36 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/202 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.202.9b5f1e65-6194-45dd-bada-f6ae7fa73594.tmp
26/01/04 17:33:37 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.202.9b5f1e65-6194-45dd-bada-f6ae7fa73594.tmp to file:/tmp/spark-checkpoint-enrichment/commits/202
26/01/04 17:33:37 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T17:33:36.036Z",
  "batchId" : 202,
  "numInputRows" : 30,
  "inputRowsPerSecond" : 1875.0,
  "processedRowsPerSecond" : 29.440628066732092,
  "durationMs" : {
    "addBatch" : 823,
    "commitOffsets" : 80,
    "getBatch" : 0,
    "latestOffset" : 2,
    "queryPlanning" : 18,
    "triggerExecution" : 1019,
    "walCommit" : 96
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "0" : 1410
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "0" : 1440
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "0" : 1440
      }
    },
    "numInputRows" : 30,
    "inputRowsPerSecond" : 1875.0,
    "processedRowsPerSecond" : 29.440628066732092,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 31
  }
}
26/01/04 17:33:47 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/203 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.203.2f19cb24-4877-46dd-8430-e133ae81861d.tmp
26/01/04 17:33:47 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.203.2f19cb24-4877-46dd-8430-e133ae81861d.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/203
26/01/04 17:33:47 INFO MicroBatchExecution: Committed offsets for batch 203. Metadata OffsetSeqMetadata(0,1767548027058,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 17:33:47 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:33:47 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:33:47 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:33:47 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:33:47 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:33:47 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:33:47 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#164695 - airline_prefix.nullCount#164694) > 0)
26/01/04 17:33:47 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#164730 - min_flight_num.nullCount#164729) > 0)
26/01/04 17:33:47 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#164725 - max_flight_num.nullCount#164724) > 0)
26/01/04 17:33:47 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:33:47 INFO DAGScheduler: Got job 409 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 17:33:47 INFO DAGScheduler: Final stage: ResultStage 410 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:33:47 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:33:47 INFO DAGScheduler: Missing parents: List()
26/01/04 17:33:47 INFO DAGScheduler: Submitting ResultStage 410 (MapPartitionsRDD[2053] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:33:47 INFO MemoryStore: Block broadcast_612 stored as values in memory (estimated size 38.5 KiB, free 434.3 MiB)
26/01/04 17:33:47 INFO MemoryStore: Block broadcast_612_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.3 MiB)
26/01/04 17:33:47 INFO BlockManagerInfo: Removed broadcast_611_piece0 on spark-master:33535 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:33:47 INFO BlockManagerInfo: Added broadcast_612_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:33:47 INFO SparkContext: Created broadcast 612 from broadcast at DAGScheduler.scala:1585
26/01/04 17:33:47 INFO BlockManagerInfo: Removed broadcast_611_piece0 on 172.18.0.13:44401 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:33:47 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 410 (MapPartitionsRDD[2053] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 17:33:47 INFO TaskSchedulerImpl: Adding task set 410.0 with 2 tasks resource profile 0
26/01/04 17:33:47 INFO TaskSetManager: Starting task 0.0 in stage 410.0 (TID 650) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 17:33:47 INFO TaskSetManager: Starting task 1.0 in stage 410.0 (TID 651) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 17:33:47 INFO BlockManagerInfo: Removed broadcast_610_piece0 on spark-master:33535 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:33:47 INFO BlockManagerInfo: Removed broadcast_610_piece0 on 172.18.0.13:44401 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:33:47 INFO BlockManagerInfo: Added broadcast_612_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:33:47 INFO BlockManagerInfo: Added broadcast_612_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:33:47 INFO TaskSetManager: Finished task 1.0 in stage 410.0 (TID 651) in 25 ms on 172.18.0.13 (executor 0) (1/2)
26/01/04 17:33:47 INFO TaskSetManager: Finished task 0.0 in stage 410.0 (TID 650) in 78 ms on 172.18.0.14 (executor 1) (2/2)
26/01/04 17:33:47 INFO TaskSchedulerImpl: Removed TaskSet 410.0, whose tasks have all completed, from pool 
26/01/04 17:33:47 INFO DAGScheduler: ResultStage 410 (start at NativeMethodAccessorImpl.java:0) finished in 0.089 s
26/01/04 17:33:47 INFO DAGScheduler: Job 409 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:33:47 INFO TaskSchedulerImpl: Killing all running tasks in stage 410: Stage finished
26/01/04 17:33:47 INFO DAGScheduler: Job 409 finished: start at NativeMethodAccessorImpl.java:0, took 0.091222 s
26/01/04 17:33:47 INFO MemoryStore: Block broadcast_613_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.3 MiB)
26/01/04 17:33:47 INFO BlockManagerInfo: Removed broadcast_612_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:33:47 INFO BlockManagerInfo: Added broadcast_613_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:33:47 INFO SparkContext: Created broadcast 613 from start at NativeMethodAccessorImpl.java:0
26/01/04 17:33:47 INFO BlockManagerInfo: Removed broadcast_612_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:33:47 INFO BlockManagerInfo: Removed broadcast_612_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:33:47 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 203, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@7797c8d9]. The input RDD has 1 partitions.
26/01/04 17:33:47 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:33:47 INFO DAGScheduler: Got job 410 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/01/04 17:33:47 INFO DAGScheduler: Final stage: ResultStage 411 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:33:47 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:33:47 INFO DAGScheduler: Missing parents: List()
26/01/04 17:33:47 INFO DAGScheduler: Submitting ResultStage 411 (MapPartitionsRDD[2058] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:33:47 INFO MemoryStore: Block broadcast_614 stored as values in memory (estimated size 52.9 KiB, free 434.3 MiB)
26/01/04 17:33:47 INFO MemoryStore: Block broadcast_614_piece0 stored as bytes in memory (estimated size 20.2 KiB, free 434.3 MiB)
26/01/04 17:33:47 INFO BlockManagerInfo: Added broadcast_614_piece0 in memory on spark-master:33535 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:33:47 INFO SparkContext: Created broadcast 614 from broadcast at DAGScheduler.scala:1585
26/01/04 17:33:47 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 411 (MapPartitionsRDD[2058] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/01/04 17:33:47 INFO TaskSchedulerImpl: Adding task set 411.0 with 1 tasks resource profile 0
26/01/04 17:33:47 INFO TaskSetManager: Starting task 0.0 in stage 411.0 (TID 652) (172.18.0.13, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:33:47 INFO BlockManagerInfo: Added broadcast_614_piece0 in memory on 172.18.0.13:44401 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:33:47 INFO BlockManagerInfo: Added broadcast_613_piece0 in memory on 172.18.0.13:44401 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:33:47 INFO TaskSetManager: Finished task 0.0 in stage 411.0 (TID 652) in 577 ms on 172.18.0.13 (executor 0) (1/1)
26/01/04 17:33:47 INFO TaskSchedulerImpl: Removed TaskSet 411.0, whose tasks have all completed, from pool 
26/01/04 17:33:47 INFO DAGScheduler: ResultStage 411 (start at NativeMethodAccessorImpl.java:0) finished in 0.584 s
26/01/04 17:33:47 INFO DAGScheduler: Job 410 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:33:47 INFO TaskSchedulerImpl: Killing all running tasks in stage 411: Stage finished
26/01/04 17:33:47 INFO DAGScheduler: Job 410 finished: start at NativeMethodAccessorImpl.java:0, took 0.586516 s
26/01/04 17:33:47 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 203, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@7797c8d9] is committing.
26/01/04 17:33:47 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 203, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@7797c8d9] committed.
26/01/04 17:33:47 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/203 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.203.520ffdf8-c084-4581-a090-e1868023b9e3.tmp
26/01/04 17:33:48 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.203.520ffdf8-c084-4581-a090-e1868023b9e3.tmp to file:/tmp/spark-checkpoint-enrichment/commits/203
26/01/04 17:33:48 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T17:33:47.055Z",
  "batchId" : 203,
  "numInputRows" : 30,
  "inputRowsPerSecond" : 1764.705882352941,
  "processedRowsPerSecond" : 31.54574132492114,
  "durationMs" : {
    "addBatch" : 757,
    "commitOffsets" : 88,
    "getBatch" : 0,
    "latestOffset" : 3,
    "queryPlanning" : 24,
    "triggerExecution" : 951,
    "walCommit" : 79
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "0" : 1440
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "0" : 1470
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "0" : 1470
      }
    },
    "numInputRows" : 30,
    "inputRowsPerSecond" : 1764.705882352941,
    "processedRowsPerSecond" : 31.54574132492114,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 31
  }
}
26/01/04 17:33:58 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/01/04 17:33:58 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/204 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.204.c960e93d-b4ab-4995-91ca-e0954b9413be.tmp
26/01/04 17:33:58 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.204.c960e93d-b4ab-4995-91ca-e0954b9413be.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/204
26/01/04 17:33:58 INFO MicroBatchExecution: Committed offsets for batch 204. Metadata OffsetSeqMetadata(0,1767548038073,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 17:33:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:33:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:33:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:33:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:33:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:33:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:33:58 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#165499 - airline_prefix.nullCount#165498) > 0)
26/01/04 17:33:58 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#165534 - min_flight_num.nullCount#165533) > 0)
26/01/04 17:33:58 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#165529 - max_flight_num.nullCount#165528) > 0)
26/01/04 17:33:58 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:33:58 INFO DAGScheduler: Got job 411 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 17:33:58 INFO DAGScheduler: Final stage: ResultStage 412 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:33:58 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:33:58 INFO DAGScheduler: Missing parents: List()
26/01/04 17:33:58 INFO DAGScheduler: Submitting ResultStage 412 (MapPartitionsRDD[2063] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:33:58 INFO MemoryStore: Block broadcast_615 stored as values in memory (estimated size 38.5 KiB, free 434.3 MiB)
26/01/04 17:33:58 INFO MemoryStore: Block broadcast_615_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.3 MiB)
26/01/04 17:33:58 INFO BlockManagerInfo: Added broadcast_615_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:33:58 INFO SparkContext: Created broadcast 615 from broadcast at DAGScheduler.scala:1585
26/01/04 17:33:58 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 412 (MapPartitionsRDD[2063] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 17:33:58 INFO TaskSchedulerImpl: Adding task set 412.0 with 2 tasks resource profile 0
26/01/04 17:33:58 INFO BlockManagerInfo: Removed broadcast_613_piece0 on 172.18.0.13:44401 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:33:58 INFO BlockManagerInfo: Removed broadcast_613_piece0 on spark-master:33535 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:33:58 INFO TaskSetManager: Starting task 1.0 in stage 412.0 (TID 653) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 17:33:58 INFO TaskSetManager: Starting task 0.0 in stage 412.0 (TID 654) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 17:33:58 INFO BlockManagerInfo: Removed broadcast_614_piece0 on spark-master:33535 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:33:58 INFO BlockManagerInfo: Removed broadcast_614_piece0 on 172.18.0.13:44401 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:33:58 INFO BlockManagerInfo: Added broadcast_615_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:33:58 INFO BlockManagerInfo: Added broadcast_615_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:33:58 INFO TaskSetManager: Finished task 1.0 in stage 412.0 (TID 653) in 55 ms on 172.18.0.13 (executor 0) (1/2)
26/01/04 17:33:58 INFO TaskSetManager: Finished task 0.0 in stage 412.0 (TID 654) in 109 ms on 172.18.0.14 (executor 1) (2/2)
26/01/04 17:33:58 INFO TaskSchedulerImpl: Removed TaskSet 412.0, whose tasks have all completed, from pool 
26/01/04 17:33:58 INFO DAGScheduler: ResultStage 412 (start at NativeMethodAccessorImpl.java:0) finished in 0.121 s
26/01/04 17:33:58 INFO DAGScheduler: Job 411 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:33:58 INFO TaskSchedulerImpl: Killing all running tasks in stage 412: Stage finished
26/01/04 17:33:58 INFO DAGScheduler: Job 411 finished: start at NativeMethodAccessorImpl.java:0, took 0.122868 s
26/01/04 17:33:58 INFO BlockManagerInfo: Removed broadcast_615_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:33:58 INFO MemoryStore: Block broadcast_616_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.4 MiB)
26/01/04 17:33:58 INFO BlockManagerInfo: Removed broadcast_615_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:33:58 INFO BlockManagerInfo: Added broadcast_616_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:33:58 INFO SparkContext: Created broadcast 616 from start at NativeMethodAccessorImpl.java:0
26/01/04 17:33:58 INFO BlockManagerInfo: Removed broadcast_615_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:33:58 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 204, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@cb147aa]. The input RDD has 1 partitions.
26/01/04 17:33:58 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:33:58 INFO DAGScheduler: Got job 412 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/01/04 17:33:58 INFO DAGScheduler: Final stage: ResultStage 413 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:33:58 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:33:58 INFO DAGScheduler: Missing parents: List()
26/01/04 17:33:58 INFO DAGScheduler: Submitting ResultStage 413 (MapPartitionsRDD[2068] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:33:58 INFO MemoryStore: Block broadcast_617 stored as values in memory (estimated size 52.9 KiB, free 434.3 MiB)
26/01/04 17:33:58 INFO MemoryStore: Block broadcast_617_piece0 stored as bytes in memory (estimated size 20.2 KiB, free 434.3 MiB)
26/01/04 17:33:58 INFO BlockManagerInfo: Added broadcast_617_piece0 in memory on spark-master:33535 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:33:58 INFO SparkContext: Created broadcast 617 from broadcast at DAGScheduler.scala:1585
26/01/04 17:33:58 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 413 (MapPartitionsRDD[2068] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/01/04 17:33:58 INFO TaskSchedulerImpl: Adding task set 413.0 with 1 tasks resource profile 0
26/01/04 17:33:58 INFO TaskSetManager: Starting task 0.0 in stage 413.0 (TID 655) (172.18.0.13, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:33:58 INFO BlockManagerInfo: Added broadcast_617_piece0 in memory on 172.18.0.13:44401 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:33:58 INFO BlockManagerInfo: Added broadcast_616_piece0 in memory on 172.18.0.13:44401 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:33:59 INFO TaskSetManager: Finished task 0.0 in stage 413.0 (TID 655) in 639 ms on 172.18.0.13 (executor 0) (1/1)
26/01/04 17:33:59 INFO TaskSchedulerImpl: Removed TaskSet 413.0, whose tasks have all completed, from pool 
26/01/04 17:33:59 INFO DAGScheduler: ResultStage 413 (start at NativeMethodAccessorImpl.java:0) finished in 0.670 s
26/01/04 17:33:59 INFO DAGScheduler: Job 412 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:33:59 INFO TaskSchedulerImpl: Killing all running tasks in stage 413: Stage finished
26/01/04 17:33:59 INFO DAGScheduler: Job 412 finished: start at NativeMethodAccessorImpl.java:0, took 0.678673 s
26/01/04 17:33:59 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 204, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@cb147aa] is committing.
26/01/04 17:33:59 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 204, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@cb147aa] committed.
26/01/04 17:33:59 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/204 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.204.7a1382c0-cd0f-4737-a391-139199c67e48.tmp
26/01/04 17:33:59 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.204.7a1382c0-cd0f-4737-a391-139199c67e48.tmp to file:/tmp/spark-checkpoint-enrichment/commits/204
26/01/04 17:33:59 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T17:33:58.071Z",
  "batchId" : 204,
  "numInputRows" : 30,
  "inputRowsPerSecond" : 2500.0,
  "processedRowsPerSecond" : 23.828435266084195,
  "durationMs" : {
    "addBatch" : 910,
    "commitOffsets" : 161,
    "getBatch" : 0,
    "latestOffset" : 2,
    "queryPlanning" : 47,
    "triggerExecution" : 1259,
    "walCommit" : 136
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "0" : 1470
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "0" : 1500
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "0" : 1500
      }
    },
    "numInputRows" : 30,
    "inputRowsPerSecond" : 2500.0,
    "processedRowsPerSecond" : 23.828435266084195,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 31
  }
}
26/01/04 17:34:09 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/205 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.205.38fa3a27-5276-4eac-8ac1-d673d2d7f25a.tmp
26/01/04 17:34:09 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.205.38fa3a27-5276-4eac-8ac1-d673d2d7f25a.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/205
26/01/04 17:34:09 INFO MicroBatchExecution: Committed offsets for batch 205. Metadata OffsetSeqMetadata(0,1767548049088,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 17:34:09 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:34:09 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:34:09 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:34:09 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:34:09 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:34:09 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:34:09 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#166303 - airline_prefix.nullCount#166302) > 0)
26/01/04 17:34:09 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#166338 - min_flight_num.nullCount#166337) > 0)
26/01/04 17:34:09 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#166333 - max_flight_num.nullCount#166332) > 0)
26/01/04 17:34:09 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:34:09 INFO DAGScheduler: Got job 413 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 17:34:09 INFO DAGScheduler: Final stage: ResultStage 414 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:34:09 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:34:09 INFO DAGScheduler: Missing parents: List()
26/01/04 17:34:09 INFO DAGScheduler: Submitting ResultStage 414 (MapPartitionsRDD[2073] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:34:09 INFO MemoryStore: Block broadcast_618 stored as values in memory (estimated size 38.5 KiB, free 434.3 MiB)
26/01/04 17:34:09 INFO MemoryStore: Block broadcast_618_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.3 MiB)
26/01/04 17:34:09 INFO BlockManagerInfo: Added broadcast_618_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:34:09 INFO SparkContext: Created broadcast 618 from broadcast at DAGScheduler.scala:1585
26/01/04 17:34:09 INFO BlockManagerInfo: Removed broadcast_616_piece0 on spark-master:33535 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:34:09 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 414 (MapPartitionsRDD[2073] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 17:34:09 INFO TaskSchedulerImpl: Adding task set 414.0 with 2 tasks resource profile 0
26/01/04 17:34:09 INFO BlockManagerInfo: Removed broadcast_616_piece0 on 172.18.0.13:44401 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:34:09 INFO TaskSetManager: Starting task 1.0 in stage 414.0 (TID 656) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 17:34:09 INFO TaskSetManager: Starting task 0.0 in stage 414.0 (TID 657) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 17:34:09 INFO BlockManagerInfo: Removed broadcast_617_piece0 on spark-master:33535 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:34:09 INFO BlockManagerInfo: Removed broadcast_617_piece0 on 172.18.0.13:44401 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:34:09 INFO BlockManagerInfo: Added broadcast_618_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:34:09 INFO BlockManagerInfo: Added broadcast_618_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:34:09 INFO TaskSetManager: Finished task 1.0 in stage 414.0 (TID 656) in 48 ms on 172.18.0.13 (executor 0) (1/2)
26/01/04 17:34:09 INFO TaskSetManager: Finished task 0.0 in stage 414.0 (TID 657) in 174 ms on 172.18.0.14 (executor 1) (2/2)
26/01/04 17:34:09 INFO TaskSchedulerImpl: Removed TaskSet 414.0, whose tasks have all completed, from pool 
26/01/04 17:34:09 INFO DAGScheduler: ResultStage 414 (start at NativeMethodAccessorImpl.java:0) finished in 0.198 s
26/01/04 17:34:09 INFO DAGScheduler: Job 413 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:34:09 INFO TaskSchedulerImpl: Killing all running tasks in stage 414: Stage finished
26/01/04 17:34:09 INFO DAGScheduler: Job 413 finished: start at NativeMethodAccessorImpl.java:0, took 0.201035 s
26/01/04 17:34:09 INFO MemoryStore: Block broadcast_619_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.3 MiB)
26/01/04 17:34:09 INFO BlockManagerInfo: Added broadcast_619_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:34:09 INFO SparkContext: Created broadcast 619 from start at NativeMethodAccessorImpl.java:0
26/01/04 17:34:09 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 205, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@261777cc]. The input RDD has 1 partitions.
26/01/04 17:34:09 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:34:09 INFO DAGScheduler: Got job 414 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/01/04 17:34:09 INFO DAGScheduler: Final stage: ResultStage 415 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:34:09 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:34:09 INFO DAGScheduler: Missing parents: List()
26/01/04 17:34:09 INFO DAGScheduler: Submitting ResultStage 415 (MapPartitionsRDD[2078] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:34:09 INFO MemoryStore: Block broadcast_620 stored as values in memory (estimated size 52.9 KiB, free 434.3 MiB)
26/01/04 17:34:09 INFO MemoryStore: Block broadcast_620_piece0 stored as bytes in memory (estimated size 20.2 KiB, free 434.3 MiB)
26/01/04 17:34:09 INFO BlockManagerInfo: Added broadcast_620_piece0 in memory on spark-master:33535 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:34:09 INFO SparkContext: Created broadcast 620 from broadcast at DAGScheduler.scala:1585
26/01/04 17:34:09 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 415 (MapPartitionsRDD[2078] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/01/04 17:34:09 INFO TaskSchedulerImpl: Adding task set 415.0 with 1 tasks resource profile 0
26/01/04 17:34:09 INFO TaskSetManager: Starting task 0.0 in stage 415.0 (TID 658) (172.18.0.13, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:34:09 INFO BlockManagerInfo: Added broadcast_620_piece0 in memory on 172.18.0.13:44401 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:34:09 INFO BlockManagerInfo: Added broadcast_619_piece0 in memory on 172.18.0.13:44401 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:34:10 INFO TaskSetManager: Finished task 0.0 in stage 415.0 (TID 658) in 578 ms on 172.18.0.13 (executor 0) (1/1)
26/01/04 17:34:10 INFO TaskSchedulerImpl: Removed TaskSet 415.0, whose tasks have all completed, from pool 
26/01/04 17:34:10 INFO DAGScheduler: ResultStage 415 (start at NativeMethodAccessorImpl.java:0) finished in 0.592 s
26/01/04 17:34:10 INFO DAGScheduler: Job 414 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:34:10 INFO TaskSchedulerImpl: Killing all running tasks in stage 415: Stage finished
26/01/04 17:34:10 INFO DAGScheduler: Job 414 finished: start at NativeMethodAccessorImpl.java:0, took 0.597485 s
26/01/04 17:34:10 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 205, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@261777cc] is committing.
26/01/04 17:34:10 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 205, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@261777cc] committed.
26/01/04 17:34:10 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/205 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.205.e2f0d98a-b346-46ab-bff8-c6adde971ba1.tmp
26/01/04 17:34:10 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.205.e2f0d98a-b346-46ab-bff8-c6adde971ba1.tmp to file:/tmp/spark-checkpoint-enrichment/commits/205
26/01/04 17:34:10 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T17:34:09.086Z",
  "batchId" : 205,
  "numInputRows" : 30,
  "inputRowsPerSecond" : 2142.8571428571427,
  "processedRowsPerSecond" : 24.752475247524753,
  "durationMs" : {
    "addBatch" : 912,
    "commitOffsets" : 161,
    "getBatch" : 0,
    "latestOffset" : 2,
    "queryPlanning" : 32,
    "triggerExecution" : 1212,
    "walCommit" : 104
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "0" : 1500
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "0" : 1530
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "0" : 1530
      }
    },
    "numInputRows" : 30,
    "inputRowsPerSecond" : 2142.8571428571427,
    "processedRowsPerSecond" : 24.752475247524753,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 31
  }
}
26/01/04 17:34:20 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/206 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.206.00046523-e875-400c-86f8-38ec7f937cc5.tmp
26/01/04 17:34:20 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.206.00046523-e875-400c-86f8-38ec7f937cc5.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/206
26/01/04 17:34:20 INFO MicroBatchExecution: Committed offsets for batch 206. Metadata OffsetSeqMetadata(0,1767548060095,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 17:34:20 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:34:20 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:34:20 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:34:20 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:34:20 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:34:20 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:34:20 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#167107 - airline_prefix.nullCount#167106) > 0)
26/01/04 17:34:20 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#167142 - min_flight_num.nullCount#167141) > 0)
26/01/04 17:34:20 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#167137 - max_flight_num.nullCount#167136) > 0)
26/01/04 17:34:20 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:34:20 INFO DAGScheduler: Got job 415 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 17:34:20 INFO DAGScheduler: Final stage: ResultStage 416 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:34:20 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:34:20 INFO DAGScheduler: Missing parents: List()
26/01/04 17:34:20 INFO DAGScheduler: Submitting ResultStage 416 (MapPartitionsRDD[2083] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:34:20 INFO MemoryStore: Block broadcast_621 stored as values in memory (estimated size 38.5 KiB, free 434.2 MiB)
26/01/04 17:34:20 INFO MemoryStore: Block broadcast_621_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.2 MiB)
26/01/04 17:34:20 INFO BlockManagerInfo: Added broadcast_621_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:34:20 INFO BlockManagerInfo: Removed broadcast_620_piece0 on spark-master:33535 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:34:20 INFO SparkContext: Created broadcast 621 from broadcast at DAGScheduler.scala:1585
26/01/04 17:34:20 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 416 (MapPartitionsRDD[2083] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 17:34:20 INFO TaskSchedulerImpl: Adding task set 416.0 with 2 tasks resource profile 0
26/01/04 17:34:20 INFO BlockManagerInfo: Removed broadcast_620_piece0 on 172.18.0.13:44401 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:34:20 INFO TaskSetManager: Starting task 1.0 in stage 416.0 (TID 659) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 17:34:20 INFO TaskSetManager: Starting task 0.0 in stage 416.0 (TID 660) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 17:34:20 INFO BlockManagerInfo: Added broadcast_621_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:34:20 INFO BlockManagerInfo: Removed broadcast_619_piece0 on spark-master:33535 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:34:20 INFO BlockManagerInfo: Removed broadcast_619_piece0 on 172.18.0.13:44401 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:34:20 INFO BlockManagerInfo: Removed broadcast_618_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:34:20 INFO BlockManagerInfo: Added broadcast_621_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:34:20 INFO BlockManagerInfo: Removed broadcast_618_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:34:20 INFO BlockManagerInfo: Removed broadcast_618_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:34:20 INFO TaskSetManager: Finished task 1.0 in stage 416.0 (TID 659) in 36 ms on 172.18.0.13 (executor 0) (1/2)
26/01/04 17:34:20 INFO TaskSetManager: Finished task 0.0 in stage 416.0 (TID 660) in 74 ms on 172.18.0.14 (executor 1) (2/2)
26/01/04 17:34:20 INFO TaskSchedulerImpl: Removed TaskSet 416.0, whose tasks have all completed, from pool 
26/01/04 17:34:20 INFO DAGScheduler: ResultStage 416 (start at NativeMethodAccessorImpl.java:0) finished in 0.095 s
26/01/04 17:34:20 INFO DAGScheduler: Job 415 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:34:20 INFO TaskSchedulerImpl: Killing all running tasks in stage 416: Stage finished
26/01/04 17:34:20 INFO DAGScheduler: Job 415 finished: start at NativeMethodAccessorImpl.java:0, took 0.098161 s
26/01/04 17:34:20 INFO MemoryStore: Block broadcast_622_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.3 MiB)
26/01/04 17:34:20 INFO BlockManagerInfo: Added broadcast_622_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:34:20 INFO SparkContext: Created broadcast 622 from start at NativeMethodAccessorImpl.java:0
26/01/04 17:34:20 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 206, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@33a04e09]. The input RDD has 1 partitions.
26/01/04 17:34:20 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:34:20 INFO DAGScheduler: Got job 416 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/01/04 17:34:20 INFO DAGScheduler: Final stage: ResultStage 417 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:34:20 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:34:20 INFO DAGScheduler: Missing parents: List()
26/01/04 17:34:20 INFO DAGScheduler: Submitting ResultStage 417 (MapPartitionsRDD[2088] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:34:20 INFO MemoryStore: Block broadcast_623 stored as values in memory (estimated size 52.9 KiB, free 434.3 MiB)
26/01/04 17:34:20 INFO MemoryStore: Block broadcast_623_piece0 stored as bytes in memory (estimated size 20.2 KiB, free 434.3 MiB)
26/01/04 17:34:20 INFO BlockManagerInfo: Added broadcast_623_piece0 in memory on spark-master:33535 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:34:20 INFO SparkContext: Created broadcast 623 from broadcast at DAGScheduler.scala:1585
26/01/04 17:34:20 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 417 (MapPartitionsRDD[2088] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/01/04 17:34:20 INFO TaskSchedulerImpl: Adding task set 417.0 with 1 tasks resource profile 0
26/01/04 17:34:20 INFO TaskSetManager: Starting task 0.0 in stage 417.0 (TID 661) (172.18.0.13, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:34:20 INFO BlockManagerInfo: Added broadcast_623_piece0 in memory on 172.18.0.13:44401 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:34:20 INFO BlockManagerInfo: Added broadcast_622_piece0 in memory on 172.18.0.13:44401 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:34:20 INFO TaskSetManager: Finished task 0.0 in stage 417.0 (TID 661) in 547 ms on 172.18.0.13 (executor 0) (1/1)
26/01/04 17:34:20 INFO TaskSchedulerImpl: Removed TaskSet 417.0, whose tasks have all completed, from pool 
26/01/04 17:34:20 INFO DAGScheduler: ResultStage 417 (start at NativeMethodAccessorImpl.java:0) finished in 0.555 s
26/01/04 17:34:20 INFO DAGScheduler: Job 416 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:34:20 INFO TaskSchedulerImpl: Killing all running tasks in stage 417: Stage finished
26/01/04 17:34:20 INFO DAGScheduler: Job 416 finished: start at NativeMethodAccessorImpl.java:0, took 0.557204 s
26/01/04 17:34:20 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 206, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@33a04e09] is committing.
26/01/04 17:34:20 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 206, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@33a04e09] committed.
26/01/04 17:34:21 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/206 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.206.95c95f79-f021-4cf6-9932-819324c1e556.tmp
26/01/04 17:34:21 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.206.95c95f79-f021-4cf6-9932-819324c1e556.tmp to file:/tmp/spark-checkpoint-enrichment/commits/206
26/01/04 17:34:21 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T17:34:20.091Z",
  "batchId" : 206,
  "numInputRows" : 1,
  "inputRowsPerSecond" : 76.92307692307692,
  "processedRowsPerSecond" : 1.0111223458038423,
  "durationMs" : {
    "addBatch" : 740,
    "commitOffsets" : 95,
    "getBatch" : 0,
    "latestOffset" : 4,
    "queryPlanning" : 56,
    "triggerExecution" : 989,
    "walCommit" : 93
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "0" : 1530
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "0" : 1531
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "0" : 1531
      }
    },
    "numInputRows" : 1,
    "inputRowsPerSecond" : 76.92307692307692,
    "processedRowsPerSecond" : 1.0111223458038423,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 1
  }
}
26/01/04 17:34:21 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/207 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.207.19aad5e0-9eed-452f-ab67-547b50abf9f3.tmp
26/01/04 17:34:21 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.207.19aad5e0-9eed-452f-ab67-547b50abf9f3.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/207
26/01/04 17:34:21 INFO MicroBatchExecution: Committed offsets for batch 207. Metadata OffsetSeqMetadata(0,1767548061082,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 17:34:21 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:34:21 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:34:21 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:34:21 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:34:21 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:34:21 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:34:21 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#167911 - airline_prefix.nullCount#167910) > 0)
26/01/04 17:34:21 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#167946 - min_flight_num.nullCount#167945) > 0)
26/01/04 17:34:21 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#167941 - max_flight_num.nullCount#167940) > 0)
26/01/04 17:34:21 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:34:21 INFO DAGScheduler: Got job 417 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 17:34:21 INFO DAGScheduler: Final stage: ResultStage 418 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:34:21 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:34:21 INFO DAGScheduler: Missing parents: List()
26/01/04 17:34:21 INFO DAGScheduler: Submitting ResultStage 418 (MapPartitionsRDD[2093] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:34:21 INFO MemoryStore: Block broadcast_624 stored as values in memory (estimated size 38.5 KiB, free 434.2 MiB)
26/01/04 17:34:21 INFO MemoryStore: Block broadcast_624_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.2 MiB)
26/01/04 17:34:21 INFO BlockManagerInfo: Added broadcast_624_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:34:21 INFO BlockManagerInfo: Removed broadcast_622_piece0 on spark-master:33535 in memory (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:34:21 INFO SparkContext: Created broadcast 624 from broadcast at DAGScheduler.scala:1585
26/01/04 17:34:21 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 418 (MapPartitionsRDD[2093] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 17:34:21 INFO TaskSchedulerImpl: Adding task set 418.0 with 2 tasks resource profile 0
26/01/04 17:34:21 INFO BlockManagerInfo: Removed broadcast_622_piece0 on 172.18.0.13:44401 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:34:21 INFO TaskSetManager: Starting task 0.0 in stage 418.0 (TID 662) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 17:34:21 INFO TaskSetManager: Starting task 1.0 in stage 418.0 (TID 663) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 17:34:21 INFO BlockManagerInfo: Removed broadcast_621_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:34:21 INFO BlockManagerInfo: Removed broadcast_621_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:34:21 INFO BlockManagerInfo: Removed broadcast_621_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:34:21 INFO BlockManagerInfo: Added broadcast_624_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:34:21 INFO BlockManagerInfo: Removed broadcast_623_piece0 on 172.18.0.13:44401 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:34:21 INFO BlockManagerInfo: Removed broadcast_623_piece0 on spark-master:33535 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:34:21 INFO BlockManagerInfo: Added broadcast_624_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:34:21 INFO TaskSetManager: Finished task 1.0 in stage 418.0 (TID 663) in 49 ms on 172.18.0.13 (executor 0) (1/2)
26/01/04 17:34:21 INFO TaskSetManager: Finished task 0.0 in stage 418.0 (TID 662) in 93 ms on 172.18.0.14 (executor 1) (2/2)
26/01/04 17:34:21 INFO TaskSchedulerImpl: Removed TaskSet 418.0, whose tasks have all completed, from pool 
26/01/04 17:34:21 INFO DAGScheduler: ResultStage 418 (start at NativeMethodAccessorImpl.java:0) finished in 0.109 s
26/01/04 17:34:21 INFO DAGScheduler: Job 417 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:34:21 INFO TaskSchedulerImpl: Killing all running tasks in stage 418: Stage finished
26/01/04 17:34:21 INFO DAGScheduler: Job 417 finished: start at NativeMethodAccessorImpl.java:0, took 0.113652 s
26/01/04 17:34:21 INFO MemoryStore: Block broadcast_625_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.3 MiB)
26/01/04 17:34:21 INFO BlockManagerInfo: Added broadcast_625_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:34:21 INFO SparkContext: Created broadcast 625 from start at NativeMethodAccessorImpl.java:0
26/01/04 17:34:21 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 207, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@61ea05eb]. The input RDD has 1 partitions.
26/01/04 17:34:21 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:34:21 INFO DAGScheduler: Got job 418 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/01/04 17:34:21 INFO DAGScheduler: Final stage: ResultStage 419 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:34:21 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:34:21 INFO DAGScheduler: Missing parents: List()
26/01/04 17:34:21 INFO DAGScheduler: Submitting ResultStage 419 (MapPartitionsRDD[2098] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:34:21 INFO MemoryStore: Block broadcast_626 stored as values in memory (estimated size 52.9 KiB, free 434.3 MiB)
26/01/04 17:34:21 INFO BlockManagerInfo: Removed broadcast_624_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:34:21 INFO MemoryStore: Block broadcast_626_piece0 stored as bytes in memory (estimated size 20.2 KiB, free 434.3 MiB)
26/01/04 17:34:21 INFO BlockManagerInfo: Added broadcast_626_piece0 in memory on spark-master:33535 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:34:21 INFO BlockManagerInfo: Removed broadcast_624_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:34:21 INFO SparkContext: Created broadcast 626 from broadcast at DAGScheduler.scala:1585
26/01/04 17:34:21 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 419 (MapPartitionsRDD[2098] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/01/04 17:34:21 INFO TaskSchedulerImpl: Adding task set 419.0 with 1 tasks resource profile 0
26/01/04 17:34:21 INFO TaskSetManager: Starting task 0.0 in stage 419.0 (TID 664) (172.18.0.13, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:34:21 INFO BlockManagerInfo: Removed broadcast_624_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:34:21 INFO BlockManagerInfo: Added broadcast_626_piece0 in memory on 172.18.0.13:44401 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:34:21 INFO BlockManagerInfo: Added broadcast_625_piece0 in memory on 172.18.0.13:44401 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:34:21 INFO TaskSetManager: Finished task 0.0 in stage 419.0 (TID 664) in 91 ms on 172.18.0.13 (executor 0) (1/1)
26/01/04 17:34:21 INFO TaskSchedulerImpl: Removed TaskSet 419.0, whose tasks have all completed, from pool 
26/01/04 17:34:21 INFO DAGScheduler: ResultStage 419 (start at NativeMethodAccessorImpl.java:0) finished in 0.122 s
26/01/04 17:34:21 INFO DAGScheduler: Job 418 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:34:21 INFO TaskSchedulerImpl: Killing all running tasks in stage 419: Stage finished
26/01/04 17:34:21 INFO DAGScheduler: Job 418 finished: start at NativeMethodAccessorImpl.java:0, took 0.125423 s
26/01/04 17:34:21 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 207, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@61ea05eb] is committing.
26/01/04 17:34:21 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 207, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@61ea05eb] committed.
26/01/04 17:34:21 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/207 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.207.ff2e4547-a9a6-4256-85e4-c16cef80b9c7.tmp
26/01/04 17:34:21 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.207.ff2e4547-a9a6-4256-85e4-c16cef80b9c7.tmp to file:/tmp/spark-checkpoint-enrichment/commits/207
26/01/04 17:34:21 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T17:34:21.081Z",
  "batchId" : 207,
  "numInputRows" : 29,
  "inputRowsPerSecond" : 29.292929292929294,
  "processedRowsPerSecond" : 57.76892430278885,
  "durationMs" : {
    "addBatch" : 317,
    "commitOffsets" : 85,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 21,
    "triggerExecution" : 502,
    "walCommit" : 77
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "0" : 1531
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "0" : 1560
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "0" : 1560
      }
    },
    "numInputRows" : 29,
    "inputRowsPerSecond" : 29.292929292929294,
    "processedRowsPerSecond" : 57.76892430278885,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 30
  }
}
26/01/04 17:34:31 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/208 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.208.636afdcf-58aa-445a-9d2c-e76b6fe7fb68.tmp
26/01/04 17:34:31 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.208.636afdcf-58aa-445a-9d2c-e76b6fe7fb68.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/208
26/01/04 17:34:31 INFO MicroBatchExecution: Committed offsets for batch 208. Metadata OffsetSeqMetadata(0,1767548071120,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 17:34:31 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:34:31 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:34:31 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:34:31 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:34:31 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:34:31 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:34:31 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#168715 - airline_prefix.nullCount#168714) > 0)
26/01/04 17:34:31 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#168750 - min_flight_num.nullCount#168749) > 0)
26/01/04 17:34:31 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#168745 - max_flight_num.nullCount#168744) > 0)
26/01/04 17:34:31 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:34:31 INFO DAGScheduler: Got job 419 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 17:34:31 INFO DAGScheduler: Final stage: ResultStage 420 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:34:31 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:34:31 INFO DAGScheduler: Missing parents: List()
26/01/04 17:34:31 INFO DAGScheduler: Submitting ResultStage 420 (MapPartitionsRDD[2103] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:34:31 INFO MemoryStore: Block broadcast_627 stored as values in memory (estimated size 38.5 KiB, free 434.3 MiB)
26/01/04 17:34:31 INFO MemoryStore: Block broadcast_627_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.3 MiB)
26/01/04 17:34:31 INFO BlockManagerInfo: Removed broadcast_625_piece0 on spark-master:33535 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:34:31 INFO BlockManagerInfo: Added broadcast_627_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:34:31 INFO BlockManagerInfo: Removed broadcast_625_piece0 on 172.18.0.13:44401 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:34:31 INFO SparkContext: Created broadcast 627 from broadcast at DAGScheduler.scala:1585
26/01/04 17:34:31 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 420 (MapPartitionsRDD[2103] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 17:34:31 INFO TaskSchedulerImpl: Adding task set 420.0 with 2 tasks resource profile 0
26/01/04 17:34:31 INFO TaskSetManager: Starting task 0.0 in stage 420.0 (TID 665) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 17:34:31 INFO TaskSetManager: Starting task 1.0 in stage 420.0 (TID 666) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 17:34:31 INFO BlockManagerInfo: Removed broadcast_626_piece0 on spark-master:33535 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:34:31 INFO BlockManagerInfo: Removed broadcast_626_piece0 on 172.18.0.13:44401 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:34:31 INFO BlockManagerInfo: Added broadcast_627_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:34:31 INFO TaskSetManager: Finished task 1.0 in stage 420.0 (TID 666) in 33 ms on 172.18.0.13 (executor 0) (1/2)
26/01/04 17:34:31 INFO BlockManagerInfo: Added broadcast_627_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:34:31 INFO TaskSetManager: Finished task 0.0 in stage 420.0 (TID 665) in 71 ms on 172.18.0.14 (executor 1) (2/2)
26/01/04 17:34:31 INFO TaskSchedulerImpl: Removed TaskSet 420.0, whose tasks have all completed, from pool 
26/01/04 17:34:31 INFO DAGScheduler: ResultStage 420 (start at NativeMethodAccessorImpl.java:0) finished in 0.084 s
26/01/04 17:34:31 INFO DAGScheduler: Job 419 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:34:31 INFO TaskSchedulerImpl: Killing all running tasks in stage 420: Stage finished
26/01/04 17:34:31 INFO DAGScheduler: Job 419 finished: start at NativeMethodAccessorImpl.java:0, took 0.085403 s
26/01/04 17:34:31 INFO MemoryStore: Block broadcast_628_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.3 MiB)
26/01/04 17:34:31 INFO BlockManagerInfo: Added broadcast_628_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:34:31 INFO SparkContext: Created broadcast 628 from start at NativeMethodAccessorImpl.java:0
26/01/04 17:34:31 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 208, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@163344df]. The input RDD has 1 partitions.
26/01/04 17:34:31 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:34:31 INFO DAGScheduler: Got job 420 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/01/04 17:34:31 INFO DAGScheduler: Final stage: ResultStage 421 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:34:31 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:34:31 INFO DAGScheduler: Missing parents: List()
26/01/04 17:34:31 INFO DAGScheduler: Submitting ResultStage 421 (MapPartitionsRDD[2108] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:34:31 INFO MemoryStore: Block broadcast_629 stored as values in memory (estimated size 52.9 KiB, free 434.3 MiB)
26/01/04 17:34:31 INFO BlockManagerInfo: Removed broadcast_627_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:34:31 INFO MemoryStore: Block broadcast_629_piece0 stored as bytes in memory (estimated size 20.2 KiB, free 434.3 MiB)
26/01/04 17:34:31 INFO BlockManagerInfo: Added broadcast_629_piece0 in memory on spark-master:33535 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:34:31 INFO SparkContext: Created broadcast 629 from broadcast at DAGScheduler.scala:1585
26/01/04 17:34:31 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 421 (MapPartitionsRDD[2108] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/01/04 17:34:31 INFO TaskSchedulerImpl: Adding task set 421.0 with 1 tasks resource profile 0
26/01/04 17:34:31 INFO BlockManagerInfo: Removed broadcast_627_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:34:31 INFO TaskSetManager: Starting task 0.0 in stage 421.0 (TID 667) (172.18.0.13, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:34:31 INFO BlockManagerInfo: Removed broadcast_627_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:34:31 INFO BlockManagerInfo: Added broadcast_629_piece0 in memory on 172.18.0.13:44401 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:34:31 INFO BlockManagerInfo: Added broadcast_628_piece0 in memory on 172.18.0.13:44401 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:34:31 INFO TaskSetManager: Finished task 0.0 in stage 421.0 (TID 667) in 549 ms on 172.18.0.13 (executor 0) (1/1)
26/01/04 17:34:31 INFO TaskSchedulerImpl: Removed TaskSet 421.0, whose tasks have all completed, from pool 
26/01/04 17:34:31 INFO DAGScheduler: ResultStage 421 (start at NativeMethodAccessorImpl.java:0) finished in 0.561 s
26/01/04 17:34:31 INFO DAGScheduler: Job 420 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:34:31 INFO TaskSchedulerImpl: Killing all running tasks in stage 421: Stage finished
26/01/04 17:34:31 INFO DAGScheduler: Job 420 finished: start at NativeMethodAccessorImpl.java:0, took 0.562363 s
26/01/04 17:34:31 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 208, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@163344df] is committing.
26/01/04 17:34:31 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 208, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@163344df] committed.
26/01/04 17:34:31 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/208 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.208.85c83c8a-5bab-4778-907c-ff7f2c2afebf.tmp
26/01/04 17:34:32 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.208.85c83c8a-5bab-4778-907c-ff7f2c2afebf.tmp to file:/tmp/spark-checkpoint-enrichment/commits/208
26/01/04 17:34:32 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T17:34:31.118Z",
  "batchId" : 208,
  "numInputRows" : 30,
  "inputRowsPerSecond" : 2727.2727272727275,
  "processedRowsPerSecond" : 33.557046979865774,
  "durationMs" : {
    "addBatch" : 726,
    "commitOffsets" : 71,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 17,
    "triggerExecution" : 894,
    "walCommit" : 77
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "0" : 1560
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "0" : 1590
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "0" : 1590
      }
    },
    "numInputRows" : 30,
    "inputRowsPerSecond" : 2727.2727272727275,
    "processedRowsPerSecond" : 33.557046979865774,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 31
  }
}
26/01/04 17:34:42 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/01/04 17:34:42 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/209 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.209.4614ab75-f95b-499c-a8c5-4700177520f2.tmp
26/01/04 17:34:42 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.209.4614ab75-f95b-499c-a8c5-4700177520f2.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/209
26/01/04 17:34:42 INFO MicroBatchExecution: Committed offsets for batch 209. Metadata OffsetSeqMetadata(0,1767548082132,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 17:34:42 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:34:42 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:34:42 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:34:42 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:34:42 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:34:42 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:34:42 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#169519 - airline_prefix.nullCount#169518) > 0)
26/01/04 17:34:42 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#169554 - min_flight_num.nullCount#169553) > 0)
26/01/04 17:34:42 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#169549 - max_flight_num.nullCount#169548) > 0)
26/01/04 17:34:42 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:34:42 INFO DAGScheduler: Got job 421 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 17:34:42 INFO DAGScheduler: Final stage: ResultStage 422 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:34:42 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:34:42 INFO DAGScheduler: Missing parents: List()
26/01/04 17:34:42 INFO DAGScheduler: Submitting ResultStage 422 (MapPartitionsRDD[2113] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:34:42 INFO MemoryStore: Block broadcast_630 stored as values in memory (estimated size 38.5 KiB, free 434.3 MiB)
26/01/04 17:34:42 INFO MemoryStore: Block broadcast_630_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.3 MiB)
26/01/04 17:34:42 INFO BlockManagerInfo: Added broadcast_630_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:34:42 INFO SparkContext: Created broadcast 630 from broadcast at DAGScheduler.scala:1585
26/01/04 17:34:42 INFO BlockManagerInfo: Removed broadcast_629_piece0 on spark-master:33535 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:34:42 INFO BlockManagerInfo: Removed broadcast_629_piece0 on 172.18.0.13:44401 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:34:42 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 422 (MapPartitionsRDD[2113] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 17:34:42 INFO TaskSchedulerImpl: Adding task set 422.0 with 2 tasks resource profile 0
26/01/04 17:34:42 INFO TaskSetManager: Starting task 1.0 in stage 422.0 (TID 668) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 17:34:42 INFO TaskSetManager: Starting task 0.0 in stage 422.0 (TID 669) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 17:34:42 INFO BlockManagerInfo: Added broadcast_630_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:34:42 INFO BlockManagerInfo: Removed broadcast_628_piece0 on spark-master:33535 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:34:42 INFO BlockManagerInfo: Removed broadcast_628_piece0 on 172.18.0.13:44401 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:34:42 INFO TaskSetManager: Finished task 1.0 in stage 422.0 (TID 668) in 31 ms on 172.18.0.13 (executor 0) (1/2)
26/01/04 17:34:42 INFO BlockManagerInfo: Added broadcast_630_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:34:42 INFO TaskSetManager: Finished task 0.0 in stage 422.0 (TID 669) in 93 ms on 172.18.0.14 (executor 1) (2/2)
26/01/04 17:34:42 INFO TaskSchedulerImpl: Removed TaskSet 422.0, whose tasks have all completed, from pool 
26/01/04 17:34:42 INFO DAGScheduler: ResultStage 422 (start at NativeMethodAccessorImpl.java:0) finished in 0.116 s
26/01/04 17:34:42 INFO DAGScheduler: Job 421 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:34:42 INFO TaskSchedulerImpl: Killing all running tasks in stage 422: Stage finished
26/01/04 17:34:42 INFO DAGScheduler: Job 421 finished: start at NativeMethodAccessorImpl.java:0, took 0.120222 s
26/01/04 17:34:42 INFO MemoryStore: Block broadcast_631_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.3 MiB)
26/01/04 17:34:42 INFO BlockManagerInfo: Added broadcast_631_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:34:42 INFO SparkContext: Created broadcast 631 from start at NativeMethodAccessorImpl.java:0
26/01/04 17:34:42 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 209, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@4fe8fef6]. The input RDD has 1 partitions.
26/01/04 17:34:42 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:34:42 INFO DAGScheduler: Got job 422 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/01/04 17:34:42 INFO DAGScheduler: Final stage: ResultStage 423 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:34:42 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:34:42 INFO DAGScheduler: Missing parents: List()
26/01/04 17:34:42 INFO DAGScheduler: Submitting ResultStage 423 (MapPartitionsRDD[2118] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:34:42 INFO MemoryStore: Block broadcast_632 stored as values in memory (estimated size 52.9 KiB, free 434.3 MiB)
26/01/04 17:34:42 INFO MemoryStore: Block broadcast_632_piece0 stored as bytes in memory (estimated size 20.2 KiB, free 434.3 MiB)
26/01/04 17:34:42 INFO BlockManagerInfo: Added broadcast_632_piece0 in memory on spark-master:33535 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:34:42 INFO SparkContext: Created broadcast 632 from broadcast at DAGScheduler.scala:1585
26/01/04 17:34:42 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 423 (MapPartitionsRDD[2118] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/01/04 17:34:42 INFO TaskSchedulerImpl: Adding task set 423.0 with 1 tasks resource profile 0
26/01/04 17:34:42 INFO TaskSetManager: Starting task 0.0 in stage 423.0 (TID 670) (172.18.0.13, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:34:42 INFO BlockManagerInfo: Added broadcast_632_piece0 in memory on 172.18.0.13:44401 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:34:42 INFO BlockManagerInfo: Added broadcast_631_piece0 in memory on 172.18.0.13:44401 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:34:43 INFO TaskSetManager: Finished task 0.0 in stage 423.0 (TID 670) in 561 ms on 172.18.0.13 (executor 0) (1/1)
26/01/04 17:34:43 INFO TaskSchedulerImpl: Removed TaskSet 423.0, whose tasks have all completed, from pool 
26/01/04 17:34:43 INFO DAGScheduler: ResultStage 423 (start at NativeMethodAccessorImpl.java:0) finished in 0.568 s
26/01/04 17:34:43 INFO DAGScheduler: Job 422 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:34:43 INFO TaskSchedulerImpl: Killing all running tasks in stage 423: Stage finished
26/01/04 17:34:43 INFO DAGScheduler: Job 422 finished: start at NativeMethodAccessorImpl.java:0, took 0.570540 s
26/01/04 17:34:43 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 209, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@4fe8fef6] is committing.
26/01/04 17:34:43 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 209, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@4fe8fef6] committed.
26/01/04 17:34:43 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/209 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.209.8cc62a77-8fcb-448a-af58-01394a9cc6c5.tmp
26/01/04 17:34:43 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.209.8cc62a77-8fcb-448a-af58-01394a9cc6c5.tmp to file:/tmp/spark-checkpoint-enrichment/commits/209
26/01/04 17:34:43 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T17:34:42.119Z",
  "batchId" : 209,
  "numInputRows" : 1,
  "inputRowsPerSecond" : 76.92307692307692,
  "processedRowsPerSecond" : 0.9891196834817014,
  "durationMs" : {
    "addBatch" : 795,
    "commitOffsets" : 65,
    "getBatch" : 0,
    "latestOffset" : 13,
    "queryPlanning" : 31,
    "triggerExecution" : 1011,
    "walCommit" : 107
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "0" : 1590
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "0" : 1591
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "0" : 1591
      }
    },
    "numInputRows" : 1,
    "inputRowsPerSecond" : 76.92307692307692,
    "processedRowsPerSecond" : 0.9891196834817014,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 1
  }
}
26/01/04 17:34:43 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/210 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.210.986181c0-721f-4a7b-8ab0-f7299a295e9e.tmp
26/01/04 17:34:43 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.210.986181c0-721f-4a7b-8ab0-f7299a295e9e.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/210
26/01/04 17:34:43 INFO MicroBatchExecution: Committed offsets for batch 210. Metadata OffsetSeqMetadata(0,1767548083133,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 17:34:43 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:34:43 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:34:43 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:34:43 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:34:43 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:34:43 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:34:43 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#170323 - airline_prefix.nullCount#170322) > 0)
26/01/04 17:34:43 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#170358 - min_flight_num.nullCount#170357) > 0)
26/01/04 17:34:43 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#170353 - max_flight_num.nullCount#170352) > 0)
26/01/04 17:34:43 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:34:43 INFO DAGScheduler: Got job 423 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 17:34:43 INFO DAGScheduler: Final stage: ResultStage 424 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:34:43 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:34:43 INFO DAGScheduler: Missing parents: List()
26/01/04 17:34:43 INFO DAGScheduler: Submitting ResultStage 424 (MapPartitionsRDD[2123] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:34:43 INFO MemoryStore: Block broadcast_633 stored as values in memory (estimated size 38.5 KiB, free 434.2 MiB)
26/01/04 17:34:43 INFO MemoryStore: Block broadcast_633_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.2 MiB)
26/01/04 17:34:43 INFO BlockManagerInfo: Added broadcast_633_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:34:43 INFO SparkContext: Created broadcast 633 from broadcast at DAGScheduler.scala:1585
26/01/04 17:34:43 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 424 (MapPartitionsRDD[2123] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 17:34:43 INFO TaskSchedulerImpl: Adding task set 424.0 with 2 tasks resource profile 0
26/01/04 17:34:43 INFO TaskSetManager: Starting task 0.0 in stage 424.0 (TID 671) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 17:34:43 INFO TaskSetManager: Starting task 1.0 in stage 424.0 (TID 672) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 17:34:43 INFO BlockManagerInfo: Added broadcast_633_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:34:43 INFO BlockManagerInfo: Added broadcast_633_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:34:43 INFO TaskSetManager: Finished task 1.0 in stage 424.0 (TID 672) in 13 ms on 172.18.0.13 (executor 0) (1/2)
26/01/04 17:34:43 INFO TaskSetManager: Finished task 0.0 in stage 424.0 (TID 671) in 36 ms on 172.18.0.14 (executor 1) (2/2)
26/01/04 17:34:43 INFO TaskSchedulerImpl: Removed TaskSet 424.0, whose tasks have all completed, from pool 
26/01/04 17:34:43 INFO DAGScheduler: ResultStage 424 (start at NativeMethodAccessorImpl.java:0) finished in 0.044 s
26/01/04 17:34:43 INFO DAGScheduler: Job 423 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:34:43 INFO TaskSchedulerImpl: Killing all running tasks in stage 424: Stage finished
26/01/04 17:34:43 INFO DAGScheduler: Job 423 finished: start at NativeMethodAccessorImpl.java:0, took 0.045907 s
26/01/04 17:34:43 INFO MemoryStore: Block broadcast_634_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.2 MiB)
26/01/04 17:34:43 INFO BlockManagerInfo: Added broadcast_634_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:34:43 INFO SparkContext: Created broadcast 634 from start at NativeMethodAccessorImpl.java:0
26/01/04 17:34:43 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 210, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@65e62075]. The input RDD has 1 partitions.
26/01/04 17:34:43 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:34:43 INFO DAGScheduler: Got job 424 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/01/04 17:34:43 INFO DAGScheduler: Final stage: ResultStage 425 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:34:43 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:34:43 INFO DAGScheduler: Missing parents: List()
26/01/04 17:34:43 INFO DAGScheduler: Submitting ResultStage 425 (MapPartitionsRDD[2128] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:34:43 INFO MemoryStore: Block broadcast_635 stored as values in memory (estimated size 52.9 KiB, free 434.2 MiB)
26/01/04 17:34:43 INFO MemoryStore: Block broadcast_635_piece0 stored as bytes in memory (estimated size 20.2 KiB, free 434.1 MiB)
26/01/04 17:34:43 INFO BlockManagerInfo: Added broadcast_635_piece0 in memory on spark-master:33535 (size: 20.2 KiB, free: 434.3 MiB)
26/01/04 17:34:43 INFO SparkContext: Created broadcast 635 from broadcast at DAGScheduler.scala:1585
26/01/04 17:34:43 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 425 (MapPartitionsRDD[2128] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/01/04 17:34:43 INFO TaskSchedulerImpl: Adding task set 425.0 with 1 tasks resource profile 0
26/01/04 17:34:43 INFO TaskSetManager: Starting task 0.0 in stage 425.0 (TID 673) (172.18.0.13, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:34:43 INFO BlockManagerInfo: Added broadcast_635_piece0 in memory on 172.18.0.13:44401 (size: 20.2 KiB, free: 434.3 MiB)
26/01/04 17:34:43 INFO BlockManagerInfo: Added broadcast_634_piece0 in memory on 172.18.0.13:44401 (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:34:43 INFO TaskSetManager: Finished task 0.0 in stage 425.0 (TID 673) in 41 ms on 172.18.0.13 (executor 0) (1/1)
26/01/04 17:34:43 INFO TaskSchedulerImpl: Removed TaskSet 425.0, whose tasks have all completed, from pool 
26/01/04 17:34:43 INFO DAGScheduler: ResultStage 425 (start at NativeMethodAccessorImpl.java:0) finished in 0.070 s
26/01/04 17:34:43 INFO DAGScheduler: Job 424 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:34:43 INFO TaskSchedulerImpl: Killing all running tasks in stage 425: Stage finished
26/01/04 17:34:43 INFO DAGScheduler: Job 424 finished: start at NativeMethodAccessorImpl.java:0, took 0.071509 s
26/01/04 17:34:43 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 210, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@65e62075] is committing.
26/01/04 17:34:43 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 210, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@65e62075] committed.
26/01/04 17:34:43 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/210 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.210.8e8f0a78-3eeb-4893-a5f1-ecf2c1060abf.tmp
26/01/04 17:34:43 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.210.8e8f0a78-3eeb-4893-a5f1-ecf2c1060abf.tmp to file:/tmp/spark-checkpoint-enrichment/commits/210
26/01/04 17:34:43 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T17:34:43.130Z",
  "batchId" : 210,
  "numInputRows" : 29,
  "inputRowsPerSecond" : 28.68447082096934,
  "processedRowsPerSecond" : 75.91623036649214,
  "durationMs" : {
    "addBatch" : 208,
    "commitOffsets" : 70,
    "getBatch" : 0,
    "latestOffset" : 3,
    "queryPlanning" : 22,
    "triggerExecution" : 382,
    "walCommit" : 78
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "0" : 1591
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "0" : 1620
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "0" : 1620
      }
    },
    "numInputRows" : 29,
    "inputRowsPerSecond" : 28.68447082096934,
    "processedRowsPerSecond" : 75.91623036649214,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 30
  }
}
26/01/04 17:34:53 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/211 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.211.ef4dec8b-11ee-44bc-ad09-ad1852610bb6.tmp
26/01/04 17:34:53 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.211.ef4dec8b-11ee-44bc-ad09-ad1852610bb6.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/211
26/01/04 17:34:53 INFO MicroBatchExecution: Committed offsets for batch 211. Metadata OffsetSeqMetadata(0,1767548093164,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 17:34:53 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:34:53 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:34:53 INFO BlockManagerInfo: Removed broadcast_631_piece0 on spark-master:33535 in memory (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:34:53 INFO BlockManagerInfo: Removed broadcast_631_piece0 on 172.18.0.13:44401 in memory (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:34:53 INFO BlockManagerInfo: Removed broadcast_634_piece0 on spark-master:33535 in memory (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:34:53 INFO BlockManagerInfo: Removed broadcast_634_piece0 on 172.18.0.13:44401 in memory (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:34:53 INFO BlockManagerInfo: Removed broadcast_635_piece0 on spark-master:33535 in memory (size: 20.2 KiB, free: 434.3 MiB)
26/01/04 17:34:53 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:34:53 INFO BlockManagerInfo: Removed broadcast_635_piece0 on 172.18.0.13:44401 in memory (size: 20.2 KiB, free: 434.3 MiB)
26/01/04 17:34:53 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:34:53 INFO BlockManagerInfo: Removed broadcast_632_piece0 on spark-master:33535 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:34:53 INFO BlockManagerInfo: Removed broadcast_632_piece0 on 172.18.0.13:44401 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:34:53 INFO BlockManagerInfo: Removed broadcast_633_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:34:53 INFO BlockManagerInfo: Removed broadcast_633_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:34:53 INFO BlockManagerInfo: Removed broadcast_633_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:34:53 INFO BlockManagerInfo: Removed broadcast_630_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:34:53 INFO BlockManagerInfo: Removed broadcast_630_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:34:53 INFO BlockManagerInfo: Removed broadcast_630_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:34:53 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:34:53 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:34:53 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#171127 - airline_prefix.nullCount#171126) > 0)
26/01/04 17:34:53 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#171162 - min_flight_num.nullCount#171161) > 0)
26/01/04 17:34:53 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#171157 - max_flight_num.nullCount#171156) > 0)
26/01/04 17:34:53 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:34:53 INFO DAGScheduler: Got job 425 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 17:34:53 INFO DAGScheduler: Final stage: ResultStage 426 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:34:53 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:34:53 INFO DAGScheduler: Missing parents: List()
26/01/04 17:34:53 INFO DAGScheduler: Submitting ResultStage 426 (MapPartitionsRDD[2133] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:34:53 INFO MemoryStore: Block broadcast_636 stored as values in memory (estimated size 38.5 KiB, free 434.4 MiB)
26/01/04 17:34:53 INFO MemoryStore: Block broadcast_636_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.3 MiB)
26/01/04 17:34:53 INFO BlockManagerInfo: Added broadcast_636_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:34:53 INFO SparkContext: Created broadcast 636 from broadcast at DAGScheduler.scala:1585
26/01/04 17:34:53 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 426 (MapPartitionsRDD[2133] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 17:34:53 INFO TaskSchedulerImpl: Adding task set 426.0 with 2 tasks resource profile 0
26/01/04 17:34:53 INFO TaskSetManager: Starting task 1.0 in stage 426.0 (TID 674) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 17:34:53 INFO TaskSetManager: Starting task 0.0 in stage 426.0 (TID 675) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 17:34:53 INFO BlockManagerInfo: Added broadcast_636_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:34:53 INFO BlockManagerInfo: Added broadcast_636_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:34:53 INFO TaskSetManager: Finished task 1.0 in stage 426.0 (TID 674) in 15 ms on 172.18.0.13 (executor 0) (1/2)
26/01/04 17:34:53 INFO TaskSetManager: Finished task 0.0 in stage 426.0 (TID 675) in 122 ms on 172.18.0.14 (executor 1) (2/2)
26/01/04 17:34:53 INFO TaskSchedulerImpl: Removed TaskSet 426.0, whose tasks have all completed, from pool 
26/01/04 17:34:53 INFO DAGScheduler: ResultStage 426 (start at NativeMethodAccessorImpl.java:0) finished in 0.128 s
26/01/04 17:34:53 INFO DAGScheduler: Job 425 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:34:53 INFO TaskSchedulerImpl: Killing all running tasks in stage 426: Stage finished
26/01/04 17:34:53 INFO DAGScheduler: Job 425 finished: start at NativeMethodAccessorImpl.java:0, took 0.129023 s
26/01/04 17:34:53 INFO MemoryStore: Block broadcast_637_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.3 MiB)
26/01/04 17:34:53 INFO BlockManagerInfo: Added broadcast_637_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:34:53 INFO SparkContext: Created broadcast 637 from start at NativeMethodAccessorImpl.java:0
26/01/04 17:34:53 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 211, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@34852eb2]. The input RDD has 1 partitions.
26/01/04 17:34:53 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:34:53 INFO DAGScheduler: Got job 426 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/01/04 17:34:53 INFO DAGScheduler: Final stage: ResultStage 427 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:34:53 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:34:53 INFO DAGScheduler: Missing parents: List()
26/01/04 17:34:53 INFO DAGScheduler: Submitting ResultStage 427 (MapPartitionsRDD[2138] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:34:53 INFO MemoryStore: Block broadcast_638 stored as values in memory (estimated size 52.9 KiB, free 434.3 MiB)
26/01/04 17:34:53 INFO MemoryStore: Block broadcast_638_piece0 stored as bytes in memory (estimated size 20.2 KiB, free 434.3 MiB)
26/01/04 17:34:53 INFO BlockManagerInfo: Added broadcast_638_piece0 in memory on spark-master:33535 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:34:53 INFO SparkContext: Created broadcast 638 from broadcast at DAGScheduler.scala:1585
26/01/04 17:34:53 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 427 (MapPartitionsRDD[2138] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/01/04 17:34:53 INFO TaskSchedulerImpl: Adding task set 427.0 with 1 tasks resource profile 0
26/01/04 17:34:53 INFO TaskSetManager: Starting task 0.0 in stage 427.0 (TID 676) (172.18.0.13, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:34:53 INFO BlockManagerInfo: Added broadcast_638_piece0 in memory on 172.18.0.13:44401 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:34:53 INFO BlockManagerInfo: Added broadcast_637_piece0 in memory on 172.18.0.13:44401 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:34:54 INFO TaskSetManager: Finished task 0.0 in stage 427.0 (TID 676) in 547 ms on 172.18.0.13 (executor 0) (1/1)
26/01/04 17:34:54 INFO TaskSchedulerImpl: Removed TaskSet 427.0, whose tasks have all completed, from pool 
26/01/04 17:34:54 INFO DAGScheduler: ResultStage 427 (start at NativeMethodAccessorImpl.java:0) finished in 0.553 s
26/01/04 17:34:54 INFO DAGScheduler: Job 426 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:34:54 INFO TaskSchedulerImpl: Killing all running tasks in stage 427: Stage finished
26/01/04 17:34:54 INFO DAGScheduler: Job 426 finished: start at NativeMethodAccessorImpl.java:0, took 0.554256 s
26/01/04 17:34:54 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 211, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@34852eb2] is committing.
26/01/04 17:34:54 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 211, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@34852eb2] committed.
26/01/04 17:34:54 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/211 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.211.23a2e98b-5ab2-4676-8d81-e92101b60b9b.tmp
26/01/04 17:34:54 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.211.23a2e98b-5ab2-4676-8d81-e92101b60b9b.tmp to file:/tmp/spark-checkpoint-enrichment/commits/211
26/01/04 17:34:54 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T17:34:53.162Z",
  "batchId" : 211,
  "numInputRows" : 30,
  "inputRowsPerSecond" : 2500.0,
  "processedRowsPerSecond" : 31.41361256544503,
  "durationMs" : {
    "addBatch" : 776,
    "commitOffsets" : 64,
    "getBatch" : 0,
    "latestOffset" : 2,
    "queryPlanning" : 44,
    "triggerExecution" : 955,
    "walCommit" : 68
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "0" : 1620
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "0" : 1650
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "0" : 1650
      }
    },
    "numInputRows" : 30,
    "inputRowsPerSecond" : 2500.0,
    "processedRowsPerSecond" : 31.41361256544503,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 31
  }
}
26/01/04 17:35:04 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/01/04 17:35:04 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/212 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.212.2e9b6a7e-4324-4696-8523-7ffaf6986cea.tmp
26/01/04 17:35:04 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.212.2e9b6a7e-4324-4696-8523-7ffaf6986cea.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/212
26/01/04 17:35:04 INFO MicroBatchExecution: Committed offsets for batch 212. Metadata OffsetSeqMetadata(0,1767548104170,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 17:35:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:35:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:35:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:35:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:35:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:35:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:35:04 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#171931 - airline_prefix.nullCount#171930) > 0)
26/01/04 17:35:04 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#171966 - min_flight_num.nullCount#171965) > 0)
26/01/04 17:35:04 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#171961 - max_flight_num.nullCount#171960) > 0)
26/01/04 17:35:04 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:35:04 INFO DAGScheduler: Got job 427 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 17:35:04 INFO DAGScheduler: Final stage: ResultStage 428 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:35:04 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:35:04 INFO DAGScheduler: Missing parents: List()
26/01/04 17:35:04 INFO DAGScheduler: Submitting ResultStage 428 (MapPartitionsRDD[2143] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:35:04 INFO MemoryStore: Block broadcast_639 stored as values in memory (estimated size 38.5 KiB, free 434.2 MiB)
26/01/04 17:35:04 INFO MemoryStore: Block broadcast_639_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.2 MiB)
26/01/04 17:35:04 INFO BlockManagerInfo: Added broadcast_639_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:35:04 INFO SparkContext: Created broadcast 639 from broadcast at DAGScheduler.scala:1585
26/01/04 17:35:04 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 428 (MapPartitionsRDD[2143] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 17:35:04 INFO TaskSchedulerImpl: Adding task set 428.0 with 2 tasks resource profile 0
26/01/04 17:35:04 INFO TaskSetManager: Starting task 0.0 in stage 428.0 (TID 677) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 17:35:04 INFO TaskSetManager: Starting task 1.0 in stage 428.0 (TID 678) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 17:35:04 INFO BlockManagerInfo: Added broadcast_639_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:35:04 INFO TaskSetManager: Finished task 1.0 in stage 428.0 (TID 678) in 17 ms on 172.18.0.13 (executor 0) (1/2)
26/01/04 17:35:04 INFO BlockManagerInfo: Added broadcast_639_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:35:04 INFO TaskSetManager: Finished task 0.0 in stage 428.0 (TID 677) in 39 ms on 172.18.0.14 (executor 1) (2/2)
26/01/04 17:35:04 INFO TaskSchedulerImpl: Removed TaskSet 428.0, whose tasks have all completed, from pool 
26/01/04 17:35:04 INFO DAGScheduler: ResultStage 428 (start at NativeMethodAccessorImpl.java:0) finished in 0.045 s
26/01/04 17:35:04 INFO DAGScheduler: Job 427 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:35:04 INFO TaskSchedulerImpl: Killing all running tasks in stage 428: Stage finished
26/01/04 17:35:04 INFO DAGScheduler: Job 427 finished: start at NativeMethodAccessorImpl.java:0, took 0.046274 s
26/01/04 17:35:04 INFO MemoryStore: Block broadcast_640_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.2 MiB)
26/01/04 17:35:04 INFO BlockManagerInfo: Added broadcast_640_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:35:04 INFO SparkContext: Created broadcast 640 from start at NativeMethodAccessorImpl.java:0
26/01/04 17:35:04 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 212, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@615abcde]. The input RDD has 1 partitions.
26/01/04 17:35:04 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:35:04 INFO DAGScheduler: Got job 428 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/01/04 17:35:04 INFO DAGScheduler: Final stage: ResultStage 429 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:35:04 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:35:04 INFO DAGScheduler: Missing parents: List()
26/01/04 17:35:04 INFO DAGScheduler: Submitting ResultStage 429 (MapPartitionsRDD[2148] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:35:04 INFO MemoryStore: Block broadcast_641 stored as values in memory (estimated size 52.9 KiB, free 434.2 MiB)
26/01/04 17:35:04 INFO MemoryStore: Block broadcast_641_piece0 stored as bytes in memory (estimated size 20.2 KiB, free 434.1 MiB)
26/01/04 17:35:04 INFO BlockManagerInfo: Added broadcast_641_piece0 in memory on spark-master:33535 (size: 20.2 KiB, free: 434.3 MiB)
26/01/04 17:35:04 INFO BlockManagerInfo: Removed broadcast_639_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:35:04 INFO SparkContext: Created broadcast 641 from broadcast at DAGScheduler.scala:1585
26/01/04 17:35:04 INFO BlockManagerInfo: Removed broadcast_639_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:35:04 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 429 (MapPartitionsRDD[2148] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/01/04 17:35:04 INFO TaskSchedulerImpl: Adding task set 429.0 with 1 tasks resource profile 0
26/01/04 17:35:04 INFO TaskSetManager: Starting task 0.0 in stage 429.0 (TID 679) (172.18.0.13, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:35:04 INFO BlockManagerInfo: Removed broadcast_639_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:35:04 INFO BlockManagerInfo: Removed broadcast_637_piece0 on spark-master:33535 in memory (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:35:04 INFO BlockManagerInfo: Removed broadcast_637_piece0 on 172.18.0.13:44401 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:35:04 INFO BlockManagerInfo: Added broadcast_641_piece0 in memory on 172.18.0.13:44401 (size: 20.2 KiB, free: 434.3 MiB)
26/01/04 17:35:04 INFO BlockManagerInfo: Removed broadcast_638_piece0 on spark-master:33535 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:35:04 INFO BlockManagerInfo: Removed broadcast_638_piece0 on 172.18.0.13:44401 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:35:04 INFO BlockManagerInfo: Removed broadcast_636_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:35:04 INFO BlockManagerInfo: Removed broadcast_636_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:35:04 INFO BlockManagerInfo: Added broadcast_640_piece0 in memory on 172.18.0.13:44401 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:35:04 INFO BlockManagerInfo: Removed broadcast_636_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:35:04 INFO TaskSetManager: Finished task 0.0 in stage 429.0 (TID 679) in 544 ms on 172.18.0.13 (executor 0) (1/1)
26/01/04 17:35:04 INFO TaskSchedulerImpl: Removed TaskSet 429.0, whose tasks have all completed, from pool 
26/01/04 17:35:04 INFO DAGScheduler: ResultStage 429 (start at NativeMethodAccessorImpl.java:0) finished in 0.558 s
26/01/04 17:35:04 INFO DAGScheduler: Job 428 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:35:04 INFO TaskSchedulerImpl: Killing all running tasks in stage 429: Stage finished
26/01/04 17:35:04 INFO DAGScheduler: Job 428 finished: start at NativeMethodAccessorImpl.java:0, took 0.559369 s
26/01/04 17:35:04 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 212, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@615abcde] is committing.
26/01/04 17:35:04 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 212, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@615abcde] committed.
26/01/04 17:35:04 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/212 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.212.82fea99d-5b09-4fe7-82cc-1a879ec700b2.tmp
26/01/04 17:35:05 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.212.82fea99d-5b09-4fe7-82cc-1a879ec700b2.tmp to file:/tmp/spark-checkpoint-enrichment/commits/212
26/01/04 17:35:05 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T17:35:04.168Z",
  "batchId" : 212,
  "numInputRows" : 30,
  "inputRowsPerSecond" : 2500.0,
  "processedRowsPerSecond" : 34.32494279176201,
  "durationMs" : {
    "addBatch" : 669,
    "commitOffsets" : 69,
    "getBatch" : 0,
    "latestOffset" : 2,
    "queryPlanning" : 25,
    "triggerExecution" : 874,
    "walCommit" : 108
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "0" : 1650
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "0" : 1680
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "0" : 1680
      }
    },
    "numInputRows" : 30,
    "inputRowsPerSecond" : 2500.0,
    "processedRowsPerSecond" : 34.32494279176201,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 31
  }
}
26/01/04 17:35:15 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/01/04 17:35:15 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/213 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.213.a2ef0012-8aa5-4bfe-a942-0774885591aa.tmp
26/01/04 17:35:15 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.213.a2ef0012-8aa5-4bfe-a942-0774885591aa.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/213
26/01/04 17:35:15 INFO MicroBatchExecution: Committed offsets for batch 213. Metadata OffsetSeqMetadata(0,1767548115184,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 17:35:15 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:35:15 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:35:15 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:35:15 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:35:15 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:35:15 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:35:15 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#172735 - airline_prefix.nullCount#172734) > 0)
26/01/04 17:35:15 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#172770 - min_flight_num.nullCount#172769) > 0)
26/01/04 17:35:15 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#172765 - max_flight_num.nullCount#172764) > 0)
26/01/04 17:35:15 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:35:15 INFO DAGScheduler: Got job 429 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 17:35:15 INFO DAGScheduler: Final stage: ResultStage 430 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:35:15 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:35:15 INFO DAGScheduler: Missing parents: List()
26/01/04 17:35:15 INFO DAGScheduler: Submitting ResultStage 430 (MapPartitionsRDD[2153] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:35:15 INFO MemoryStore: Block broadcast_642 stored as values in memory (estimated size 38.5 KiB, free 434.3 MiB)
26/01/04 17:35:15 INFO MemoryStore: Block broadcast_642_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.3 MiB)
26/01/04 17:35:15 INFO BlockManagerInfo: Added broadcast_642_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:35:15 INFO BlockManagerInfo: Removed broadcast_641_piece0 on spark-master:33535 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:35:15 INFO SparkContext: Created broadcast 642 from broadcast at DAGScheduler.scala:1585
26/01/04 17:35:15 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 430 (MapPartitionsRDD[2153] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 17:35:15 INFO TaskSchedulerImpl: Adding task set 430.0 with 2 tasks resource profile 0
26/01/04 17:35:15 INFO BlockManagerInfo: Removed broadcast_641_piece0 on 172.18.0.13:44401 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:35:15 INFO TaskSetManager: Starting task 1.0 in stage 430.0 (TID 680) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 17:35:15 INFO TaskSetManager: Starting task 0.0 in stage 430.0 (TID 681) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 17:35:15 INFO BlockManagerInfo: Removed broadcast_640_piece0 on spark-master:33535 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:35:15 INFO BlockManagerInfo: Removed broadcast_640_piece0 on 172.18.0.13:44401 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:35:15 INFO BlockManagerInfo: Added broadcast_642_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:35:15 INFO BlockManagerInfo: Added broadcast_642_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:35:15 INFO TaskSetManager: Finished task 1.0 in stage 430.0 (TID 680) in 17 ms on 172.18.0.13 (executor 0) (1/2)
26/01/04 17:35:15 INFO TaskSetManager: Finished task 0.0 in stage 430.0 (TID 681) in 43 ms on 172.18.0.14 (executor 1) (2/2)
26/01/04 17:35:15 INFO TaskSchedulerImpl: Removed TaskSet 430.0, whose tasks have all completed, from pool 
26/01/04 17:35:15 INFO DAGScheduler: ResultStage 430 (start at NativeMethodAccessorImpl.java:0) finished in 0.055 s
26/01/04 17:35:15 INFO DAGScheduler: Job 429 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:35:15 INFO TaskSchedulerImpl: Killing all running tasks in stage 430: Stage finished
26/01/04 17:35:15 INFO DAGScheduler: Job 429 finished: start at NativeMethodAccessorImpl.java:0, took 0.056134 s
26/01/04 17:35:15 INFO MemoryStore: Block broadcast_643_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.3 MiB)
26/01/04 17:35:15 INFO BlockManagerInfo: Added broadcast_643_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:35:15 INFO SparkContext: Created broadcast 643 from start at NativeMethodAccessorImpl.java:0
26/01/04 17:35:15 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 213, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@2afba84e]. The input RDD has 1 partitions.
26/01/04 17:35:15 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:35:15 INFO DAGScheduler: Got job 430 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/01/04 17:35:15 INFO DAGScheduler: Final stage: ResultStage 431 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:35:15 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:35:15 INFO DAGScheduler: Missing parents: List()
26/01/04 17:35:15 INFO DAGScheduler: Submitting ResultStage 431 (MapPartitionsRDD[2158] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:35:15 INFO MemoryStore: Block broadcast_644 stored as values in memory (estimated size 52.9 KiB, free 434.3 MiB)
26/01/04 17:35:15 INFO MemoryStore: Block broadcast_644_piece0 stored as bytes in memory (estimated size 20.2 KiB, free 434.3 MiB)
26/01/04 17:35:15 INFO BlockManagerInfo: Removed broadcast_642_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:35:15 INFO BlockManagerInfo: Added broadcast_644_piece0 in memory on spark-master:33535 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:35:15 INFO SparkContext: Created broadcast 644 from broadcast at DAGScheduler.scala:1585
26/01/04 17:35:15 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 431 (MapPartitionsRDD[2158] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/01/04 17:35:15 INFO TaskSchedulerImpl: Adding task set 431.0 with 1 tasks resource profile 0
26/01/04 17:35:15 INFO BlockManagerInfo: Removed broadcast_642_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:35:15 INFO TaskSetManager: Starting task 0.0 in stage 431.0 (TID 682) (172.18.0.13, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:35:15 INFO BlockManagerInfo: Added broadcast_644_piece0 in memory on 172.18.0.13:44401 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:35:15 INFO BlockManagerInfo: Removed broadcast_642_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:35:15 INFO BlockManagerInfo: Added broadcast_643_piece0 in memory on 172.18.0.13:44401 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:35:15 INFO TaskSetManager: Finished task 0.0 in stage 431.0 (TID 682) in 541 ms on 172.18.0.13 (executor 0) (1/1)
26/01/04 17:35:15 INFO TaskSchedulerImpl: Removed TaskSet 431.0, whose tasks have all completed, from pool 
26/01/04 17:35:15 INFO DAGScheduler: ResultStage 431 (start at NativeMethodAccessorImpl.java:0) finished in 0.551 s
26/01/04 17:35:15 INFO DAGScheduler: Job 430 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:35:15 INFO TaskSchedulerImpl: Killing all running tasks in stage 431: Stage finished
26/01/04 17:35:15 INFO DAGScheduler: Job 430 finished: start at NativeMethodAccessorImpl.java:0, took 0.552043 s
26/01/04 17:35:15 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 213, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@2afba84e] is committing.
26/01/04 17:35:15 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 213, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@2afba84e] committed.
26/01/04 17:35:15 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/213 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.213.e4a2c344-c823-467e-a18f-dd0284badbd1.tmp
26/01/04 17:35:16 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.213.e4a2c344-c823-467e-a18f-dd0284badbd1.tmp to file:/tmp/spark-checkpoint-enrichment/commits/213
26/01/04 17:35:16 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T17:35:15.182Z",
  "batchId" : 213,
  "numInputRows" : 30,
  "inputRowsPerSecond" : 2500.0,
  "processedRowsPerSecond" : 36.014405762304925,
  "durationMs" : {
    "addBatch" : 667,
    "commitOffsets" : 69,
    "getBatch" : 0,
    "latestOffset" : 2,
    "queryPlanning" : 19,
    "triggerExecution" : 833,
    "walCommit" : 76
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "0" : 1680
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "0" : 1710
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "0" : 1710
      }
    },
    "numInputRows" : 30,
    "inputRowsPerSecond" : 2500.0,
    "processedRowsPerSecond" : 36.014405762304925,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 31
  }
}
26/01/04 17:35:26 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/01/04 17:35:26 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/214 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.214.ba8a0b64-6323-4045-9d35-4bc7b802c9d2.tmp
26/01/04 17:35:26 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.214.ba8a0b64-6323-4045-9d35-4bc7b802c9d2.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/214
26/01/04 17:35:26 INFO MicroBatchExecution: Committed offsets for batch 214. Metadata OffsetSeqMetadata(0,1767548126207,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 17:35:26 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:35:26 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:35:26 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:35:26 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:35:26 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:35:26 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:35:26 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#173539 - airline_prefix.nullCount#173538) > 0)
26/01/04 17:35:26 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#173574 - min_flight_num.nullCount#173573) > 0)
26/01/04 17:35:26 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#173569 - max_flight_num.nullCount#173568) > 0)
26/01/04 17:35:26 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:35:26 INFO DAGScheduler: Got job 431 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 17:35:26 INFO DAGScheduler: Final stage: ResultStage 432 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:35:26 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:35:26 INFO DAGScheduler: Missing parents: List()
26/01/04 17:35:26 INFO DAGScheduler: Submitting ResultStage 432 (MapPartitionsRDD[2163] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:35:26 INFO MemoryStore: Block broadcast_645 stored as values in memory (estimated size 38.5 KiB, free 434.3 MiB)
26/01/04 17:35:26 INFO MemoryStore: Block broadcast_645_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.3 MiB)
26/01/04 17:35:26 INFO BlockManagerInfo: Added broadcast_645_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:35:26 INFO SparkContext: Created broadcast 645 from broadcast at DAGScheduler.scala:1585
26/01/04 17:35:26 INFO BlockManagerInfo: Removed broadcast_644_piece0 on spark-master:33535 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:35:26 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 432 (MapPartitionsRDD[2163] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 17:35:26 INFO TaskSchedulerImpl: Adding task set 432.0 with 2 tasks resource profile 0
26/01/04 17:35:26 INFO BlockManagerInfo: Removed broadcast_644_piece0 on 172.18.0.13:44401 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:35:26 INFO TaskSetManager: Starting task 0.0 in stage 432.0 (TID 683) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 17:35:26 INFO TaskSetManager: Starting task 1.0 in stage 432.0 (TID 684) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 17:35:26 INFO BlockManagerInfo: Removed broadcast_643_piece0 on spark-master:33535 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:35:26 INFO BlockManagerInfo: Removed broadcast_643_piece0 on 172.18.0.13:44401 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:35:26 INFO BlockManagerInfo: Added broadcast_645_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:35:26 INFO BlockManagerInfo: Added broadcast_645_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:35:26 INFO TaskSetManager: Finished task 1.0 in stage 432.0 (TID 684) in 21 ms on 172.18.0.13 (executor 0) (1/2)
26/01/04 17:35:26 INFO TaskSetManager: Finished task 0.0 in stage 432.0 (TID 683) in 48 ms on 172.18.0.14 (executor 1) (2/2)
26/01/04 17:35:26 INFO TaskSchedulerImpl: Removed TaskSet 432.0, whose tasks have all completed, from pool 
26/01/04 17:35:26 INFO DAGScheduler: ResultStage 432 (start at NativeMethodAccessorImpl.java:0) finished in 0.058 s
26/01/04 17:35:26 INFO DAGScheduler: Job 431 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:35:26 INFO TaskSchedulerImpl: Killing all running tasks in stage 432: Stage finished
26/01/04 17:35:26 INFO DAGScheduler: Job 431 finished: start at NativeMethodAccessorImpl.java:0, took 0.059025 s
26/01/04 17:35:26 INFO MemoryStore: Block broadcast_646_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.3 MiB)
26/01/04 17:35:26 INFO BlockManagerInfo: Added broadcast_646_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:35:26 INFO SparkContext: Created broadcast 646 from start at NativeMethodAccessorImpl.java:0
26/01/04 17:35:26 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 214, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@749efcb4]. The input RDD has 1 partitions.
26/01/04 17:35:26 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:35:26 INFO DAGScheduler: Got job 432 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/01/04 17:35:26 INFO DAGScheduler: Final stage: ResultStage 433 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:35:26 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:35:26 INFO DAGScheduler: Missing parents: List()
26/01/04 17:35:26 INFO DAGScheduler: Submitting ResultStage 433 (MapPartitionsRDD[2168] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:35:26 INFO MemoryStore: Block broadcast_647 stored as values in memory (estimated size 52.9 KiB, free 434.3 MiB)
26/01/04 17:35:26 INFO MemoryStore: Block broadcast_647_piece0 stored as bytes in memory (estimated size 20.2 KiB, free 434.3 MiB)
26/01/04 17:35:26 INFO BlockManagerInfo: Added broadcast_647_piece0 in memory on spark-master:33535 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:35:26 INFO SparkContext: Created broadcast 647 from broadcast at DAGScheduler.scala:1585
26/01/04 17:35:26 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 433 (MapPartitionsRDD[2168] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/01/04 17:35:26 INFO TaskSchedulerImpl: Adding task set 433.0 with 1 tasks resource profile 0
26/01/04 17:35:26 INFO TaskSetManager: Starting task 0.0 in stage 433.0 (TID 685) (172.18.0.13, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:35:26 INFO BlockManagerInfo: Added broadcast_647_piece0 in memory on 172.18.0.13:44401 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:35:26 INFO BlockManagerInfo: Added broadcast_646_piece0 in memory on 172.18.0.13:44401 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:35:26 INFO TaskSetManager: Finished task 0.0 in stage 433.0 (TID 685) in 557 ms on 172.18.0.13 (executor 0) (1/1)
26/01/04 17:35:26 INFO TaskSchedulerImpl: Removed TaskSet 433.0, whose tasks have all completed, from pool 
26/01/04 17:35:27 INFO DAGScheduler: ResultStage 433 (start at NativeMethodAccessorImpl.java:0) finished in 0.562 s
26/01/04 17:35:27 INFO DAGScheduler: Job 432 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:35:27 INFO TaskSchedulerImpl: Killing all running tasks in stage 433: Stage finished
26/01/04 17:35:27 INFO DAGScheduler: Job 432 finished: start at NativeMethodAccessorImpl.java:0, took 0.562545 s
26/01/04 17:35:27 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 214, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@749efcb4] is committing.
26/01/04 17:35:27 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 214, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@749efcb4] committed.
26/01/04 17:35:27 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/214 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.214.c48911f8-ba08-4a82-b9f0-fe5ee4bededb.tmp
26/01/04 17:35:27 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.214.c48911f8-ba08-4a82-b9f0-fe5ee4bededb.tmp to file:/tmp/spark-checkpoint-enrichment/commits/214
26/01/04 17:35:27 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T17:35:26.205Z",
  "batchId" : 214,
  "numInputRows" : 30,
  "inputRowsPerSecond" : 2500.0,
  "processedRowsPerSecond" : 34.602076124567475,
  "durationMs" : {
    "addBatch" : 681,
    "commitOffsets" : 72,
    "getBatch" : 0,
    "latestOffset" : 2,
    "queryPlanning" : 30,
    "triggerExecution" : 867,
    "walCommit" : 80
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "0" : 1710
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "0" : 1740
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "0" : 1740
      }
    },
    "numInputRows" : 30,
    "inputRowsPerSecond" : 2500.0,
    "processedRowsPerSecond" : 34.602076124567475,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 31
  }
}
26/01/04 17:35:37 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/01/04 17:35:37 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/215 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.215.0d077d43-cc0d-40ed-a965-109f86d05253.tmp
26/01/04 17:35:37 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.215.0d077d43-cc0d-40ed-a965-109f86d05253.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/215
26/01/04 17:35:37 INFO MicroBatchExecution: Committed offsets for batch 215. Metadata OffsetSeqMetadata(0,1767548137223,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 17:35:37 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:35:37 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:35:37 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:35:37 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:35:37 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:35:37 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:35:37 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#174343 - airline_prefix.nullCount#174342) > 0)
26/01/04 17:35:37 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#174378 - min_flight_num.nullCount#174377) > 0)
26/01/04 17:35:37 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#174373 - max_flight_num.nullCount#174372) > 0)
26/01/04 17:35:37 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:35:37 INFO DAGScheduler: Got job 433 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 17:35:37 INFO DAGScheduler: Final stage: ResultStage 434 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:35:37 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:35:37 INFO DAGScheduler: Missing parents: List()
26/01/04 17:35:37 INFO DAGScheduler: Submitting ResultStage 434 (MapPartitionsRDD[2173] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:35:37 INFO MemoryStore: Block broadcast_648 stored as values in memory (estimated size 38.5 KiB, free 434.2 MiB)
26/01/04 17:35:37 INFO MemoryStore: Block broadcast_648_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.2 MiB)
26/01/04 17:35:37 INFO BlockManagerInfo: Added broadcast_648_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:35:37 INFO SparkContext: Created broadcast 648 from broadcast at DAGScheduler.scala:1585
26/01/04 17:35:37 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 434 (MapPartitionsRDD[2173] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 17:35:37 INFO TaskSchedulerImpl: Adding task set 434.0 with 2 tasks resource profile 0
26/01/04 17:35:37 INFO TaskSetManager: Starting task 1.0 in stage 434.0 (TID 686) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 17:35:37 INFO TaskSetManager: Starting task 0.0 in stage 434.0 (TID 687) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 17:35:37 INFO BlockManagerInfo: Added broadcast_648_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:35:37 INFO BlockManagerInfo: Added broadcast_648_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:35:37 INFO TaskSetManager: Finished task 1.0 in stage 434.0 (TID 686) in 16 ms on 172.18.0.13 (executor 0) (1/2)
26/01/04 17:35:37 INFO TaskSetManager: Finished task 0.0 in stage 434.0 (TID 687) in 29 ms on 172.18.0.14 (executor 1) (2/2)
26/01/04 17:35:37 INFO TaskSchedulerImpl: Removed TaskSet 434.0, whose tasks have all completed, from pool 
26/01/04 17:35:37 INFO DAGScheduler: ResultStage 434 (start at NativeMethodAccessorImpl.java:0) finished in 0.033 s
26/01/04 17:35:37 INFO DAGScheduler: Job 433 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:35:37 INFO TaskSchedulerImpl: Killing all running tasks in stage 434: Stage finished
26/01/04 17:35:37 INFO DAGScheduler: Job 433 finished: start at NativeMethodAccessorImpl.java:0, took 0.034709 s
26/01/04 17:35:37 INFO MemoryStore: Block broadcast_649_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.2 MiB)
26/01/04 17:35:37 INFO BlockManagerInfo: Added broadcast_649_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:35:37 INFO SparkContext: Created broadcast 649 from start at NativeMethodAccessorImpl.java:0
26/01/04 17:35:37 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 215, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@3380eda1]. The input RDD has 1 partitions.
26/01/04 17:35:37 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:35:37 INFO DAGScheduler: Got job 434 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/01/04 17:35:37 INFO DAGScheduler: Final stage: ResultStage 435 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:35:37 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:35:37 INFO DAGScheduler: Missing parents: List()
26/01/04 17:35:37 INFO DAGScheduler: Submitting ResultStage 435 (MapPartitionsRDD[2178] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:35:37 INFO MemoryStore: Block broadcast_650 stored as values in memory (estimated size 52.9 KiB, free 434.2 MiB)
26/01/04 17:35:37 INFO MemoryStore: Block broadcast_650_piece0 stored as bytes in memory (estimated size 20.2 KiB, free 434.1 MiB)
26/01/04 17:35:37 INFO BlockManagerInfo: Added broadcast_650_piece0 in memory on spark-master:33535 (size: 20.2 KiB, free: 434.3 MiB)
26/01/04 17:35:37 INFO SparkContext: Created broadcast 650 from broadcast at DAGScheduler.scala:1585
26/01/04 17:35:37 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 435 (MapPartitionsRDD[2178] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/01/04 17:35:37 INFO TaskSchedulerImpl: Adding task set 435.0 with 1 tasks resource profile 0
26/01/04 17:35:37 INFO TaskSetManager: Starting task 0.0 in stage 435.0 (TID 688) (172.18.0.13, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:35:37 INFO BlockManagerInfo: Added broadcast_650_piece0 in memory on 172.18.0.13:44401 (size: 20.2 KiB, free: 434.3 MiB)
26/01/04 17:35:37 INFO BlockManagerInfo: Added broadcast_649_piece0 in memory on 172.18.0.13:44401 (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:35:37 INFO TaskSetManager: Finished task 0.0 in stage 435.0 (TID 688) in 550 ms on 172.18.0.13 (executor 0) (1/1)
26/01/04 17:35:37 INFO TaskSchedulerImpl: Removed TaskSet 435.0, whose tasks have all completed, from pool 
26/01/04 17:35:37 INFO DAGScheduler: ResultStage 435 (start at NativeMethodAccessorImpl.java:0) finished in 0.559 s
26/01/04 17:35:37 INFO DAGScheduler: Job 434 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:35:37 INFO TaskSchedulerImpl: Killing all running tasks in stage 435: Stage finished
26/01/04 17:35:37 INFO DAGScheduler: Job 434 finished: start at NativeMethodAccessorImpl.java:0, took 0.559566 s
26/01/04 17:35:37 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 215, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@3380eda1] is committing.
26/01/04 17:35:37 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 215, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@3380eda1] committed.
26/01/04 17:35:37 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/215 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.215.41f8f324-9eb7-4cfb-bd60-36e28ed716c8.tmp
26/01/04 17:35:38 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.215.41f8f324-9eb7-4cfb-bd60-36e28ed716c8.tmp to file:/tmp/spark-checkpoint-enrichment/commits/215
26/01/04 17:35:38 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T17:35:37.220Z",
  "batchId" : 215,
  "numInputRows" : 30,
  "inputRowsPerSecond" : 2500.0,
  "processedRowsPerSecond" : 36.144578313253014,
  "durationMs" : {
    "addBatch" : 643,
    "commitOffsets" : 85,
    "getBatch" : 0,
    "latestOffset" : 3,
    "queryPlanning" : 30,
    "triggerExecution" : 830,
    "walCommit" : 68
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "0" : 1740
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "0" : 1770
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "0" : 1770
      }
    },
    "numInputRows" : 30,
    "inputRowsPerSecond" : 2500.0,
    "processedRowsPerSecond" : 36.144578313253014,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 31
  }
}
26/01/04 17:35:38 INFO BlockManagerInfo: Removed broadcast_645_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:35:38 INFO BlockManagerInfo: Removed broadcast_645_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:35:38 INFO BlockManagerInfo: Removed broadcast_645_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:35:38 INFO BlockManagerInfo: Removed broadcast_646_piece0 on spark-master:33535 in memory (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:35:38 INFO BlockManagerInfo: Removed broadcast_646_piece0 on 172.18.0.13:44401 in memory (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:35:38 INFO BlockManagerInfo: Removed broadcast_650_piece0 on spark-master:33535 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:35:38 INFO BlockManagerInfo: Removed broadcast_650_piece0 on 172.18.0.13:44401 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:35:38 INFO BlockManagerInfo: Removed broadcast_647_piece0 on spark-master:33535 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:35:38 INFO BlockManagerInfo: Removed broadcast_647_piece0 on 172.18.0.13:44401 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:35:39 INFO BlockManagerInfo: Removed broadcast_648_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:35:39 INFO BlockManagerInfo: Removed broadcast_648_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:35:39 INFO BlockManagerInfo: Removed broadcast_648_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:35:48 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/01/04 17:35:48 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/216 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.216.fcac77f9-22a9-4508-b87f-2382e8687a47.tmp
26/01/04 17:35:48 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.216.fcac77f9-22a9-4508-b87f-2382e8687a47.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/216
26/01/04 17:35:48 INFO MicroBatchExecution: Committed offsets for batch 216. Metadata OffsetSeqMetadata(0,1767548148241,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 17:35:48 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:35:48 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:35:48 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:35:48 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:35:48 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:35:48 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:35:48 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#175147 - airline_prefix.nullCount#175146) > 0)
26/01/04 17:35:48 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#175182 - min_flight_num.nullCount#175181) > 0)
26/01/04 17:35:48 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#175177 - max_flight_num.nullCount#175176) > 0)
26/01/04 17:35:48 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:35:48 INFO DAGScheduler: Got job 435 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 17:35:48 INFO DAGScheduler: Final stage: ResultStage 436 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:35:48 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:35:48 INFO DAGScheduler: Missing parents: List()
26/01/04 17:35:48 INFO DAGScheduler: Submitting ResultStage 436 (MapPartitionsRDD[2183] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:35:48 INFO MemoryStore: Block broadcast_651 stored as values in memory (estimated size 38.5 KiB, free 434.4 MiB)
26/01/04 17:35:48 INFO MemoryStore: Block broadcast_651_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.3 MiB)
26/01/04 17:35:48 INFO BlockManagerInfo: Added broadcast_651_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:35:48 INFO SparkContext: Created broadcast 651 from broadcast at DAGScheduler.scala:1585
26/01/04 17:35:48 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 436 (MapPartitionsRDD[2183] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 17:35:48 INFO TaskSchedulerImpl: Adding task set 436.0 with 2 tasks resource profile 0
26/01/04 17:35:48 INFO TaskSetManager: Starting task 1.0 in stage 436.0 (TID 689) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 17:35:48 INFO TaskSetManager: Starting task 0.0 in stage 436.0 (TID 690) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 17:35:48 INFO BlockManagerInfo: Added broadcast_651_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:35:48 INFO BlockManagerInfo: Added broadcast_651_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:35:48 INFO TaskSetManager: Finished task 1.0 in stage 436.0 (TID 689) in 15 ms on 172.18.0.13 (executor 0) (1/2)
26/01/04 17:35:48 INFO TaskSetManager: Finished task 0.0 in stage 436.0 (TID 690) in 26 ms on 172.18.0.14 (executor 1) (2/2)
26/01/04 17:35:48 INFO TaskSchedulerImpl: Removed TaskSet 436.0, whose tasks have all completed, from pool 
26/01/04 17:35:48 INFO DAGScheduler: ResultStage 436 (start at NativeMethodAccessorImpl.java:0) finished in 0.032 s
26/01/04 17:35:48 INFO DAGScheduler: Job 435 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:35:48 INFO TaskSchedulerImpl: Killing all running tasks in stage 436: Stage finished
26/01/04 17:35:48 INFO DAGScheduler: Job 435 finished: start at NativeMethodAccessorImpl.java:0, took 0.033522 s
26/01/04 17:35:48 INFO MemoryStore: Block broadcast_652_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.3 MiB)
26/01/04 17:35:48 INFO BlockManagerInfo: Added broadcast_652_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:35:48 INFO SparkContext: Created broadcast 652 from start at NativeMethodAccessorImpl.java:0
26/01/04 17:35:48 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 216, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@3f1dd40f]. The input RDD has 1 partitions.
26/01/04 17:35:48 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:35:48 INFO DAGScheduler: Got job 436 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/01/04 17:35:48 INFO DAGScheduler: Final stage: ResultStage 437 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:35:48 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:35:48 INFO DAGScheduler: Missing parents: List()
26/01/04 17:35:48 INFO DAGScheduler: Submitting ResultStage 437 (MapPartitionsRDD[2188] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:35:48 INFO MemoryStore: Block broadcast_653 stored as values in memory (estimated size 52.9 KiB, free 434.3 MiB)
26/01/04 17:35:48 INFO MemoryStore: Block broadcast_653_piece0 stored as bytes in memory (estimated size 20.2 KiB, free 434.3 MiB)
26/01/04 17:35:48 INFO BlockManagerInfo: Added broadcast_653_piece0 in memory on spark-master:33535 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:35:48 INFO SparkContext: Created broadcast 653 from broadcast at DAGScheduler.scala:1585
26/01/04 17:35:48 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 437 (MapPartitionsRDD[2188] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/01/04 17:35:48 INFO TaskSchedulerImpl: Adding task set 437.0 with 1 tasks resource profile 0
26/01/04 17:35:48 INFO TaskSetManager: Starting task 0.0 in stage 437.0 (TID 691) (172.18.0.13, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:35:48 INFO BlockManagerInfo: Added broadcast_653_piece0 in memory on 172.18.0.13:44401 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:35:48 INFO BlockManagerInfo: Added broadcast_652_piece0 in memory on 172.18.0.13:44401 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:35:49 INFO TaskSetManager: Finished task 0.0 in stage 437.0 (TID 691) in 544 ms on 172.18.0.13 (executor 0) (1/1)
26/01/04 17:35:49 INFO TaskSchedulerImpl: Removed TaskSet 437.0, whose tasks have all completed, from pool 
26/01/04 17:35:49 INFO DAGScheduler: ResultStage 437 (start at NativeMethodAccessorImpl.java:0) finished in 0.548 s
26/01/04 17:35:49 INFO DAGScheduler: Job 436 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:35:49 INFO TaskSchedulerImpl: Killing all running tasks in stage 437: Stage finished
26/01/04 17:35:49 INFO DAGScheduler: Job 436 finished: start at NativeMethodAccessorImpl.java:0, took 0.549775 s
26/01/04 17:35:49 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 216, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@3f1dd40f] is committing.
26/01/04 17:35:49 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 216, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@3f1dd40f] committed.
26/01/04 17:35:49 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/216 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.216.5ee07a8f-63bb-4151-8896-6c1e1359b888.tmp
26/01/04 17:35:49 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.216.5ee07a8f-63bb-4151-8896-6c1e1359b888.tmp to file:/tmp/spark-checkpoint-enrichment/commits/216
26/01/04 17:35:49 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T17:35:48.240Z",
  "batchId" : 216,
  "numInputRows" : 30,
  "inputRowsPerSecond" : 2500.0,
  "processedRowsPerSecond" : 34.84320557491289,
  "durationMs" : {
    "addBatch" : 643,
    "commitOffsets" : 64,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 30,
    "triggerExecution" : 861,
    "walCommit" : 123
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "0" : 1770
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "0" : 1800
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "0" : 1800
      }
    },
    "numInputRows" : 30,
    "inputRowsPerSecond" : 2500.0,
    "processedRowsPerSecond" : 34.84320557491289,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 31
  }
}
26/01/04 17:35:56 INFO BlockManagerInfo: Removed broadcast_651_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:35:56 INFO BlockManagerInfo: Removed broadcast_651_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:35:56 INFO BlockManagerInfo: Removed broadcast_651_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:35:56 INFO BlockManagerInfo: Removed broadcast_653_piece0 on spark-master:33535 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:35:56 INFO BlockManagerInfo: Removed broadcast_653_piece0 on 172.18.0.13:44401 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:35:56 INFO BlockManagerInfo: Removed broadcast_649_piece0 on spark-master:33535 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:35:56 INFO BlockManagerInfo: Removed broadcast_649_piece0 on 172.18.0.13:44401 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:35:59 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/01/04 17:35:59 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/217 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.217.1b05bb80-61e6-40a1-bb8c-ec383b70d94f.tmp
26/01/04 17:35:59 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.217.1b05bb80-61e6-40a1-bb8c-ec383b70d94f.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/217
26/01/04 17:35:59 INFO MicroBatchExecution: Committed offsets for batch 217. Metadata OffsetSeqMetadata(0,1767548159254,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 17:35:59 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:35:59 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:35:59 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:35:59 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:35:59 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:35:59 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:35:59 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#175951 - airline_prefix.nullCount#175950) > 0)
26/01/04 17:35:59 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#175986 - min_flight_num.nullCount#175985) > 0)
26/01/04 17:35:59 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#175981 - max_flight_num.nullCount#175980) > 0)
26/01/04 17:35:59 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:35:59 INFO DAGScheduler: Got job 437 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 17:35:59 INFO DAGScheduler: Final stage: ResultStage 438 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:35:59 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:35:59 INFO DAGScheduler: Missing parents: List()
26/01/04 17:35:59 INFO DAGScheduler: Submitting ResultStage 438 (MapPartitionsRDD[2193] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:35:59 INFO MemoryStore: Block broadcast_654 stored as values in memory (estimated size 38.5 KiB, free 434.4 MiB)
26/01/04 17:35:59 INFO MemoryStore: Block broadcast_654_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.3 MiB)
26/01/04 17:35:59 INFO BlockManagerInfo: Added broadcast_654_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:35:59 INFO BlockManagerInfo: Removed broadcast_652_piece0 on spark-master:33535 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:35:59 INFO SparkContext: Created broadcast 654 from broadcast at DAGScheduler.scala:1585
26/01/04 17:35:59 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 438 (MapPartitionsRDD[2193] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 17:35:59 INFO TaskSchedulerImpl: Adding task set 438.0 with 2 tasks resource profile 0
26/01/04 17:35:59 INFO BlockManagerInfo: Removed broadcast_652_piece0 on 172.18.0.13:44401 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:35:59 INFO TaskSetManager: Starting task 1.0 in stage 438.0 (TID 692) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 17:35:59 INFO TaskSetManager: Starting task 0.0 in stage 438.0 (TID 693) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 17:35:59 INFO BlockManagerInfo: Added broadcast_654_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:35:59 INFO BlockManagerInfo: Added broadcast_654_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:35:59 INFO TaskSetManager: Finished task 1.0 in stage 438.0 (TID 692) in 13 ms on 172.18.0.13 (executor 0) (1/2)
26/01/04 17:35:59 INFO TaskSetManager: Finished task 0.0 in stage 438.0 (TID 693) in 32 ms on 172.18.0.14 (executor 1) (2/2)
26/01/04 17:35:59 INFO TaskSchedulerImpl: Removed TaskSet 438.0, whose tasks have all completed, from pool 
26/01/04 17:35:59 INFO DAGScheduler: ResultStage 438 (start at NativeMethodAccessorImpl.java:0) finished in 0.040 s
26/01/04 17:35:59 INFO DAGScheduler: Job 437 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:35:59 INFO TaskSchedulerImpl: Killing all running tasks in stage 438: Stage finished
26/01/04 17:35:59 INFO DAGScheduler: Job 437 finished: start at NativeMethodAccessorImpl.java:0, took 0.041213 s
26/01/04 17:35:59 INFO MemoryStore: Block broadcast_655_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.3 MiB)
26/01/04 17:35:59 INFO BlockManagerInfo: Added broadcast_655_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:35:59 INFO SparkContext: Created broadcast 655 from start at NativeMethodAccessorImpl.java:0
26/01/04 17:35:59 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 217, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@6d176077]. The input RDD has 1 partitions.
26/01/04 17:35:59 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:35:59 INFO DAGScheduler: Got job 438 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/01/04 17:35:59 INFO DAGScheduler: Final stage: ResultStage 439 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:35:59 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:35:59 INFO DAGScheduler: Missing parents: List()
26/01/04 17:35:59 INFO DAGScheduler: Submitting ResultStage 439 (MapPartitionsRDD[2198] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:35:59 INFO MemoryStore: Block broadcast_656 stored as values in memory (estimated size 52.9 KiB, free 434.3 MiB)
26/01/04 17:35:59 INFO MemoryStore: Block broadcast_656_piece0 stored as bytes in memory (estimated size 20.2 KiB, free 434.3 MiB)
26/01/04 17:35:59 INFO BlockManagerInfo: Added broadcast_656_piece0 in memory on spark-master:33535 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:35:59 INFO SparkContext: Created broadcast 656 from broadcast at DAGScheduler.scala:1585
26/01/04 17:35:59 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 439 (MapPartitionsRDD[2198] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/01/04 17:35:59 INFO TaskSchedulerImpl: Adding task set 439.0 with 1 tasks resource profile 0
26/01/04 17:35:59 INFO TaskSetManager: Starting task 0.0 in stage 439.0 (TID 694) (172.18.0.13, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:35:59 INFO BlockManagerInfo: Added broadcast_656_piece0 in memory on 172.18.0.13:44401 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:35:59 INFO BlockManagerInfo: Added broadcast_655_piece0 in memory on 172.18.0.13:44401 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:36:00 INFO TaskSetManager: Finished task 0.0 in stage 439.0 (TID 694) in 536 ms on 172.18.0.13 (executor 0) (1/1)
26/01/04 17:36:00 INFO TaskSchedulerImpl: Removed TaskSet 439.0, whose tasks have all completed, from pool 
26/01/04 17:36:00 INFO DAGScheduler: ResultStage 439 (start at NativeMethodAccessorImpl.java:0) finished in 0.540 s
26/01/04 17:36:00 INFO DAGScheduler: Job 438 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:36:00 INFO TaskSchedulerImpl: Killing all running tasks in stage 439: Stage finished
26/01/04 17:36:00 INFO DAGScheduler: Job 438 finished: start at NativeMethodAccessorImpl.java:0, took 0.541178 s
26/01/04 17:36:00 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 217, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@6d176077] is committing.
26/01/04 17:36:00 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 217, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@6d176077] committed.
26/01/04 17:36:00 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/217 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.217.8f4045e1-59cf-4edf-86c6-b120432ab33e.tmp
26/01/04 17:36:00 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.217.8f4045e1-59cf-4edf-86c6-b120432ab33e.tmp to file:/tmp/spark-checkpoint-enrichment/commits/217
26/01/04 17:36:00 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T17:35:59.252Z",
  "batchId" : 217,
  "numInputRows" : 30,
  "inputRowsPerSecond" : 2500.0,
  "processedRowsPerSecond" : 34.64203233256351,
  "durationMs" : {
    "addBatch" : 650,
    "commitOffsets" : 95,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 29,
    "triggerExecution" : 866,
    "walCommit" : 89
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "0" : 1800
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "0" : 1830
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "0" : 1830
      }
    },
    "numInputRows" : 30,
    "inputRowsPerSecond" : 2500.0,
    "processedRowsPerSecond" : 34.64203233256351,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 31
  }
}
26/01/04 17:36:10 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/01/04 17:36:10 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/218 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.218.e70c1fa6-2220-4be4-b917-edb5926353b3.tmp
26/01/04 17:36:10 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.218.e70c1fa6-2220-4be4-b917-edb5926353b3.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/218
26/01/04 17:36:10 INFO MicroBatchExecution: Committed offsets for batch 218. Metadata OffsetSeqMetadata(0,1767548170272,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 17:36:10 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:36:10 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:36:10 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:36:10 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:36:10 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:36:10 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:36:10 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#176755 - airline_prefix.nullCount#176754) > 0)
26/01/04 17:36:10 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#176790 - min_flight_num.nullCount#176789) > 0)
26/01/04 17:36:10 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#176785 - max_flight_num.nullCount#176784) > 0)
26/01/04 17:36:10 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:36:10 INFO DAGScheduler: Got job 439 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 17:36:10 INFO DAGScheduler: Final stage: ResultStage 440 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:36:10 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:36:10 INFO DAGScheduler: Missing parents: List()
26/01/04 17:36:10 INFO DAGScheduler: Submitting ResultStage 440 (MapPartitionsRDD[2203] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:36:10 INFO MemoryStore: Block broadcast_657 stored as values in memory (estimated size 38.5 KiB, free 434.2 MiB)
26/01/04 17:36:10 INFO MemoryStore: Block broadcast_657_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.2 MiB)
26/01/04 17:36:10 INFO BlockManagerInfo: Added broadcast_657_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:36:10 INFO SparkContext: Created broadcast 657 from broadcast at DAGScheduler.scala:1585
26/01/04 17:36:10 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 440 (MapPartitionsRDD[2203] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 17:36:10 INFO TaskSchedulerImpl: Adding task set 440.0 with 2 tasks resource profile 0
26/01/04 17:36:10 INFO TaskSetManager: Starting task 0.0 in stage 440.0 (TID 695) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 17:36:10 INFO TaskSetManager: Starting task 1.0 in stage 440.0 (TID 696) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 17:36:10 INFO BlockManagerInfo: Added broadcast_657_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.3 MiB)
26/01/04 17:36:10 INFO BlockManagerInfo: Added broadcast_657_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:36:10 INFO TaskSetManager: Finished task 1.0 in stage 440.0 (TID 696) in 14 ms on 172.18.0.13 (executor 0) (1/2)
26/01/04 17:36:10 INFO TaskSetManager: Finished task 0.0 in stage 440.0 (TID 695) in 39 ms on 172.18.0.14 (executor 1) (2/2)
26/01/04 17:36:10 INFO TaskSchedulerImpl: Removed TaskSet 440.0, whose tasks have all completed, from pool 
26/01/04 17:36:10 INFO DAGScheduler: ResultStage 440 (start at NativeMethodAccessorImpl.java:0) finished in 0.042 s
26/01/04 17:36:10 INFO DAGScheduler: Job 439 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:36:10 INFO TaskSchedulerImpl: Killing all running tasks in stage 440: Stage finished
26/01/04 17:36:10 INFO DAGScheduler: Job 439 finished: start at NativeMethodAccessorImpl.java:0, took 0.043552 s
26/01/04 17:36:10 INFO MemoryStore: Block broadcast_658_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.2 MiB)
26/01/04 17:36:10 INFO BlockManagerInfo: Added broadcast_658_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:36:10 INFO SparkContext: Created broadcast 658 from start at NativeMethodAccessorImpl.java:0
26/01/04 17:36:10 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 218, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@210f2ffe]. The input RDD has 1 partitions.
26/01/04 17:36:10 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:36:10 INFO DAGScheduler: Got job 440 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/01/04 17:36:10 INFO DAGScheduler: Final stage: ResultStage 441 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:36:10 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:36:10 INFO DAGScheduler: Missing parents: List()
26/01/04 17:36:10 INFO DAGScheduler: Submitting ResultStage 441 (MapPartitionsRDD[2208] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:36:10 INFO MemoryStore: Block broadcast_659 stored as values in memory (estimated size 52.9 KiB, free 434.2 MiB)
26/01/04 17:36:10 INFO MemoryStore: Block broadcast_659_piece0 stored as bytes in memory (estimated size 20.2 KiB, free 434.1 MiB)
26/01/04 17:36:10 INFO BlockManagerInfo: Added broadcast_659_piece0 in memory on spark-master:33535 (size: 20.2 KiB, free: 434.3 MiB)
26/01/04 17:36:10 INFO SparkContext: Created broadcast 659 from broadcast at DAGScheduler.scala:1585
26/01/04 17:36:10 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 441 (MapPartitionsRDD[2208] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/01/04 17:36:10 INFO TaskSchedulerImpl: Adding task set 441.0 with 1 tasks resource profile 0
26/01/04 17:36:10 INFO TaskSetManager: Starting task 0.0 in stage 441.0 (TID 697) (172.18.0.13, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:36:10 INFO BlockManagerInfo: Added broadcast_659_piece0 in memory on 172.18.0.13:44401 (size: 20.2 KiB, free: 434.3 MiB)
26/01/04 17:36:10 INFO BlockManagerInfo: Added broadcast_658_piece0 in memory on 172.18.0.13:44401 (size: 2.7 KiB, free: 434.3 MiB)
26/01/04 17:36:11 INFO TaskSetManager: Finished task 0.0 in stage 441.0 (TID 697) in 546 ms on 172.18.0.13 (executor 0) (1/1)
26/01/04 17:36:11 INFO TaskSchedulerImpl: Removed TaskSet 441.0, whose tasks have all completed, from pool 
26/01/04 17:36:11 INFO DAGScheduler: ResultStage 441 (start at NativeMethodAccessorImpl.java:0) finished in 0.551 s
26/01/04 17:36:11 INFO DAGScheduler: Job 440 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:36:11 INFO TaskSchedulerImpl: Killing all running tasks in stage 441: Stage finished
26/01/04 17:36:11 INFO DAGScheduler: Job 440 finished: start at NativeMethodAccessorImpl.java:0, took 0.552453 s
26/01/04 17:36:11 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 218, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@210f2ffe] is committing.
26/01/04 17:36:11 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 218, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@210f2ffe] committed.
26/01/04 17:36:11 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/218 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.218.64a26462-ce43-467b-8288-3a8003c3be43.tmp
26/01/04 17:36:11 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.218.64a26462-ce43-467b-8288-3a8003c3be43.tmp to file:/tmp/spark-checkpoint-enrichment/commits/218
26/01/04 17:36:11 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T17:36:10.270Z",
  "batchId" : 218,
  "numInputRows" : 30,
  "inputRowsPerSecond" : 2500.0,
  "processedRowsPerSecond" : 37.174721189591075,
  "durationMs" : {
    "addBatch" : 647,
    "commitOffsets" : 61,
    "getBatch" : 0,
    "latestOffset" : 2,
    "queryPlanning" : 19,
    "triggerExecution" : 807,
    "walCommit" : 77
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "0" : 1830
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "0" : 1860
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "0" : 1860
      }
    },
    "numInputRows" : 30,
    "inputRowsPerSecond" : 2500.0,
    "processedRowsPerSecond" : 37.174721189591075,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 31
  }
}
26/01/04 17:36:12 INFO BlockManagerInfo: Removed broadcast_656_piece0 on spark-master:33535 in memory (size: 20.2 KiB, free: 434.3 MiB)
26/01/04 17:36:12 INFO BlockManagerInfo: Removed broadcast_656_piece0 on 172.18.0.13:44401 in memory (size: 20.2 KiB, free: 434.3 MiB)
26/01/04 17:36:12 INFO BlockManagerInfo: Removed broadcast_657_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:36:12 INFO BlockManagerInfo: Removed broadcast_657_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:36:12 INFO BlockManagerInfo: Removed broadcast_657_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:36:12 INFO BlockManagerInfo: Removed broadcast_654_piece0 on spark-master:33535 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:36:12 INFO BlockManagerInfo: Removed broadcast_654_piece0 on 172.18.0.13:44401 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:36:12 INFO BlockManagerInfo: Removed broadcast_654_piece0 on 172.18.0.14:43523 in memory (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:36:12 INFO BlockManagerInfo: Removed broadcast_655_piece0 on spark-master:33535 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:36:12 INFO BlockManagerInfo: Removed broadcast_655_piece0 on 172.18.0.13:44401 in memory (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:36:12 INFO BlockManagerInfo: Removed broadcast_659_piece0 on spark-master:33535 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:36:12 INFO BlockManagerInfo: Removed broadcast_659_piece0 on 172.18.0.13:44401 in memory (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:36:21 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/01/04 17:36:21 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/219 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.219.420381af-a0a3-4a22-ad1e-48327fdd8657.tmp
26/01/04 17:36:21 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.219.420381af-a0a3-4a22-ad1e-48327fdd8657.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/219
26/01/04 17:36:21 INFO MicroBatchExecution: Committed offsets for batch 219. Metadata OffsetSeqMetadata(0,1767548181291,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/01/04 17:36:21 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:36:21 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:36:21 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:36:21 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:36:21 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:36:21 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/01/04 17:36:21 INFO DefaultCachedBatchSerializer: Predicate isnotnull(airline_prefix#3) generates partition filter: ((airline_prefix.count#177559 - airline_prefix.nullCount#177558) > 0)
26/01/04 17:36:21 INFO DefaultCachedBatchSerializer: Predicate isnotnull(min_flight_num#10L) generates partition filter: ((min_flight_num.count#177594 - min_flight_num.nullCount#177593) > 0)
26/01/04 17:36:21 INFO DefaultCachedBatchSerializer: Predicate isnotnull(max_flight_num#9L) generates partition filter: ((max_flight_num.count#177589 - max_flight_num.nullCount#177588) > 0)
26/01/04 17:36:21 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:36:21 INFO DAGScheduler: Got job 441 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/01/04 17:36:21 INFO DAGScheduler: Final stage: ResultStage 442 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:36:21 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:36:21 INFO DAGScheduler: Missing parents: List()
26/01/04 17:36:21 INFO DAGScheduler: Submitting ResultStage 442 (MapPartitionsRDD[2213] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:36:21 INFO MemoryStore: Block broadcast_660 stored as values in memory (estimated size 38.5 KiB, free 434.4 MiB)
26/01/04 17:36:21 INFO MemoryStore: Block broadcast_660_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.3 MiB)
26/01/04 17:36:21 INFO BlockManagerInfo: Added broadcast_660_piece0 in memory on spark-master:33535 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:36:21 INFO SparkContext: Created broadcast 660 from broadcast at DAGScheduler.scala:1585
26/01/04 17:36:21 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 442 (MapPartitionsRDD[2213] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/01/04 17:36:21 INFO TaskSchedulerImpl: Adding task set 442.0 with 2 tasks resource profile 0
26/01/04 17:36:21 INFO TaskSetManager: Starting task 0.0 in stage 442.0 (TID 698) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 9237 bytes) 
26/01/04 17:36:21 INFO TaskSetManager: Starting task 1.0 in stage 442.0 (TID 699) (172.18.0.13, executor 0, partition 1, PROCESS_LOCAL, 9424 bytes) 
26/01/04 17:36:21 INFO BlockManagerInfo: Added broadcast_660_piece0 in memory on 172.18.0.13:44401 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:36:21 INFO BlockManagerInfo: Added broadcast_660_piece0 in memory on 172.18.0.14:43523 (size: 15.9 KiB, free: 434.4 MiB)
26/01/04 17:36:21 INFO TaskSetManager: Finished task 1.0 in stage 442.0 (TID 699) in 21 ms on 172.18.0.13 (executor 0) (1/2)
26/01/04 17:36:21 INFO TaskSetManager: Finished task 0.0 in stage 442.0 (TID 698) in 35 ms on 172.18.0.14 (executor 1) (2/2)
26/01/04 17:36:21 INFO TaskSchedulerImpl: Removed TaskSet 442.0, whose tasks have all completed, from pool 
26/01/04 17:36:21 INFO DAGScheduler: ResultStage 442 (start at NativeMethodAccessorImpl.java:0) finished in 0.042 s
26/01/04 17:36:21 INFO DAGScheduler: Job 441 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:36:21 INFO TaskSchedulerImpl: Killing all running tasks in stage 442: Stage finished
26/01/04 17:36:21 INFO DAGScheduler: Job 441 finished: start at NativeMethodAccessorImpl.java:0, took 0.043016 s
26/01/04 17:36:21 INFO MemoryStore: Block broadcast_661_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 434.3 MiB)
26/01/04 17:36:21 INFO BlockManagerInfo: Added broadcast_661_piece0 in memory on spark-master:33535 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:36:21 INFO SparkContext: Created broadcast 661 from start at NativeMethodAccessorImpl.java:0
26/01/04 17:36:21 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 219, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@4b6dbb8b]. The input RDD has 1 partitions.
26/01/04 17:36:21 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/01/04 17:36:21 INFO DAGScheduler: Got job 442 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/01/04 17:36:21 INFO DAGScheduler: Final stage: ResultStage 443 (start at NativeMethodAccessorImpl.java:0)
26/01/04 17:36:21 INFO DAGScheduler: Parents of final stage: List()
26/01/04 17:36:21 INFO DAGScheduler: Missing parents: List()
26/01/04 17:36:21 INFO DAGScheduler: Submitting ResultStage 443 (MapPartitionsRDD[2218] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/01/04 17:36:21 INFO MemoryStore: Block broadcast_662 stored as values in memory (estimated size 52.9 KiB, free 434.3 MiB)
26/01/04 17:36:21 INFO MemoryStore: Block broadcast_662_piece0 stored as bytes in memory (estimated size 20.2 KiB, free 434.3 MiB)
26/01/04 17:36:21 INFO BlockManagerInfo: Added broadcast_662_piece0 in memory on spark-master:33535 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:36:21 INFO SparkContext: Created broadcast 662 from broadcast at DAGScheduler.scala:1585
26/01/04 17:36:21 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 443 (MapPartitionsRDD[2218] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/01/04 17:36:21 INFO TaskSchedulerImpl: Adding task set 443.0 with 1 tasks resource profile 0
26/01/04 17:36:21 INFO TaskSetManager: Starting task 0.0 in stage 443.0 (TID 700) (172.18.0.13, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/01/04 17:36:21 INFO BlockManagerInfo: Added broadcast_662_piece0 in memory on 172.18.0.13:44401 (size: 20.2 KiB, free: 434.4 MiB)
26/01/04 17:36:21 INFO BlockManagerInfo: Added broadcast_661_piece0 in memory on 172.18.0.13:44401 (size: 2.7 KiB, free: 434.4 MiB)
26/01/04 17:36:22 INFO TaskSetManager: Finished task 0.0 in stage 443.0 (TID 700) in 543 ms on 172.18.0.13 (executor 0) (1/1)
26/01/04 17:36:22 INFO TaskSchedulerImpl: Removed TaskSet 443.0, whose tasks have all completed, from pool 
26/01/04 17:36:22 INFO DAGScheduler: ResultStage 443 (start at NativeMethodAccessorImpl.java:0) finished in 0.550 s
26/01/04 17:36:22 INFO DAGScheduler: Job 442 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/04 17:36:22 INFO TaskSchedulerImpl: Killing all running tasks in stage 443: Stage finished
26/01/04 17:36:22 INFO DAGScheduler: Job 442 finished: start at NativeMethodAccessorImpl.java:0, took 0.550298 s
26/01/04 17:36:22 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 219, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@4b6dbb8b] is committing.
26/01/04 17:36:22 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 219, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@4b6dbb8b] committed.
26/01/04 17:36:22 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/219 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.219.a52e0cd0-ec3c-449b-83f7-b8fb7eaeca51.tmp
26/01/04 17:36:22 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.219.a52e0cd0-ec3c-449b-83f7-b8fb7eaeca51.tmp to file:/tmp/spark-checkpoint-enrichment/commits/219
26/01/04 17:36:22 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7ee758be-50e4-4375-8595-7bf053b44670",
  "runId" : "2b3a52fc-8b7c-44f0-bd17-98d89af32a25",
  "name" : null,
  "timestamp" : "2026-01-04T17:36:21.289Z",
  "batchId" : 219,
  "numInputRows" : 30,
  "inputRowsPerSecond" : 2500.0,
  "processedRowsPerSecond" : 36.496350364963504,
  "durationMs" : {
    "addBatch" : 639,
    "commitOffsets" : 99,
    "getBatch" : 1,
    "latestOffset" : 2,
    "queryPlanning" : 14,
    "triggerExecution" : 822,
    "walCommit" : 67
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "0" : 1860
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "0" : 1890
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "0" : 1890
      }
    },
    "numInputRows" : 30,
    "inputRowsPerSecond" : 2500.0,
    "processedRowsPerSecond" : 36.496350364963504,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@387c9e7",
    "numOutputRows" : 31
  }
}
