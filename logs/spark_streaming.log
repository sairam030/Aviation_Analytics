============================================
Starting Spark Streaming Enrichment Service
============================================
[Fri Feb 13 12:01:15 UTC 2026] Waiting for Kafka and topics to be ready...
  Waiting 40 seconds for Kafka initialization...
✓ Kafka should be ready now
[Fri Feb 13 12:01:55 UTC 2026] Submitting Spark Streaming job...
============================================================
Starting Spark Streaming Enrichment
============================================================
26/02/13 12:02:03 INFO SparkContext: Running Spark version 3.5.1
26/02/13 12:02:03 INFO SparkContext: OS info Linux, 6.17.0-14-generic, amd64
26/02/13 12:02:03 INFO SparkContext: Java version 17.0.17
26/02/13 12:02:03 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
26/02/13 12:02:04 INFO ResourceUtils: ==============================================================
26/02/13 12:02:04 INFO ResourceUtils: No custom resources configured for spark.driver.
26/02/13 12:02:04 INFO ResourceUtils: ==============================================================
26/02/13 12:02:04 INFO SparkContext: Submitted application: FlightStreamEnrichment
26/02/13 12:02:04 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
26/02/13 12:02:04 INFO ResourceProfile: Limiting resource is cpu
26/02/13 12:02:04 INFO ResourceProfileManager: Added ResourceProfile id: 0
26/02/13 12:02:04 INFO SecurityManager: Changing view acls to: root
26/02/13 12:02:04 INFO SecurityManager: Changing modify acls to: root
26/02/13 12:02:04 INFO SecurityManager: Changing view acls groups to: 
26/02/13 12:02:04 INFO SecurityManager: Changing modify acls groups to: 
26/02/13 12:02:04 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY
26/02/13 12:02:05 INFO Utils: Successfully started service 'sparkDriver' on port 34315.
26/02/13 12:02:05 INFO SparkEnv: Registering MapOutputTracker
26/02/13 12:02:05 INFO SparkEnv: Registering BlockManagerMaster
26/02/13 12:02:05 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
26/02/13 12:02:05 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
26/02/13 12:02:05 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
26/02/13 12:02:06 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-a34d3e76-4399-4334-9bf8-2002c941bac6
26/02/13 12:02:06 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
26/02/13 12:02:06 INFO SparkEnv: Registering OutputCommitCoordinator
26/02/13 12:02:06 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
26/02/13 12:02:06 INFO Utils: Successfully started service 'SparkUI' on port 4040.
26/02/13 12:02:07 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...
26/02/13 12:02:07 INFO TransportClientFactory: Successfully created connection to spark-master/172.18.0.10:7077 after 72 ms (0 ms spent in bootstraps)
26/02/13 12:02:07 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20260213120207-0001
26/02/13 12:02:07 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20260213120207-0001/0 on worker-20260213114143-172.18.0.15-35515 (172.18.0.15:35515) with 2 core(s)
26/02/13 12:02:07 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 42073.
26/02/13 12:02:07 INFO NettyBlockTransferService: Server created on spark-master 0.0.0.0:42073
26/02/13 12:02:07 INFO StandaloneSchedulerBackend: Granted executor ID app-20260213120207-0001/0 on hostPort 172.18.0.15:35515 with 2 core(s), 1024.0 MiB RAM
26/02/13 12:02:07 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20260213120207-0001/1 on worker-20260213114143-172.18.0.14-33335 (172.18.0.14:33335) with 2 core(s)
26/02/13 12:02:07 INFO StandaloneSchedulerBackend: Granted executor ID app-20260213120207-0001/1 on hostPort 172.18.0.14:33335 with 2 core(s), 1024.0 MiB RAM
26/02/13 12:02:07 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
26/02/13 12:02:07 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, spark-master, 42073, None)
26/02/13 12:02:07 INFO BlockManagerMasterEndpoint: Registering block manager spark-master:42073 with 434.4 MiB RAM, BlockManagerId(driver, spark-master, 42073, None)
26/02/13 12:02:07 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, spark-master, 42073, None)
26/02/13 12:02:07 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, spark-master, 42073, None)
26/02/13 12:02:08 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
26/02/13 12:02:08 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20260213120207-0001/0 is now RUNNING
26/02/13 12:02:08 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20260213120207-0001/1 is now RUNNING
✓ Spark session created
✓ Reading from: aviation-india-states
✓ Writing to: aviation-enriched-states
[DEBUG] Creating route mapping table...
Loading routes from CSV: /opt/airflow/data/routes.csv
26/02/13 12:02:09 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
26/02/13 12:02:09 INFO SharedState: Warehouse path is 'file:/opt/airflow/spark-warehouse'.
26/02/13 12:02:12 INFO InMemoryFileIndex: It took 108 ms to list leaf files for 1 paths.
26/02/13 12:02:12 INFO InMemoryFileIndex: It took 5 ms to list leaf files for 1 paths.
26/02/13 12:02:16 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.14:40320) with ID 1,  ResourceProfileId 0
26/02/13 12:02:17 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.15:48682) with ID 0,  ResourceProfileId 0
26/02/13 12:02:17 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.14:37571 with 434.4 MiB RAM, BlockManagerId(1, 172.18.0.14, 37571, None)
26/02/13 12:02:17 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.15:37717 with 434.4 MiB RAM, BlockManagerId(0, 172.18.0.15, 37717, None)
26/02/13 12:02:19 INFO FileSourceStrategy: Pushed Filters: 
26/02/13 12:02:19 INFO FileSourceStrategy: Post-Scan Filters: (length(trim(value#0, None)) > 0)
26/02/13 12:02:20 INFO CodeGenerator: Code generated in 447.022027 ms
26/02/13 12:02:21 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 200.4 KiB, free 434.2 MiB)
26/02/13 12:02:21 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 434.2 MiB)
26/02/13 12:02:21 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on spark-master:42073 (size: 34.5 KiB, free: 434.4 MiB)
26/02/13 12:02:21 INFO SparkContext: Created broadcast 0 from csv at NativeMethodAccessorImpl.java:0
26/02/13 12:02:21 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
26/02/13 12:02:21 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0
26/02/13 12:02:21 INFO DAGScheduler: Got job 0 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/02/13 12:02:21 INFO DAGScheduler: Final stage: ResultStage 0 (csv at NativeMethodAccessorImpl.java:0)
26/02/13 12:02:21 INFO DAGScheduler: Parents of final stage: List()
26/02/13 12:02:21 INFO DAGScheduler: Missing parents: List()
26/02/13 12:02:21 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:02:21 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 13.5 KiB, free 434.2 MiB)
26/02/13 12:02:21 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.4 KiB, free 434.2 MiB)
26/02/13 12:02:21 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on spark-master:42073 (size: 6.4 KiB, free: 434.4 MiB)
26/02/13 12:02:21 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1585
26/02/13 12:02:21 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/02/13 12:02:21 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
26/02/13 12:02:21 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8217 bytes) 
26/02/13 12:02:22 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.18.0.14:37571 (size: 6.4 KiB, free: 434.4 MiB)
26/02/13 12:02:23 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.18.0.14:37571 (size: 34.5 KiB, free: 434.4 MiB)
26/02/13 12:02:23 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 2238 ms on 172.18.0.14 (executor 1) (1/1)
26/02/13 12:02:23 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
26/02/13 12:02:23 INFO DAGScheduler: ResultStage 0 (csv at NativeMethodAccessorImpl.java:0) finished in 2.454 s
26/02/13 12:02:23 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 12:02:23 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
26/02/13 12:02:23 INFO DAGScheduler: Job 0 finished: csv at NativeMethodAccessorImpl.java:0, took 2.533778 s
26/02/13 12:02:23 INFO CodeGenerator: Code generated in 21.136175 ms
26/02/13 12:02:24 INFO FileSourceStrategy: Pushed Filters: 
26/02/13 12:02:24 INFO FileSourceStrategy: Post-Scan Filters: 
26/02/13 12:02:24 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 200.4 KiB, free 434.0 MiB)
26/02/13 12:02:24 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.9 MiB)
26/02/13 12:02:24 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 172.18.0.14:37571 in memory (size: 6.4 KiB, free: 434.4 MiB)
26/02/13 12:02:24 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on spark-master:42073 (size: 34.5 KiB, free: 434.3 MiB)
26/02/13 12:02:24 INFO BlockManagerInfo: Removed broadcast_1_piece0 on spark-master:42073 in memory (size: 6.4 KiB, free: 434.3 MiB)
26/02/13 12:02:24 INFO SparkContext: Created broadcast 2 from csv at NativeMethodAccessorImpl.java:0
26/02/13 12:02:24 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
26/02/13 12:02:24 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0
26/02/13 12:02:24 INFO DAGScheduler: Got job 1 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/02/13 12:02:24 INFO DAGScheduler: Final stage: ResultStage 1 (csv at NativeMethodAccessorImpl.java:0)
26/02/13 12:02:24 INFO DAGScheduler: Parents of final stage: List()
26/02/13 12:02:24 INFO DAGScheduler: Missing parents: List()
26/02/13 12:02:24 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[9] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:02:24 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 27.8 KiB, free 433.9 MiB)
26/02/13 12:02:24 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 12.8 KiB, free 433.9 MiB)
26/02/13 12:02:24 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on spark-master:42073 (size: 12.8 KiB, free: 434.3 MiB)
26/02/13 12:02:24 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1585
26/02/13 12:02:24 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[9] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/02/13 12:02:24 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
26/02/13 12:02:24 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8217 bytes) 
26/02/13 12:02:24 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 172.18.0.14:37571 (size: 12.8 KiB, free: 434.4 MiB)
26/02/13 12:02:26 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.18.0.14:37571 (size: 34.5 KiB, free: 434.3 MiB)
26/02/13 12:02:26 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 2172 ms on 172.18.0.14 (executor 1) (1/1)
26/02/13 12:02:26 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
26/02/13 12:02:26 INFO DAGScheduler: ResultStage 1 (csv at NativeMethodAccessorImpl.java:0) finished in 2.251 s
26/02/13 12:02:26 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 12:02:26 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
26/02/13 12:02:26 INFO DAGScheduler: Job 1 finished: csv at NativeMethodAccessorImpl.java:0, took 2.273979 s
26/02/13 12:02:27 INFO BlockManagerInfo: Removed broadcast_3_piece0 on spark-master:42073 in memory (size: 12.8 KiB, free: 434.3 MiB)
26/02/13 12:02:27 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 172.18.0.14:37571 in memory (size: 12.8 KiB, free: 434.3 MiB)
26/02/13 12:02:27 INFO FileSourceStrategy: Pushed Filters: IsNotNull(FlightNo),IsNotNull(Origin),IsNotNull(Destination)
26/02/13 12:02:27 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(FlightNo#17),isnotnull(Origin#18),isnotnull(Destination#19),NOT (Origin#18 = Destination#19)
26/02/13 12:02:28 INFO CodeGenerator: Code generated in 190.546998 ms
26/02/13 12:02:28 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 200.3 KiB, free 433.7 MiB)
26/02/13 12:02:28 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.7 MiB)
26/02/13 12:02:28 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on spark-master:42073 (size: 34.5 KiB, free: 434.3 MiB)
26/02/13 12:02:28 INFO SparkContext: Created broadcast 4 from count at NativeMethodAccessorImpl.java:0
26/02/13 12:02:28 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
26/02/13 12:02:28 INFO BlockManagerInfo: Removed broadcast_0_piece0 on spark-master:42073 in memory (size: 34.5 KiB, free: 434.3 MiB)
26/02/13 12:02:28 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 172.18.0.14:37571 in memory (size: 34.5 KiB, free: 434.4 MiB)
26/02/13 12:02:28 INFO DAGScheduler: Registering RDD 13 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 0
26/02/13 12:02:28 INFO DAGScheduler: Got map stage job 2 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/02/13 12:02:28 INFO DAGScheduler: Final stage: ShuffleMapStage 2 (count at NativeMethodAccessorImpl.java:0)
26/02/13 12:02:28 INFO DAGScheduler: Parents of final stage: List()
26/02/13 12:02:28 INFO DAGScheduler: Missing parents: List()
26/02/13 12:02:28 INFO DAGScheduler: Submitting ShuffleMapStage 2 (MapPartitionsRDD[13] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:02:28 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 51.6 KiB, free 433.9 MiB)
26/02/13 12:02:28 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 21.5 KiB, free 433.9 MiB)
26/02/13 12:02:28 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on spark-master:42073 (size: 21.5 KiB, free: 434.3 MiB)
26/02/13 12:02:28 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1585
26/02/13 12:02:28 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[13] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/02/13 12:02:28 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
26/02/13 12:02:28 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8206 bytes) 
26/02/13 12:02:28 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 172.18.0.14:37571 (size: 21.5 KiB, free: 434.3 MiB)
26/02/13 12:02:28 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 172.18.0.14:37571 (size: 34.5 KiB, free: 434.3 MiB)
26/02/13 12:02:29 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 868 ms on 172.18.0.14 (executor 1) (1/1)
26/02/13 12:02:29 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool 
26/02/13 12:02:29 INFO DAGScheduler: ShuffleMapStage 2 (count at NativeMethodAccessorImpl.java:0) finished in 0.907 s
26/02/13 12:02:29 INFO DAGScheduler: looking for newly runnable stages
26/02/13 12:02:29 INFO DAGScheduler: running: Set()
26/02/13 12:02:29 INFO DAGScheduler: waiting: Set()
26/02/13 12:02:29 INFO DAGScheduler: failed: Set()
26/02/13 12:02:29 INFO ShufflePartitionsUtil: For shuffle(0), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
26/02/13 12:02:29 INFO CodeGenerator: Code generated in 47.58471 ms
26/02/13 12:02:29 INFO DAGScheduler: Registering RDD 16 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 1
26/02/13 12:02:29 INFO DAGScheduler: Got map stage job 3 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/02/13 12:02:29 INFO DAGScheduler: Final stage: ShuffleMapStage 4 (count at NativeMethodAccessorImpl.java:0)
26/02/13 12:02:29 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 3)
26/02/13 12:02:29 INFO DAGScheduler: Missing parents: List()
26/02/13 12:02:29 INFO DAGScheduler: Submitting ShuffleMapStage 4 (MapPartitionsRDD[16] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:02:29 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 54.7 KiB, free 433.8 MiB)
26/02/13 12:02:29 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 24.2 KiB, free 433.8 MiB)
26/02/13 12:02:29 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on spark-master:42073 (size: 24.2 KiB, free: 434.3 MiB)
26/02/13 12:02:29 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1585
26/02/13 12:02:29 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 4 (MapPartitionsRDD[16] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/02/13 12:02:29 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0
26/02/13 12:02:29 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 3) (172.18.0.14, executor 1, partition 0, NODE_LOCAL, 7608 bytes) 
26/02/13 12:02:29 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 172.18.0.14:37571 (size: 24.2 KiB, free: 434.3 MiB)
26/02/13 12:02:29 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 172.18.0.14:40320
26/02/13 12:02:29 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 3) in 349 ms on 172.18.0.14 (executor 1) (1/1)
26/02/13 12:02:29 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool 
26/02/13 12:02:29 INFO DAGScheduler: ShuffleMapStage 4 (count at NativeMethodAccessorImpl.java:0) finished in 0.371 s
26/02/13 12:02:29 INFO DAGScheduler: looking for newly runnable stages
26/02/13 12:02:29 INFO DAGScheduler: running: Set()
26/02/13 12:02:29 INFO DAGScheduler: waiting: Set()
26/02/13 12:02:29 INFO DAGScheduler: failed: Set()
26/02/13 12:02:29 INFO CodeGenerator: Code generated in 10.878602 ms
26/02/13 12:02:29 INFO SparkContext: Starting job: count at NativeMethodAccessorImpl.java:0
26/02/13 12:02:29 INFO DAGScheduler: Got job 4 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/02/13 12:02:29 INFO DAGScheduler: Final stage: ResultStage 7 (count at NativeMethodAccessorImpl.java:0)
26/02/13 12:02:29 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 6)
26/02/13 12:02:29 INFO DAGScheduler: Missing parents: List()
26/02/13 12:02:29 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[19] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:02:29 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 12.5 KiB, free 433.8 MiB)
26/02/13 12:02:29 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 5.9 KiB, free 433.8 MiB)
26/02/13 12:02:29 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on spark-master:42073 (size: 5.9 KiB, free: 434.3 MiB)
26/02/13 12:02:29 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1585
26/02/13 12:02:29 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[19] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/02/13 12:02:29 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks resource profile 0
26/02/13 12:02:29 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 4) (172.18.0.14, executor 1, partition 0, NODE_LOCAL, 7619 bytes) 
26/02/13 12:02:29 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 172.18.0.14:37571 (size: 5.9 KiB, free: 434.3 MiB)
26/02/13 12:02:29 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 172.18.0.14:40320
26/02/13 12:02:29 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 4) in 77 ms on 172.18.0.14 (executor 1) (1/1)
26/02/13 12:02:29 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool 
26/02/13 12:02:29 INFO DAGScheduler: ResultStage 7 (count at NativeMethodAccessorImpl.java:0) finished in 0.089 s
26/02/13 12:02:29 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 12:02:29 INFO TaskSchedulerImpl: Killing all running tasks in stage 7: Stage finished
26/02/13 12:02:29 INFO DAGScheduler: Job 4 finished: count at NativeMethodAccessorImpl.java:0, took 0.095420 s
✓ Loaded 2773 routes from CSV: /opt/airflow/data/routes.csv
✓ Using flight number-based route matching
✓ Sample callsigns: IGO102, AIC176, etc.
26/02/13 12:02:29 INFO FileSourceStrategy: Pushed Filters: IsNotNull(FlightNo),IsNotNull(Origin),IsNotNull(Destination)
26/02/13 12:02:29 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(FlightNo#17),isnotnull(Origin#18),isnotnull(Destination#19),NOT (Origin#18 = Destination#19)
26/02/13 12:02:30 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
26/02/13 12:02:30 INFO BlockManagerInfo: Removed broadcast_6_piece0 on spark-master:42073 in memory (size: 24.2 KiB, free: 434.3 MiB)
26/02/13 12:02:30 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 172.18.0.14:37571 in memory (size: 24.2 KiB, free: 434.3 MiB)
26/02/13 12:02:30 INFO BlockManagerInfo: Removed broadcast_7_piece0 on spark-master:42073 in memory (size: 5.9 KiB, free: 434.3 MiB)
26/02/13 12:02:30 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 172.18.0.14:37571 in memory (size: 5.9 KiB, free: 434.3 MiB)
26/02/13 12:02:30 INFO BlockManagerInfo: Removed broadcast_5_piece0 on spark-master:42073 in memory (size: 21.5 KiB, free: 434.3 MiB)
26/02/13 12:02:30 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 172.18.0.14:37571 in memory (size: 21.5 KiB, free: 434.3 MiB)
26/02/13 12:02:30 INFO BlockManagerInfo: Removed broadcast_4_piece0 on spark-master:42073 in memory (size: 34.5 KiB, free: 434.4 MiB)
26/02/13 12:02:30 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 172.18.0.14:37571 in memory (size: 34.5 KiB, free: 434.4 MiB)
26/02/13 12:02:30 INFO BlockManagerInfo: Removed broadcast_2_piece0 on spark-master:42073 in memory (size: 34.5 KiB, free: 434.4 MiB)
26/02/13 12:02:30 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 172.18.0.14:37571 in memory (size: 34.5 KiB, free: 434.4 MiB)
26/02/13 12:02:30 INFO CodeGenerator: Code generated in 211.68615 ms
26/02/13 12:02:30 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 200.3 KiB, free 434.2 MiB)
26/02/13 12:02:30 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 434.2 MiB)
26/02/13 12:02:30 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on spark-master:42073 (size: 34.5 KiB, free: 434.4 MiB)
26/02/13 12:02:30 INFO SparkContext: Created broadcast 8 from count at NativeMethodAccessorImpl.java:0
26/02/13 12:02:30 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
26/02/13 12:02:30 INFO DAGScheduler: Registering RDD 24 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 2
26/02/13 12:02:30 INFO DAGScheduler: Got map stage job 5 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/02/13 12:02:30 INFO DAGScheduler: Final stage: ShuffleMapStage 8 (count at NativeMethodAccessorImpl.java:0)
26/02/13 12:02:30 INFO DAGScheduler: Parents of final stage: List()
26/02/13 12:02:30 INFO DAGScheduler: Missing parents: List()
26/02/13 12:02:30 INFO DAGScheduler: Submitting ShuffleMapStage 8 (MapPartitionsRDD[24] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:02:30 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 111.9 KiB, free 434.1 MiB)
26/02/13 12:02:30 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 434.0 MiB)
26/02/13 12:02:30 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on spark-master:42073 (size: 34.0 KiB, free: 434.3 MiB)
26/02/13 12:02:30 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1585
26/02/13 12:02:30 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 8 (MapPartitionsRDD[24] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/02/13 12:02:30 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks resource profile 0
26/02/13 12:02:30 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 5) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8206 bytes) 
26/02/13 12:02:30 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 172.18.0.14:37571 (size: 34.0 KiB, free: 434.4 MiB)
26/02/13 12:02:30 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 172.18.0.14:37571 (size: 34.5 KiB, free: 434.3 MiB)
26/02/13 12:02:31 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 5) in 813 ms on 172.18.0.14 (executor 1) (1/1)
26/02/13 12:02:31 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool 
26/02/13 12:02:31 INFO DAGScheduler: ShuffleMapStage 8 (count at NativeMethodAccessorImpl.java:0) finished in 0.831 s
26/02/13 12:02:31 INFO DAGScheduler: looking for newly runnable stages
26/02/13 12:02:31 INFO DAGScheduler: running: Set()
26/02/13 12:02:31 INFO DAGScheduler: waiting: Set()
26/02/13 12:02:31 INFO DAGScheduler: failed: Set()
26/02/13 12:02:31 INFO CodeGenerator: Code generated in 16.514236 ms
26/02/13 12:02:31 INFO DAGScheduler: Got job 6 (count at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 12:02:31 INFO DAGScheduler: Final stage: ResultStage 10 (count at NativeMethodAccessorImpl.java:0)
26/02/13 12:02:31 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 9)
26/02/13 12:02:31 INFO DAGScheduler: Missing parents: List()
26/02/13 12:02:31 INFO DAGScheduler: Submitting ResultStage 10 (AdaptiveSparkPlan isFinalPlan=false
+- SortAggregate(key=[callsign#67], functions=[first(flight_number#25, false), first(origin_code#26, false), first(destination_code#27, false), first(airline_iata#32, false), first(airline_name#37, false), first(airline_prefix#43, false), first(airline_icao#50, false), first(flight_num_only#58, false), first(origin_city#77, false), first(origin_airport#88, false), first(origin_lat#100, false), first(origin_lon#113, false), first(destination_city#127, false), first(destination_airport#142, false), first(destination_lat#158, false), first(destination_lon#175, false)], output=[flight_number#251, origin_code#253, destination_code#255, airline_iata#257, airline_name#259, airline_prefix#261, airline_icao#263, flight_num_only#265, callsign#67, origin_city#267, origin_airport#269, origin_lat#271, origin_lon#273, destination_city#275, destination_airport#277, destination_lat#279, destination_lon#281])
   +- Sort [callsign#67 ASC NULLS FIRST], false, 0
      +- Exchange hashpartit... MapPartitionsRDD[29] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:02:31 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 89.6 KiB, free 433.9 MiB)
26/02/13 12:02:31 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 29.7 KiB, free 433.9 MiB)
26/02/13 12:02:31 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on spark-master:42073 (size: 29.7 KiB, free: 434.3 MiB)
26/02/13 12:02:31 INFO SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:1585
26/02/13 12:02:31 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 10 (AdaptiveSparkPlan isFinalPlan=false
+- SortAggregate(key=[callsign#67], functions=[first(flight_number#25, false), first(origin_code#26, false), first(destination_code#27, false), first(airline_iata#32, false), first(airline_name#37, false), first(airline_prefix#43, false), first(airline_icao#50, false), first(flight_num_only#58, false), first(origin_city#77, false), first(origin_airport#88, false), first(origin_lat#100, false), first(origin_lon#113, false), first(destination_city#127, false), first(destination_airport#142, false), first(destination_lat#158, false), first(destination_lon#175, false)], output=[flight_number#251, origin_code#253, destination_code#255, airline_iata#257, airline_name#259, airline_prefix#261, airline_icao#263, flight_num_only#265, callsign#67, origin_city#267, origin_airport#269, origin_lat#271, origin_lon#273, destination_city#275, destination_airport#277, destination_lat#279, destination_lon#281])
   +- Sort [callsign#67 ASC NULLS FIRST], false, 0
      +- Exchange hashpartit... MapPartitionsRDD[29] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 12:02:31 INFO TaskSchedulerImpl: Adding task set 10.0 with 4 tasks resource profile 0
26/02/13 12:02:31 INFO TaskSetManager: Starting task 0.0 in stage 10.0 (TID 6) (172.18.0.14, executor 1, partition 0, NODE_LOCAL, 7619 bytes) 
26/02/13 12:02:31 INFO TaskSetManager: Starting task 1.0 in stage 10.0 (TID 7) (172.18.0.14, executor 1, partition 1, NODE_LOCAL, 7619 bytes) 
26/02/13 12:02:31 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 172.18.0.14:37571 (size: 29.7 KiB, free: 434.3 MiB)
26/02/13 12:02:31 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 2 to 172.18.0.14:40320
26/02/13 12:02:32 INFO BlockManagerInfo: Added rdd_29_1 in memory on 172.18.0.14:37571 (size: 52.1 KiB, free: 434.3 MiB)
26/02/13 12:02:32 INFO BlockManagerInfo: Added rdd_29_0 in memory on 172.18.0.14:37571 (size: 51.7 KiB, free: 434.2 MiB)
26/02/13 12:02:32 INFO TaskSetManager: Starting task 2.0 in stage 10.0 (TID 8) (172.18.0.14, executor 1, partition 2, NODE_LOCAL, 7619 bytes) 
26/02/13 12:02:32 INFO TaskSetManager: Starting task 3.0 in stage 10.0 (TID 9) (172.18.0.14, executor 1, partition 3, NODE_LOCAL, 7619 bytes) 
26/02/13 12:02:32 INFO TaskSetManager: Finished task 1.0 in stage 10.0 (TID 7) in 847 ms on 172.18.0.14 (executor 1) (1/4)
26/02/13 12:02:32 INFO TaskSetManager: Finished task 0.0 in stage 10.0 (TID 6) in 850 ms on 172.18.0.14 (executor 1) (2/4)
26/02/13 12:02:32 INFO BlockManagerInfo: Added rdd_29_3 in memory on 172.18.0.14:37571 (size: 51.3 KiB, free: 434.2 MiB)
26/02/13 12:02:32 INFO BlockManagerInfo: Added rdd_29_2 in memory on 172.18.0.14:37571 (size: 49.7 KiB, free: 434.1 MiB)
26/02/13 12:02:32 INFO TaskSetManager: Finished task 3.0 in stage 10.0 (TID 9) in 245 ms on 172.18.0.14 (executor 1) (3/4)
26/02/13 12:02:32 INFO TaskSetManager: Finished task 2.0 in stage 10.0 (TID 8) in 268 ms on 172.18.0.14 (executor 1) (4/4)
26/02/13 12:02:32 INFO DAGScheduler: ResultStage 10 (count at NativeMethodAccessorImpl.java:0) finished in 1.130 s
26/02/13 12:02:32 INFO TaskSchedulerImpl: Removed TaskSet 10.0, whose tasks have all completed, from pool 
26/02/13 12:02:32 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 12:02:32 INFO TaskSchedulerImpl: Killing all running tasks in stage 10: Stage finished
26/02/13 12:02:32 INFO CodeGenerator: Code generated in 45.666914 ms
26/02/13 12:02:32 INFO DAGScheduler: Registering RDD 34 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 3
26/02/13 12:02:32 INFO DAGScheduler: Got map stage job 7 (count at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 12:02:32 INFO DAGScheduler: Final stage: ShuffleMapStage 12 (count at NativeMethodAccessorImpl.java:0)
26/02/13 12:02:32 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 11)
26/02/13 12:02:32 INFO DAGScheduler: Missing parents: List()
26/02/13 12:02:32 INFO DAGScheduler: Submitting ShuffleMapStage 12 (MapPartitionsRDD[34] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:02:32 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 95.1 KiB, free 433.8 MiB)
26/02/13 12:02:32 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 32.0 KiB, free 433.8 MiB)
26/02/13 12:02:32 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on spark-master:42073 (size: 32.0 KiB, free: 434.3 MiB)
26/02/13 12:02:32 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1585
26/02/13 12:02:32 INFO DAGScheduler: Submitting 4 missing tasks from ShuffleMapStage 12 (MapPartitionsRDD[34] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 12:02:32 INFO TaskSchedulerImpl: Adding task set 12.0 with 4 tasks resource profile 0
26/02/13 12:02:32 INFO TaskSetManager: Starting task 0.0 in stage 12.0 (TID 10) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 7608 bytes) 
26/02/13 12:02:32 INFO TaskSetManager: Starting task 1.0 in stage 12.0 (TID 11) (172.18.0.14, executor 1, partition 1, PROCESS_LOCAL, 7608 bytes) 
26/02/13 12:02:32 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 172.18.0.14:37571 (size: 32.0 KiB, free: 434.1 MiB)
26/02/13 12:02:33 INFO TaskSetManager: Starting task 2.0 in stage 12.0 (TID 12) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 7608 bytes) 
26/02/13 12:02:33 INFO TaskSetManager: Starting task 3.0 in stage 12.0 (TID 13) (172.18.0.14, executor 1, partition 3, PROCESS_LOCAL, 7608 bytes) 
26/02/13 12:02:33 INFO TaskSetManager: Finished task 1.0 in stage 12.0 (TID 11) in 216 ms on 172.18.0.14 (executor 1) (1/4)
26/02/13 12:02:33 INFO TaskSetManager: Finished task 0.0 in stage 12.0 (TID 10) in 220 ms on 172.18.0.14 (executor 1) (2/4)
26/02/13 12:02:33 INFO TaskSetManager: Finished task 2.0 in stage 12.0 (TID 12) in 49 ms on 172.18.0.14 (executor 1) (3/4)
26/02/13 12:02:33 INFO TaskSetManager: Finished task 3.0 in stage 12.0 (TID 13) in 59 ms on 172.18.0.14 (executor 1) (4/4)
26/02/13 12:02:33 INFO TaskSchedulerImpl: Removed TaskSet 12.0, whose tasks have all completed, from pool 
26/02/13 12:02:33 INFO DAGScheduler: ShuffleMapStage 12 (count at NativeMethodAccessorImpl.java:0) finished in 0.298 s
26/02/13 12:02:33 INFO DAGScheduler: looking for newly runnable stages
26/02/13 12:02:33 INFO DAGScheduler: running: Set()
26/02/13 12:02:33 INFO DAGScheduler: waiting: Set()
26/02/13 12:02:33 INFO DAGScheduler: failed: Set()
26/02/13 12:02:33 INFO CodeGenerator: Code generated in 21.090851 ms
26/02/13 12:02:33 INFO SparkContext: Starting job: count at NativeMethodAccessorImpl.java:0
26/02/13 12:02:33 INFO DAGScheduler: Got job 8 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/02/13 12:02:33 INFO DAGScheduler: Final stage: ResultStage 15 (count at NativeMethodAccessorImpl.java:0)
26/02/13 12:02:33 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 14)
26/02/13 12:02:33 INFO DAGScheduler: Missing parents: List()
26/02/13 12:02:33 INFO DAGScheduler: Submitting ResultStage 15 (MapPartitionsRDD[37] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:02:33 INFO BlockManagerInfo: Removed broadcast_11_piece0 on spark-master:42073 in memory (size: 32.0 KiB, free: 434.3 MiB)
26/02/13 12:02:33 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 12.5 KiB, free 433.9 MiB)
26/02/13 12:02:33 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 5.9 KiB, free 433.9 MiB)
26/02/13 12:02:33 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on spark-master:42073 (size: 5.9 KiB, free: 434.3 MiB)
26/02/13 12:02:33 INFO SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:1585
26/02/13 12:02:33 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 15 (MapPartitionsRDD[37] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/02/13 12:02:33 INFO TaskSchedulerImpl: Adding task set 15.0 with 1 tasks resource profile 0
26/02/13 12:02:33 INFO BlockManagerInfo: Removed broadcast_11_piece0 on 172.18.0.14:37571 in memory (size: 32.0 KiB, free: 434.1 MiB)
26/02/13 12:02:33 INFO TaskSetManager: Starting task 0.0 in stage 15.0 (TID 14) (172.18.0.14, executor 1, partition 0, NODE_LOCAL, 7619 bytes) 
26/02/13 12:02:33 INFO BlockManagerInfo: Removed broadcast_10_piece0 on spark-master:42073 in memory (size: 29.7 KiB, free: 434.3 MiB)
26/02/13 12:02:33 INFO BlockManagerInfo: Removed broadcast_10_piece0 on 172.18.0.14:37571 in memory (size: 29.7 KiB, free: 434.1 MiB)
26/02/13 12:02:33 INFO BlockManagerInfo: Removed broadcast_9_piece0 on 172.18.0.14:37571 in memory (size: 34.0 KiB, free: 434.2 MiB)
26/02/13 12:02:33 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 172.18.0.14:37571 (size: 5.9 KiB, free: 434.2 MiB)
26/02/13 12:02:33 INFO BlockManagerInfo: Removed broadcast_9_piece0 on spark-master:42073 in memory (size: 34.0 KiB, free: 434.4 MiB)
26/02/13 12:02:33 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 3 to 172.18.0.14:40320
26/02/13 12:02:33 INFO TaskSetManager: Finished task 0.0 in stage 15.0 (TID 14) in 124 ms on 172.18.0.14 (executor 1) (1/1)
26/02/13 12:02:33 INFO TaskSchedulerImpl: Removed TaskSet 15.0, whose tasks have all completed, from pool 
26/02/13 12:02:33 INFO DAGScheduler: ResultStage 15 (count at NativeMethodAccessorImpl.java:0) finished in 0.150 s
26/02/13 12:02:33 INFO DAGScheduler: Job 8 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 12:02:33 INFO TaskSchedulerImpl: Killing all running tasks in stage 15: Stage finished
26/02/13 12:02:33 INFO DAGScheduler: Job 8 finished: count at NativeMethodAccessorImpl.java:0, took 0.178666 s
✓ Route mapping table created (2773 routes)
[DEBUG] Setting up Kafka input stream...
✓ Connected to Kafka input stream
26/02/13 12:02:33 INFO BlockManagerInfo: Removed broadcast_12_piece0 on spark-master:42073 in memory (size: 5.9 KiB, free: 434.4 MiB)
[DEBUG] Parsing JSON and extracting fields...
26/02/13 12:02:33 INFO BlockManagerInfo: Removed broadcast_12_piece0 on 172.18.0.14:37571 in memory (size: 5.9 KiB, free: 434.2 MiB)
[DEBUG] Joining with route mapping...
  Using CSV-based exact callsign matching
[DEBUG] Filtering records with valid routes...
✓ Filtering enabled: Only flights with routes will be sent to next topic
[DEBUG] Setting up output stream to Kafka...
26/02/13 12:02:34 INFO StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint
26/02/13 12:02:34 INFO ResolveWriteToStream: Checkpoint root /tmp/spark-checkpoint-enrichment resolved to file:/tmp/spark-checkpoint-enrichment.
26/02/13 12:02:34 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
26/02/13 12:02:34 INFO MicroBatchExecution: Starting [id = ebfab33f-0465-4e53-8f7a-961711cffb90, runId = 4a161d66-225b-4b29-812d-ddec3c05364e]. Use file:/tmp/spark-checkpoint-enrichment to store the query checkpoint.
============================================================
✓ Streaming query started successfully!
✓ Enriching flights with route information...
✓ Query ID: ebfab33f-0465-4e53-8f7a-961711cffb90
============================================================
[DEBUG] Waiting for stream termination...
26/02/13 12:02:34 INFO MicroBatchExecution: Reading table [org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@67ef27c4] from DataSourceV2 named 'kafka' [org.apache.spark.sql.kafka010.KafkaSourceProvider@3ee8c8b0]
26/02/13 12:02:34 INFO OffsetSeqLog: BatchIds found from listing: 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122
26/02/13 12:02:34 INFO OffsetSeqLog: Getting latest batch 122
26/02/13 12:02:34 INFO OffsetSeqLog: BatchIds found from listing: 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122
26/02/13 12:02:34 INFO OffsetSeqLog: Getting latest batch 122
26/02/13 12:02:34 INFO CommitLog: BatchIds found from listing: 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122
26/02/13 12:02:34 INFO CommitLog: Getting latest batch 122
26/02/13 12:02:34 INFO MicroBatchExecution: Resuming at batch 123 with committed offsets {KafkaV2[Subscribe[aviation-india-states]]: {"aviation-india-states":{"2":7613,"1":8550,"0":9619}}} and available offsets {KafkaV2[Subscribe[aviation-india-states]]: {"aviation-india-states":{"2":7613,"1":8550,"0":9619}}}
26/02/13 12:02:34 INFO MicroBatchExecution: Stream started from {KafkaV2[Subscribe[aviation-india-states]]: {"aviation-india-states":{"2":7613,"1":8550,"0":9619}}}
26/02/13 12:02:34 INFO AdminClientConfig: AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.servers = [kafka:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

26/02/13 12:02:34 INFO AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
26/02/13 12:02:34 INFO AppInfoParser: Kafka version: 3.5.1
26/02/13 12:02:34 INFO AppInfoParser: Kafka commitId: 2c6fb6c54472e90a
26/02/13 12:02:34 INFO AppInfoParser: Kafka startTimeMs: 1770984154694
26/02/13 12:02:35 WARN KafkaOffsetReaderAdmin: Found incorrect offsets in some partitions (partition, previous offset, fetched offset): ArrayBuffer((aviation-india-states-2,7613,560), (aviation-india-states-1,8550,695), (aviation-india-states-0,9619,502))
26/02/13 12:02:35 WARN KafkaOffsetReaderAdmin: Retrying to fetch latest offsets because of incorrect offsets
26/02/13 12:02:36 WARN KafkaOffsetReaderAdmin: Found incorrect offsets in some partitions (partition, previous offset, fetched offset): ArrayBuffer((aviation-india-states-2,7613,560), (aviation-india-states-1,8550,695), (aviation-india-states-0,9619,502))
26/02/13 12:02:36 WARN KafkaOffsetReaderAdmin: Retrying to fetch latest offsets because of incorrect offsets
26/02/13 12:02:37 WARN KafkaOffsetReaderAdmin: Found incorrect offsets in some partitions (partition, previous offset, fetched offset): ArrayBuffer((aviation-india-states-2,7613,560), (aviation-india-states-1,8550,695), (aviation-india-states-0,9619,502))
26/02/13 12:02:37 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/123 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.123.2e1d93c1-4de2-4741-b61b-4bc1723a2893.tmp
26/02/13 12:02:37 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.123.2e1d93c1-4de2-4741-b61b-4bc1723a2893.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/123
26/02/13 12:02:37 INFO MicroBatchExecution: Committed offsets for batch 123. Metadata OffsetSeqMetadata(0,1770984157099,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 12:02:37 INFO OffsetSeqLog: BatchIds found from listing: 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 123
26/02/13 12:02:37 INFO CommitLog: BatchIds found from listing: 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122
26/02/13 12:02:37 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:02:37 WARN KafkaMicroBatchStream: Partition aviation-india-states-2's offset was changed from 7613 to 560, some data may have been missed. 
Some data may have been lost because they are not available in Kafka any more; either the
 data was aged out by Kafka or the topic may have been deleted before all the data in the
 topic was processed. If you want your streaming query to fail on such cases, set the source
 option "failOnDataLoss" to "true".
    
26/02/13 12:02:37 WARN KafkaMicroBatchStream: Partition aviation-india-states-1's offset was changed from 8550 to 695, some data may have been missed. 
Some data may have been lost because they are not available in Kafka any more; either the
 data was aged out by Kafka or the topic may have been deleted before all the data in the
 topic was processed. If you want your streaming query to fail on such cases, set the source
 option "failOnDataLoss" to "true".
    
26/02/13 12:02:37 WARN KafkaMicroBatchStream: Partition aviation-india-states-0's offset was changed from 9619 to 502, some data may have been missed. 
Some data may have been lost because they are not available in Kafka any more; either the
 data was aged out by Kafka or the topic may have been deleted before all the data in the
 topic was processed. If you want your streaming query to fail on such cases, set the source
 option "failOnDataLoss" to "true".
    
26/02/13 12:02:37 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:02:37 WARN KafkaMicroBatchStream: Partition aviation-india-states-2's offset was changed from 7613 to 560, some data may have been missed. 
Some data may have been lost because they are not available in Kafka any more; either the
 data was aged out by Kafka or the topic may have been deleted before all the data in the
 topic was processed. If you want your streaming query to fail on such cases, set the source
 option "failOnDataLoss" to "true".
    
26/02/13 12:02:37 WARN KafkaMicroBatchStream: Partition aviation-india-states-1's offset was changed from 8550 to 695, some data may have been missed. 
Some data may have been lost because they are not available in Kafka any more; either the
 data was aged out by Kafka or the topic may have been deleted before all the data in the
 topic was processed. If you want your streaming query to fail on such cases, set the source
 option "failOnDataLoss" to "true".
    
26/02/13 12:02:37 WARN KafkaMicroBatchStream: Partition aviation-india-states-0's offset was changed from 9619 to 502, some data may have been missed. 
Some data may have been lost because they are not available in Kafka any more; either the
 data was aged out by Kafka or the topic may have been deleted before all the data in the
 topic was processed. If you want your streaming query to fail on such cases, set the source
 option "failOnDataLoss" to "true".
    
26/02/13 12:02:37 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:02:37 WARN KafkaMicroBatchStream: Partition aviation-india-states-2's offset was changed from 7613 to 560, some data may have been missed. 
Some data may have been lost because they are not available in Kafka any more; either the
 data was aged out by Kafka or the topic may have been deleted before all the data in the
 topic was processed. If you want your streaming query to fail on such cases, set the source
 option "failOnDataLoss" to "true".
    
26/02/13 12:02:37 WARN KafkaMicroBatchStream: Partition aviation-india-states-1's offset was changed from 8550 to 695, some data may have been missed. 
Some data may have been lost because they are not available in Kafka any more; either the
 data was aged out by Kafka or the topic may have been deleted before all the data in the
 topic was processed. If you want your streaming query to fail on such cases, set the source
 option "failOnDataLoss" to "true".
    
26/02/13 12:02:37 WARN KafkaMicroBatchStream: Partition aviation-india-states-0's offset was changed from 9619 to 502, some data may have been missed. 
Some data may have been lost because they are not available in Kafka any more; either the
 data was aged out by Kafka or the topic may have been deleted before all the data in the
 topic was processed. If you want your streaming query to fail on such cases, set the source
 option "failOnDataLoss" to "true".
    
26/02/13 12:02:37 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:02:37 WARN KafkaMicroBatchStream: Partition aviation-india-states-2's offset was changed from 7613 to 560, some data may have been missed. 
Some data may have been lost because they are not available in Kafka any more; either the
 data was aged out by Kafka or the topic may have been deleted before all the data in the
 topic was processed. If you want your streaming query to fail on such cases, set the source
 option "failOnDataLoss" to "true".
    
26/02/13 12:02:37 WARN KafkaMicroBatchStream: Partition aviation-india-states-1's offset was changed from 8550 to 695, some data may have been missed. 
Some data may have been lost because they are not available in Kafka any more; either the
 data was aged out by Kafka or the topic may have been deleted before all the data in the
 topic was processed. If you want your streaming query to fail on such cases, set the source
 option "failOnDataLoss" to "true".
    
26/02/13 12:02:37 WARN KafkaMicroBatchStream: Partition aviation-india-states-0's offset was changed from 9619 to 502, some data may have been missed. 
Some data may have been lost because they are not available in Kafka any more; either the
 data was aged out by Kafka or the topic may have been deleted before all the data in the
 topic was processed. If you want your streaming query to fail on such cases, set the source
 option "failOnDataLoss" to "true".
    
26/02/13 12:02:37 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:02:37 WARN KafkaMicroBatchStream: Partition aviation-india-states-2's offset was changed from 7613 to 560, some data may have been missed. 
Some data may have been lost because they are not available in Kafka any more; either the
 data was aged out by Kafka or the topic may have been deleted before all the data in the
 topic was processed. If you want your streaming query to fail on such cases, set the source
 option "failOnDataLoss" to "true".
    
26/02/13 12:02:37 WARN KafkaMicroBatchStream: Partition aviation-india-states-1's offset was changed from 8550 to 695, some data may have been missed. 
Some data may have been lost because they are not available in Kafka any more; either the
 data was aged out by Kafka or the topic may have been deleted before all the data in the
 topic was processed. If you want your streaming query to fail on such cases, set the source
 option "failOnDataLoss" to "true".
    
26/02/13 12:02:37 WARN KafkaMicroBatchStream: Partition aviation-india-states-0's offset was changed from 9619 to 502, some data may have been missed. 
Some data may have been lost because they are not available in Kafka any more; either the
 data was aged out by Kafka or the topic may have been deleted before all the data in the
 topic was processed. If you want your streaming query to fail on such cases, set the source
 option "failOnDataLoss" to "true".
    
26/02/13 12:02:37 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:02:37 WARN KafkaMicroBatchStream: Partition aviation-india-states-2's offset was changed from 7613 to 560, some data may have been missed. 
Some data may have been lost because they are not available in Kafka any more; either the
 data was aged out by Kafka or the topic may have been deleted before all the data in the
 topic was processed. If you want your streaming query to fail on such cases, set the source
 option "failOnDataLoss" to "true".
    
26/02/13 12:02:37 WARN KafkaMicroBatchStream: Partition aviation-india-states-1's offset was changed from 8550 to 695, some data may have been missed. 
Some data may have been lost because they are not available in Kafka any more; either the
 data was aged out by Kafka or the topic may have been deleted before all the data in the
 topic was processed. If you want your streaming query to fail on such cases, set the source
 option "failOnDataLoss" to "true".
    
26/02/13 12:02:37 WARN KafkaMicroBatchStream: Partition aviation-india-states-0's offset was changed from 9619 to 502, some data may have been missed. 
Some data may have been lost because they are not available in Kafka any more; either the
 data was aged out by Kafka or the topic may have been deleted before all the data in the
 topic was processed. If you want your streaming query to fail on such cases, set the source
 option "failOnDataLoss" to "true".
    
26/02/13 12:02:37 INFO CodeGenerator: Code generated in 16.709744 ms
26/02/13 12:02:37 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#1951 - origin_code.nullCount#1950) > 0)
26/02/13 12:02:37 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#1956 - destination_code.nullCount#1955) > 0)
26/02/13 12:02:37 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#1986 - callsign.nullCount#1985) > 0)
26/02/13 12:02:37 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 12:02:37 INFO DAGScheduler: Got job 9 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 12:02:37 INFO DAGScheduler: Final stage: ResultStage 17 (start at NativeMethodAccessorImpl.java:0)
26/02/13 12:02:37 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 16)
26/02/13 12:02:37 INFO DAGScheduler: Missing parents: List()
26/02/13 12:02:37 INFO DAGScheduler: Submitting ResultStage 17 (MapPartitionsRDD[42] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:02:37 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 99.3 KiB, free 434.1 MiB)
26/02/13 12:02:37 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 434.0 MiB)
26/02/13 12:02:37 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on spark-master:42073 (size: 33.3 KiB, free: 434.3 MiB)
26/02/13 12:02:37 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1585
26/02/13 12:02:37 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 17 (MapPartitionsRDD[42] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 12:02:37 INFO TaskSchedulerImpl: Adding task set 17.0 with 4 tasks resource profile 0
26/02/13 12:02:37 INFO TaskSetManager: Starting task 0.0 in stage 17.0 (TID 15) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:02:37 INFO TaskSetManager: Starting task 1.0 in stage 17.0 (TID 16) (172.18.0.14, executor 1, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:02:37 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on 172.18.0.14:37571 (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 12:02:37 INFO TaskSetManager: Starting task 2.0 in stage 17.0 (TID 17) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:02:37 INFO TaskSetManager: Starting task 3.0 in stage 17.0 (TID 18) (172.18.0.14, executor 1, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:02:37 INFO TaskSetManager: Finished task 1.0 in stage 17.0 (TID 16) in 145 ms on 172.18.0.14 (executor 1) (1/4)
26/02/13 12:02:37 INFO TaskSetManager: Finished task 0.0 in stage 17.0 (TID 15) in 147 ms on 172.18.0.14 (executor 1) (2/4)
26/02/13 12:02:37 INFO TaskSetManager: Finished task 2.0 in stage 17.0 (TID 17) in 63 ms on 172.18.0.14 (executor 1) (3/4)
26/02/13 12:02:37 INFO TaskSetManager: Finished task 3.0 in stage 17.0 (TID 18) in 64 ms on 172.18.0.14 (executor 1) (4/4)
26/02/13 12:02:37 INFO TaskSchedulerImpl: Removed TaskSet 17.0, whose tasks have all completed, from pool 
26/02/13 12:02:37 INFO DAGScheduler: ResultStage 17 (start at NativeMethodAccessorImpl.java:0) finished in 0.224 s
26/02/13 12:02:37 INFO DAGScheduler: Job 9 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 12:02:37 INFO TaskSchedulerImpl: Killing all running tasks in stage 17: Stage finished
26/02/13 12:02:37 INFO DAGScheduler: Job 9 finished: start at NativeMethodAccessorImpl.java:0, took 0.232045 s
26/02/13 12:02:37 INFO CodeGenerator: Code generated in 13.912175 ms
26/02/13 12:02:38 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.9 MiB)
26/02/13 12:02:38 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on spark-master:42073 (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 12:02:38 INFO SparkContext: Created broadcast 14 from start at NativeMethodAccessorImpl.java:0
26/02/13 12:02:38 INFO CodeGenerator: Code generated in 72.533244 ms
26/02/13 12:02:38 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 123, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@2c04f7]. The input RDD has 1 partitions.
26/02/13 12:02:38 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 12:02:38 INFO DAGScheduler: Got job 10 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/02/13 12:02:38 INFO DAGScheduler: Final stage: ResultStage 18 (start at NativeMethodAccessorImpl.java:0)
26/02/13 12:02:38 INFO DAGScheduler: Parents of final stage: List()
26/02/13 12:02:38 INFO DAGScheduler: Missing parents: List()
26/02/13 12:02:38 INFO DAGScheduler: Submitting ResultStage 18 (ParallelCollectionRDD[49] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:02:38 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 4.4 KiB, free 433.9 MiB)
26/02/13 12:02:38 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 2.6 KiB, free 433.9 MiB)
26/02/13 12:02:38 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on spark-master:42073 (size: 2.6 KiB, free: 434.2 MiB)
26/02/13 12:02:38 INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1585
26/02/13 12:02:38 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 18 (ParallelCollectionRDD[49] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/02/13 12:02:38 INFO TaskSchedulerImpl: Adding task set 18.0 with 1 tasks resource profile 0
26/02/13 12:02:38 INFO TaskSetManager: Starting task 0.0 in stage 18.0 (TID 19) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 7640 bytes) 
26/02/13 12:02:38 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on 172.18.0.14:37571 (size: 2.6 KiB, free: 434.1 MiB)
26/02/13 12:02:38 INFO TaskSetManager: Finished task 0.0 in stage 18.0 (TID 19) in 191 ms on 172.18.0.14 (executor 1) (1/1)
26/02/13 12:02:38 INFO TaskSchedulerImpl: Removed TaskSet 18.0, whose tasks have all completed, from pool 
26/02/13 12:02:38 INFO DAGScheduler: ResultStage 18 (start at NativeMethodAccessorImpl.java:0) finished in 0.204 s
26/02/13 12:02:38 INFO DAGScheduler: Job 10 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 12:02:38 INFO TaskSchedulerImpl: Killing all running tasks in stage 18: Stage finished
26/02/13 12:02:38 INFO DAGScheduler: Job 10 finished: start at NativeMethodAccessorImpl.java:0, took 0.217129 s
26/02/13 12:02:38 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 123, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@2c04f7] is committing.
26/02/13 12:02:38 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 123, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@2c04f7] committed.
26/02/13 12:02:38 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/123 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.123.e822e7e8-92af-4eef-ad68-f81fe84d6ff6.tmp
26/02/13 12:02:38 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.123.e822e7e8-92af-4eef-ad68-f81fe84d6ff6.tmp to file:/tmp/spark-checkpoint-enrichment/commits/123
26/02/13 12:02:38 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "4a161d66-225b-4b29-812d-ddec3c05364e",
  "name" : null,
  "timestamp" : "2026-02-13T12:02:34.385Z",
  "batchId" : 123,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "addBatch" : 1000,
    "commitOffsets" : 97,
    "getBatch" : 3,
    "latestOffset" : 2587,
    "queryPlanning" : 240,
    "triggerExecution" : 4144,
    "walCommit" : 77
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 7613,
        "1" : 8550,
        "0" : 9619
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 560,
        "1" : 695,
        "0" : 502
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 560,
        "1" : 695,
        "0" : 502
      }
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@1a6a42c2",
    "numOutputRows" : 0
  }
}
26/02/13 12:02:42 INFO BlockManagerInfo: Removed broadcast_13_piece0 on spark-master:42073 in memory (size: 33.3 KiB, free: 434.3 MiB)
26/02/13 12:02:42 INFO BlockManagerInfo: Removed broadcast_13_piece0 on 172.18.0.14:37571 in memory (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 12:02:42 INFO BlockManagerInfo: Removed broadcast_15_piece0 on spark-master:42073 in memory (size: 2.6 KiB, free: 434.3 MiB)
26/02/13 12:02:42 INFO BlockManagerInfo: Removed broadcast_15_piece0 on 172.18.0.14:37571 in memory (size: 2.6 KiB, free: 434.2 MiB)
26/02/13 12:02:45 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/124 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.124.0c35f4b1-a74c-4e30-bf0f-20f6f590d1ce.tmp
26/02/13 12:02:45 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.124.0c35f4b1-a74c-4e30-bf0f-20f6f590d1ce.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/124
26/02/13 12:02:45 INFO MicroBatchExecution: Committed offsets for batch 124. Metadata OffsetSeqMetadata(0,1770984165635,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 12:02:45 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:02:45 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:02:46 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:02:46 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:02:46 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:02:46 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:02:46 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#2805 - origin_code.nullCount#2804) > 0)
26/02/13 12:02:46 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#2810 - destination_code.nullCount#2809) > 0)
26/02/13 12:02:46 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#2840 - callsign.nullCount#2839) > 0)
26/02/13 12:02:46 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 12:02:46 INFO DAGScheduler: Got job 11 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 12:02:46 INFO DAGScheduler: Final stage: ResultStage 20 (start at NativeMethodAccessorImpl.java:0)
26/02/13 12:02:46 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 19)
26/02/13 12:02:46 INFO DAGScheduler: Missing parents: List()
26/02/13 12:02:46 INFO DAGScheduler: Submitting ResultStage 20 (MapPartitionsRDD[54] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:02:46 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 99.3 KiB, free 434.0 MiB)
26/02/13 12:02:46 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 433.9 MiB)
26/02/13 12:02:46 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on spark-master:42073 (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 12:02:46 INFO SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1585
26/02/13 12:02:46 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 20 (MapPartitionsRDD[54] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 12:02:46 INFO TaskSchedulerImpl: Adding task set 20.0 with 4 tasks resource profile 0
26/02/13 12:02:46 INFO TaskSetManager: Starting task 0.0 in stage 20.0 (TID 20) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:02:46 INFO TaskSetManager: Starting task 1.0 in stage 20.0 (TID 21) (172.18.0.14, executor 1, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:02:46 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on 172.18.0.14:37571 (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 12:02:46 INFO TaskSetManager: Starting task 2.0 in stage 20.0 (TID 22) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:02:46 INFO BlockManagerInfo: Removed broadcast_14_piece0 on spark-master:42073 in memory (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 12:02:46 INFO TaskSetManager: Starting task 3.0 in stage 20.0 (TID 23) (172.18.0.14, executor 1, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:02:46 INFO TaskSetManager: Finished task 1.0 in stage 20.0 (TID 21) in 105 ms on 172.18.0.14 (executor 1) (1/4)
26/02/13 12:02:46 INFO TaskSetManager: Finished task 0.0 in stage 20.0 (TID 20) in 110 ms on 172.18.0.14 (executor 1) (2/4)
26/02/13 12:02:46 INFO TaskSetManager: Finished task 3.0 in stage 20.0 (TID 23) in 56 ms on 172.18.0.14 (executor 1) (3/4)
26/02/13 12:02:46 INFO TaskSetManager: Finished task 2.0 in stage 20.0 (TID 22) in 62 ms on 172.18.0.14 (executor 1) (4/4)
26/02/13 12:02:46 INFO TaskSchedulerImpl: Removed TaskSet 20.0, whose tasks have all completed, from pool 
26/02/13 12:02:46 INFO DAGScheduler: ResultStage 20 (start at NativeMethodAccessorImpl.java:0) finished in 0.181 s
26/02/13 12:02:46 INFO DAGScheduler: Job 11 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 12:02:46 INFO TaskSchedulerImpl: Killing all running tasks in stage 20: Stage finished
26/02/13 12:02:46 INFO DAGScheduler: Job 11 finished: start at NativeMethodAccessorImpl.java:0, took 0.189795 s
26/02/13 12:02:46 INFO MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.9 MiB)
26/02/13 12:02:46 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on spark-master:42073 (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 12:02:46 INFO SparkContext: Created broadcast 17 from start at NativeMethodAccessorImpl.java:0
26/02/13 12:02:46 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 124, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@60a8d7cc]. The input RDD has 3 partitions.
26/02/13 12:02:46 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 12:02:46 INFO DAGScheduler: Got job 12 (start at NativeMethodAccessorImpl.java:0) with 3 output partitions
26/02/13 12:02:46 INFO DAGScheduler: Final stage: ResultStage 21 (start at NativeMethodAccessorImpl.java:0)
26/02/13 12:02:46 INFO DAGScheduler: Parents of final stage: List()
26/02/13 12:02:46 INFO DAGScheduler: Missing parents: List()
26/02/13 12:02:46 INFO DAGScheduler: Submitting ResultStage 21 (MapPartitionsRDD[60] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:02:46 INFO MemoryStore: Block broadcast_18 stored as values in memory (estimated size 49.9 KiB, free 433.9 MiB)
26/02/13 12:02:46 INFO MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.9 MiB)
26/02/13 12:02:46 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on spark-master:42073 (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 12:02:46 INFO SparkContext: Created broadcast 18 from broadcast at DAGScheduler.scala:1585
26/02/13 12:02:46 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 21 (MapPartitionsRDD[60] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
26/02/13 12:02:46 INFO TaskSchedulerImpl: Adding task set 21.0 with 3 tasks resource profile 0
26/02/13 12:02:46 INFO TaskSetManager: Starting task 1.0 in stage 21.0 (TID 24) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:02:46 INFO TaskSetManager: Starting task 0.0 in stage 21.0 (TID 25) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:02:46 INFO TaskSetManager: Starting task 2.0 in stage 21.0 (TID 26) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:02:46 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on 172.18.0.14:37571 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 12:02:46 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on 172.18.0.14:37571 (size: 107.2 KiB, free: 434.0 MiB)
26/02/13 12:02:46 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on 172.18.0.15:37717 (size: 19.5 KiB, free: 434.4 MiB)
26/02/13 12:02:48 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on 172.18.0.15:37717 (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 12:02:49 INFO TaskSetManager: Finished task 2.0 in stage 21.0 (TID 26) in 2515 ms on 172.18.0.14 (executor 1) (1/3)
26/02/13 12:02:49 INFO TaskSetManager: Finished task 0.0 in stage 21.0 (TID 25) in 2531 ms on 172.18.0.14 (executor 1) (2/3)
26/02/13 12:02:50 INFO TaskSetManager: Finished task 1.0 in stage 21.0 (TID 24) in 4210 ms on 172.18.0.15 (executor 0) (3/3)
26/02/13 12:02:50 INFO TaskSchedulerImpl: Removed TaskSet 21.0, whose tasks have all completed, from pool 
26/02/13 12:02:50 INFO DAGScheduler: ResultStage 21 (start at NativeMethodAccessorImpl.java:0) finished in 4.236 s
26/02/13 12:02:50 INFO DAGScheduler: Job 12 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 12:02:50 INFO TaskSchedulerImpl: Killing all running tasks in stage 21: Stage finished
26/02/13 12:02:50 INFO DAGScheduler: Job 12 finished: start at NativeMethodAccessorImpl.java:0, took 4.242215 s
26/02/13 12:02:50 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 124, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@60a8d7cc] is committing.
26/02/13 12:02:50 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 124, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@60a8d7cc] committed.
26/02/13 12:02:50 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/124 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.124.34e05788-282f-448b-bb65-754ead52d189.tmp
26/02/13 12:02:50 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.124.34e05788-282f-448b-bb65-754ead52d189.tmp to file:/tmp/spark-checkpoint-enrichment/commits/124
26/02/13 12:02:50 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "4a161d66-225b-4b29-812d-ddec3c05364e",
  "name" : null,
  "timestamp" : "2026-02-13T12:02:45.622Z",
  "batchId" : 124,
  "numInputRows" : 67,
  "inputRowsPerSecond" : 2913.0434782608695,
  "processedRowsPerSecond" : 13.024883359253499,
  "durationMs" : {
    "addBatch" : 4736,
    "commitOffsets" : 62,
    "getBatch" : 0,
    "latestOffset" : 12,
    "queryPlanning" : 219,
    "triggerExecution" : 5144,
    "walCommit" : 111
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 560,
        "1" : 695,
        "0" : 502
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 582,
        "1" : 722,
        "0" : 520
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 582,
        "1" : 722,
        "0" : 520
      }
    },
    "numInputRows" : 67,
    "inputRowsPerSecond" : 2913.0434782608695,
    "processedRowsPerSecond" : 13.024883359253499,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@1a6a42c2",
    "numOutputRows" : 17
  }
}
26/02/13 12:02:50 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/125 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.125.2f529519-8262-4bd9-9ad7-0876c387d95a.tmp
26/02/13 12:02:50 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.125.2f529519-8262-4bd9-9ad7-0876c387d95a.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/125
26/02/13 12:02:50 INFO MicroBatchExecution: Committed offsets for batch 125. Metadata OffsetSeqMetadata(0,1770984170771,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 12:02:50 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:02:50 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:02:50 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:02:50 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:02:50 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:02:50 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:02:50 INFO BlockManagerInfo: Removed broadcast_16_piece0 on spark-master:42073 in memory (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 12:02:51 INFO BlockManagerInfo: Removed broadcast_16_piece0 on 172.18.0.14:37571 in memory (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 12:02:51 INFO BlockManagerInfo: Removed broadcast_18_piece0 on spark-master:42073 in memory (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 12:02:51 INFO BlockManagerInfo: Removed broadcast_18_piece0 on 172.18.0.14:37571 in memory (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 12:02:51 INFO BlockManagerInfo: Removed broadcast_18_piece0 on 172.18.0.15:37717 in memory (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 12:02:51 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#3659 - origin_code.nullCount#3658) > 0)
26/02/13 12:02:51 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#3664 - destination_code.nullCount#3663) > 0)
26/02/13 12:02:51 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#3694 - callsign.nullCount#3693) > 0)
26/02/13 12:02:51 INFO BlockManagerInfo: Removed broadcast_17_piece0 on spark-master:42073 in memory (size: 107.2 KiB, free: 434.4 MiB)
26/02/13 12:02:51 INFO BlockManagerInfo: Removed broadcast_17_piece0 on 172.18.0.14:37571 in memory (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 12:02:51 INFO BlockManagerInfo: Removed broadcast_17_piece0 on 172.18.0.15:37717 in memory (size: 107.2 KiB, free: 434.4 MiB)
26/02/13 12:02:51 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 12:02:51 INFO DAGScheduler: Got job 13 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 12:02:51 INFO DAGScheduler: Final stage: ResultStage 23 (start at NativeMethodAccessorImpl.java:0)
26/02/13 12:02:51 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 22)
26/02/13 12:02:51 INFO DAGScheduler: Missing parents: List()
26/02/13 12:02:51 INFO DAGScheduler: Submitting ResultStage 23 (MapPartitionsRDD[65] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:02:51 INFO MemoryStore: Block broadcast_19 stored as values in memory (estimated size 99.3 KiB, free 434.1 MiB)
26/02/13 12:02:51 INFO MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 434.0 MiB)
26/02/13 12:02:51 INFO BlockManagerInfo: Added broadcast_19_piece0 in memory on spark-master:42073 (size: 33.3 KiB, free: 434.3 MiB)
26/02/13 12:02:51 INFO SparkContext: Created broadcast 19 from broadcast at DAGScheduler.scala:1585
26/02/13 12:02:51 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 23 (MapPartitionsRDD[65] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 12:02:51 INFO TaskSchedulerImpl: Adding task set 23.0 with 4 tasks resource profile 0
26/02/13 12:02:51 INFO TaskSetManager: Starting task 0.0 in stage 23.0 (TID 27) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:02:51 INFO TaskSetManager: Starting task 1.0 in stage 23.0 (TID 28) (172.18.0.14, executor 1, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:02:51 INFO BlockManagerInfo: Added broadcast_19_piece0 in memory on 172.18.0.14:37571 (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 12:02:51 INFO TaskSetManager: Starting task 2.0 in stage 23.0 (TID 29) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:02:51 INFO TaskSetManager: Finished task 0.0 in stage 23.0 (TID 27) in 50 ms on 172.18.0.14 (executor 1) (1/4)
26/02/13 12:02:51 INFO TaskSetManager: Starting task 3.0 in stage 23.0 (TID 30) (172.18.0.14, executor 1, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:02:51 INFO TaskSetManager: Finished task 1.0 in stage 23.0 (TID 28) in 56 ms on 172.18.0.14 (executor 1) (2/4)
26/02/13 12:02:51 INFO TaskSetManager: Finished task 2.0 in stage 23.0 (TID 29) in 51 ms on 172.18.0.14 (executor 1) (3/4)
26/02/13 12:02:51 INFO TaskSetManager: Finished task 3.0 in stage 23.0 (TID 30) in 55 ms on 172.18.0.14 (executor 1) (4/4)
26/02/13 12:02:51 INFO TaskSchedulerImpl: Removed TaskSet 23.0, whose tasks have all completed, from pool 
26/02/13 12:02:51 INFO DAGScheduler: ResultStage 23 (start at NativeMethodAccessorImpl.java:0) finished in 0.123 s
26/02/13 12:02:51 INFO DAGScheduler: Job 13 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 12:02:51 INFO TaskSchedulerImpl: Killing all running tasks in stage 23: Stage finished
26/02/13 12:02:51 INFO DAGScheduler: Job 13 finished: start at NativeMethodAccessorImpl.java:0, took 0.130037 s
26/02/13 12:02:51 INFO MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.9 MiB)
26/02/13 12:02:51 INFO BlockManagerInfo: Added broadcast_20_piece0 in memory on spark-master:42073 (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 12:02:51 INFO SparkContext: Created broadcast 20 from start at NativeMethodAccessorImpl.java:0
26/02/13 12:02:51 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 125, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@2b164f88]. The input RDD has 3 partitions.
26/02/13 12:02:51 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 12:02:51 INFO DAGScheduler: Got job 14 (start at NativeMethodAccessorImpl.java:0) with 3 output partitions
26/02/13 12:02:51 INFO DAGScheduler: Final stage: ResultStage 24 (start at NativeMethodAccessorImpl.java:0)
26/02/13 12:02:51 INFO DAGScheduler: Parents of final stage: List()
26/02/13 12:02:51 INFO DAGScheduler: Missing parents: List()
26/02/13 12:02:51 INFO DAGScheduler: Submitting ResultStage 24 (MapPartitionsRDD[71] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:02:51 INFO MemoryStore: Block broadcast_21 stored as values in memory (estimated size 49.9 KiB, free 433.9 MiB)
26/02/13 12:02:51 INFO MemoryStore: Block broadcast_21_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.9 MiB)
26/02/13 12:02:51 INFO BlockManagerInfo: Added broadcast_21_piece0 in memory on spark-master:42073 (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 12:02:51 INFO SparkContext: Created broadcast 21 from broadcast at DAGScheduler.scala:1585
26/02/13 12:02:51 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 24 (MapPartitionsRDD[71] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
26/02/13 12:02:51 INFO TaskSchedulerImpl: Adding task set 24.0 with 3 tasks resource profile 0
26/02/13 12:02:51 INFO TaskSetManager: Starting task 0.0 in stage 24.0 (TID 31) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:02:51 INFO TaskSetManager: Starting task 1.0 in stage 24.0 (TID 32) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:02:51 INFO TaskSetManager: Starting task 2.0 in stage 24.0 (TID 33) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:02:51 INFO BlockManagerInfo: Added broadcast_21_piece0 in memory on 172.18.0.14:37571 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 12:02:51 INFO BlockManagerInfo: Added broadcast_21_piece0 in memory on 172.18.0.15:37717 (size: 19.5 KiB, free: 434.4 MiB)
26/02/13 12:02:51 INFO BlockManagerInfo: Added broadcast_20_piece0 in memory on 172.18.0.14:37571 (size: 107.2 KiB, free: 434.0 MiB)
26/02/13 12:02:51 INFO BlockManagerInfo: Added broadcast_20_piece0 in memory on 172.18.0.15:37717 (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 12:02:51 INFO TaskSetManager: Finished task 2.0 in stage 24.0 (TID 33) in 280 ms on 172.18.0.14 (executor 1) (1/3)
26/02/13 12:02:51 INFO TaskSetManager: Finished task 0.0 in stage 24.0 (TID 31) in 333 ms on 172.18.0.14 (executor 1) (2/3)
26/02/13 12:02:51 INFO TaskSetManager: Finished task 1.0 in stage 24.0 (TID 32) in 505 ms on 172.18.0.15 (executor 0) (3/3)
26/02/13 12:02:51 INFO TaskSchedulerImpl: Removed TaskSet 24.0, whose tasks have all completed, from pool 
26/02/13 12:02:51 INFO DAGScheduler: ResultStage 24 (start at NativeMethodAccessorImpl.java:0) finished in 0.514 s
26/02/13 12:02:51 INFO DAGScheduler: Job 14 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 12:02:51 INFO TaskSchedulerImpl: Killing all running tasks in stage 24: Stage finished
26/02/13 12:02:51 INFO DAGScheduler: Job 14 finished: start at NativeMethodAccessorImpl.java:0, took 0.517818 s
26/02/13 12:02:51 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 125, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@2b164f88] is committing.
26/02/13 12:02:51 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 125, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@2b164f88] committed.
26/02/13 12:02:51 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/125 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.125.3f52c63d-647d-4931-ac74-45d3be27d957.tmp
26/02/13 12:02:51 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.125.3f52c63d-647d-4931-ac74-45d3be27d957.tmp to file:/tmp/spark-checkpoint-enrichment/commits/125
26/02/13 12:02:51 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "4a161d66-225b-4b29-812d-ddec3c05364e",
  "name" : null,
  "timestamp" : "2026-02-13T12:02:50.768Z",
  "batchId" : 125,
  "numInputRows" : 181,
  "inputRowsPerSecond" : 35.172949863972015,
  "processedRowsPerSecond" : 169.9530516431925,
  "durationMs" : {
    "addBatch" : 853,
    "commitOffsets" : 93,
    "getBatch" : 0,
    "latestOffset" : 3,
    "queryPlanning" : 50,
    "triggerExecution" : 1065,
    "walCommit" : 65
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 582,
        "1" : 722,
        "0" : 520
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 640,
        "1" : 790,
        "0" : 575
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 640,
        "1" : 790,
        "0" : 575
      }
    },
    "numInputRows" : 181,
    "inputRowsPerSecond" : 35.172949863972015,
    "processedRowsPerSecond" : 169.9530516431925,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@1a6a42c2",
    "numOutputRows" : 34
  }
}
26/02/13 12:02:56 INFO BlockManagerInfo: Removed broadcast_21_piece0 on spark-master:42073 in memory (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 12:02:56 INFO BlockManagerInfo: Removed broadcast_21_piece0 on 172.18.0.14:37571 in memory (size: 19.5 KiB, free: 434.0 MiB)
26/02/13 12:02:56 INFO BlockManagerInfo: Removed broadcast_21_piece0 on 172.18.0.15:37717 in memory (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 12:02:56 INFO BlockManagerInfo: Removed broadcast_19_piece0 on spark-master:42073 in memory (size: 33.3 KiB, free: 434.3 MiB)
26/02/13 12:02:56 INFO BlockManagerInfo: Removed broadcast_19_piece0 on 172.18.0.14:37571 in memory (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 12:03:01 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:03:02 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/126 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.126.b021f94a-a0c1-4ffc-bf90-afe4cf56b053.tmp
26/02/13 12:03:02 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.126.b021f94a-a0c1-4ffc-bf90-afe4cf56b053.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/126
26/02/13 12:03:02 INFO MicroBatchExecution: Committed offsets for batch 126. Metadata OffsetSeqMetadata(0,1770984182280,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 12:03:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:03:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:03:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:03:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:03:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:03:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:03:02 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#4513 - origin_code.nullCount#4512) > 0)
26/02/13 12:03:02 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#4518 - destination_code.nullCount#4517) > 0)
26/02/13 12:03:02 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#4548 - callsign.nullCount#4547) > 0)
26/02/13 12:03:02 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 12:03:02 INFO DAGScheduler: Got job 15 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 12:03:02 INFO DAGScheduler: Final stage: ResultStage 26 (start at NativeMethodAccessorImpl.java:0)
26/02/13 12:03:02 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 25)
26/02/13 12:03:02 INFO DAGScheduler: Missing parents: List()
26/02/13 12:03:02 INFO DAGScheduler: Submitting ResultStage 26 (MapPartitionsRDD[76] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:03:02 INFO MemoryStore: Block broadcast_22 stored as values in memory (estimated size 99.3 KiB, free 434.0 MiB)
26/02/13 12:03:02 INFO MemoryStore: Block broadcast_22_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 433.9 MiB)
26/02/13 12:03:02 INFO BlockManagerInfo: Added broadcast_22_piece0 in memory on spark-master:42073 (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 12:03:02 INFO SparkContext: Created broadcast 22 from broadcast at DAGScheduler.scala:1585
26/02/13 12:03:02 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 26 (MapPartitionsRDD[76] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 12:03:02 INFO TaskSchedulerImpl: Adding task set 26.0 with 4 tasks resource profile 0
26/02/13 12:03:02 INFO TaskSetManager: Starting task 0.0 in stage 26.0 (TID 34) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:03:02 INFO TaskSetManager: Starting task 1.0 in stage 26.0 (TID 35) (172.18.0.14, executor 1, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:03:02 INFO BlockManagerInfo: Added broadcast_22_piece0 in memory on 172.18.0.14:37571 (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 12:03:02 INFO TaskSetManager: Starting task 2.0 in stage 26.0 (TID 36) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:03:02 INFO TaskSetManager: Finished task 1.0 in stage 26.0 (TID 35) in 79 ms on 172.18.0.14 (executor 1) (1/4)
26/02/13 12:03:02 INFO TaskSetManager: Starting task 3.0 in stage 26.0 (TID 37) (172.18.0.14, executor 1, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:03:02 INFO TaskSetManager: Finished task 0.0 in stage 26.0 (TID 34) in 104 ms on 172.18.0.14 (executor 1) (2/4)
26/02/13 12:03:02 INFO TaskSetManager: Finished task 2.0 in stage 26.0 (TID 36) in 74 ms on 172.18.0.14 (executor 1) (3/4)
26/02/13 12:03:02 INFO TaskSetManager: Finished task 3.0 in stage 26.0 (TID 37) in 53 ms on 172.18.0.14 (executor 1) (4/4)
26/02/13 12:03:02 INFO TaskSchedulerImpl: Removed TaskSet 26.0, whose tasks have all completed, from pool 
26/02/13 12:03:02 INFO DAGScheduler: ResultStage 26 (start at NativeMethodAccessorImpl.java:0) finished in 0.174 s
26/02/13 12:03:02 INFO DAGScheduler: Job 15 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 12:03:02 INFO TaskSchedulerImpl: Killing all running tasks in stage 26: Stage finished
26/02/13 12:03:02 INFO DAGScheduler: Job 15 finished: start at NativeMethodAccessorImpl.java:0, took 0.180202 s
26/02/13 12:03:02 INFO MemoryStore: Block broadcast_23_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.8 MiB)
26/02/13 12:03:02 INFO BlockManagerInfo: Added broadcast_23_piece0 in memory on spark-master:42073 (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 12:03:02 INFO SparkContext: Created broadcast 23 from start at NativeMethodAccessorImpl.java:0
26/02/13 12:03:02 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 126, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@33c3a72f]. The input RDD has 3 partitions.
26/02/13 12:03:02 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 12:03:02 INFO DAGScheduler: Got job 16 (start at NativeMethodAccessorImpl.java:0) with 3 output partitions
26/02/13 12:03:02 INFO DAGScheduler: Final stage: ResultStage 27 (start at NativeMethodAccessorImpl.java:0)
26/02/13 12:03:02 INFO DAGScheduler: Parents of final stage: List()
26/02/13 12:03:02 INFO DAGScheduler: Missing parents: List()
26/02/13 12:03:02 INFO DAGScheduler: Submitting ResultStage 27 (MapPartitionsRDD[82] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:03:02 INFO MemoryStore: Block broadcast_24 stored as values in memory (estimated size 49.9 KiB, free 433.8 MiB)
26/02/13 12:03:02 INFO MemoryStore: Block broadcast_24_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.8 MiB)
26/02/13 12:03:02 INFO BlockManagerInfo: Added broadcast_24_piece0 in memory on spark-master:42073 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 12:03:02 INFO SparkContext: Created broadcast 24 from broadcast at DAGScheduler.scala:1585
26/02/13 12:03:02 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 27 (MapPartitionsRDD[82] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
26/02/13 12:03:02 INFO TaskSchedulerImpl: Adding task set 27.0 with 3 tasks resource profile 0
26/02/13 12:03:02 INFO TaskSetManager: Starting task 1.0 in stage 27.0 (TID 38) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:03:02 INFO TaskSetManager: Starting task 0.0 in stage 27.0 (TID 39) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:03:02 INFO TaskSetManager: Starting task 2.0 in stage 27.0 (TID 40) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:03:02 INFO BlockManagerInfo: Added broadcast_24_piece0 in memory on 172.18.0.14:37571 (size: 19.5 KiB, free: 434.0 MiB)
26/02/13 12:03:02 INFO BlockManagerInfo: Added broadcast_24_piece0 in memory on 172.18.0.15:37717 (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 12:03:02 INFO BlockManagerInfo: Added broadcast_23_piece0 in memory on 172.18.0.14:37571 (size: 107.2 KiB, free: 433.9 MiB)
26/02/13 12:03:03 INFO BlockManagerInfo: Added broadcast_23_piece0 in memory on 172.18.0.15:37717 (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 12:03:03 INFO TaskSetManager: Finished task 0.0 in stage 27.0 (TID 39) in 722 ms on 172.18.0.14 (executor 1) (1/3)
26/02/13 12:03:03 INFO TaskSetManager: Finished task 2.0 in stage 27.0 (TID 40) in 771 ms on 172.18.0.14 (executor 1) (2/3)
26/02/13 12:03:03 INFO TaskSetManager: Finished task 1.0 in stage 27.0 (TID 38) in 840 ms on 172.18.0.15 (executor 0) (3/3)
26/02/13 12:03:03 INFO TaskSchedulerImpl: Removed TaskSet 27.0, whose tasks have all completed, from pool 
26/02/13 12:03:03 INFO DAGScheduler: ResultStage 27 (start at NativeMethodAccessorImpl.java:0) finished in 0.863 s
26/02/13 12:03:03 INFO DAGScheduler: Job 16 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 12:03:03 INFO TaskSchedulerImpl: Killing all running tasks in stage 27: Stage finished
26/02/13 12:03:03 INFO DAGScheduler: Job 16 finished: start at NativeMethodAccessorImpl.java:0, took 0.870887 s
26/02/13 12:03:03 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 126, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@33c3a72f] is committing.
26/02/13 12:03:03 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 126, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@33c3a72f] committed.
26/02/13 12:03:03 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/126 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.126.3adbd414-a257-4914-a1ea-a12324404f3c.tmp
26/02/13 12:03:03 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.126.3adbd414-a257-4914-a1ea-a12324404f3c.tmp to file:/tmp/spark-checkpoint-enrichment/commits/126
26/02/13 12:03:03 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "4a161d66-225b-4b29-812d-ddec3c05364e",
  "name" : null,
  "timestamp" : "2026-02-13T12:03:02.278Z",
  "batchId" : 126,
  "numInputRows" : 92,
  "inputRowsPerSecond" : 7076.923076923077,
  "processedRowsPerSecond" : 61.37424949966644,
  "durationMs" : {
    "addBatch" : 1297,
    "commitOffsets" : 58,
    "getBatch" : 0,
    "latestOffset" : 2,
    "queryPlanning" : 67,
    "triggerExecution" : 1499,
    "walCommit" : 74
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 640,
        "1" : 790,
        "0" : 575
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 668,
        "1" : 827,
        "0" : 602
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 668,
        "1" : 827,
        "0" : 602
      }
    },
    "numInputRows" : 92,
    "inputRowsPerSecond" : 7076.923076923077,
    "processedRowsPerSecond" : 61.37424949966644,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@1a6a42c2",
    "numOutputRows" : 24
  }
}
26/02/13 12:03:03 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/127 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.127.592b0e0a-d6a4-46dc-a002-2d2c3860fb3c.tmp
26/02/13 12:03:03 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.127.592b0e0a-d6a4-46dc-a002-2d2c3860fb3c.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/127
26/02/13 12:03:03 INFO MicroBatchExecution: Committed offsets for batch 127. Metadata OffsetSeqMetadata(0,1770984183781,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 12:03:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:03:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:03:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:03:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:03:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:03:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:03:04 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#5367 - origin_code.nullCount#5366) > 0)
26/02/13 12:03:04 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#5372 - destination_code.nullCount#5371) > 0)
26/02/13 12:03:04 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#5402 - callsign.nullCount#5401) > 0)
26/02/13 12:03:04 INFO BlockManagerInfo: Removed broadcast_24_piece0 on spark-master:42073 in memory (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 12:03:04 INFO BlockManagerInfo: Removed broadcast_24_piece0 on 172.18.0.14:37571 in memory (size: 19.5 KiB, free: 433.9 MiB)
26/02/13 12:03:04 INFO BlockManagerInfo: Removed broadcast_24_piece0 on 172.18.0.15:37717 in memory (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 12:03:04 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 12:03:04 INFO DAGScheduler: Got job 17 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 12:03:04 INFO BlockManagerInfo: Removed broadcast_23_piece0 on spark-master:42073 in memory (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 12:03:04 INFO DAGScheduler: Final stage: ResultStage 29 (start at NativeMethodAccessorImpl.java:0)
26/02/13 12:03:04 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 28)
26/02/13 12:03:04 INFO DAGScheduler: Missing parents: List()
26/02/13 12:03:04 INFO BlockManagerInfo: Removed broadcast_23_piece0 on 172.18.0.14:37571 in memory (size: 107.2 KiB, free: 434.0 MiB)
26/02/13 12:03:04 INFO BlockManagerInfo: Removed broadcast_23_piece0 on 172.18.0.15:37717 in memory (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 12:03:04 INFO DAGScheduler: Submitting ResultStage 29 (MapPartitionsRDD[87] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:03:04 INFO MemoryStore: Block broadcast_25 stored as values in memory (estimated size 99.3 KiB, free 433.8 MiB)
26/02/13 12:03:04 INFO BlockManagerInfo: Removed broadcast_22_piece0 on spark-master:42073 in memory (size: 33.3 KiB, free: 434.3 MiB)
26/02/13 12:03:04 INFO MemoryStore: Block broadcast_25_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 433.9 MiB)
26/02/13 12:03:04 INFO BlockManagerInfo: Added broadcast_25_piece0 in memory on spark-master:42073 (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 12:03:04 INFO SparkContext: Created broadcast 25 from broadcast at DAGScheduler.scala:1585
26/02/13 12:03:04 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 29 (MapPartitionsRDD[87] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 12:03:04 INFO TaskSchedulerImpl: Adding task set 29.0 with 4 tasks resource profile 0
26/02/13 12:03:04 INFO BlockManagerInfo: Removed broadcast_22_piece0 on 172.18.0.14:37571 in memory (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 12:03:04 INFO TaskSetManager: Starting task 0.0 in stage 29.0 (TID 41) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:03:04 INFO TaskSetManager: Starting task 1.0 in stage 29.0 (TID 42) (172.18.0.14, executor 1, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:03:04 INFO BlockManagerInfo: Added broadcast_25_piece0 in memory on 172.18.0.14:37571 (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 12:03:04 INFO TaskSetManager: Starting task 2.0 in stage 29.0 (TID 43) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:03:04 INFO TaskSetManager: Starting task 3.0 in stage 29.0 (TID 44) (172.18.0.14, executor 1, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:03:04 INFO TaskSetManager: Finished task 0.0 in stage 29.0 (TID 41) in 38 ms on 172.18.0.14 (executor 1) (1/4)
26/02/13 12:03:04 INFO TaskSetManager: Finished task 1.0 in stage 29.0 (TID 42) in 38 ms on 172.18.0.14 (executor 1) (2/4)
26/02/13 12:03:04 INFO TaskSetManager: Finished task 3.0 in stage 29.0 (TID 44) in 28 ms on 172.18.0.14 (executor 1) (3/4)
26/02/13 12:03:04 INFO TaskSetManager: Finished task 2.0 in stage 29.0 (TID 43) in 34 ms on 172.18.0.14 (executor 1) (4/4)
26/02/13 12:03:04 INFO TaskSchedulerImpl: Removed TaskSet 29.0, whose tasks have all completed, from pool 
26/02/13 12:03:04 INFO DAGScheduler: ResultStage 29 (start at NativeMethodAccessorImpl.java:0) finished in 0.091 s
26/02/13 12:03:04 INFO DAGScheduler: Job 17 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 12:03:04 INFO TaskSchedulerImpl: Killing all running tasks in stage 29: Stage finished
26/02/13 12:03:04 INFO DAGScheduler: Job 17 finished: start at NativeMethodAccessorImpl.java:0, took 0.106390 s
26/02/13 12:03:04 INFO MemoryStore: Block broadcast_26_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.8 MiB)
26/02/13 12:03:04 INFO BlockManagerInfo: Added broadcast_26_piece0 in memory on spark-master:42073 (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 12:03:04 INFO SparkContext: Created broadcast 26 from start at NativeMethodAccessorImpl.java:0
26/02/13 12:03:04 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 127, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@3e0fc296]. The input RDD has 3 partitions.
26/02/13 12:03:04 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 12:03:04 INFO DAGScheduler: Got job 18 (start at NativeMethodAccessorImpl.java:0) with 3 output partitions
26/02/13 12:03:04 INFO DAGScheduler: Final stage: ResultStage 30 (start at NativeMethodAccessorImpl.java:0)
26/02/13 12:03:04 INFO DAGScheduler: Parents of final stage: List()
26/02/13 12:03:04 INFO DAGScheduler: Missing parents: List()
26/02/13 12:03:04 INFO DAGScheduler: Submitting ResultStage 30 (MapPartitionsRDD[93] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:03:04 INFO MemoryStore: Block broadcast_27 stored as values in memory (estimated size 49.9 KiB, free 433.8 MiB)
26/02/13 12:03:04 INFO MemoryStore: Block broadcast_27_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.8 MiB)
26/02/13 12:03:04 INFO BlockManagerInfo: Added broadcast_27_piece0 in memory on spark-master:42073 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 12:03:04 INFO SparkContext: Created broadcast 27 from broadcast at DAGScheduler.scala:1585
26/02/13 12:03:04 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 30 (MapPartitionsRDD[93] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
26/02/13 12:03:04 INFO TaskSchedulerImpl: Adding task set 30.0 with 3 tasks resource profile 0
26/02/13 12:03:04 INFO TaskSetManager: Starting task 1.0 in stage 30.0 (TID 45) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:03:04 INFO TaskSetManager: Starting task 0.0 in stage 30.0 (TID 46) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:03:04 INFO TaskSetManager: Starting task 2.0 in stage 30.0 (TID 47) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:03:04 INFO BlockManagerInfo: Added broadcast_27_piece0 in memory on 172.18.0.14:37571 (size: 19.5 KiB, free: 434.0 MiB)
26/02/13 12:03:04 INFO BlockManagerInfo: Added broadcast_27_piece0 in memory on 172.18.0.15:37717 (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 12:03:04 INFO BlockManagerInfo: Added broadcast_26_piece0 in memory on 172.18.0.14:37571 (size: 107.2 KiB, free: 433.9 MiB)
26/02/13 12:03:04 INFO BlockManagerInfo: Added broadcast_26_piece0 in memory on 172.18.0.15:37717 (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 12:03:04 INFO TaskSetManager: Finished task 0.0 in stage 30.0 (TID 46) in 294 ms on 172.18.0.14 (executor 1) (1/3)
26/02/13 12:03:04 INFO TaskSetManager: Finished task 2.0 in stage 30.0 (TID 47) in 294 ms on 172.18.0.14 (executor 1) (2/3)
26/02/13 12:03:04 INFO TaskSetManager: Finished task 1.0 in stage 30.0 (TID 45) in 410 ms on 172.18.0.15 (executor 0) (3/3)
26/02/13 12:03:04 INFO TaskSchedulerImpl: Removed TaskSet 30.0, whose tasks have all completed, from pool 
26/02/13 12:03:04 INFO DAGScheduler: ResultStage 30 (start at NativeMethodAccessorImpl.java:0) finished in 0.423 s
26/02/13 12:03:04 INFO DAGScheduler: Job 18 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 12:03:04 INFO TaskSchedulerImpl: Killing all running tasks in stage 30: Stage finished
26/02/13 12:03:04 INFO DAGScheduler: Job 18 finished: start at NativeMethodAccessorImpl.java:0, took 0.430318 s
26/02/13 12:03:04 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 127, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@3e0fc296] is committing.
26/02/13 12:03:04 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 127, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@3e0fc296] committed.
26/02/13 12:03:04 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/127 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.127.09af33dc-3526-497e-aa43-50265210b0ec.tmp
26/02/13 12:03:04 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.127.09af33dc-3526-497e-aa43-50265210b0ec.tmp to file:/tmp/spark-checkpoint-enrichment/commits/127
26/02/13 12:03:04 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "4a161d66-225b-4b29-812d-ddec3c05364e",
  "name" : null,
  "timestamp" : "2026-02-13T12:03:03.779Z",
  "batchId" : 127,
  "numInputRows" : 155,
  "inputRowsPerSecond" : 103.26449033977349,
  "processedRowsPerSecond" : 166.13076098606643,
  "durationMs" : {
    "addBatch" : 730,
    "commitOffsets" : 90,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 54,
    "triggerExecution" : 933,
    "walCommit" : 57
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 668,
        "1" : 827,
        "0" : 602
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 719,
        "1" : 884,
        "0" : 649
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 719,
        "1" : 884,
        "0" : 649
      }
    },
    "numInputRows" : 155,
    "inputRowsPerSecond" : 103.26449033977349,
    "processedRowsPerSecond" : 166.13076098606643,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@1a6a42c2",
    "numOutputRows" : 27
  }
}
26/02/13 12:03:11 INFO BlockManagerInfo: Removed broadcast_27_piece0 on spark-master:42073 in memory (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 12:03:11 INFO BlockManagerInfo: Removed broadcast_27_piece0 on 172.18.0.14:37571 in memory (size: 19.5 KiB, free: 433.9 MiB)
26/02/13 12:03:11 INFO BlockManagerInfo: Removed broadcast_27_piece0 on 172.18.0.15:37717 in memory (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 12:03:11 INFO BlockManagerInfo: Removed broadcast_25_piece0 on spark-master:42073 in memory (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 12:03:11 INFO BlockManagerInfo: Removed broadcast_25_piece0 on 172.18.0.14:37571 in memory (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 12:03:14 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:03:22 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/128 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.128.4596734c-4530-401a-aaa2-d00635b37f1a.tmp
26/02/13 12:03:22 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.128.4596734c-4530-401a-aaa2-d00635b37f1a.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/128
26/02/13 12:03:22 INFO MicroBatchExecution: Committed offsets for batch 128. Metadata OffsetSeqMetadata(0,1770984202786,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 12:03:22 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:03:22 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:03:22 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:03:22 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:03:23 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:03:23 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:03:23 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#6221 - origin_code.nullCount#6220) > 0)
26/02/13 12:03:23 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#6226 - destination_code.nullCount#6225) > 0)
26/02/13 12:03:23 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#6256 - callsign.nullCount#6255) > 0)
26/02/13 12:03:23 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 12:03:23 INFO DAGScheduler: Got job 19 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 12:03:23 INFO DAGScheduler: Final stage: ResultStage 32 (start at NativeMethodAccessorImpl.java:0)
26/02/13 12:03:23 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 31)
26/02/13 12:03:23 INFO DAGScheduler: Missing parents: List()
26/02/13 12:03:23 INFO DAGScheduler: Submitting ResultStage 32 (MapPartitionsRDD[98] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:03:23 INFO MemoryStore: Block broadcast_28 stored as values in memory (estimated size 99.3 KiB, free 433.9 MiB)
26/02/13 12:03:23 INFO MemoryStore: Block broadcast_28_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 433.8 MiB)
26/02/13 12:03:23 INFO BlockManagerInfo: Added broadcast_28_piece0 in memory on spark-master:42073 (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 12:03:23 INFO BlockManagerInfo: Removed broadcast_20_piece0 on spark-master:42073 in memory (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 12:03:23 INFO BlockManagerInfo: Removed broadcast_20_piece0 on 172.18.0.15:37717 in memory (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 12:03:23 INFO SparkContext: Created broadcast 28 from broadcast at DAGScheduler.scala:1585
26/02/13 12:03:23 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 32 (MapPartitionsRDD[98] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 12:03:23 INFO BlockManagerInfo: Removed broadcast_20_piece0 on 172.18.0.14:37571 in memory (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 12:03:23 INFO TaskSchedulerImpl: Adding task set 32.0 with 4 tasks resource profile 0
26/02/13 12:03:23 INFO TaskSetManager: Starting task 0.0 in stage 32.0 (TID 48) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:03:23 INFO TaskSetManager: Starting task 1.0 in stage 32.0 (TID 49) (172.18.0.14, executor 1, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:03:23 INFO BlockManagerInfo: Added broadcast_28_piece0 in memory on 172.18.0.14:37571 (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 12:03:23 INFO TaskSetManager: Starting task 2.0 in stage 32.0 (TID 50) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:03:23 INFO TaskSetManager: Finished task 0.0 in stage 32.0 (TID 48) in 35 ms on 172.18.0.14 (executor 1) (1/4)
26/02/13 12:03:23 INFO TaskSetManager: Starting task 3.0 in stage 32.0 (TID 51) (172.18.0.14, executor 1, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:03:23 INFO TaskSetManager: Finished task 1.0 in stage 32.0 (TID 49) in 38 ms on 172.18.0.14 (executor 1) (2/4)
26/02/13 12:03:23 INFO TaskSetManager: Finished task 2.0 in stage 32.0 (TID 50) in 31 ms on 172.18.0.14 (executor 1) (3/4)
26/02/13 12:03:23 INFO TaskSetManager: Finished task 3.0 in stage 32.0 (TID 51) in 34 ms on 172.18.0.14 (executor 1) (4/4)
26/02/13 12:03:23 INFO TaskSchedulerImpl: Removed TaskSet 32.0, whose tasks have all completed, from pool 
26/02/13 12:03:23 INFO DAGScheduler: ResultStage 32 (start at NativeMethodAccessorImpl.java:0) finished in 0.094 s
26/02/13 12:03:23 INFO DAGScheduler: Job 19 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 12:03:23 INFO TaskSchedulerImpl: Killing all running tasks in stage 32: Stage finished
26/02/13 12:03:23 INFO DAGScheduler: Job 19 finished: start at NativeMethodAccessorImpl.java:0, took 0.099901 s
26/02/13 12:03:23 INFO MemoryStore: Block broadcast_29_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.8 MiB)
26/02/13 12:03:23 INFO BlockManagerInfo: Added broadcast_29_piece0 in memory on spark-master:42073 (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 12:03:23 INFO SparkContext: Created broadcast 29 from start at NativeMethodAccessorImpl.java:0
26/02/13 12:03:23 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 128, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@6d78afe3]. The input RDD has 2 partitions.
26/02/13 12:03:23 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 12:03:23 INFO DAGScheduler: Got job 20 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/02/13 12:03:23 INFO DAGScheduler: Final stage: ResultStage 33 (start at NativeMethodAccessorImpl.java:0)
26/02/13 12:03:23 INFO DAGScheduler: Parents of final stage: List()
26/02/13 12:03:23 INFO DAGScheduler: Missing parents: List()
26/02/13 12:03:23 INFO DAGScheduler: Submitting ResultStage 33 (MapPartitionsRDD[104] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:03:23 INFO MemoryStore: Block broadcast_30 stored as values in memory (estimated size 49.9 KiB, free 433.8 MiB)
26/02/13 12:03:23 INFO MemoryStore: Block broadcast_30_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.8 MiB)
26/02/13 12:03:23 INFO BlockManagerInfo: Added broadcast_30_piece0 in memory on spark-master:42073 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 12:03:23 INFO SparkContext: Created broadcast 30 from broadcast at DAGScheduler.scala:1585
26/02/13 12:03:23 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 33 (MapPartitionsRDD[104] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/02/13 12:03:23 INFO TaskSchedulerImpl: Adding task set 33.0 with 2 tasks resource profile 0
26/02/13 12:03:23 INFO TaskSetManager: Starting task 1.0 in stage 33.0 (TID 52) (172.18.0.14, executor 1, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:03:23 INFO TaskSetManager: Starting task 0.0 in stage 33.0 (TID 53) (172.18.0.15, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:03:23 INFO BlockManagerInfo: Added broadcast_30_piece0 in memory on 172.18.0.14:37571 (size: 19.5 KiB, free: 434.0 MiB)
26/02/13 12:03:23 INFO BlockManagerInfo: Added broadcast_30_piece0 in memory on 172.18.0.15:37717 (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 12:03:23 INFO BlockManagerInfo: Added broadcast_29_piece0 in memory on 172.18.0.14:37571 (size: 107.2 KiB, free: 433.9 MiB)
26/02/13 12:03:23 INFO BlockManagerInfo: Added broadcast_29_piece0 in memory on 172.18.0.15:37717 (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 12:03:23 INFO TaskSetManager: Finished task 1.0 in stage 33.0 (TID 52) in 598 ms on 172.18.0.14 (executor 1) (1/2)
26/02/13 12:03:23 INFO TaskSetManager: Finished task 0.0 in stage 33.0 (TID 53) in 636 ms on 172.18.0.15 (executor 0) (2/2)
26/02/13 12:03:23 INFO TaskSchedulerImpl: Removed TaskSet 33.0, whose tasks have all completed, from pool 
26/02/13 12:03:23 INFO DAGScheduler: ResultStage 33 (start at NativeMethodAccessorImpl.java:0) finished in 0.644 s
26/02/13 12:03:23 INFO DAGScheduler: Job 20 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 12:03:23 INFO TaskSchedulerImpl: Killing all running tasks in stage 33: Stage finished
26/02/13 12:03:23 INFO DAGScheduler: Job 20 finished: start at NativeMethodAccessorImpl.java:0, took 0.651158 s
26/02/13 12:03:23 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 128, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@6d78afe3] is committing.
26/02/13 12:03:23 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 128, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@6d78afe3] committed.
26/02/13 12:03:23 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/128 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.128.f4b741cf-f48e-4094-98ce-f53c541e3632.tmp
26/02/13 12:03:23 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.128.f4b741cf-f48e-4094-98ce-f53c541e3632.tmp to file:/tmp/spark-checkpoint-enrichment/commits/128
26/02/13 12:03:23 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "4a161d66-225b-4b29-812d-ddec3c05364e",
  "name" : null,
  "timestamp" : "2026-02-13T12:03:22.784Z",
  "batchId" : 128,
  "numInputRows" : 3,
  "inputRowsPerSecond" : 230.76923076923077,
  "processedRowsPerSecond" : 2.6785714285714284,
  "durationMs" : {
    "addBatch" : 897,
    "commitOffsets" : 65,
    "getBatch" : 0,
    "latestOffset" : 2,
    "queryPlanning" : 61,
    "triggerExecution" : 1120,
    "walCommit" : 94
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 719,
        "1" : 884,
        "0" : 649
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 719,
        "1" : 886,
        "0" : 650
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 719,
        "1" : 886,
        "0" : 650
      }
    },
    "numInputRows" : 3,
    "inputRowsPerSecond" : 230.76923076923077,
    "processedRowsPerSecond" : 2.6785714285714284,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@1a6a42c2",
    "numOutputRows" : 0
  }
}
26/02/13 12:03:23 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/129 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.129.49ef9758-b8ee-4296-8494-936528032cfe.tmp
26/02/13 12:03:23 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.129.49ef9758-b8ee-4296-8494-936528032cfe.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/129
26/02/13 12:03:23 INFO MicroBatchExecution: Committed offsets for batch 129. Metadata OffsetSeqMetadata(0,1770984203908,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 12:03:24 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:03:24 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:03:24 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:03:24 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:03:24 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:03:24 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:03:24 INFO BlockManagerInfo: Removed broadcast_26_piece0 on spark-master:42073 in memory (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 12:03:24 INFO BlockManagerInfo: Removed broadcast_26_piece0 on 172.18.0.15:37717 in memory (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 12:03:24 INFO BlockManagerInfo: Removed broadcast_26_piece0 on 172.18.0.14:37571 in memory (size: 107.2 KiB, free: 434.0 MiB)
26/02/13 12:03:24 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#7075 - origin_code.nullCount#7074) > 0)
26/02/13 12:03:24 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#7080 - destination_code.nullCount#7079) > 0)
26/02/13 12:03:24 INFO BlockManagerInfo: Removed broadcast_28_piece0 on spark-master:42073 in memory (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 12:03:24 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#7110 - callsign.nullCount#7109) > 0)
26/02/13 12:03:24 INFO BlockManagerInfo: Removed broadcast_28_piece0 on 172.18.0.14:37571 in memory (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 12:03:24 INFO BlockManagerInfo: Removed broadcast_30_piece0 on spark-master:42073 in memory (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 12:03:24 INFO BlockManagerInfo: Removed broadcast_30_piece0 on 172.18.0.14:37571 in memory (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 12:03:24 INFO BlockManagerInfo: Removed broadcast_30_piece0 on 172.18.0.15:37717 in memory (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 12:03:24 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 12:03:24 INFO BlockManagerInfo: Removed broadcast_29_piece0 on spark-master:42073 in memory (size: 107.2 KiB, free: 434.4 MiB)
26/02/13 12:03:24 INFO DAGScheduler: Got job 21 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 12:03:24 INFO DAGScheduler: Final stage: ResultStage 35 (start at NativeMethodAccessorImpl.java:0)
26/02/13 12:03:24 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 34)
26/02/13 12:03:24 INFO DAGScheduler: Missing parents: List()
26/02/13 12:03:24 INFO BlockManagerInfo: Removed broadcast_29_piece0 on 172.18.0.14:37571 in memory (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 12:03:24 INFO DAGScheduler: Submitting ResultStage 35 (MapPartitionsRDD[109] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:03:24 INFO BlockManagerInfo: Removed broadcast_29_piece0 on 172.18.0.15:37717 in memory (size: 107.2 KiB, free: 434.4 MiB)
26/02/13 12:03:24 INFO MemoryStore: Block broadcast_31 stored as values in memory (estimated size 99.3 KiB, free 434.1 MiB)
26/02/13 12:03:24 INFO MemoryStore: Block broadcast_31_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 434.0 MiB)
26/02/13 12:03:24 INFO BlockManagerInfo: Added broadcast_31_piece0 in memory on spark-master:42073 (size: 33.3 KiB, free: 434.3 MiB)
26/02/13 12:03:24 INFO SparkContext: Created broadcast 31 from broadcast at DAGScheduler.scala:1585
26/02/13 12:03:24 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 35 (MapPartitionsRDD[109] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 12:03:24 INFO TaskSchedulerImpl: Adding task set 35.0 with 4 tasks resource profile 0
26/02/13 12:03:24 INFO TaskSetManager: Starting task 0.0 in stage 35.0 (TID 54) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:03:24 INFO TaskSetManager: Starting task 1.0 in stage 35.0 (TID 55) (172.18.0.14, executor 1, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:03:24 INFO BlockManagerInfo: Added broadcast_31_piece0 in memory on 172.18.0.14:37571 (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 12:03:24 INFO TaskSetManager: Starting task 2.0 in stage 35.0 (TID 56) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:03:24 INFO TaskSetManager: Starting task 3.0 in stage 35.0 (TID 57) (172.18.0.14, executor 1, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:03:24 INFO TaskSetManager: Finished task 0.0 in stage 35.0 (TID 54) in 43 ms on 172.18.0.14 (executor 1) (1/4)
26/02/13 12:03:24 INFO TaskSetManager: Finished task 1.0 in stage 35.0 (TID 55) in 44 ms on 172.18.0.14 (executor 1) (2/4)
26/02/13 12:03:24 INFO TaskSetManager: Finished task 2.0 in stage 35.0 (TID 56) in 51 ms on 172.18.0.14 (executor 1) (3/4)
26/02/13 12:03:24 INFO TaskSetManager: Finished task 3.0 in stage 35.0 (TID 57) in 51 ms on 172.18.0.14 (executor 1) (4/4)
26/02/13 12:03:24 INFO TaskSchedulerImpl: Removed TaskSet 35.0, whose tasks have all completed, from pool 
26/02/13 12:03:24 INFO DAGScheduler: ResultStage 35 (start at NativeMethodAccessorImpl.java:0) finished in 0.105 s
26/02/13 12:03:24 INFO DAGScheduler: Job 21 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 12:03:24 INFO TaskSchedulerImpl: Killing all running tasks in stage 35: Stage finished
26/02/13 12:03:24 INFO DAGScheduler: Job 21 finished: start at NativeMethodAccessorImpl.java:0, took 0.113237 s
26/02/13 12:03:24 INFO MemoryStore: Block broadcast_32_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.9 MiB)
26/02/13 12:03:24 INFO BlockManagerInfo: Added broadcast_32_piece0 in memory on spark-master:42073 (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 12:03:24 INFO SparkContext: Created broadcast 32 from start at NativeMethodAccessorImpl.java:0
26/02/13 12:03:24 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 129, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@38c24c61]. The input RDD has 3 partitions.
26/02/13 12:03:24 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 12:03:24 INFO DAGScheduler: Got job 22 (start at NativeMethodAccessorImpl.java:0) with 3 output partitions
26/02/13 12:03:24 INFO DAGScheduler: Final stage: ResultStage 36 (start at NativeMethodAccessorImpl.java:0)
26/02/13 12:03:24 INFO DAGScheduler: Parents of final stage: List()
26/02/13 12:03:24 INFO DAGScheduler: Missing parents: List()
26/02/13 12:03:24 INFO DAGScheduler: Submitting ResultStage 36 (MapPartitionsRDD[115] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:03:24 INFO MemoryStore: Block broadcast_33 stored as values in memory (estimated size 49.9 KiB, free 433.9 MiB)
26/02/13 12:03:24 INFO MemoryStore: Block broadcast_33_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.9 MiB)
26/02/13 12:03:24 INFO BlockManagerInfo: Added broadcast_33_piece0 in memory on spark-master:42073 (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 12:03:24 INFO SparkContext: Created broadcast 33 from broadcast at DAGScheduler.scala:1585
26/02/13 12:03:24 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 36 (MapPartitionsRDD[115] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
26/02/13 12:03:24 INFO TaskSchedulerImpl: Adding task set 36.0 with 3 tasks resource profile 0
26/02/13 12:03:24 INFO TaskSetManager: Starting task 0.0 in stage 36.0 (TID 58) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:03:24 INFO TaskSetManager: Starting task 1.0 in stage 36.0 (TID 59) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:03:24 INFO TaskSetManager: Starting task 2.0 in stage 36.0 (TID 60) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:03:24 INFO BlockManagerInfo: Added broadcast_33_piece0 in memory on 172.18.0.14:37571 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 12:03:24 INFO BlockManagerInfo: Added broadcast_33_piece0 in memory on 172.18.0.15:37717 (size: 19.5 KiB, free: 434.4 MiB)
26/02/13 12:03:24 INFO BlockManagerInfo: Added broadcast_32_piece0 in memory on 172.18.0.14:37571 (size: 107.2 KiB, free: 434.0 MiB)
26/02/13 12:03:24 INFO BlockManagerInfo: Added broadcast_32_piece0 in memory on 172.18.0.15:37717 (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 12:03:24 INFO TaskSetManager: Finished task 2.0 in stage 36.0 (TID 60) in 191 ms on 172.18.0.14 (executor 1) (1/3)
26/02/13 12:03:24 INFO TaskSetManager: Finished task 1.0 in stage 36.0 (TID 59) in 251 ms on 172.18.0.15 (executor 0) (2/3)
26/02/13 12:03:24 INFO TaskSetManager: Finished task 0.0 in stage 36.0 (TID 58) in 661 ms on 172.18.0.14 (executor 1) (3/3)
26/02/13 12:03:24 INFO TaskSchedulerImpl: Removed TaskSet 36.0, whose tasks have all completed, from pool 
26/02/13 12:03:24 INFO DAGScheduler: ResultStage 36 (start at NativeMethodAccessorImpl.java:0) finished in 0.670 s
26/02/13 12:03:24 INFO DAGScheduler: Job 22 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 12:03:24 INFO TaskSchedulerImpl: Killing all running tasks in stage 36: Stage finished
26/02/13 12:03:24 INFO DAGScheduler: Job 22 finished: start at NativeMethodAccessorImpl.java:0, took 0.673729 s
26/02/13 12:03:24 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 129, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@38c24c61] is committing.
26/02/13 12:03:24 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 129, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@38c24c61] committed.
26/02/13 12:03:24 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/129 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.129.30f3e7ac-d50c-43c4-8883-60a45e96b5cc.tmp
26/02/13 12:03:25 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.129.30f3e7ac-d50c-43c4-8883-60a45e96b5cc.tmp to file:/tmp/spark-checkpoint-enrichment/commits/129
26/02/13 12:03:25 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "4a161d66-225b-4b29-812d-ddec3c05364e",
  "name" : null,
  "timestamp" : "2026-02-13T12:03:23.906Z",
  "batchId" : 129,
  "numInputRows" : 243,
  "inputRowsPerSecond" : 216.57754010695186,
  "processedRowsPerSecond" : 219.11632100991883,
  "durationMs" : {
    "addBatch" : 945,
    "commitOffsets" : 60,
    "getBatch" : 0,
    "latestOffset" : 2,
    "queryPlanning" : 45,
    "triggerExecution" : 1109,
    "walCommit" : 56
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 719,
        "1" : 886,
        "0" : 650
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 798,
        "1" : 978,
        "0" : 722
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 798,
        "1" : 978,
        "0" : 722
      }
    },
    "numInputRows" : 243,
    "inputRowsPerSecond" : 216.57754010695186,
    "processedRowsPerSecond" : 219.11632100991883,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@1a6a42c2",
    "numOutputRows" : 51
  }
}
26/02/13 12:03:31 INFO BlockManagerInfo: Removed broadcast_31_piece0 on spark-master:42073 in memory (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 12:03:31 INFO BlockManagerInfo: Removed broadcast_31_piece0 on 172.18.0.14:37571 in memory (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 12:03:31 INFO BlockManagerInfo: Removed broadcast_33_piece0 on spark-master:42073 in memory (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 12:03:31 INFO BlockManagerInfo: Removed broadcast_33_piece0 on 172.18.0.14:37571 in memory (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 12:03:31 INFO BlockManagerInfo: Removed broadcast_33_piece0 on 172.18.0.15:37717 in memory (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 12:03:35 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:03:39 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/130 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.130.57411147-dd0c-46d9-8d32-ecad8d46b58a.tmp
26/02/13 12:03:39 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.130.57411147-dd0c-46d9-8d32-ecad8d46b58a.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/130
26/02/13 12:03:39 INFO MicroBatchExecution: Committed offsets for batch 130. Metadata OffsetSeqMetadata(0,1770984219345,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 12:03:39 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:03:39 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:03:39 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:03:39 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:03:39 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:03:39 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:03:39 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#7929 - origin_code.nullCount#7928) > 0)
26/02/13 12:03:39 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#7934 - destination_code.nullCount#7933) > 0)
26/02/13 12:03:39 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#7964 - callsign.nullCount#7963) > 0)
26/02/13 12:03:39 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 12:03:39 INFO DAGScheduler: Got job 23 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 12:03:39 INFO DAGScheduler: Final stage: ResultStage 38 (start at NativeMethodAccessorImpl.java:0)
26/02/13 12:03:39 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 37)
26/02/13 12:03:39 INFO DAGScheduler: Missing parents: List()
26/02/13 12:03:39 INFO DAGScheduler: Submitting ResultStage 38 (MapPartitionsRDD[120] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:03:39 INFO MemoryStore: Block broadcast_34 stored as values in memory (estimated size 99.3 KiB, free 434.0 MiB)
26/02/13 12:03:39 INFO MemoryStore: Block broadcast_34_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 433.9 MiB)
26/02/13 12:03:39 INFO BlockManagerInfo: Added broadcast_34_piece0 in memory on spark-master:42073 (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 12:03:39 INFO SparkContext: Created broadcast 34 from broadcast at DAGScheduler.scala:1585
26/02/13 12:03:39 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 38 (MapPartitionsRDD[120] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 12:03:39 INFO TaskSchedulerImpl: Adding task set 38.0 with 4 tasks resource profile 0
26/02/13 12:03:39 INFO TaskSetManager: Starting task 0.0 in stage 38.0 (TID 61) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:03:39 INFO TaskSetManager: Starting task 1.0 in stage 38.0 (TID 62) (172.18.0.14, executor 1, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:03:39 INFO BlockManagerInfo: Added broadcast_34_piece0 in memory on 172.18.0.14:37571 (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 12:03:39 INFO TaskSetManager: Starting task 2.0 in stage 38.0 (TID 63) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:03:39 INFO TaskSetManager: Finished task 0.0 in stage 38.0 (TID 61) in 38 ms on 172.18.0.14 (executor 1) (1/4)
26/02/13 12:03:39 INFO TaskSetManager: Starting task 3.0 in stage 38.0 (TID 64) (172.18.0.14, executor 1, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:03:39 INFO TaskSetManager: Finished task 1.0 in stage 38.0 (TID 62) in 43 ms on 172.18.0.14 (executor 1) (2/4)
26/02/13 12:03:39 INFO TaskSetManager: Finished task 2.0 in stage 38.0 (TID 63) in 32 ms on 172.18.0.14 (executor 1) (3/4)
26/02/13 12:03:39 INFO TaskSetManager: Finished task 3.0 in stage 38.0 (TID 64) in 38 ms on 172.18.0.14 (executor 1) (4/4)
26/02/13 12:03:39 INFO TaskSchedulerImpl: Removed TaskSet 38.0, whose tasks have all completed, from pool 
26/02/13 12:03:39 INFO DAGScheduler: ResultStage 38 (start at NativeMethodAccessorImpl.java:0) finished in 0.089 s
26/02/13 12:03:39 INFO DAGScheduler: Job 23 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 12:03:39 INFO TaskSchedulerImpl: Killing all running tasks in stage 38: Stage finished
26/02/13 12:03:39 INFO DAGScheduler: Job 23 finished: start at NativeMethodAccessorImpl.java:0, took 0.093127 s
26/02/13 12:03:39 INFO MemoryStore: Block broadcast_35_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.8 MiB)
26/02/13 12:03:39 INFO BlockManagerInfo: Added broadcast_35_piece0 in memory on spark-master:42073 (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 12:03:39 INFO SparkContext: Created broadcast 35 from start at NativeMethodAccessorImpl.java:0
26/02/13 12:03:39 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 130, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@2d4bd70c]. The input RDD has 3 partitions.
26/02/13 12:03:39 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 12:03:39 INFO DAGScheduler: Got job 24 (start at NativeMethodAccessorImpl.java:0) with 3 output partitions
26/02/13 12:03:39 INFO DAGScheduler: Final stage: ResultStage 39 (start at NativeMethodAccessorImpl.java:0)
26/02/13 12:03:39 INFO DAGScheduler: Parents of final stage: List()
26/02/13 12:03:39 INFO DAGScheduler: Missing parents: List()
26/02/13 12:03:39 INFO DAGScheduler: Submitting ResultStage 39 (MapPartitionsRDD[126] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:03:39 INFO MemoryStore: Block broadcast_36 stored as values in memory (estimated size 49.9 KiB, free 433.8 MiB)
26/02/13 12:03:39 INFO MemoryStore: Block broadcast_36_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.8 MiB)
26/02/13 12:03:39 INFO BlockManagerInfo: Added broadcast_36_piece0 in memory on spark-master:42073 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 12:03:39 INFO SparkContext: Created broadcast 36 from broadcast at DAGScheduler.scala:1585
26/02/13 12:03:39 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 39 (MapPartitionsRDD[126] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
26/02/13 12:03:39 INFO TaskSchedulerImpl: Adding task set 39.0 with 3 tasks resource profile 0
26/02/13 12:03:39 INFO TaskSetManager: Starting task 1.0 in stage 39.0 (TID 65) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:03:39 INFO TaskSetManager: Starting task 0.0 in stage 39.0 (TID 66) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:03:39 INFO TaskSetManager: Starting task 2.0 in stage 39.0 (TID 67) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:03:39 INFO BlockManagerInfo: Added broadcast_36_piece0 in memory on 172.18.0.14:37571 (size: 19.5 KiB, free: 434.0 MiB)
26/02/13 12:03:39 INFO BlockManagerInfo: Added broadcast_36_piece0 in memory on 172.18.0.15:37717 (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 12:03:39 INFO BlockManagerInfo: Added broadcast_35_piece0 in memory on 172.18.0.14:37571 (size: 107.2 KiB, free: 433.9 MiB)
26/02/13 12:03:39 INFO BlockManagerInfo: Added broadcast_35_piece0 in memory on 172.18.0.15:37717 (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 12:03:40 INFO TaskSetManager: Finished task 0.0 in stage 39.0 (TID 66) in 613 ms on 172.18.0.14 (executor 1) (1/3)
26/02/13 12:03:40 INFO TaskSetManager: Finished task 2.0 in stage 39.0 (TID 67) in 615 ms on 172.18.0.14 (executor 1) (2/3)
26/02/13 12:03:40 INFO TaskSetManager: Finished task 1.0 in stage 39.0 (TID 65) in 661 ms on 172.18.0.15 (executor 0) (3/3)
26/02/13 12:03:40 INFO TaskSchedulerImpl: Removed TaskSet 39.0, whose tasks have all completed, from pool 
26/02/13 12:03:40 INFO DAGScheduler: ResultStage 39 (start at NativeMethodAccessorImpl.java:0) finished in 0.667 s
26/02/13 12:03:40 INFO DAGScheduler: Job 24 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 12:03:40 INFO TaskSchedulerImpl: Killing all running tasks in stage 39: Stage finished
26/02/13 12:03:40 INFO DAGScheduler: Job 24 finished: start at NativeMethodAccessorImpl.java:0, took 0.669798 s
26/02/13 12:03:40 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 130, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@2d4bd70c] is committing.
26/02/13 12:03:40 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 130, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@2d4bd70c] committed.
26/02/13 12:03:40 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/130 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.130.ced3af03-e13a-4c80-98de-719bbc6ae5f9.tmp
26/02/13 12:03:40 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.130.ced3af03-e13a-4c80-98de-719bbc6ae5f9.tmp to file:/tmp/spark-checkpoint-enrichment/commits/130
26/02/13 12:03:40 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "4a161d66-225b-4b29-812d-ddec3c05364e",
  "name" : null,
  "timestamp" : "2026-02-13T12:03:39.343Z",
  "batchId" : 130,
  "numInputRows" : 92,
  "inputRowsPerSecond" : 7666.666666666666,
  "processedRowsPerSecond" : 81.9964349376114,
  "durationMs" : {
    "addBatch" : 919,
    "commitOffsets" : 60,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 50,
    "triggerExecution" : 1122,
    "walCommit" : 89
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 798,
        "1" : 978,
        "0" : 722
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 826,
        "1" : 1015,
        "0" : 749
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 826,
        "1" : 1015,
        "0" : 749
      }
    },
    "numInputRows" : 92,
    "inputRowsPerSecond" : 7666.666666666666,
    "processedRowsPerSecond" : 81.9964349376114,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@1a6a42c2",
    "numOutputRows" : 24
  }
}
26/02/13 12:03:40 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/131 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.131.90d851bb-7b71-49be-a020-051a5ea363d4.tmp
26/02/13 12:03:40 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.131.90d851bb-7b71-49be-a020-051a5ea363d4.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/131
26/02/13 12:03:40 INFO MicroBatchExecution: Committed offsets for batch 131. Metadata OffsetSeqMetadata(0,1770984220467,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 12:03:40 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:03:40 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:03:40 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:03:40 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:03:40 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:03:40 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:03:40 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#8783 - origin_code.nullCount#8782) > 0)
26/02/13 12:03:40 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#8788 - destination_code.nullCount#8787) > 0)
26/02/13 12:03:40 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#8818 - callsign.nullCount#8817) > 0)
26/02/13 12:03:40 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 12:03:40 INFO DAGScheduler: Got job 25 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 12:03:40 INFO DAGScheduler: Final stage: ResultStage 41 (start at NativeMethodAccessorImpl.java:0)
26/02/13 12:03:40 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 40)
26/02/13 12:03:40 INFO DAGScheduler: Missing parents: List()
26/02/13 12:03:40 INFO DAGScheduler: Submitting ResultStage 41 (MapPartitionsRDD[131] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:03:40 INFO MemoryStore: Block broadcast_37 stored as values in memory (estimated size 99.3 KiB, free 433.7 MiB)
26/02/13 12:03:40 INFO MemoryStore: Block broadcast_37_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 433.6 MiB)
26/02/13 12:03:40 INFO BlockManagerInfo: Added broadcast_37_piece0 in memory on spark-master:42073 (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 12:03:40 INFO SparkContext: Created broadcast 37 from broadcast at DAGScheduler.scala:1585
26/02/13 12:03:40 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 41 (MapPartitionsRDD[131] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 12:03:40 INFO TaskSchedulerImpl: Adding task set 41.0 with 4 tasks resource profile 0
26/02/13 12:03:40 INFO TaskSetManager: Starting task 0.0 in stage 41.0 (TID 68) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:03:40 INFO TaskSetManager: Starting task 1.0 in stage 41.0 (TID 69) (172.18.0.14, executor 1, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:03:40 INFO BlockManagerInfo: Added broadcast_37_piece0 in memory on 172.18.0.14:37571 (size: 33.3 KiB, free: 433.9 MiB)
26/02/13 12:03:40 INFO TaskSetManager: Starting task 2.0 in stage 41.0 (TID 70) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:03:40 INFO TaskSetManager: Starting task 3.0 in stage 41.0 (TID 71) (172.18.0.14, executor 1, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:03:40 INFO TaskSetManager: Finished task 0.0 in stage 41.0 (TID 68) in 36 ms on 172.18.0.14 (executor 1) (1/4)
26/02/13 12:03:40 INFO TaskSetManager: Finished task 1.0 in stage 41.0 (TID 69) in 35 ms on 172.18.0.14 (executor 1) (2/4)
26/02/13 12:03:40 INFO TaskSetManager: Finished task 2.0 in stage 41.0 (TID 70) in 29 ms on 172.18.0.14 (executor 1) (3/4)
26/02/13 12:03:40 INFO TaskSetManager: Finished task 3.0 in stage 41.0 (TID 71) in 36 ms on 172.18.0.14 (executor 1) (4/4)
26/02/13 12:03:40 INFO TaskSchedulerImpl: Removed TaskSet 41.0, whose tasks have all completed, from pool 
26/02/13 12:03:40 INFO DAGScheduler: ResultStage 41 (start at NativeMethodAccessorImpl.java:0) finished in 0.081 s
26/02/13 12:03:40 INFO DAGScheduler: Job 25 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 12:03:40 INFO TaskSchedulerImpl: Killing all running tasks in stage 41: Stage finished
26/02/13 12:03:40 INFO DAGScheduler: Job 25 finished: start at NativeMethodAccessorImpl.java:0, took 0.086412 s
26/02/13 12:03:40 INFO BlockManagerInfo: Removed broadcast_36_piece0 on spark-master:42073 in memory (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 12:03:40 INFO BlockManagerInfo: Removed broadcast_36_piece0 on 172.18.0.15:37717 in memory (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 12:03:40 INFO BlockManagerInfo: Removed broadcast_36_piece0 on 172.18.0.14:37571 in memory (size: 19.5 KiB, free: 433.9 MiB)
26/02/13 12:03:40 INFO MemoryStore: Block broadcast_38_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.6 MiB)
26/02/13 12:03:40 INFO BlockManagerInfo: Added broadcast_38_piece0 in memory on spark-master:42073 (size: 107.2 KiB, free: 434.0 MiB)
26/02/13 12:03:40 INFO SparkContext: Created broadcast 38 from start at NativeMethodAccessorImpl.java:0
26/02/13 12:03:40 INFO BlockManagerInfo: Removed broadcast_35_piece0 on spark-master:42073 in memory (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 12:03:40 INFO BlockManagerInfo: Removed broadcast_35_piece0 on 172.18.0.14:37571 in memory (size: 107.2 KiB, free: 434.0 MiB)
26/02/13 12:03:40 INFO BlockManagerInfo: Removed broadcast_35_piece0 on 172.18.0.15:37717 in memory (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 12:03:40 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 131, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@13edfac4]. The input RDD has 3 partitions.
26/02/13 12:03:40 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 12:03:40 INFO DAGScheduler: Got job 26 (start at NativeMethodAccessorImpl.java:0) with 3 output partitions
26/02/13 12:03:40 INFO DAGScheduler: Final stage: ResultStage 42 (start at NativeMethodAccessorImpl.java:0)
26/02/13 12:03:40 INFO DAGScheduler: Parents of final stage: List()
26/02/13 12:03:40 INFO DAGScheduler: Missing parents: List()
26/02/13 12:03:40 INFO DAGScheduler: Submitting ResultStage 42 (MapPartitionsRDD[137] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:03:40 INFO MemoryStore: Block broadcast_39 stored as values in memory (estimated size 49.9 KiB, free 433.7 MiB)
26/02/13 12:03:40 INFO BlockManagerInfo: Removed broadcast_37_piece0 on spark-master:42073 in memory (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 12:03:40 INFO BlockManagerInfo: Removed broadcast_37_piece0 on 172.18.0.14:37571 in memory (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 12:03:40 INFO MemoryStore: Block broadcast_39_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.7 MiB)
26/02/13 12:03:40 INFO BlockManagerInfo: Added broadcast_39_piece0 in memory on spark-master:42073 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 12:03:40 INFO SparkContext: Created broadcast 39 from broadcast at DAGScheduler.scala:1585
26/02/13 12:03:40 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 42 (MapPartitionsRDD[137] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
26/02/13 12:03:40 INFO TaskSchedulerImpl: Adding task set 42.0 with 3 tasks resource profile 0
26/02/13 12:03:40 INFO TaskSetManager: Starting task 0.0 in stage 42.0 (TID 72) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:03:40 INFO TaskSetManager: Starting task 1.0 in stage 42.0 (TID 73) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:03:40 INFO TaskSetManager: Starting task 2.0 in stage 42.0 (TID 74) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:03:40 INFO BlockManagerInfo: Removed broadcast_34_piece0 on spark-master:42073 in memory (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 12:03:40 INFO BlockManagerInfo: Removed broadcast_34_piece0 on 172.18.0.14:37571 in memory (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 12:03:40 INFO BlockManagerInfo: Added broadcast_39_piece0 in memory on 172.18.0.14:37571 (size: 19.5 KiB, free: 434.0 MiB)
26/02/13 12:03:40 INFO BlockManagerInfo: Added broadcast_39_piece0 in memory on 172.18.0.15:37717 (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 12:03:40 INFO BlockManagerInfo: Added broadcast_38_piece0 in memory on 172.18.0.14:37571 (size: 107.2 KiB, free: 433.9 MiB)
26/02/13 12:03:40 INFO BlockManagerInfo: Added broadcast_38_piece0 in memory on 172.18.0.15:37717 (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 12:03:41 INFO TaskSetManager: Finished task 0.0 in stage 42.0 (TID 72) in 216 ms on 172.18.0.14 (executor 1) (1/3)
26/02/13 12:03:41 INFO TaskSetManager: Finished task 2.0 in stage 42.0 (TID 74) in 230 ms on 172.18.0.14 (executor 1) (2/3)
26/02/13 12:03:41 INFO TaskSetManager: Finished task 1.0 in stage 42.0 (TID 73) in 343 ms on 172.18.0.15 (executor 0) (3/3)
26/02/13 12:03:41 INFO TaskSchedulerImpl: Removed TaskSet 42.0, whose tasks have all completed, from pool 
26/02/13 12:03:41 INFO DAGScheduler: ResultStage 42 (start at NativeMethodAccessorImpl.java:0) finished in 0.364 s
26/02/13 12:03:41 INFO DAGScheduler: Job 26 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 12:03:41 INFO TaskSchedulerImpl: Killing all running tasks in stage 42: Stage finished
26/02/13 12:03:41 INFO DAGScheduler: Job 26 finished: start at NativeMethodAccessorImpl.java:0, took 0.379009 s
26/02/13 12:03:41 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 131, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@13edfac4] is committing.
26/02/13 12:03:41 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 131, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@13edfac4] committed.
26/02/13 12:03:41 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/131 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.131.2a4a0e2b-0fe1-482f-a073-dc7126e071bc.tmp
26/02/13 12:03:41 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.131.2a4a0e2b-0fe1-482f-a073-dc7126e071bc.tmp to file:/tmp/spark-checkpoint-enrichment/commits/131
26/02/13 12:03:41 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "4a161d66-225b-4b29-812d-ddec3c05364e",
  "name" : null,
  "timestamp" : "2026-02-13T12:03:40.466Z",
  "batchId" : 131,
  "numInputRows" : 154,
  "inputRowsPerSecond" : 137.1326803205699,
  "processedRowsPerSecond" : 183.7708830548926,
  "durationMs" : {
    "addBatch" : 635,
    "commitOffsets" : 85,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 58,
    "triggerExecution" : 838,
    "walCommit" : 57
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 826,
        "1" : 1015,
        "0" : 749
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 878,
        "1" : 1071,
        "0" : 795
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 878,
        "1" : 1071,
        "0" : 795
      }
    },
    "numInputRows" : 154,
    "inputRowsPerSecond" : 137.1326803205699,
    "processedRowsPerSecond" : 183.7708830548926,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@1a6a42c2",
    "numOutputRows" : 28
  }
}
26/02/13 12:03:50 INFO BlockManagerInfo: Removed broadcast_39_piece0 on spark-master:42073 in memory (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 12:03:50 INFO BlockManagerInfo: Removed broadcast_39_piece0 on 172.18.0.14:37571 in memory (size: 19.5 KiB, free: 434.0 MiB)
26/02/13 12:03:50 INFO BlockManagerInfo: Removed broadcast_39_piece0 on 172.18.0.15:37717 in memory (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 12:03:51 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:03:57 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/132 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.132.b59efa9f-2a99-4da7-b754-87a4b502119b.tmp
26/02/13 12:03:57 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.132.b59efa9f-2a99-4da7-b754-87a4b502119b.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/132
26/02/13 12:03:57 INFO MicroBatchExecution: Committed offsets for batch 132. Metadata OffsetSeqMetadata(0,1770984237566,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 12:03:57 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:03:57 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:03:57 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:03:57 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:03:57 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:03:57 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:03:57 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#9637 - origin_code.nullCount#9636) > 0)
26/02/13 12:03:57 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#9642 - destination_code.nullCount#9641) > 0)
26/02/13 12:03:57 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#9672 - callsign.nullCount#9671) > 0)
26/02/13 12:03:57 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 12:03:57 INFO DAGScheduler: Got job 27 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 12:03:57 INFO DAGScheduler: Final stage: ResultStage 44 (start at NativeMethodAccessorImpl.java:0)
26/02/13 12:03:57 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 43)
26/02/13 12:03:57 INFO DAGScheduler: Missing parents: List()
26/02/13 12:03:57 INFO DAGScheduler: Submitting ResultStage 44 (MapPartitionsRDD[142] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:03:57 INFO MemoryStore: Block broadcast_40 stored as values in memory (estimated size 99.3 KiB, free 433.9 MiB)
26/02/13 12:03:57 INFO MemoryStore: Block broadcast_40_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 433.8 MiB)
26/02/13 12:03:57 INFO BlockManagerInfo: Added broadcast_40_piece0 in memory on spark-master:42073 (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 12:03:57 INFO SparkContext: Created broadcast 40 from broadcast at DAGScheduler.scala:1585
26/02/13 12:03:57 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 44 (MapPartitionsRDD[142] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 12:03:57 INFO TaskSchedulerImpl: Adding task set 44.0 with 4 tasks resource profile 0
26/02/13 12:03:57 INFO TaskSetManager: Starting task 0.0 in stage 44.0 (TID 75) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:03:57 INFO TaskSetManager: Starting task 1.0 in stage 44.0 (TID 76) (172.18.0.14, executor 1, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:03:57 INFO BlockManagerInfo: Added broadcast_40_piece0 in memory on 172.18.0.14:37571 (size: 33.3 KiB, free: 433.9 MiB)
26/02/13 12:03:57 INFO TaskSetManager: Starting task 2.0 in stage 44.0 (TID 77) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:03:57 INFO TaskSetManager: Finished task 0.0 in stage 44.0 (TID 75) in 30 ms on 172.18.0.14 (executor 1) (1/4)
26/02/13 12:03:57 INFO TaskSetManager: Finished task 1.0 in stage 44.0 (TID 76) in 30 ms on 172.18.0.14 (executor 1) (2/4)
26/02/13 12:03:57 INFO TaskSetManager: Starting task 3.0 in stage 44.0 (TID 78) (172.18.0.14, executor 1, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:03:57 INFO TaskSetManager: Finished task 2.0 in stage 44.0 (TID 77) in 24 ms on 172.18.0.14 (executor 1) (3/4)
26/02/13 12:03:57 INFO TaskSetManager: Finished task 3.0 in stage 44.0 (TID 78) in 24 ms on 172.18.0.14 (executor 1) (4/4)
26/02/13 12:03:57 INFO TaskSchedulerImpl: Removed TaskSet 44.0, whose tasks have all completed, from pool 
26/02/13 12:03:57 INFO DAGScheduler: ResultStage 44 (start at NativeMethodAccessorImpl.java:0) finished in 0.066 s
26/02/13 12:03:57 INFO DAGScheduler: Job 27 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 12:03:57 INFO TaskSchedulerImpl: Killing all running tasks in stage 44: Stage finished
26/02/13 12:03:57 INFO DAGScheduler: Job 27 finished: start at NativeMethodAccessorImpl.java:0, took 0.069007 s
26/02/13 12:03:57 INFO MemoryStore: Block broadcast_41_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.7 MiB)
26/02/13 12:03:57 INFO BlockManagerInfo: Added broadcast_41_piece0 in memory on spark-master:42073 (size: 107.2 KiB, free: 434.0 MiB)
26/02/13 12:03:57 INFO SparkContext: Created broadcast 41 from start at NativeMethodAccessorImpl.java:0
26/02/13 12:03:57 INFO BlockManagerInfo: Removed broadcast_32_piece0 on spark-master:42073 in memory (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 12:03:57 INFO BlockManagerInfo: Removed broadcast_32_piece0 on 172.18.0.14:37571 in memory (size: 107.2 KiB, free: 434.0 MiB)
26/02/13 12:03:57 INFO BlockManagerInfo: Removed broadcast_32_piece0 on 172.18.0.15:37717 in memory (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 12:03:57 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 132, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@16fdcb56]. The input RDD has 3 partitions.
26/02/13 12:03:57 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 12:03:57 INFO DAGScheduler: Got job 28 (start at NativeMethodAccessorImpl.java:0) with 3 output partitions
26/02/13 12:03:57 INFO DAGScheduler: Final stage: ResultStage 45 (start at NativeMethodAccessorImpl.java:0)
26/02/13 12:03:57 INFO DAGScheduler: Parents of final stage: List()
26/02/13 12:03:57 INFO DAGScheduler: Missing parents: List()
26/02/13 12:03:57 INFO DAGScheduler: Submitting ResultStage 45 (MapPartitionsRDD[148] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:03:57 INFO MemoryStore: Block broadcast_42 stored as values in memory (estimated size 49.9 KiB, free 433.8 MiB)
26/02/13 12:03:57 INFO MemoryStore: Block broadcast_42_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.8 MiB)
26/02/13 12:03:57 INFO BlockManagerInfo: Added broadcast_42_piece0 in memory on spark-master:42073 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 12:03:57 INFO SparkContext: Created broadcast 42 from broadcast at DAGScheduler.scala:1585
26/02/13 12:03:57 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 45 (MapPartitionsRDD[148] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
26/02/13 12:03:57 INFO TaskSchedulerImpl: Adding task set 45.0 with 3 tasks resource profile 0
26/02/13 12:03:57 INFO TaskSetManager: Starting task 0.0 in stage 45.0 (TID 79) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:03:57 INFO TaskSetManager: Starting task 1.0 in stage 45.0 (TID 80) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:03:57 INFO TaskSetManager: Starting task 2.0 in stage 45.0 (TID 81) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:03:57 INFO BlockManagerInfo: Added broadcast_42_piece0 in memory on 172.18.0.15:37717 (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 12:03:57 INFO BlockManagerInfo: Added broadcast_42_piece0 in memory on 172.18.0.14:37571 (size: 19.5 KiB, free: 434.0 MiB)
26/02/13 12:03:57 INFO BlockManagerInfo: Added broadcast_41_piece0 in memory on 172.18.0.14:37571 (size: 107.2 KiB, free: 433.9 MiB)
26/02/13 12:03:58 INFO BlockManagerInfo: Added broadcast_41_piece0 in memory on 172.18.0.15:37717 (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 12:03:58 INFO TaskSetManager: Finished task 2.0 in stage 45.0 (TID 81) in 604 ms on 172.18.0.14 (executor 1) (1/3)
26/02/13 12:03:58 INFO TaskSetManager: Finished task 0.0 in stage 45.0 (TID 79) in 605 ms on 172.18.0.14 (executor 1) (2/3)
26/02/13 12:03:58 INFO TaskSetManager: Finished task 1.0 in stage 45.0 (TID 80) in 640 ms on 172.18.0.15 (executor 0) (3/3)
26/02/13 12:03:58 INFO TaskSchedulerImpl: Removed TaskSet 45.0, whose tasks have all completed, from pool 
26/02/13 12:03:58 INFO DAGScheduler: ResultStage 45 (start at NativeMethodAccessorImpl.java:0) finished in 0.648 s
26/02/13 12:03:58 INFO DAGScheduler: Job 28 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 12:03:58 INFO TaskSchedulerImpl: Killing all running tasks in stage 45: Stage finished
26/02/13 12:03:58 INFO DAGScheduler: Job 28 finished: start at NativeMethodAccessorImpl.java:0, took 0.650136 s
26/02/13 12:03:58 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 132, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@16fdcb56] is committing.
26/02/13 12:03:58 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 132, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@16fdcb56] committed.
26/02/13 12:03:58 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/132 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.132.3a24ea6f-d82c-4caa-bd45-bd05f0446efe.tmp
26/02/13 12:03:58 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.132.3a24ea6f-d82c-4caa-bd45-bd05f0446efe.tmp to file:/tmp/spark-checkpoint-enrichment/commits/132
26/02/13 12:03:58 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "4a161d66-225b-4b29-812d-ddec3c05364e",
  "name" : null,
  "timestamp" : "2026-02-13T12:03:57.564Z",
  "batchId" : 132,
  "numInputRows" : 111,
  "inputRowsPerSecond" : 10090.909090909092,
  "processedRowsPerSecond" : 102.58780036968577,
  "durationMs" : {
    "addBatch" : 863,
    "commitOffsets" : 53,
    "getBatch" : 0,
    "latestOffset" : 2,
    "queryPlanning" : 58,
    "triggerExecution" : 1082,
    "walCommit" : 106
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 878,
        "1" : 1071,
        "0" : 795
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 915,
        "1" : 1108,
        "0" : 832
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 915,
        "1" : 1108,
        "0" : 832
      }
    },
    "numInputRows" : 111,
    "inputRowsPerSecond" : 10090.909090909092,
    "processedRowsPerSecond" : 102.58780036968577,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@1a6a42c2",
    "numOutputRows" : 27
  }
}
26/02/13 12:03:58 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/133 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.133.cb6186e3-80b1-411b-b7f7-7ec7ffa26cea.tmp
26/02/13 12:03:58 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.133.cb6186e3-80b1-411b-b7f7-7ec7ffa26cea.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/133
26/02/13 12:03:58 INFO MicroBatchExecution: Committed offsets for batch 133. Metadata OffsetSeqMetadata(0,1770984238648,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 12:03:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:03:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:03:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:03:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:03:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:03:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:03:58 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#10491 - origin_code.nullCount#10490) > 0)
26/02/13 12:03:58 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#10496 - destination_code.nullCount#10495) > 0)
26/02/13 12:03:58 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#10526 - callsign.nullCount#10525) > 0)
26/02/13 12:03:58 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 12:03:58 INFO DAGScheduler: Got job 29 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 12:03:58 INFO DAGScheduler: Final stage: ResultStage 47 (start at NativeMethodAccessorImpl.java:0)
26/02/13 12:03:58 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 46)
26/02/13 12:03:58 INFO DAGScheduler: Missing parents: List()
26/02/13 12:03:58 INFO DAGScheduler: Submitting ResultStage 47 (MapPartitionsRDD[153] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:03:58 INFO MemoryStore: Block broadcast_43 stored as values in memory (estimated size 99.3 KiB, free 433.7 MiB)
26/02/13 12:03:58 INFO MemoryStore: Block broadcast_43_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 433.6 MiB)
26/02/13 12:03:58 INFO BlockManagerInfo: Added broadcast_43_piece0 in memory on spark-master:42073 (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 12:03:58 INFO SparkContext: Created broadcast 43 from broadcast at DAGScheduler.scala:1585
26/02/13 12:03:58 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 47 (MapPartitionsRDD[153] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 12:03:58 INFO TaskSchedulerImpl: Adding task set 47.0 with 4 tasks resource profile 0
26/02/13 12:03:58 INFO TaskSetManager: Starting task 0.0 in stage 47.0 (TID 82) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:03:58 INFO TaskSetManager: Starting task 1.0 in stage 47.0 (TID 83) (172.18.0.14, executor 1, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:03:58 INFO BlockManagerInfo: Added broadcast_43_piece0 in memory on 172.18.0.14:37571 (size: 33.3 KiB, free: 433.9 MiB)
26/02/13 12:03:58 INFO TaskSetManager: Starting task 2.0 in stage 47.0 (TID 84) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:03:58 INFO TaskSetManager: Finished task 1.0 in stage 47.0 (TID 83) in 26 ms on 172.18.0.14 (executor 1) (1/4)
26/02/13 12:03:58 INFO TaskSetManager: Starting task 3.0 in stage 47.0 (TID 85) (172.18.0.14, executor 1, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:03:58 INFO TaskSetManager: Finished task 0.0 in stage 47.0 (TID 82) in 30 ms on 172.18.0.14 (executor 1) (2/4)
26/02/13 12:03:58 INFO TaskSetManager: Finished task 2.0 in stage 47.0 (TID 84) in 17 ms on 172.18.0.14 (executor 1) (3/4)
26/02/13 12:03:58 INFO TaskSetManager: Finished task 3.0 in stage 47.0 (TID 85) in 17 ms on 172.18.0.14 (executor 1) (4/4)
26/02/13 12:03:58 INFO TaskSchedulerImpl: Removed TaskSet 47.0, whose tasks have all completed, from pool 
26/02/13 12:03:58 INFO DAGScheduler: ResultStage 47 (start at NativeMethodAccessorImpl.java:0) finished in 0.055 s
26/02/13 12:03:58 INFO DAGScheduler: Job 29 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 12:03:58 INFO TaskSchedulerImpl: Killing all running tasks in stage 47: Stage finished
26/02/13 12:03:58 INFO DAGScheduler: Job 29 finished: start at NativeMethodAccessorImpl.java:0, took 0.058999 s
26/02/13 12:03:58 INFO MemoryStore: Block broadcast_44_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.5 MiB)
26/02/13 12:03:58 INFO BlockManagerInfo: Added broadcast_44_piece0 in memory on spark-master:42073 (size: 107.2 KiB, free: 434.0 MiB)
26/02/13 12:03:58 INFO SparkContext: Created broadcast 44 from start at NativeMethodAccessorImpl.java:0
26/02/13 12:03:58 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 133, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@17ad2efe]. The input RDD has 3 partitions.
26/02/13 12:03:58 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 12:03:58 INFO DAGScheduler: Got job 30 (start at NativeMethodAccessorImpl.java:0) with 3 output partitions
26/02/13 12:03:58 INFO DAGScheduler: Final stage: ResultStage 48 (start at NativeMethodAccessorImpl.java:0)
26/02/13 12:03:58 INFO DAGScheduler: Parents of final stage: List()
26/02/13 12:03:58 INFO DAGScheduler: Missing parents: List()
26/02/13 12:03:58 INFO BlockManagerInfo: Removed broadcast_38_piece0 on spark-master:42073 in memory (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 12:03:58 INFO DAGScheduler: Submitting ResultStage 48 (MapPartitionsRDD[159] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:03:58 INFO BlockManagerInfo: Removed broadcast_38_piece0 on 172.18.0.14:37571 in memory (size: 107.2 KiB, free: 434.0 MiB)
26/02/13 12:03:58 INFO MemoryStore: Block broadcast_45 stored as values in memory (estimated size 49.9 KiB, free 433.6 MiB)
26/02/13 12:03:58 INFO BlockManagerInfo: Removed broadcast_38_piece0 on 172.18.0.15:37717 in memory (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 12:03:58 INFO MemoryStore: Block broadcast_45_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.6 MiB)
26/02/13 12:03:58 INFO BlockManagerInfo: Added broadcast_45_piece0 in memory on spark-master:42073 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 12:03:58 INFO SparkContext: Created broadcast 45 from broadcast at DAGScheduler.scala:1585
26/02/13 12:03:58 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 48 (MapPartitionsRDD[159] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
26/02/13 12:03:58 INFO TaskSchedulerImpl: Adding task set 48.0 with 3 tasks resource profile 0
26/02/13 12:03:58 INFO TaskSetManager: Starting task 1.0 in stage 48.0 (TID 86) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:03:58 INFO TaskSetManager: Starting task 0.0 in stage 48.0 (TID 87) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:03:58 INFO TaskSetManager: Starting task 2.0 in stage 48.0 (TID 88) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:03:58 INFO BlockManagerInfo: Removed broadcast_41_piece0 on spark-master:42073 in memory (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 12:03:58 INFO BlockManagerInfo: Removed broadcast_41_piece0 on 172.18.0.15:37717 in memory (size: 107.2 KiB, free: 434.4 MiB)
26/02/13 12:03:58 INFO BlockManagerInfo: Removed broadcast_41_piece0 on 172.18.0.14:37571 in memory (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 12:03:58 INFO BlockManagerInfo: Removed broadcast_43_piece0 on spark-master:42073 in memory (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 12:03:58 INFO BlockManagerInfo: Removed broadcast_43_piece0 on 172.18.0.14:37571 in memory (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 12:03:58 INFO BlockManagerInfo: Added broadcast_45_piece0 in memory on 172.18.0.15:37717 (size: 19.5 KiB, free: 434.4 MiB)
26/02/13 12:03:58 INFO BlockManagerInfo: Added broadcast_45_piece0 in memory on 172.18.0.14:37571 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 12:03:58 INFO BlockManagerInfo: Removed broadcast_40_piece0 on spark-master:42073 in memory (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 12:03:58 INFO BlockManagerInfo: Removed broadcast_40_piece0 on 172.18.0.14:37571 in memory (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 12:03:58 INFO BlockManagerInfo: Removed broadcast_42_piece0 on spark-master:42073 in memory (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 12:03:58 INFO BlockManagerInfo: Removed broadcast_42_piece0 on 172.18.0.15:37717 in memory (size: 19.5 KiB, free: 434.4 MiB)
26/02/13 12:03:58 INFO BlockManagerInfo: Removed broadcast_42_piece0 on 172.18.0.14:37571 in memory (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 12:03:58 INFO BlockManagerInfo: Added broadcast_44_piece0 in memory on 172.18.0.14:37571 (size: 107.2 KiB, free: 434.0 MiB)
26/02/13 12:03:58 INFO BlockManagerInfo: Added broadcast_44_piece0 in memory on 172.18.0.15:37717 (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 12:03:59 INFO TaskSetManager: Finished task 2.0 in stage 48.0 (TID 88) in 123 ms on 172.18.0.14 (executor 1) (1/3)
26/02/13 12:03:59 INFO TaskSetManager: Finished task 0.0 in stage 48.0 (TID 87) in 135 ms on 172.18.0.14 (executor 1) (2/3)
26/02/13 12:03:59 INFO TaskSetManager: Finished task 1.0 in stage 48.0 (TID 86) in 165 ms on 172.18.0.15 (executor 0) (3/3)
26/02/13 12:03:59 INFO TaskSchedulerImpl: Removed TaskSet 48.0, whose tasks have all completed, from pool 
26/02/13 12:03:59 INFO DAGScheduler: ResultStage 48 (start at NativeMethodAccessorImpl.java:0) finished in 0.176 s
26/02/13 12:03:59 INFO DAGScheduler: Job 30 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 12:03:59 INFO TaskSchedulerImpl: Killing all running tasks in stage 48: Stage finished
26/02/13 12:03:59 INFO DAGScheduler: Job 30 finished: start at NativeMethodAccessorImpl.java:0, took 0.179267 s
26/02/13 12:03:59 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 133, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@17ad2efe] is committing.
26/02/13 12:03:59 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 133, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@17ad2efe] committed.
26/02/13 12:03:59 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/133 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.133.8a120e6e-a345-4eb6-88bf-d06c3a32cfc4.tmp
26/02/13 12:03:59 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.133.8a120e6e-a345-4eb6-88bf-d06c3a32cfc4.tmp to file:/tmp/spark-checkpoint-enrichment/commits/133
26/02/13 12:03:59 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "4a161d66-225b-4b29-812d-ddec3c05364e",
  "name" : null,
  "timestamp" : "2026-02-13T12:03:58.647Z",
  "batchId" : 133,
  "numInputRows" : 133,
  "inputRowsPerSecond" : 122.80701754385966,
  "processedRowsPerSecond" : 264.9402390438247,
  "durationMs" : {
    "addBatch" : 349,
    "commitOffsets" : 64,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 33,
    "triggerExecution" : 502,
    "walCommit" : 54
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 915,
        "1" : 1108,
        "0" : 832
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 955,
        "1" : 1164,
        "0" : 869
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 955,
        "1" : 1164,
        "0" : 869
      }
    },
    "numInputRows" : 133,
    "inputRowsPerSecond" : 122.80701754385966,
    "processedRowsPerSecond" : 264.9402390438247,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@1a6a42c2",
    "numOutputRows" : 27
  }
}
26/02/13 12:03:59 INFO BlockManagerInfo: Removed broadcast_45_piece0 on spark-master:42073 in memory (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 12:03:59 INFO BlockManagerInfo: Removed broadcast_45_piece0 on 172.18.0.14:37571 in memory (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 12:03:59 INFO BlockManagerInfo: Removed broadcast_45_piece0 on 172.18.0.15:37717 in memory (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 12:04:09 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:04:13 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/134 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.134.5da774eb-91b1-4c0f-8fe2-03395c272df2.tmp
26/02/13 12:04:13 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.134.5da774eb-91b1-4c0f-8fe2-03395c272df2.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/134
26/02/13 12:04:13 INFO MicroBatchExecution: Committed offsets for batch 134. Metadata OffsetSeqMetadata(0,1770984253552,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 12:04:13 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:04:13 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:04:13 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:04:13 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:04:13 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:04:13 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:04:13 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#11345 - origin_code.nullCount#11344) > 0)
26/02/13 12:04:13 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#11350 - destination_code.nullCount#11349) > 0)
26/02/13 12:04:13 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#11380 - callsign.nullCount#11379) > 0)
26/02/13 12:04:13 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 12:04:13 INFO DAGScheduler: Got job 31 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 12:04:13 INFO DAGScheduler: Final stage: ResultStage 50 (start at NativeMethodAccessorImpl.java:0)
26/02/13 12:04:13 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 49)
26/02/13 12:04:13 INFO DAGScheduler: Missing parents: List()
26/02/13 12:04:13 INFO DAGScheduler: Submitting ResultStage 50 (MapPartitionsRDD[164] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:04:13 INFO MemoryStore: Block broadcast_46 stored as values in memory (estimated size 99.3 KiB, free 434.0 MiB)
26/02/13 12:04:13 INFO MemoryStore: Block broadcast_46_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 433.9 MiB)
26/02/13 12:04:13 INFO BlockManagerInfo: Added broadcast_46_piece0 in memory on spark-master:42073 (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 12:04:13 INFO SparkContext: Created broadcast 46 from broadcast at DAGScheduler.scala:1585
26/02/13 12:04:13 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 50 (MapPartitionsRDD[164] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 12:04:13 INFO TaskSchedulerImpl: Adding task set 50.0 with 4 tasks resource profile 0
26/02/13 12:04:13 INFO TaskSetManager: Starting task 0.0 in stage 50.0 (TID 89) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:04:13 INFO TaskSetManager: Starting task 1.0 in stage 50.0 (TID 90) (172.18.0.14, executor 1, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:04:13 INFO BlockManagerInfo: Added broadcast_46_piece0 in memory on 172.18.0.14:37571 (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 12:04:13 INFO TaskSetManager: Starting task 2.0 in stage 50.0 (TID 91) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:04:13 INFO TaskSetManager: Finished task 0.0 in stage 50.0 (TID 89) in 31 ms on 172.18.0.14 (executor 1) (1/4)
26/02/13 12:04:13 INFO TaskSetManager: Starting task 3.0 in stage 50.0 (TID 92) (172.18.0.14, executor 1, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:04:13 INFO TaskSetManager: Finished task 1.0 in stage 50.0 (TID 90) in 33 ms on 172.18.0.14 (executor 1) (2/4)
26/02/13 12:04:13 INFO TaskSetManager: Finished task 2.0 in stage 50.0 (TID 91) in 26 ms on 172.18.0.14 (executor 1) (3/4)
26/02/13 12:04:13 INFO TaskSetManager: Finished task 3.0 in stage 50.0 (TID 92) in 35 ms on 172.18.0.14 (executor 1) (4/4)
26/02/13 12:04:13 INFO TaskSchedulerImpl: Removed TaskSet 50.0, whose tasks have all completed, from pool 
26/02/13 12:04:13 INFO DAGScheduler: ResultStage 50 (start at NativeMethodAccessorImpl.java:0) finished in 0.080 s
26/02/13 12:04:13 INFO DAGScheduler: Job 31 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 12:04:13 INFO TaskSchedulerImpl: Killing all running tasks in stage 50: Stage finished
26/02/13 12:04:13 INFO BlockManagerInfo: Removed broadcast_44_piece0 on spark-master:42073 in memory (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 12:04:13 INFO DAGScheduler: Job 31 finished: start at NativeMethodAccessorImpl.java:0, took 0.085533 s
26/02/13 12:04:13 INFO BlockManagerInfo: Removed broadcast_44_piece0 on 172.18.0.14:37571 in memory (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 12:04:13 INFO BlockManagerInfo: Removed broadcast_44_piece0 on 172.18.0.15:37717 in memory (size: 107.2 KiB, free: 434.4 MiB)
26/02/13 12:04:13 INFO MemoryStore: Block broadcast_47_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.9 MiB)
26/02/13 12:04:13 INFO BlockManagerInfo: Added broadcast_47_piece0 in memory on spark-master:42073 (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 12:04:13 INFO SparkContext: Created broadcast 47 from start at NativeMethodAccessorImpl.java:0
26/02/13 12:04:13 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 134, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@509aef70]. The input RDD has 2 partitions.
26/02/13 12:04:13 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 12:04:13 INFO DAGScheduler: Got job 32 (start at NativeMethodAccessorImpl.java:0) with 2 output partitions
26/02/13 12:04:13 INFO DAGScheduler: Final stage: ResultStage 51 (start at NativeMethodAccessorImpl.java:0)
26/02/13 12:04:13 INFO DAGScheduler: Parents of final stage: List()
26/02/13 12:04:13 INFO DAGScheduler: Missing parents: List()
26/02/13 12:04:13 INFO DAGScheduler: Submitting ResultStage 51 (MapPartitionsRDD[170] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:04:13 INFO MemoryStore: Block broadcast_48 stored as values in memory (estimated size 49.9 KiB, free 433.9 MiB)
26/02/13 12:04:13 INFO MemoryStore: Block broadcast_48_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.9 MiB)
26/02/13 12:04:13 INFO BlockManagerInfo: Added broadcast_48_piece0 in memory on spark-master:42073 (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 12:04:13 INFO SparkContext: Created broadcast 48 from broadcast at DAGScheduler.scala:1585
26/02/13 12:04:13 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 51 (MapPartitionsRDD[170] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
26/02/13 12:04:13 INFO TaskSchedulerImpl: Adding task set 51.0 with 2 tasks resource profile 0
26/02/13 12:04:13 INFO TaskSetManager: Starting task 0.0 in stage 51.0 (TID 93) (172.18.0.15, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:04:13 INFO TaskSetManager: Starting task 1.0 in stage 51.0 (TID 94) (172.18.0.14, executor 1, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:04:13 INFO BlockManagerInfo: Added broadcast_48_piece0 in memory on 172.18.0.14:37571 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 12:04:13 INFO BlockManagerInfo: Added broadcast_48_piece0 in memory on 172.18.0.15:37717 (size: 19.5 KiB, free: 434.4 MiB)
26/02/13 12:04:13 INFO BlockManagerInfo: Added broadcast_47_piece0 in memory on 172.18.0.14:37571 (size: 107.2 KiB, free: 434.0 MiB)
26/02/13 12:04:13 INFO BlockManagerInfo: Added broadcast_47_piece0 in memory on 172.18.0.15:37717 (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 12:04:14 INFO TaskSetManager: Finished task 1.0 in stage 51.0 (TID 94) in 587 ms on 172.18.0.14 (executor 1) (1/2)
26/02/13 12:04:14 INFO TaskSetManager: Finished task 0.0 in stage 51.0 (TID 93) in 616 ms on 172.18.0.15 (executor 0) (2/2)
26/02/13 12:04:14 INFO TaskSchedulerImpl: Removed TaskSet 51.0, whose tasks have all completed, from pool 
26/02/13 12:04:14 INFO DAGScheduler: ResultStage 51 (start at NativeMethodAccessorImpl.java:0) finished in 0.624 s
26/02/13 12:04:14 INFO DAGScheduler: Job 32 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 12:04:14 INFO TaskSchedulerImpl: Killing all running tasks in stage 51: Stage finished
26/02/13 12:04:14 INFO DAGScheduler: Job 32 finished: start at NativeMethodAccessorImpl.java:0, took 0.630107 s
26/02/13 12:04:14 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 134, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@509aef70] is committing.
26/02/13 12:04:14 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 134, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@509aef70] committed.
26/02/13 12:04:14 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/134 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.134.7e1739ce-81c7-41b9-83d6-7c8f6e8c07f2.tmp
26/02/13 12:04:14 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.134.7e1739ce-81c7-41b9-83d6-7c8f6e8c07f2.tmp to file:/tmp/spark-checkpoint-enrichment/commits/134
26/02/13 12:04:14 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "4a161d66-225b-4b29-812d-ddec3c05364e",
  "name" : null,
  "timestamp" : "2026-02-13T12:04:13.550Z",
  "batchId" : 134,
  "numInputRows" : 7,
  "inputRowsPerSecond" : 583.3333333333334,
  "processedRowsPerSecond" : 6.673021925643471,
  "durationMs" : {
    "addBatch" : 844,
    "commitOffsets" : 61,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 58,
    "triggerExecution" : 1049,
    "walCommit" : 83
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 955,
        "1" : 1164,
        "0" : 869
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 955,
        "1" : 1169,
        "0" : 871
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 955,
        "1" : 1169,
        "0" : 871
      }
    },
    "numInputRows" : 7,
    "inputRowsPerSecond" : 583.3333333333334,
    "processedRowsPerSecond" : 6.673021925643471,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@1a6a42c2",
    "numOutputRows" : 1
  }
}
26/02/13 12:04:14 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/135 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.135.4d33cd74-4de0-4fcb-b9f6-064be5a58c1e.tmp
26/02/13 12:04:14 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.135.4d33cd74-4de0-4fcb-b9f6-064be5a58c1e.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/135
26/02/13 12:04:14 INFO MicroBatchExecution: Committed offsets for batch 135. Metadata OffsetSeqMetadata(0,1770984254602,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 12:04:14 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:04:14 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:04:14 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:04:14 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:04:14 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:04:14 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:04:14 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#12199 - origin_code.nullCount#12198) > 0)
26/02/13 12:04:14 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#12204 - destination_code.nullCount#12203) > 0)
26/02/13 12:04:14 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#12234 - callsign.nullCount#12233) > 0)
26/02/13 12:04:14 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 12:04:14 INFO DAGScheduler: Got job 33 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 12:04:14 INFO DAGScheduler: Final stage: ResultStage 53 (start at NativeMethodAccessorImpl.java:0)
26/02/13 12:04:14 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 52)
26/02/13 12:04:14 INFO DAGScheduler: Missing parents: List()
26/02/13 12:04:14 INFO DAGScheduler: Submitting ResultStage 53 (MapPartitionsRDD[175] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:04:14 INFO MemoryStore: Block broadcast_49 stored as values in memory (estimated size 99.3 KiB, free 433.8 MiB)
26/02/13 12:04:14 INFO MemoryStore: Block broadcast_49_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 433.7 MiB)
26/02/13 12:04:14 INFO BlockManagerInfo: Added broadcast_49_piece0 in memory on spark-master:42073 (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 12:04:14 INFO SparkContext: Created broadcast 49 from broadcast at DAGScheduler.scala:1585
26/02/13 12:04:14 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 53 (MapPartitionsRDD[175] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 12:04:14 INFO TaskSchedulerImpl: Adding task set 53.0 with 4 tasks resource profile 0
26/02/13 12:04:14 INFO TaskSetManager: Starting task 0.0 in stage 53.0 (TID 95) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:04:14 INFO TaskSetManager: Starting task 1.0 in stage 53.0 (TID 96) (172.18.0.14, executor 1, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:04:14 INFO BlockManagerInfo: Added broadcast_49_piece0 in memory on 172.18.0.14:37571 (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 12:04:14 INFO TaskSetManager: Starting task 2.0 in stage 53.0 (TID 97) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:04:14 INFO TaskSetManager: Finished task 0.0 in stage 53.0 (TID 95) in 31 ms on 172.18.0.14 (executor 1) (1/4)
26/02/13 12:04:14 INFO TaskSetManager: Starting task 3.0 in stage 53.0 (TID 98) (172.18.0.14, executor 1, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:04:14 INFO TaskSetManager: Finished task 1.0 in stage 53.0 (TID 96) in 33 ms on 172.18.0.14 (executor 1) (2/4)
26/02/13 12:04:14 INFO TaskSetManager: Finished task 2.0 in stage 53.0 (TID 97) in 29 ms on 172.18.0.14 (executor 1) (3/4)
26/02/13 12:04:14 INFO TaskSetManager: Finished task 3.0 in stage 53.0 (TID 98) in 31 ms on 172.18.0.14 (executor 1) (4/4)
26/02/13 12:04:14 INFO TaskSchedulerImpl: Removed TaskSet 53.0, whose tasks have all completed, from pool 
26/02/13 12:04:14 INFO DAGScheduler: ResultStage 53 (start at NativeMethodAccessorImpl.java:0) finished in 0.071 s
26/02/13 12:04:14 INFO DAGScheduler: Job 33 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 12:04:14 INFO TaskSchedulerImpl: Killing all running tasks in stage 53: Stage finished
26/02/13 12:04:14 INFO DAGScheduler: Job 33 finished: start at NativeMethodAccessorImpl.java:0, took 0.073351 s
26/02/13 12:04:14 INFO MemoryStore: Block broadcast_50_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.6 MiB)
26/02/13 12:04:14 INFO BlockManagerInfo: Added broadcast_50_piece0 in memory on spark-master:42073 (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 12:04:14 INFO SparkContext: Created broadcast 50 from start at NativeMethodAccessorImpl.java:0
26/02/13 12:04:14 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 135, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@7d0763cf]. The input RDD has 3 partitions.
26/02/13 12:04:14 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 12:04:14 INFO DAGScheduler: Got job 34 (start at NativeMethodAccessorImpl.java:0) with 3 output partitions
26/02/13 12:04:14 INFO DAGScheduler: Final stage: ResultStage 54 (start at NativeMethodAccessorImpl.java:0)
26/02/13 12:04:14 INFO DAGScheduler: Parents of final stage: List()
26/02/13 12:04:14 INFO DAGScheduler: Missing parents: List()
26/02/13 12:04:14 INFO DAGScheduler: Submitting ResultStage 54 (MapPartitionsRDD[181] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:04:14 INFO MemoryStore: Block broadcast_51 stored as values in memory (estimated size 49.9 KiB, free 433.6 MiB)
26/02/13 12:04:14 INFO MemoryStore: Block broadcast_51_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.6 MiB)
26/02/13 12:04:14 INFO BlockManagerInfo: Added broadcast_51_piece0 in memory on spark-master:42073 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 12:04:14 INFO SparkContext: Created broadcast 51 from broadcast at DAGScheduler.scala:1585
26/02/13 12:04:14 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 54 (MapPartitionsRDD[181] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
26/02/13 12:04:14 INFO TaskSchedulerImpl: Adding task set 54.0 with 3 tasks resource profile 0
26/02/13 12:04:14 INFO TaskSetManager: Starting task 1.0 in stage 54.0 (TID 99) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:04:14 INFO TaskSetManager: Starting task 0.0 in stage 54.0 (TID 100) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:04:14 INFO TaskSetManager: Starting task 2.0 in stage 54.0 (TID 101) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:04:14 INFO BlockManagerInfo: Removed broadcast_46_piece0 on spark-master:42073 in memory (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 12:04:14 INFO BlockManagerInfo: Removed broadcast_46_piece0 on 172.18.0.14:37571 in memory (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 12:04:14 INFO BlockManagerInfo: Added broadcast_51_piece0 in memory on 172.18.0.14:37571 (size: 19.5 KiB, free: 434.0 MiB)
26/02/13 12:04:14 INFO BlockManagerInfo: Removed broadcast_47_piece0 on spark-master:42073 in memory (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 12:04:14 INFO BlockManagerInfo: Removed broadcast_47_piece0 on 172.18.0.14:37571 in memory (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 12:04:14 INFO BlockManagerInfo: Removed broadcast_47_piece0 on 172.18.0.15:37717 in memory (size: 107.2 KiB, free: 434.4 MiB)
26/02/13 12:04:14 INFO BlockManagerInfo: Added broadcast_51_piece0 in memory on 172.18.0.15:37717 (size: 19.5 KiB, free: 434.4 MiB)
26/02/13 12:04:14 INFO BlockManagerInfo: Removed broadcast_49_piece0 on spark-master:42073 in memory (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 12:04:14 INFO BlockManagerInfo: Removed broadcast_49_piece0 on 172.18.0.14:37571 in memory (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 12:04:14 INFO BlockManagerInfo: Added broadcast_50_piece0 in memory on 172.18.0.14:37571 (size: 107.2 KiB, free: 434.0 MiB)
26/02/13 12:04:14 INFO BlockManagerInfo: Removed broadcast_48_piece0 on spark-master:42073 in memory (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 12:04:14 INFO BlockManagerInfo: Removed broadcast_48_piece0 on 172.18.0.15:37717 in memory (size: 19.5 KiB, free: 434.4 MiB)
26/02/13 12:04:14 INFO BlockManagerInfo: Removed broadcast_48_piece0 on 172.18.0.14:37571 in memory (size: 19.5 KiB, free: 434.0 MiB)
26/02/13 12:04:14 INFO BlockManagerInfo: Added broadcast_50_piece0 in memory on 172.18.0.15:37717 (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 12:04:15 INFO TaskSetManager: Finished task 2.0 in stage 54.0 (TID 101) in 118 ms on 172.18.0.14 (executor 1) (1/3)
26/02/13 12:04:15 INFO TaskSetManager: Finished task 1.0 in stage 54.0 (TID 99) in 164 ms on 172.18.0.15 (executor 0) (2/3)
26/02/13 12:04:15 INFO TaskSetManager: Finished task 0.0 in stage 54.0 (TID 100) in 626 ms on 172.18.0.14 (executor 1) (3/3)
26/02/13 12:04:15 INFO TaskSchedulerImpl: Removed TaskSet 54.0, whose tasks have all completed, from pool 
26/02/13 12:04:15 INFO DAGScheduler: ResultStage 54 (start at NativeMethodAccessorImpl.java:0) finished in 0.638 s
26/02/13 12:04:15 INFO DAGScheduler: Job 34 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 12:04:15 INFO TaskSchedulerImpl: Killing all running tasks in stage 54: Stage finished
26/02/13 12:04:15 INFO DAGScheduler: Job 34 finished: start at NativeMethodAccessorImpl.java:0, took 0.640636 s
26/02/13 12:04:15 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 135, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@7d0763cf] is committing.
26/02/13 12:04:15 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 135, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@7d0763cf] committed.
26/02/13 12:04:15 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/135 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.135.7fcfe160-3934-4111-9757-3dd6542a6fa5.tmp
26/02/13 12:04:15 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.135.7fcfe160-3934-4111-9757-3dd6542a6fa5.tmp to file:/tmp/spark-checkpoint-enrichment/commits/135
26/02/13 12:04:15 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "4a161d66-225b-4b29-812d-ddec3c05364e",
  "name" : null,
  "timestamp" : "2026-02-13T12:04:14.600Z",
  "batchId" : 135,
  "numInputRows" : 234,
  "inputRowsPerSecond" : 222.85714285714286,
  "processedRowsPerSecond" : 236.12512613521696,
  "durationMs" : {
    "addBatch" : 846,
    "commitOffsets" : 56,
    "getBatch" : 0,
    "latestOffset" : 2,
    "queryPlanning" : 32,
    "triggerExecution" : 991,
    "walCommit" : 54
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 955,
        "1" : 1169,
        "0" : 871
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 1031,
        "1" : 1256,
        "0" : 942
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 1031,
        "1" : 1256,
        "0" : 942
      }
    },
    "numInputRows" : 234,
    "inputRowsPerSecond" : 222.85714285714286,
    "processedRowsPerSecond" : 236.12512613521696,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@1a6a42c2",
    "numOutputRows" : 52
  }
}
26/02/13 12:04:25 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:04:33 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/136 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.136.aa15f27b-6852-4d03-9d9e-9e7ad394d286.tmp
26/02/13 12:04:33 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.136.aa15f27b-6852-4d03-9d9e-9e7ad394d286.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/136
26/02/13 12:04:33 INFO MicroBatchExecution: Committed offsets for batch 136. Metadata OffsetSeqMetadata(0,1770984273715,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 12:04:33 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:04:33 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:04:33 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:04:33 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:04:33 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:04:33 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:04:33 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#13053 - origin_code.nullCount#13052) > 0)
26/02/13 12:04:33 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#13058 - destination_code.nullCount#13057) > 0)
26/02/13 12:04:33 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#13088 - callsign.nullCount#13087) > 0)
26/02/13 12:04:33 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 12:04:33 INFO DAGScheduler: Got job 35 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 12:04:33 INFO DAGScheduler: Final stage: ResultStage 56 (start at NativeMethodAccessorImpl.java:0)
26/02/13 12:04:33 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 55)
26/02/13 12:04:33 INFO DAGScheduler: Missing parents: List()
26/02/13 12:04:33 INFO DAGScheduler: Submitting ResultStage 56 (MapPartitionsRDD[186] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:04:33 INFO MemoryStore: Block broadcast_52 stored as values in memory (estimated size 99.3 KiB, free 433.9 MiB)
26/02/13 12:04:33 INFO MemoryStore: Block broadcast_52_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 433.9 MiB)
26/02/13 12:04:33 INFO BlockManagerInfo: Added broadcast_52_piece0 in memory on spark-master:42073 (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 12:04:33 INFO SparkContext: Created broadcast 52 from broadcast at DAGScheduler.scala:1585
26/02/13 12:04:33 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 56 (MapPartitionsRDD[186] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 12:04:33 INFO TaskSchedulerImpl: Adding task set 56.0 with 4 tasks resource profile 0
26/02/13 12:04:33 INFO TaskSetManager: Starting task 0.0 in stage 56.0 (TID 102) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:04:33 INFO TaskSetManager: Starting task 1.0 in stage 56.0 (TID 103) (172.18.0.14, executor 1, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:04:34 INFO BlockManagerInfo: Added broadcast_52_piece0 in memory on 172.18.0.14:37571 (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 12:04:34 INFO TaskSetManager: Starting task 2.0 in stage 56.0 (TID 104) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:04:34 INFO TaskSetManager: Finished task 0.0 in stage 56.0 (TID 102) in 23 ms on 172.18.0.14 (executor 1) (1/4)
26/02/13 12:04:34 INFO TaskSetManager: Starting task 3.0 in stage 56.0 (TID 105) (172.18.0.14, executor 1, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:04:34 INFO TaskSetManager: Finished task 1.0 in stage 56.0 (TID 103) in 25 ms on 172.18.0.14 (executor 1) (2/4)
26/02/13 12:04:34 INFO TaskSetManager: Finished task 3.0 in stage 56.0 (TID 105) in 19 ms on 172.18.0.14 (executor 1) (3/4)
26/02/13 12:04:34 INFO TaskSetManager: Finished task 2.0 in stage 56.0 (TID 104) in 23 ms on 172.18.0.14 (executor 1) (4/4)
26/02/13 12:04:34 INFO TaskSchedulerImpl: Removed TaskSet 56.0, whose tasks have all completed, from pool 
26/02/13 12:04:34 INFO DAGScheduler: ResultStage 56 (start at NativeMethodAccessorImpl.java:0) finished in 0.054 s
26/02/13 12:04:34 INFO DAGScheduler: Job 35 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 12:04:34 INFO TaskSchedulerImpl: Killing all running tasks in stage 56: Stage finished
26/02/13 12:04:34 INFO DAGScheduler: Job 35 finished: start at NativeMethodAccessorImpl.java:0, took 0.057589 s
26/02/13 12:04:34 INFO BlockManagerInfo: Removed broadcast_50_piece0 on spark-master:42073 in memory (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 12:04:34 INFO BlockManagerInfo: Removed broadcast_50_piece0 on 172.18.0.15:37717 in memory (size: 107.2 KiB, free: 434.4 MiB)
26/02/13 12:04:34 INFO BlockManagerInfo: Removed broadcast_50_piece0 on 172.18.0.14:37571 in memory (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 12:04:34 INFO MemoryStore: Block broadcast_53_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.9 MiB)
26/02/13 12:04:34 INFO BlockManagerInfo: Added broadcast_53_piece0 in memory on spark-master:42073 (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 12:04:34 INFO SparkContext: Created broadcast 53 from start at NativeMethodAccessorImpl.java:0
26/02/13 12:04:34 INFO BlockManagerInfo: Removed broadcast_51_piece0 on 172.18.0.15:37717 in memory (size: 19.5 KiB, free: 434.4 MiB)
26/02/13 12:04:34 INFO BlockManagerInfo: Removed broadcast_51_piece0 on spark-master:42073 in memory (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 12:04:34 INFO BlockManagerInfo: Removed broadcast_51_piece0 on 172.18.0.14:37571 in memory (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 12:04:34 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 136, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@f9fbf07]. The input RDD has 3 partitions.
26/02/13 12:04:34 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 12:04:34 INFO DAGScheduler: Got job 36 (start at NativeMethodAccessorImpl.java:0) with 3 output partitions
26/02/13 12:04:34 INFO DAGScheduler: Final stage: ResultStage 57 (start at NativeMethodAccessorImpl.java:0)
26/02/13 12:04:34 INFO DAGScheduler: Parents of final stage: List()
26/02/13 12:04:34 INFO DAGScheduler: Missing parents: List()
26/02/13 12:04:34 INFO DAGScheduler: Submitting ResultStage 57 (MapPartitionsRDD[192] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:04:34 INFO MemoryStore: Block broadcast_54 stored as values in memory (estimated size 49.9 KiB, free 433.9 MiB)
26/02/13 12:04:34 INFO MemoryStore: Block broadcast_54_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.9 MiB)
26/02/13 12:04:34 INFO BlockManagerInfo: Added broadcast_54_piece0 in memory on spark-master:42073 (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 12:04:34 INFO SparkContext: Created broadcast 54 from broadcast at DAGScheduler.scala:1585
26/02/13 12:04:34 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 57 (MapPartitionsRDD[192] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
26/02/13 12:04:34 INFO TaskSchedulerImpl: Adding task set 57.0 with 3 tasks resource profile 0
26/02/13 12:04:34 INFO TaskSetManager: Starting task 1.0 in stage 57.0 (TID 106) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:04:34 INFO TaskSetManager: Starting task 0.0 in stage 57.0 (TID 107) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:04:34 INFO TaskSetManager: Starting task 2.0 in stage 57.0 (TID 108) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:04:34 INFO BlockManagerInfo: Added broadcast_54_piece0 in memory on 172.18.0.14:37571 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 12:04:34 INFO BlockManagerInfo: Added broadcast_54_piece0 in memory on 172.18.0.15:37717 (size: 19.5 KiB, free: 434.4 MiB)
26/02/13 12:04:34 INFO BlockManagerInfo: Added broadcast_53_piece0 in memory on 172.18.0.14:37571 (size: 107.2 KiB, free: 434.0 MiB)
26/02/13 12:04:34 INFO BlockManagerInfo: Added broadcast_53_piece0 in memory on 172.18.0.15:37717 (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 12:04:34 INFO TaskSetManager: Finished task 2.0 in stage 57.0 (TID 108) in 594 ms on 172.18.0.14 (executor 1) (1/3)
26/02/13 12:04:34 INFO TaskSetManager: Finished task 0.0 in stage 57.0 (TID 107) in 595 ms on 172.18.0.14 (executor 1) (2/3)
26/02/13 12:04:34 INFO TaskSetManager: Finished task 1.0 in stage 57.0 (TID 106) in 629 ms on 172.18.0.15 (executor 0) (3/3)
26/02/13 12:04:34 INFO TaskSchedulerImpl: Removed TaskSet 57.0, whose tasks have all completed, from pool 
26/02/13 12:04:34 INFO DAGScheduler: ResultStage 57 (start at NativeMethodAccessorImpl.java:0) finished in 0.635 s
26/02/13 12:04:34 INFO DAGScheduler: Job 36 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 12:04:34 INFO TaskSchedulerImpl: Killing all running tasks in stage 57: Stage finished
26/02/13 12:04:34 INFO DAGScheduler: Job 36 finished: start at NativeMethodAccessorImpl.java:0, took 0.639067 s
26/02/13 12:04:34 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 136, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@f9fbf07] is committing.
26/02/13 12:04:34 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 136, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@f9fbf07] committed.
26/02/13 12:04:34 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/136 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.136.24b9d228-0a82-4a1b-a48c-867b10022de0.tmp
26/02/13 12:04:34 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.136.24b9d228-0a82-4a1b-a48c-867b10022de0.tmp to file:/tmp/spark-checkpoint-enrichment/commits/136
26/02/13 12:04:34 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "4a161d66-225b-4b29-812d-ddec3c05364e",
  "name" : null,
  "timestamp" : "2026-02-13T12:04:33.714Z",
  "batchId" : 136,
  "numInputRows" : 87,
  "inputRowsPerSecond" : 7250.0,
  "processedRowsPerSecond" : 81.15671641791045,
  "durationMs" : {
    "addBatch" : 826,
    "commitOffsets" : 68,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 57,
    "triggerExecution" : 1072,
    "walCommit" : 119
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 1031,
        "1" : 1256,
        "0" : 942
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 1058,
        "1" : 1291,
        "0" : 967
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 1058,
        "1" : 1291,
        "0" : 967
      }
    },
    "numInputRows" : 87,
    "inputRowsPerSecond" : 7250.0,
    "processedRowsPerSecond" : 81.15671641791045,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@1a6a42c2",
    "numOutputRows" : 23
  }
}
26/02/13 12:04:34 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/137 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.137.0c598489-e2a5-4c6e-94fd-47ab092dea18.tmp
26/02/13 12:04:34 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.137.0c598489-e2a5-4c6e-94fd-47ab092dea18.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/137
26/02/13 12:04:34 INFO MicroBatchExecution: Committed offsets for batch 137. Metadata OffsetSeqMetadata(0,1770984274788,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 12:04:34 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:04:34 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:04:34 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:04:34 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:04:34 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:04:34 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:04:34 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#13907 - origin_code.nullCount#13906) > 0)
26/02/13 12:04:34 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#13912 - destination_code.nullCount#13911) > 0)
26/02/13 12:04:34 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#13942 - callsign.nullCount#13941) > 0)
26/02/13 12:04:34 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 12:04:34 INFO DAGScheduler: Got job 37 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 12:04:34 INFO DAGScheduler: Final stage: ResultStage 59 (start at NativeMethodAccessorImpl.java:0)
26/02/13 12:04:34 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 58)
26/02/13 12:04:34 INFO DAGScheduler: Missing parents: List()
26/02/13 12:04:34 INFO DAGScheduler: Submitting ResultStage 59 (MapPartitionsRDD[197] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:04:34 INFO MemoryStore: Block broadcast_55 stored as values in memory (estimated size 99.3 KiB, free 433.8 MiB)
26/02/13 12:04:34 INFO MemoryStore: Block broadcast_55_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 433.7 MiB)
26/02/13 12:04:34 INFO BlockManagerInfo: Added broadcast_55_piece0 in memory on spark-master:42073 (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 12:04:34 INFO SparkContext: Created broadcast 55 from broadcast at DAGScheduler.scala:1585
26/02/13 12:04:34 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 59 (MapPartitionsRDD[197] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 12:04:34 INFO TaskSchedulerImpl: Adding task set 59.0 with 4 tasks resource profile 0
26/02/13 12:04:34 INFO TaskSetManager: Starting task 0.0 in stage 59.0 (TID 109) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:04:34 INFO TaskSetManager: Starting task 1.0 in stage 59.0 (TID 110) (172.18.0.14, executor 1, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:04:34 INFO BlockManagerInfo: Added broadcast_55_piece0 in memory on 172.18.0.14:37571 (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 12:04:34 INFO TaskSetManager: Starting task 2.0 in stage 59.0 (TID 111) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:04:34 INFO TaskSetManager: Finished task 0.0 in stage 59.0 (TID 109) in 19 ms on 172.18.0.14 (executor 1) (1/4)
26/02/13 12:04:34 INFO TaskSetManager: Starting task 3.0 in stage 59.0 (TID 112) (172.18.0.14, executor 1, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:04:34 INFO TaskSetManager: Finished task 1.0 in stage 59.0 (TID 110) in 29 ms on 172.18.0.14 (executor 1) (2/4)
26/02/13 12:04:34 INFO TaskSetManager: Finished task 2.0 in stage 59.0 (TID 111) in 16 ms on 172.18.0.14 (executor 1) (3/4)
26/02/13 12:04:35 INFO TaskSetManager: Finished task 3.0 in stage 59.0 (TID 112) in 17 ms on 172.18.0.14 (executor 1) (4/4)
26/02/13 12:04:35 INFO TaskSchedulerImpl: Removed TaskSet 59.0, whose tasks have all completed, from pool 
26/02/13 12:04:35 INFO DAGScheduler: ResultStage 59 (start at NativeMethodAccessorImpl.java:0) finished in 0.053 s
26/02/13 12:04:35 INFO DAGScheduler: Job 37 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 12:04:35 INFO TaskSchedulerImpl: Killing all running tasks in stage 59: Stage finished
26/02/13 12:04:35 INFO DAGScheduler: Job 37 finished: start at NativeMethodAccessorImpl.java:0, took 0.056754 s
26/02/13 12:04:35 INFO MemoryStore: Block broadcast_56_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.6 MiB)
26/02/13 12:04:35 INFO BlockManagerInfo: Added broadcast_56_piece0 in memory on spark-master:42073 (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 12:04:35 INFO SparkContext: Created broadcast 56 from start at NativeMethodAccessorImpl.java:0
26/02/13 12:04:35 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 137, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@77bf0d25]. The input RDD has 3 partitions.
26/02/13 12:04:35 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 12:04:35 INFO DAGScheduler: Got job 38 (start at NativeMethodAccessorImpl.java:0) with 3 output partitions
26/02/13 12:04:35 INFO DAGScheduler: Final stage: ResultStage 60 (start at NativeMethodAccessorImpl.java:0)
26/02/13 12:04:35 INFO DAGScheduler: Parents of final stage: List()
26/02/13 12:04:35 INFO DAGScheduler: Missing parents: List()
26/02/13 12:04:35 INFO DAGScheduler: Submitting ResultStage 60 (MapPartitionsRDD[203] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:04:35 INFO MemoryStore: Block broadcast_57 stored as values in memory (estimated size 49.9 KiB, free 433.6 MiB)
26/02/13 12:04:35 INFO MemoryStore: Block broadcast_57_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.6 MiB)
26/02/13 12:04:35 INFO BlockManagerInfo: Removed broadcast_55_piece0 on spark-master:42073 in memory (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 12:04:35 INFO BlockManagerInfo: Added broadcast_57_piece0 in memory on spark-master:42073 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 12:04:35 INFO SparkContext: Created broadcast 57 from broadcast at DAGScheduler.scala:1585
26/02/13 12:04:35 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 60 (MapPartitionsRDD[203] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
26/02/13 12:04:35 INFO TaskSchedulerImpl: Adding task set 60.0 with 3 tasks resource profile 0
26/02/13 12:04:35 INFO BlockManagerInfo: Removed broadcast_55_piece0 on 172.18.0.14:37571 in memory (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 12:04:35 INFO TaskSetManager: Starting task 0.0 in stage 60.0 (TID 113) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:04:35 INFO TaskSetManager: Starting task 1.0 in stage 60.0 (TID 114) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:04:35 INFO TaskSetManager: Starting task 2.0 in stage 60.0 (TID 115) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:04:35 INFO BlockManagerInfo: Removed broadcast_53_piece0 on spark-master:42073 in memory (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 12:04:35 INFO BlockManagerInfo: Removed broadcast_53_piece0 on 172.18.0.15:37717 in memory (size: 107.2 KiB, free: 434.4 MiB)
26/02/13 12:04:35 INFO BlockManagerInfo: Removed broadcast_53_piece0 on 172.18.0.14:37571 in memory (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 12:04:35 INFO BlockManagerInfo: Added broadcast_57_piece0 in memory on 172.18.0.14:37571 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 12:04:35 INFO BlockManagerInfo: Removed broadcast_52_piece0 on spark-master:42073 in memory (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 12:04:35 INFO BlockManagerInfo: Removed broadcast_52_piece0 on 172.18.0.14:37571 in memory (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 12:04:35 INFO BlockManagerInfo: Added broadcast_57_piece0 in memory on 172.18.0.15:37717 (size: 19.5 KiB, free: 434.4 MiB)
26/02/13 12:04:35 INFO BlockManagerInfo: Removed broadcast_54_piece0 on spark-master:42073 in memory (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 12:04:35 INFO BlockManagerInfo: Removed broadcast_54_piece0 on 172.18.0.14:37571 in memory (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 12:04:35 INFO BlockManagerInfo: Removed broadcast_54_piece0 on 172.18.0.15:37717 in memory (size: 19.5 KiB, free: 434.4 MiB)
26/02/13 12:04:35 INFO BlockManagerInfo: Added broadcast_56_piece0 in memory on 172.18.0.14:37571 (size: 107.2 KiB, free: 434.0 MiB)
26/02/13 12:04:35 INFO BlockManagerInfo: Added broadcast_56_piece0 in memory on 172.18.0.15:37717 (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 12:04:35 INFO TaskSetManager: Finished task 0.0 in stage 60.0 (TID 113) in 95 ms on 172.18.0.14 (executor 1) (1/3)
26/02/13 12:04:35 INFO TaskSetManager: Finished task 2.0 in stage 60.0 (TID 115) in 97 ms on 172.18.0.14 (executor 1) (2/3)
26/02/13 12:04:35 INFO TaskSetManager: Finished task 1.0 in stage 60.0 (TID 114) in 127 ms on 172.18.0.15 (executor 0) (3/3)
26/02/13 12:04:35 INFO TaskSchedulerImpl: Removed TaskSet 60.0, whose tasks have all completed, from pool 
26/02/13 12:04:35 INFO DAGScheduler: ResultStage 60 (start at NativeMethodAccessorImpl.java:0) finished in 0.141 s
26/02/13 12:04:35 INFO DAGScheduler: Job 38 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 12:04:35 INFO TaskSchedulerImpl: Killing all running tasks in stage 60: Stage finished
26/02/13 12:04:35 INFO DAGScheduler: Job 38 finished: start at NativeMethodAccessorImpl.java:0, took 0.143332 s
26/02/13 12:04:35 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 137, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@77bf0d25] is committing.
26/02/13 12:04:35 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 137, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@77bf0d25] committed.
26/02/13 12:04:35 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/137 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.137.2bf0c7bc-5fa6-4eb3-b8c4-80868815413a.tmp
26/02/13 12:04:35 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.137.2bf0c7bc-5fa6-4eb3-b8c4-80868815413a.tmp to file:/tmp/spark-checkpoint-enrichment/commits/137
26/02/13 12:04:35 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "4a161d66-225b-4b29-812d-ddec3c05364e",
  "name" : null,
  "timestamp" : "2026-02-13T12:04:34.787Z",
  "batchId" : 137,
  "numInputRows" : 154,
  "inputRowsPerSecond" : 143.52283317800558,
  "processedRowsPerSecond" : 350.7972665148064,
  "durationMs" : {
    "addBatch" : 300,
    "commitOffsets" : 54,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 30,
    "triggerExecution" : 439,
    "walCommit" : 54
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 1058,
        "1" : 1291,
        "0" : 967
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 1107,
        "1" : 1348,
        "0" : 1015
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 1107,
        "1" : 1348,
        "0" : 1015
      }
    },
    "numInputRows" : 154,
    "inputRowsPerSecond" : 143.52283317800558,
    "processedRowsPerSecond" : 350.7972665148064,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@1a6a42c2",
    "numOutputRows" : 30
  }
}
26/02/13 12:04:35 INFO BlockManagerInfo: Removed broadcast_57_piece0 on spark-master:42073 in memory (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 12:04:35 INFO BlockManagerInfo: Removed broadcast_57_piece0 on 172.18.0.15:37717 in memory (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 12:04:35 INFO BlockManagerInfo: Removed broadcast_57_piece0 on 172.18.0.14:37571 in memory (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 12:04:45 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:04:50 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/138 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.138.cf13517b-ed54-4df9-b073-96aff101e341.tmp
26/02/13 12:04:50 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.138.cf13517b-ed54-4df9-b073-96aff101e341.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/138
26/02/13 12:04:50 INFO MicroBatchExecution: Committed offsets for batch 138. Metadata OffsetSeqMetadata(0,1770984290705,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 12:04:50 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:04:50 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:04:50 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:04:50 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:04:50 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:04:50 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:04:50 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#14761 - origin_code.nullCount#14760) > 0)
26/02/13 12:04:50 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#14766 - destination_code.nullCount#14765) > 0)
26/02/13 12:04:50 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#14796 - callsign.nullCount#14795) > 0)
26/02/13 12:04:50 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 12:04:50 INFO DAGScheduler: Got job 39 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 12:04:50 INFO DAGScheduler: Final stage: ResultStage 62 (start at NativeMethodAccessorImpl.java:0)
26/02/13 12:04:50 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 61)
26/02/13 12:04:50 INFO DAGScheduler: Missing parents: List()
26/02/13 12:04:50 INFO DAGScheduler: Submitting ResultStage 62 (MapPartitionsRDD[208] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:04:50 INFO MemoryStore: Block broadcast_58 stored as values in memory (estimated size 99.3 KiB, free 434.0 MiB)
26/02/13 12:04:50 INFO MemoryStore: Block broadcast_58_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 433.9 MiB)
26/02/13 12:04:50 INFO BlockManagerInfo: Added broadcast_58_piece0 in memory on spark-master:42073 (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 12:04:50 INFO SparkContext: Created broadcast 58 from broadcast at DAGScheduler.scala:1585
26/02/13 12:04:50 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 62 (MapPartitionsRDD[208] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 12:04:50 INFO TaskSchedulerImpl: Adding task set 62.0 with 4 tasks resource profile 0
26/02/13 12:04:50 INFO TaskSetManager: Starting task 0.0 in stage 62.0 (TID 116) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:04:50 INFO TaskSetManager: Starting task 1.0 in stage 62.0 (TID 117) (172.18.0.14, executor 1, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:04:50 INFO BlockManagerInfo: Added broadcast_58_piece0 in memory on 172.18.0.14:37571 (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 12:04:50 INFO TaskSetManager: Starting task 2.0 in stage 62.0 (TID 118) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:04:50 INFO TaskSetManager: Starting task 3.0 in stage 62.0 (TID 119) (172.18.0.14, executor 1, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:04:50 INFO TaskSetManager: Finished task 0.0 in stage 62.0 (TID 116) in 27 ms on 172.18.0.14 (executor 1) (1/4)
26/02/13 12:04:50 INFO TaskSetManager: Finished task 1.0 in stage 62.0 (TID 117) in 28 ms on 172.18.0.14 (executor 1) (2/4)
26/02/13 12:04:50 INFO BlockManagerInfo: Removed broadcast_56_piece0 on spark-master:42073 in memory (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 12:04:50 INFO TaskSetManager: Finished task 2.0 in stage 62.0 (TID 118) in 33 ms on 172.18.0.14 (executor 1) (3/4)
26/02/13 12:04:50 INFO BlockManagerInfo: Removed broadcast_56_piece0 on 172.18.0.14:37571 in memory (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 12:04:50 INFO BlockManagerInfo: Removed broadcast_56_piece0 on 172.18.0.15:37717 in memory (size: 107.2 KiB, free: 434.4 MiB)
26/02/13 12:04:50 INFO TaskSetManager: Finished task 3.0 in stage 62.0 (TID 119) in 33 ms on 172.18.0.14 (executor 1) (4/4)
26/02/13 12:04:50 INFO TaskSchedulerImpl: Removed TaskSet 62.0, whose tasks have all completed, from pool 
26/02/13 12:04:50 INFO DAGScheduler: ResultStage 62 (start at NativeMethodAccessorImpl.java:0) finished in 0.071 s
26/02/13 12:04:50 INFO DAGScheduler: Job 39 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 12:04:50 INFO TaskSchedulerImpl: Killing all running tasks in stage 62: Stage finished
26/02/13 12:04:50 INFO DAGScheduler: Job 39 finished: start at NativeMethodAccessorImpl.java:0, took 0.074958 s
26/02/13 12:04:50 INFO MemoryStore: Block broadcast_59_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.9 MiB)
26/02/13 12:04:50 INFO BlockManagerInfo: Added broadcast_59_piece0 in memory on spark-master:42073 (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 12:04:50 INFO SparkContext: Created broadcast 59 from start at NativeMethodAccessorImpl.java:0
26/02/13 12:04:51 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 138, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@30937bcb]. The input RDD has 1 partitions.
26/02/13 12:04:51 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 12:04:51 INFO DAGScheduler: Got job 40 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/02/13 12:04:51 INFO DAGScheduler: Final stage: ResultStage 63 (start at NativeMethodAccessorImpl.java:0)
26/02/13 12:04:51 INFO DAGScheduler: Parents of final stage: List()
26/02/13 12:04:51 INFO DAGScheduler: Missing parents: List()
26/02/13 12:04:51 INFO DAGScheduler: Submitting ResultStage 63 (MapPartitionsRDD[214] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:04:51 INFO MemoryStore: Block broadcast_60 stored as values in memory (estimated size 49.9 KiB, free 433.9 MiB)
26/02/13 12:04:51 INFO MemoryStore: Block broadcast_60_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.9 MiB)
26/02/13 12:04:51 INFO BlockManagerInfo: Added broadcast_60_piece0 in memory on spark-master:42073 (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 12:04:51 INFO SparkContext: Created broadcast 60 from broadcast at DAGScheduler.scala:1585
26/02/13 12:04:51 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 63 (MapPartitionsRDD[214] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/02/13 12:04:51 INFO TaskSchedulerImpl: Adding task set 63.0 with 1 tasks resource profile 0
26/02/13 12:04:51 INFO TaskSetManager: Starting task 0.0 in stage 63.0 (TID 120) (172.18.0.15, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:04:51 INFO BlockManagerInfo: Added broadcast_60_piece0 in memory on 172.18.0.15:37717 (size: 19.5 KiB, free: 434.4 MiB)
26/02/13 12:04:51 INFO BlockManagerInfo: Added broadcast_59_piece0 in memory on 172.18.0.15:37717 (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 12:04:51 INFO TaskSetManager: Finished task 0.0 in stage 63.0 (TID 120) in 574 ms on 172.18.0.15 (executor 0) (1/1)
26/02/13 12:04:51 INFO TaskSchedulerImpl: Removed TaskSet 63.0, whose tasks have all completed, from pool 
26/02/13 12:04:51 INFO DAGScheduler: ResultStage 63 (start at NativeMethodAccessorImpl.java:0) finished in 0.580 s
26/02/13 12:04:51 INFO DAGScheduler: Job 40 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 12:04:51 INFO TaskSchedulerImpl: Killing all running tasks in stage 63: Stage finished
26/02/13 12:04:51 INFO DAGScheduler: Job 40 finished: start at NativeMethodAccessorImpl.java:0, took 0.581505 s
26/02/13 12:04:51 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 138, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@30937bcb] is committing.
26/02/13 12:04:51 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 138, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@30937bcb] committed.
26/02/13 12:04:51 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/138 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.138.4129a523-2351-408a-8e25-a0a9ff402863.tmp
26/02/13 12:04:51 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.138.4129a523-2351-408a-8e25-a0a9ff402863.tmp to file:/tmp/spark-checkpoint-enrichment/commits/138
26/02/13 12:04:51 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "4a161d66-225b-4b29-812d-ddec3c05364e",
  "name" : null,
  "timestamp" : "2026-02-13T12:04:50.703Z",
  "batchId" : 138,
  "numInputRows" : 33,
  "inputRowsPerSecond" : 2750.0,
  "processedRowsPerSecond" : 34.51882845188285,
  "durationMs" : {
    "addBatch" : 769,
    "commitOffsets" : 76,
    "getBatch" : 0,
    "latestOffset" : 2,
    "queryPlanning" : 40,
    "triggerExecution" : 956,
    "walCommit" : 69
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 1107,
        "1" : 1348,
        "0" : 1015
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 1107,
        "1" : 1381,
        "0" : 1015
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 1107,
        "1" : 1381,
        "0" : 1015
      }
    },
    "numInputRows" : 33,
    "inputRowsPerSecond" : 2750.0,
    "processedRowsPerSecond" : 34.51882845188285,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@1a6a42c2",
    "numOutputRows" : 10
  }
}
26/02/13 12:04:51 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/139 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.139.2dd5c359-e337-4e88-ad97-675bb394f7f0.tmp
26/02/13 12:04:51 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.139.2dd5c359-e337-4e88-ad97-675bb394f7f0.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/139
26/02/13 12:04:51 INFO MicroBatchExecution: Committed offsets for batch 139. Metadata OffsetSeqMetadata(0,1770984291662,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 12:04:51 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:04:51 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:04:51 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:04:51 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:04:51 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:04:51 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:04:51 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#15615 - origin_code.nullCount#15614) > 0)
26/02/13 12:04:51 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#15620 - destination_code.nullCount#15619) > 0)
26/02/13 12:04:51 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#15650 - callsign.nullCount#15649) > 0)
26/02/13 12:04:51 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 12:04:51 INFO DAGScheduler: Got job 41 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 12:04:51 INFO DAGScheduler: Final stage: ResultStage 65 (start at NativeMethodAccessorImpl.java:0)
26/02/13 12:04:51 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 64)
26/02/13 12:04:51 INFO DAGScheduler: Missing parents: List()
26/02/13 12:04:51 INFO DAGScheduler: Submitting ResultStage 65 (MapPartitionsRDD[219] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:04:51 INFO MemoryStore: Block broadcast_61 stored as values in memory (estimated size 99.3 KiB, free 433.8 MiB)
26/02/13 12:04:51 INFO MemoryStore: Block broadcast_61_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 433.7 MiB)
26/02/13 12:04:51 INFO BlockManagerInfo: Added broadcast_61_piece0 in memory on spark-master:42073 (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 12:04:51 INFO SparkContext: Created broadcast 61 from broadcast at DAGScheduler.scala:1585
26/02/13 12:04:51 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 65 (MapPartitionsRDD[219] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 12:04:51 INFO TaskSchedulerImpl: Adding task set 65.0 with 4 tasks resource profile 0
26/02/13 12:04:51 INFO TaskSetManager: Starting task 0.0 in stage 65.0 (TID 121) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:04:51 INFO TaskSetManager: Starting task 1.0 in stage 65.0 (TID 122) (172.18.0.14, executor 1, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:04:51 INFO BlockManagerInfo: Added broadcast_61_piece0 in memory on 172.18.0.14:37571 (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 12:04:51 INFO TaskSetManager: Starting task 2.0 in stage 65.0 (TID 123) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:04:51 INFO TaskSetManager: Starting task 3.0 in stage 65.0 (TID 124) (172.18.0.14, executor 1, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:04:51 INFO TaskSetManager: Finished task 1.0 in stage 65.0 (TID 122) in 25 ms on 172.18.0.14 (executor 1) (1/4)
26/02/13 12:04:51 INFO TaskSetManager: Finished task 0.0 in stage 65.0 (TID 121) in 26 ms on 172.18.0.14 (executor 1) (2/4)
26/02/13 12:04:51 INFO TaskSetManager: Finished task 2.0 in stage 65.0 (TID 123) in 22 ms on 172.18.0.14 (executor 1) (3/4)
26/02/13 12:04:51 INFO TaskSetManager: Finished task 3.0 in stage 65.0 (TID 124) in 25 ms on 172.18.0.14 (executor 1) (4/4)
26/02/13 12:04:51 INFO TaskSchedulerImpl: Removed TaskSet 65.0, whose tasks have all completed, from pool 
26/02/13 12:04:51 INFO DAGScheduler: ResultStage 65 (start at NativeMethodAccessorImpl.java:0) finished in 0.061 s
26/02/13 12:04:51 INFO DAGScheduler: Job 41 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 12:04:51 INFO TaskSchedulerImpl: Killing all running tasks in stage 65: Stage finished
26/02/13 12:04:51 INFO DAGScheduler: Job 41 finished: start at NativeMethodAccessorImpl.java:0, took 0.065696 s
26/02/13 12:04:51 INFO MemoryStore: Block broadcast_62_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.6 MiB)
26/02/13 12:04:51 INFO BlockManagerInfo: Added broadcast_62_piece0 in memory on spark-master:42073 (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 12:04:51 INFO SparkContext: Created broadcast 62 from start at NativeMethodAccessorImpl.java:0
26/02/13 12:04:51 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 139, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@7ec6fbc7]. The input RDD has 3 partitions.
26/02/13 12:04:51 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 12:04:51 INFO DAGScheduler: Got job 42 (start at NativeMethodAccessorImpl.java:0) with 3 output partitions
26/02/13 12:04:51 INFO DAGScheduler: Final stage: ResultStage 66 (start at NativeMethodAccessorImpl.java:0)
26/02/13 12:04:51 INFO DAGScheduler: Parents of final stage: List()
26/02/13 12:04:51 INFO DAGScheduler: Missing parents: List()
26/02/13 12:04:51 INFO DAGScheduler: Submitting ResultStage 66 (MapPartitionsRDD[225] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:04:51 INFO MemoryStore: Block broadcast_63 stored as values in memory (estimated size 49.9 KiB, free 433.6 MiB)
26/02/13 12:04:51 INFO MemoryStore: Block broadcast_63_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.6 MiB)
26/02/13 12:04:51 INFO BlockManagerInfo: Added broadcast_63_piece0 in memory on spark-master:42073 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 12:04:51 INFO SparkContext: Created broadcast 63 from broadcast at DAGScheduler.scala:1585
26/02/13 12:04:51 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 66 (MapPartitionsRDD[225] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
26/02/13 12:04:51 INFO TaskSchedulerImpl: Adding task set 66.0 with 3 tasks resource profile 0
26/02/13 12:04:51 INFO BlockManagerInfo: Removed broadcast_61_piece0 on spark-master:42073 in memory (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 12:04:51 INFO TaskSetManager: Starting task 0.0 in stage 66.0 (TID 125) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:04:51 INFO BlockManagerInfo: Removed broadcast_61_piece0 on 172.18.0.14:37571 in memory (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 12:04:51 INFO TaskSetManager: Starting task 1.0 in stage 66.0 (TID 126) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:04:51 INFO TaskSetManager: Starting task 2.0 in stage 66.0 (TID 127) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:04:51 INFO BlockManagerInfo: Removed broadcast_59_piece0 on spark-master:42073 in memory (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 12:04:51 INFO BlockManagerInfo: Added broadcast_63_piece0 in memory on 172.18.0.14:37571 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 12:04:51 INFO BlockManagerInfo: Removed broadcast_59_piece0 on 172.18.0.15:37717 in memory (size: 107.2 KiB, free: 434.4 MiB)
26/02/13 12:04:51 INFO BlockManagerInfo: Added broadcast_63_piece0 in memory on 172.18.0.15:37717 (size: 19.5 KiB, free: 434.4 MiB)
26/02/13 12:04:51 INFO BlockManagerInfo: Removed broadcast_58_piece0 on spark-master:42073 in memory (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 12:04:52 INFO BlockManagerInfo: Removed broadcast_58_piece0 on 172.18.0.14:37571 in memory (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 12:04:52 INFO BlockManagerInfo: Added broadcast_62_piece0 in memory on 172.18.0.14:37571 (size: 107.2 KiB, free: 434.0 MiB)
26/02/13 12:04:52 INFO BlockManagerInfo: Removed broadcast_60_piece0 on spark-master:42073 in memory (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 12:04:52 INFO BlockManagerInfo: Removed broadcast_60_piece0 on 172.18.0.15:37717 in memory (size: 19.5 KiB, free: 434.4 MiB)
26/02/13 12:04:52 INFO BlockManagerInfo: Added broadcast_62_piece0 in memory on 172.18.0.15:37717 (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 12:04:52 INFO TaskSetManager: Finished task 1.0 in stage 66.0 (TID 126) in 146 ms on 172.18.0.15 (executor 0) (1/3)
26/02/13 12:04:52 INFO TaskSetManager: Finished task 2.0 in stage 66.0 (TID 127) in 626 ms on 172.18.0.14 (executor 1) (2/3)
26/02/13 12:04:52 INFO TaskSetManager: Finished task 0.0 in stage 66.0 (TID 125) in 631 ms on 172.18.0.14 (executor 1) (3/3)
26/02/13 12:04:52 INFO TaskSchedulerImpl: Removed TaskSet 66.0, whose tasks have all completed, from pool 
26/02/13 12:04:52 INFO DAGScheduler: ResultStage 66 (start at NativeMethodAccessorImpl.java:0) finished in 0.646 s
26/02/13 12:04:52 INFO DAGScheduler: Job 42 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 12:04:52 INFO TaskSchedulerImpl: Killing all running tasks in stage 66: Stage finished
26/02/13 12:04:52 INFO DAGScheduler: Job 42 finished: start at NativeMethodAccessorImpl.java:0, took 0.650932 s
26/02/13 12:04:52 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 139, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@7ec6fbc7] is committing.
26/02/13 12:04:52 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 139, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@7ec6fbc7] committed.
26/02/13 12:04:52 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/139 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.139.7491cd3a-e2d7-4ef8-ac14-7879b73e7d4c.tmp
26/02/13 12:04:52 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.139.7491cd3a-e2d7-4ef8-ac14-7879b73e7d4c.tmp to file:/tmp/spark-checkpoint-enrichment/commits/139
26/02/13 12:04:52 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "4a161d66-225b-4b29-812d-ddec3c05364e",
  "name" : null,
  "timestamp" : "2026-02-13T12:04:51.661Z",
  "batchId" : 139,
  "numInputRows" : 208,
  "inputRowsPerSecond" : 217.11899791231733,
  "processedRowsPerSecond" : 202.53164556962028,
  "durationMs" : {
    "addBatch" : 825,
    "commitOffsets" : 89,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 52,
    "triggerExecution" : 1027,
    "walCommit" : 60
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 1107,
        "1" : 1381,
        "0" : 1015
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 1184,
        "1" : 1441,
        "0" : 1086
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 1184,
        "1" : 1441,
        "0" : 1086
      }
    },
    "numInputRows" : 208,
    "inputRowsPerSecond" : 217.11899791231733,
    "processedRowsPerSecond" : 202.53164556962028,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@1a6a42c2",
    "numOutputRows" : 42
  }
}
26/02/13 12:04:53 INFO BlockManagerInfo: Removed broadcast_63_piece0 on spark-master:42073 in memory (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 12:04:53 INFO BlockManagerInfo: Removed broadcast_63_piece0 on 172.18.0.14:37571 in memory (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 12:04:53 INFO BlockManagerInfo: Removed broadcast_63_piece0 on 172.18.0.15:37717 in memory (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 12:05:02 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:05:09 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/140 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.140.7c977ad0-158e-440b-9a25-a223ae49fc68.tmp
26/02/13 12:05:09 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.140.7c977ad0-158e-440b-9a25-a223ae49fc68.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/140
26/02/13 12:05:09 INFO MicroBatchExecution: Committed offsets for batch 140. Metadata OffsetSeqMetadata(0,1770984309125,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 12:05:09 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:05:09 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:05:09 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:05:09 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:05:09 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:05:09 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:05:09 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#16469 - origin_code.nullCount#16468) > 0)
26/02/13 12:05:09 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#16474 - destination_code.nullCount#16473) > 0)
26/02/13 12:05:09 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#16504 - callsign.nullCount#16503) > 0)
26/02/13 12:05:09 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 12:05:09 INFO DAGScheduler: Got job 43 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 12:05:09 INFO DAGScheduler: Final stage: ResultStage 68 (start at NativeMethodAccessorImpl.java:0)
26/02/13 12:05:09 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 67)
26/02/13 12:05:09 INFO DAGScheduler: Missing parents: List()
26/02/13 12:05:09 INFO DAGScheduler: Submitting ResultStage 68 (MapPartitionsRDD[230] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:05:09 INFO MemoryStore: Block broadcast_64 stored as values in memory (estimated size 99.3 KiB, free 434.0 MiB)
26/02/13 12:05:09 INFO MemoryStore: Block broadcast_64_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 433.9 MiB)
26/02/13 12:05:09 INFO BlockManagerInfo: Added broadcast_64_piece0 in memory on spark-master:42073 (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 12:05:09 INFO SparkContext: Created broadcast 64 from broadcast at DAGScheduler.scala:1585
26/02/13 12:05:09 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 68 (MapPartitionsRDD[230] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 12:05:09 INFO TaskSchedulerImpl: Adding task set 68.0 with 4 tasks resource profile 0
26/02/13 12:05:09 INFO TaskSetManager: Starting task 0.0 in stage 68.0 (TID 128) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:05:09 INFO TaskSetManager: Starting task 1.0 in stage 68.0 (TID 129) (172.18.0.14, executor 1, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:05:09 INFO BlockManagerInfo: Added broadcast_64_piece0 in memory on 172.18.0.14:37571 (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 12:05:09 INFO TaskSetManager: Starting task 2.0 in stage 68.0 (TID 130) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:05:09 INFO TaskSetManager: Finished task 1.0 in stage 68.0 (TID 129) in 24 ms on 172.18.0.14 (executor 1) (1/4)
26/02/13 12:05:09 INFO TaskSetManager: Starting task 3.0 in stage 68.0 (TID 131) (172.18.0.14, executor 1, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:05:09 INFO TaskSetManager: Finished task 0.0 in stage 68.0 (TID 128) in 27 ms on 172.18.0.14 (executor 1) (2/4)
26/02/13 12:05:09 INFO TaskSetManager: Finished task 3.0 in stage 68.0 (TID 131) in 20 ms on 172.18.0.14 (executor 1) (3/4)
26/02/13 12:05:09 INFO TaskSetManager: Finished task 2.0 in stage 68.0 (TID 130) in 22 ms on 172.18.0.14 (executor 1) (4/4)
26/02/13 12:05:09 INFO TaskSchedulerImpl: Removed TaskSet 68.0, whose tasks have all completed, from pool 
26/02/13 12:05:09 INFO DAGScheduler: ResultStage 68 (start at NativeMethodAccessorImpl.java:0) finished in 0.058 s
26/02/13 12:05:09 INFO DAGScheduler: Job 43 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 12:05:09 INFO TaskSchedulerImpl: Killing all running tasks in stage 68: Stage finished
26/02/13 12:05:09 INFO DAGScheduler: Job 43 finished: start at NativeMethodAccessorImpl.java:0, took 0.061096 s
26/02/13 12:05:09 INFO BlockManagerInfo: Removed broadcast_62_piece0 on spark-master:42073 in memory (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 12:05:09 INFO BlockManagerInfo: Removed broadcast_62_piece0 on 172.18.0.14:37571 in memory (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 12:05:09 INFO BlockManagerInfo: Removed broadcast_62_piece0 on 172.18.0.15:37717 in memory (size: 107.2 KiB, free: 434.4 MiB)
26/02/13 12:05:09 INFO MemoryStore: Block broadcast_65_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.9 MiB)
26/02/13 12:05:09 INFO BlockManagerInfo: Added broadcast_65_piece0 in memory on spark-master:42073 (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 12:05:09 INFO SparkContext: Created broadcast 65 from start at NativeMethodAccessorImpl.java:0
26/02/13 12:05:09 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 140, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@29969ba9]. The input RDD has 3 partitions.
26/02/13 12:05:09 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 12:05:09 INFO DAGScheduler: Got job 44 (start at NativeMethodAccessorImpl.java:0) with 3 output partitions
26/02/13 12:05:09 INFO DAGScheduler: Final stage: ResultStage 69 (start at NativeMethodAccessorImpl.java:0)
26/02/13 12:05:09 INFO DAGScheduler: Parents of final stage: List()
26/02/13 12:05:09 INFO DAGScheduler: Missing parents: List()
26/02/13 12:05:09 INFO DAGScheduler: Submitting ResultStage 69 (MapPartitionsRDD[236] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:05:09 INFO MemoryStore: Block broadcast_66 stored as values in memory (estimated size 49.9 KiB, free 433.9 MiB)
26/02/13 12:05:09 INFO MemoryStore: Block broadcast_66_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.9 MiB)
26/02/13 12:05:09 INFO BlockManagerInfo: Added broadcast_66_piece0 in memory on spark-master:42073 (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 12:05:09 INFO SparkContext: Created broadcast 66 from broadcast at DAGScheduler.scala:1585
26/02/13 12:05:09 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 69 (MapPartitionsRDD[236] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
26/02/13 12:05:09 INFO TaskSchedulerImpl: Adding task set 69.0 with 3 tasks resource profile 0
26/02/13 12:05:09 INFO TaskSetManager: Starting task 1.0 in stage 69.0 (TID 132) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:05:09 INFO TaskSetManager: Starting task 0.0 in stage 69.0 (TID 133) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:05:09 INFO TaskSetManager: Starting task 2.0 in stage 69.0 (TID 134) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:05:09 INFO BlockManagerInfo: Added broadcast_66_piece0 in memory on 172.18.0.14:37571 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 12:05:09 INFO BlockManagerInfo: Added broadcast_66_piece0 in memory on 172.18.0.15:37717 (size: 19.5 KiB, free: 434.4 MiB)
26/02/13 12:05:09 INFO BlockManagerInfo: Added broadcast_65_piece0 in memory on 172.18.0.14:37571 (size: 107.2 KiB, free: 434.0 MiB)
26/02/13 12:05:09 INFO BlockManagerInfo: Added broadcast_65_piece0 in memory on 172.18.0.15:37717 (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 12:05:10 INFO TaskSetManager: Finished task 0.0 in stage 69.0 (TID 133) in 579 ms on 172.18.0.14 (executor 1) (1/3)
26/02/13 12:05:10 INFO TaskSetManager: Finished task 2.0 in stage 69.0 (TID 134) in 584 ms on 172.18.0.14 (executor 1) (2/3)
26/02/13 12:05:10 INFO TaskSetManager: Finished task 1.0 in stage 69.0 (TID 132) in 613 ms on 172.18.0.15 (executor 0) (3/3)
26/02/13 12:05:10 INFO TaskSchedulerImpl: Removed TaskSet 69.0, whose tasks have all completed, from pool 
26/02/13 12:05:10 INFO DAGScheduler: ResultStage 69 (start at NativeMethodAccessorImpl.java:0) finished in 0.618 s
26/02/13 12:05:10 INFO DAGScheduler: Job 44 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 12:05:10 INFO TaskSchedulerImpl: Killing all running tasks in stage 69: Stage finished
26/02/13 12:05:10 INFO DAGScheduler: Job 44 finished: start at NativeMethodAccessorImpl.java:0, took 0.620515 s
26/02/13 12:05:10 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 140, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@29969ba9] is committing.
26/02/13 12:05:10 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 140, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@29969ba9] committed.
26/02/13 12:05:10 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/140 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.140.c94dac20-be0d-45d3-b0a5-9eeb3cde3ffa.tmp
26/02/13 12:05:10 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.140.c94dac20-be0d-45d3-b0a5-9eeb3cde3ffa.tmp to file:/tmp/spark-checkpoint-enrichment/commits/140
26/02/13 12:05:10 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "4a161d66-225b-4b29-812d-ddec3c05364e",
  "name" : null,
  "timestamp" : "2026-02-13T12:05:09.123Z",
  "batchId" : 140,
  "numInputRows" : 51,
  "inputRowsPerSecond" : 4250.0,
  "processedRowsPerSecond" : 51.93482688391039,
  "durationMs" : {
    "addBatch" : 821,
    "commitOffsets" : 54,
    "getBatch" : 0,
    "latestOffset" : 2,
    "queryPlanning" : 39,
    "triggerExecution" : 982,
    "walCommit" : 65
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 1184,
        "1" : 1441,
        "0" : 1086
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 1199,
        "1" : 1463,
        "0" : 1100
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 1199,
        "1" : 1463,
        "0" : 1100
      }
    },
    "numInputRows" : 51,
    "inputRowsPerSecond" : 4250.0,
    "processedRowsPerSecond" : 51.93482688391039,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@1a6a42c2",
    "numOutputRows" : 13
  }
}
26/02/13 12:05:10 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/141 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.141.9723bd61-4b46-48b0-b1d8-aa604406d475.tmp
26/02/13 12:05:10 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.141.9723bd61-4b46-48b0-b1d8-aa604406d475.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/141
26/02/13 12:05:10 INFO MicroBatchExecution: Committed offsets for batch 141. Metadata OffsetSeqMetadata(0,1770984310106,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 12:05:10 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:05:10 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:05:10 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:05:10 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:05:10 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:05:10 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:05:10 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#17323 - origin_code.nullCount#17322) > 0)
26/02/13 12:05:10 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#17328 - destination_code.nullCount#17327) > 0)
26/02/13 12:05:10 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#17358 - callsign.nullCount#17357) > 0)
26/02/13 12:05:10 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 12:05:10 INFO DAGScheduler: Got job 45 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 12:05:10 INFO DAGScheduler: Final stage: ResultStage 71 (start at NativeMethodAccessorImpl.java:0)
26/02/13 12:05:10 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 70)
26/02/13 12:05:10 INFO DAGScheduler: Missing parents: List()
26/02/13 12:05:10 INFO DAGScheduler: Submitting ResultStage 71 (MapPartitionsRDD[241] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:05:10 INFO MemoryStore: Block broadcast_67 stored as values in memory (estimated size 99.3 KiB, free 433.8 MiB)
26/02/13 12:05:10 INFO MemoryStore: Block broadcast_67_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 433.7 MiB)
26/02/13 12:05:10 INFO BlockManagerInfo: Added broadcast_67_piece0 in memory on spark-master:42073 (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 12:05:10 INFO SparkContext: Created broadcast 67 from broadcast at DAGScheduler.scala:1585
26/02/13 12:05:10 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 71 (MapPartitionsRDD[241] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 12:05:10 INFO TaskSchedulerImpl: Adding task set 71.0 with 4 tasks resource profile 0
26/02/13 12:05:10 INFO TaskSetManager: Starting task 0.0 in stage 71.0 (TID 135) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:05:10 INFO TaskSetManager: Starting task 1.0 in stage 71.0 (TID 136) (172.18.0.14, executor 1, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:05:10 INFO BlockManagerInfo: Added broadcast_67_piece0 in memory on 172.18.0.14:37571 (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 12:05:10 INFO TaskSetManager: Starting task 2.0 in stage 71.0 (TID 137) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:05:10 INFO TaskSetManager: Starting task 3.0 in stage 71.0 (TID 138) (172.18.0.14, executor 1, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:05:10 INFO TaskSetManager: Finished task 0.0 in stage 71.0 (TID 135) in 28 ms on 172.18.0.14 (executor 1) (1/4)
26/02/13 12:05:10 INFO TaskSetManager: Finished task 1.0 in stage 71.0 (TID 136) in 27 ms on 172.18.0.14 (executor 1) (2/4)
26/02/13 12:05:10 INFO TaskSetManager: Finished task 2.0 in stage 71.0 (TID 137) in 15 ms on 172.18.0.14 (executor 1) (3/4)
26/02/13 12:05:10 INFO TaskSetManager: Finished task 3.0 in stage 71.0 (TID 138) in 16 ms on 172.18.0.14 (executor 1) (4/4)
26/02/13 12:05:10 INFO TaskSchedulerImpl: Removed TaskSet 71.0, whose tasks have all completed, from pool 
26/02/13 12:05:10 INFO DAGScheduler: ResultStage 71 (start at NativeMethodAccessorImpl.java:0) finished in 0.049 s
26/02/13 12:05:10 INFO DAGScheduler: Job 45 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 12:05:10 INFO TaskSchedulerImpl: Killing all running tasks in stage 71: Stage finished
26/02/13 12:05:10 INFO DAGScheduler: Job 45 finished: start at NativeMethodAccessorImpl.java:0, took 0.052082 s
26/02/13 12:05:10 INFO MemoryStore: Block broadcast_68_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.6 MiB)
26/02/13 12:05:10 INFO BlockManagerInfo: Added broadcast_68_piece0 in memory on spark-master:42073 (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 12:05:10 INFO SparkContext: Created broadcast 68 from start at NativeMethodAccessorImpl.java:0
26/02/13 12:05:10 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 141, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@37ee1700]. The input RDD has 3 partitions.
26/02/13 12:05:10 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 12:05:10 INFO DAGScheduler: Got job 46 (start at NativeMethodAccessorImpl.java:0) with 3 output partitions
26/02/13 12:05:10 INFO DAGScheduler: Final stage: ResultStage 72 (start at NativeMethodAccessorImpl.java:0)
26/02/13 12:05:10 INFO DAGScheduler: Parents of final stage: List()
26/02/13 12:05:10 INFO DAGScheduler: Missing parents: List()
26/02/13 12:05:10 INFO DAGScheduler: Submitting ResultStage 72 (MapPartitionsRDD[247] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:05:10 INFO MemoryStore: Block broadcast_69 stored as values in memory (estimated size 49.9 KiB, free 433.6 MiB)
26/02/13 12:05:10 INFO MemoryStore: Block broadcast_69_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.6 MiB)
26/02/13 12:05:10 INFO BlockManagerInfo: Removed broadcast_65_piece0 on spark-master:42073 in memory (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 12:05:10 INFO BlockManagerInfo: Added broadcast_69_piece0 in memory on spark-master:42073 (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 12:05:10 INFO SparkContext: Created broadcast 69 from broadcast at DAGScheduler.scala:1585
26/02/13 12:05:10 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 72 (MapPartitionsRDD[247] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
26/02/13 12:05:10 INFO TaskSchedulerImpl: Adding task set 72.0 with 3 tasks resource profile 0
26/02/13 12:05:10 INFO BlockManagerInfo: Removed broadcast_65_piece0 on 172.18.0.14:37571 in memory (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 12:05:10 INFO BlockManagerInfo: Removed broadcast_65_piece0 on 172.18.0.15:37717 in memory (size: 107.2 KiB, free: 434.4 MiB)
26/02/13 12:05:10 INFO TaskSetManager: Starting task 0.0 in stage 72.0 (TID 139) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:05:10 INFO TaskSetManager: Starting task 1.0 in stage 72.0 (TID 140) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:05:10 INFO TaskSetManager: Starting task 2.0 in stage 72.0 (TID 141) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:05:10 INFO BlockManagerInfo: Removed broadcast_64_piece0 on spark-master:42073 in memory (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 12:05:10 INFO BlockManagerInfo: Removed broadcast_64_piece0 on 172.18.0.14:37571 in memory (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 12:05:10 INFO BlockManagerInfo: Added broadcast_69_piece0 in memory on 172.18.0.14:37571 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 12:05:10 INFO BlockManagerInfo: Removed broadcast_67_piece0 on spark-master:42073 in memory (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 12:05:10 INFO BlockManagerInfo: Added broadcast_69_piece0 in memory on 172.18.0.15:37717 (size: 19.5 KiB, free: 434.4 MiB)
26/02/13 12:05:10 INFO BlockManagerInfo: Removed broadcast_67_piece0 on 172.18.0.14:37571 in memory (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 12:05:10 INFO BlockManagerInfo: Removed broadcast_66_piece0 on spark-master:42073 in memory (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 12:05:10 INFO BlockManagerInfo: Added broadcast_68_piece0 in memory on 172.18.0.14:37571 (size: 107.2 KiB, free: 434.0 MiB)
26/02/13 12:05:10 INFO BlockManagerInfo: Removed broadcast_66_piece0 on 172.18.0.15:37717 in memory (size: 19.5 KiB, free: 434.4 MiB)
26/02/13 12:05:10 INFO BlockManagerInfo: Removed broadcast_66_piece0 on 172.18.0.14:37571 in memory (size: 19.5 KiB, free: 434.0 MiB)
26/02/13 12:05:10 INFO BlockManagerInfo: Added broadcast_68_piece0 in memory on 172.18.0.15:37717 (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 12:05:10 INFO TaskSetManager: Finished task 0.0 in stage 72.0 (TID 139) in 116 ms on 172.18.0.14 (executor 1) (1/3)
26/02/13 12:05:10 INFO TaskSetManager: Finished task 2.0 in stage 72.0 (TID 141) in 115 ms on 172.18.0.14 (executor 1) (2/3)
26/02/13 12:05:10 INFO TaskSetManager: Finished task 1.0 in stage 72.0 (TID 140) in 156 ms on 172.18.0.15 (executor 0) (3/3)
26/02/13 12:05:10 INFO TaskSchedulerImpl: Removed TaskSet 72.0, whose tasks have all completed, from pool 
26/02/13 12:05:10 INFO DAGScheduler: ResultStage 72 (start at NativeMethodAccessorImpl.java:0) finished in 0.171 s
26/02/13 12:05:10 INFO DAGScheduler: Job 46 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 12:05:10 INFO TaskSchedulerImpl: Killing all running tasks in stage 72: Stage finished
26/02/13 12:05:10 INFO DAGScheduler: Job 46 finished: start at NativeMethodAccessorImpl.java:0, took 0.173016 s
26/02/13 12:05:10 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 141, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@37ee1700] is committing.
26/02/13 12:05:10 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 141, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@37ee1700] committed.
26/02/13 12:05:10 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/141 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.141.d102b610-1abd-48d4-b635-eff5f869bcab.tmp
26/02/13 12:05:10 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.141.d102b610-1abd-48d4-b635-eff5f869bcab.tmp to file:/tmp/spark-checkpoint-enrichment/commits/141
26/02/13 12:05:10 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "4a161d66-225b-4b29-812d-ddec3c05364e",
  "name" : null,
  "timestamp" : "2026-02-13T12:05:10.105Z",
  "batchId" : 141,
  "numInputRows" : 191,
  "inputRowsPerSecond" : 194.50101832993892,
  "processedRowsPerSecond" : 405.52016985138005,
  "durationMs" : {
    "addBatch" : 319,
    "commitOffsets" : 72,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 28,
    "triggerExecution" : 471,
    "walCommit" : 51
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 1199,
        "1" : 1463,
        "0" : 1100
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 1263,
        "1" : 1533,
        "0" : 1157
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 1263,
        "1" : 1533,
        "0" : 1157
      }
    },
    "numInputRows" : 191,
    "inputRowsPerSecond" : 194.50101832993892,
    "processedRowsPerSecond" : 405.52016985138005,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@1a6a42c2",
    "numOutputRows" : 41
  }
}
26/02/13 12:05:11 INFO BlockManagerInfo: Removed broadcast_69_piece0 on spark-master:42073 in memory (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 12:05:11 INFO BlockManagerInfo: Removed broadcast_69_piece0 on 172.18.0.14:37571 in memory (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 12:05:11 INFO BlockManagerInfo: Removed broadcast_69_piece0 on 172.18.0.15:37717 in memory (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 12:05:20 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:05:25 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/142 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.142.d6a6748b-6029-4450-842c-4c89dc970583.tmp
26/02/13 12:05:25 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.142.d6a6748b-6029-4450-842c-4c89dc970583.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/142
26/02/13 12:05:25 INFO MicroBatchExecution: Committed offsets for batch 142. Metadata OffsetSeqMetadata(0,1770984325620,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 12:05:25 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:05:25 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:05:25 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:05:25 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:05:25 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:05:25 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:05:25 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#18177 - origin_code.nullCount#18176) > 0)
26/02/13 12:05:25 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#18182 - destination_code.nullCount#18181) > 0)
26/02/13 12:05:25 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#18212 - callsign.nullCount#18211) > 0)
26/02/13 12:05:25 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 12:05:25 INFO DAGScheduler: Got job 47 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 12:05:25 INFO DAGScheduler: Final stage: ResultStage 74 (start at NativeMethodAccessorImpl.java:0)
26/02/13 12:05:25 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 73)
26/02/13 12:05:25 INFO DAGScheduler: Missing parents: List()
26/02/13 12:05:25 INFO DAGScheduler: Submitting ResultStage 74 (MapPartitionsRDD[252] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:05:25 INFO MemoryStore: Block broadcast_70 stored as values in memory (estimated size 99.3 KiB, free 434.0 MiB)
26/02/13 12:05:25 INFO MemoryStore: Block broadcast_70_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 433.9 MiB)
26/02/13 12:05:25 INFO BlockManagerInfo: Added broadcast_70_piece0 in memory on spark-master:42073 (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 12:05:25 INFO SparkContext: Created broadcast 70 from broadcast at DAGScheduler.scala:1585
26/02/13 12:05:25 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 74 (MapPartitionsRDD[252] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 12:05:25 INFO TaskSchedulerImpl: Adding task set 74.0 with 4 tasks resource profile 0
26/02/13 12:05:25 INFO TaskSetManager: Starting task 0.0 in stage 74.0 (TID 142) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:05:25 INFO TaskSetManager: Starting task 1.0 in stage 74.0 (TID 143) (172.18.0.14, executor 1, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:05:25 INFO BlockManagerInfo: Added broadcast_70_piece0 in memory on 172.18.0.14:37571 (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 12:05:25 INFO TaskSetManager: Starting task 2.0 in stage 74.0 (TID 144) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:05:25 INFO TaskSetManager: Finished task 0.0 in stage 74.0 (TID 142) in 18 ms on 172.18.0.14 (executor 1) (1/4)
26/02/13 12:05:25 INFO TaskSetManager: Starting task 3.0 in stage 74.0 (TID 145) (172.18.0.14, executor 1, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:05:25 INFO TaskSetManager: Finished task 1.0 in stage 74.0 (TID 143) in 22 ms on 172.18.0.14 (executor 1) (2/4)
26/02/13 12:05:25 INFO TaskSetManager: Finished task 2.0 in stage 74.0 (TID 144) in 18 ms on 172.18.0.14 (executor 1) (3/4)
26/02/13 12:05:25 INFO TaskSetManager: Finished task 3.0 in stage 74.0 (TID 145) in 19 ms on 172.18.0.14 (executor 1) (4/4)
26/02/13 12:05:25 INFO TaskSchedulerImpl: Removed TaskSet 74.0, whose tasks have all completed, from pool 
26/02/13 12:05:25 INFO DAGScheduler: ResultStage 74 (start at NativeMethodAccessorImpl.java:0) finished in 0.050 s
26/02/13 12:05:25 INFO DAGScheduler: Job 47 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 12:05:25 INFO TaskSchedulerImpl: Killing all running tasks in stage 74: Stage finished
26/02/13 12:05:25 INFO DAGScheduler: Job 47 finished: start at NativeMethodAccessorImpl.java:0, took 0.052281 s
26/02/13 12:05:25 INFO BlockManagerInfo: Removed broadcast_70_piece0 on spark-master:42073 in memory (size: 33.3 KiB, free: 434.3 MiB)
26/02/13 12:05:25 INFO BlockManagerInfo: Removed broadcast_70_piece0 on 172.18.0.14:37571 in memory (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 12:05:25 INFO MemoryStore: Block broadcast_71_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 434.0 MiB)
26/02/13 12:05:25 INFO BlockManagerInfo: Added broadcast_71_piece0 in memory on spark-master:42073 (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 12:05:25 INFO SparkContext: Created broadcast 71 from start at NativeMethodAccessorImpl.java:0
26/02/13 12:05:25 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 142, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@6364a0cb]. The input RDD has 3 partitions.
26/02/13 12:05:25 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 12:05:25 INFO DAGScheduler: Got job 48 (start at NativeMethodAccessorImpl.java:0) with 3 output partitions
26/02/13 12:05:25 INFO DAGScheduler: Final stage: ResultStage 75 (start at NativeMethodAccessorImpl.java:0)
26/02/13 12:05:25 INFO DAGScheduler: Parents of final stage: List()
26/02/13 12:05:25 INFO DAGScheduler: Missing parents: List()
26/02/13 12:05:25 INFO DAGScheduler: Submitting ResultStage 75 (MapPartitionsRDD[258] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:05:25 INFO MemoryStore: Block broadcast_72 stored as values in memory (estimated size 49.9 KiB, free 433.9 MiB)
26/02/13 12:05:25 INFO MemoryStore: Block broadcast_72_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.9 MiB)
26/02/13 12:05:25 INFO BlockManagerInfo: Added broadcast_72_piece0 in memory on spark-master:42073 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 12:05:25 INFO SparkContext: Created broadcast 72 from broadcast at DAGScheduler.scala:1585
26/02/13 12:05:25 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 75 (MapPartitionsRDD[258] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
26/02/13 12:05:25 INFO TaskSchedulerImpl: Adding task set 75.0 with 3 tasks resource profile 0
26/02/13 12:05:25 INFO TaskSetManager: Starting task 1.0 in stage 75.0 (TID 146) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:05:25 INFO TaskSetManager: Starting task 0.0 in stage 75.0 (TID 147) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:05:25 INFO TaskSetManager: Starting task 2.0 in stage 75.0 (TID 148) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:05:25 INFO BlockManagerInfo: Added broadcast_72_piece0 in memory on 172.18.0.14:37571 (size: 19.5 KiB, free: 434.0 MiB)
26/02/13 12:05:25 INFO BlockManagerInfo: Added broadcast_72_piece0 in memory on 172.18.0.15:37717 (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 12:05:25 INFO BlockManagerInfo: Added broadcast_71_piece0 in memory on 172.18.0.14:37571 (size: 107.2 KiB, free: 433.9 MiB)
26/02/13 12:05:25 INFO BlockManagerInfo: Added broadcast_71_piece0 in memory on 172.18.0.15:37717 (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 12:05:25 INFO BlockManagerInfo: Removed broadcast_68_piece0 on spark-master:42073 in memory (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 12:05:25 INFO BlockManagerInfo: Removed broadcast_68_piece0 on 172.18.0.15:37717 in memory (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 12:05:25 INFO BlockManagerInfo: Removed broadcast_68_piece0 on 172.18.0.14:37571 in memory (size: 107.2 KiB, free: 434.0 MiB)
26/02/13 12:05:26 INFO TaskSetManager: Finished task 2.0 in stage 75.0 (TID 148) in 580 ms on 172.18.0.14 (executor 1) (1/3)
26/02/13 12:05:26 INFO TaskSetManager: Finished task 0.0 in stage 75.0 (TID 147) in 581 ms on 172.18.0.14 (executor 1) (2/3)
26/02/13 12:05:26 INFO TaskSetManager: Finished task 1.0 in stage 75.0 (TID 146) in 618 ms on 172.18.0.15 (executor 0) (3/3)
26/02/13 12:05:26 INFO TaskSchedulerImpl: Removed TaskSet 75.0, whose tasks have all completed, from pool 
26/02/13 12:05:26 INFO DAGScheduler: ResultStage 75 (start at NativeMethodAccessorImpl.java:0) finished in 0.626 s
26/02/13 12:05:26 INFO DAGScheduler: Job 48 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 12:05:26 INFO TaskSchedulerImpl: Killing all running tasks in stage 75: Stage finished
26/02/13 12:05:26 INFO DAGScheduler: Job 48 finished: start at NativeMethodAccessorImpl.java:0, took 0.628363 s
26/02/13 12:05:26 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 142, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@6364a0cb] is committing.
26/02/13 12:05:26 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 142, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@6364a0cb] committed.
26/02/13 12:05:26 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/142 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.142.8fa6d313-8249-455c-b186-d88c4a13be73.tmp
26/02/13 12:05:26 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.142.8fa6d313-8249-455c-b186-d88c4a13be73.tmp to file:/tmp/spark-checkpoint-enrichment/commits/142
26/02/13 12:05:26 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "4a161d66-225b-4b29-812d-ddec3c05364e",
  "name" : null,
  "timestamp" : "2026-02-13T12:05:25.618Z",
  "batchId" : 142,
  "numInputRows" : 111,
  "inputRowsPerSecond" : 10090.909090909092,
  "processedRowsPerSecond" : 114.55108359133128,
  "durationMs" : {
    "addBatch" : 793,
    "commitOffsets" : 53,
    "getBatch" : 0,
    "latestOffset" : 2,
    "queryPlanning" : 40,
    "triggerExecution" : 969,
    "walCommit" : 81
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 1263,
        "1" : 1533,
        "0" : 1157
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 1300,
        "1" : 1570,
        "0" : 1194
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 1300,
        "1" : 1570,
        "0" : 1194
      }
    },
    "numInputRows" : 111,
    "inputRowsPerSecond" : 10090.909090909092,
    "processedRowsPerSecond" : 114.55108359133128,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@1a6a42c2",
    "numOutputRows" : 28
  }
}
26/02/13 12:05:26 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/143 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.143.addb4a2b-2d56-4076-8fe4-3646a3ecdacd.tmp
26/02/13 12:05:26 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.143.addb4a2b-2d56-4076-8fe4-3646a3ecdacd.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/143
26/02/13 12:05:26 INFO MicroBatchExecution: Committed offsets for batch 143. Metadata OffsetSeqMetadata(0,1770984326589,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 12:05:26 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:05:26 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:05:26 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:05:26 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:05:26 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:05:26 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:05:26 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#19031 - origin_code.nullCount#19030) > 0)
26/02/13 12:05:26 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#19036 - destination_code.nullCount#19035) > 0)
26/02/13 12:05:26 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#19066 - callsign.nullCount#19065) > 0)
26/02/13 12:05:26 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 12:05:26 INFO DAGScheduler: Got job 49 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 12:05:26 INFO DAGScheduler: Final stage: ResultStage 77 (start at NativeMethodAccessorImpl.java:0)
26/02/13 12:05:26 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 76)
26/02/13 12:05:26 INFO DAGScheduler: Missing parents: List()
26/02/13 12:05:26 INFO DAGScheduler: Submitting ResultStage 77 (MapPartitionsRDD[263] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:05:26 INFO MemoryStore: Block broadcast_73 stored as values in memory (estimated size 99.3 KiB, free 433.9 MiB)
26/02/13 12:05:26 INFO MemoryStore: Block broadcast_73_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 433.9 MiB)
26/02/13 12:05:26 INFO BlockManagerInfo: Added broadcast_73_piece0 in memory on spark-master:42073 (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 12:05:26 INFO SparkContext: Created broadcast 73 from broadcast at DAGScheduler.scala:1585
26/02/13 12:05:26 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 77 (MapPartitionsRDD[263] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 12:05:26 INFO TaskSchedulerImpl: Adding task set 77.0 with 4 tasks resource profile 0
26/02/13 12:05:26 INFO TaskSetManager: Starting task 0.0 in stage 77.0 (TID 149) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:05:26 INFO TaskSetManager: Starting task 1.0 in stage 77.0 (TID 150) (172.18.0.14, executor 1, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:05:26 INFO BlockManagerInfo: Added broadcast_73_piece0 in memory on 172.18.0.14:37571 (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 12:05:26 INFO TaskSetManager: Starting task 2.0 in stage 77.0 (TID 151) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:05:26 INFO TaskSetManager: Finished task 0.0 in stage 77.0 (TID 149) in 23 ms on 172.18.0.14 (executor 1) (1/4)
26/02/13 12:05:26 INFO TaskSetManager: Starting task 3.0 in stage 77.0 (TID 152) (172.18.0.14, executor 1, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:05:26 INFO TaskSetManager: Finished task 1.0 in stage 77.0 (TID 150) in 25 ms on 172.18.0.14 (executor 1) (2/4)
26/02/13 12:05:26 INFO TaskSetManager: Finished task 2.0 in stage 77.0 (TID 151) in 17 ms on 172.18.0.14 (executor 1) (3/4)
26/02/13 12:05:26 INFO TaskSetManager: Finished task 3.0 in stage 77.0 (TID 152) in 15 ms on 172.18.0.14 (executor 1) (4/4)
26/02/13 12:05:26 INFO TaskSchedulerImpl: Removed TaskSet 77.0, whose tasks have all completed, from pool 
26/02/13 12:05:26 INFO DAGScheduler: ResultStage 77 (start at NativeMethodAccessorImpl.java:0) finished in 0.047 s
26/02/13 12:05:26 INFO DAGScheduler: Job 49 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 12:05:26 INFO TaskSchedulerImpl: Killing all running tasks in stage 77: Stage finished
26/02/13 12:05:26 INFO DAGScheduler: Job 49 finished: start at NativeMethodAccessorImpl.java:0, took 0.049416 s
26/02/13 12:05:26 INFO MemoryStore: Block broadcast_74_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.8 MiB)
26/02/13 12:05:26 INFO BlockManagerInfo: Added broadcast_74_piece0 in memory on spark-master:42073 (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 12:05:26 INFO SparkContext: Created broadcast 74 from start at NativeMethodAccessorImpl.java:0
26/02/13 12:05:26 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 143, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@6979880e]. The input RDD has 3 partitions.
26/02/13 12:05:26 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 12:05:26 INFO DAGScheduler: Got job 50 (start at NativeMethodAccessorImpl.java:0) with 3 output partitions
26/02/13 12:05:26 INFO DAGScheduler: Final stage: ResultStage 78 (start at NativeMethodAccessorImpl.java:0)
26/02/13 12:05:26 INFO DAGScheduler: Parents of final stage: List()
26/02/13 12:05:26 INFO DAGScheduler: Missing parents: List()
26/02/13 12:05:26 INFO DAGScheduler: Submitting ResultStage 78 (MapPartitionsRDD[269] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:05:26 INFO MemoryStore: Block broadcast_75 stored as values in memory (estimated size 49.9 KiB, free 433.7 MiB)
26/02/13 12:05:26 INFO MemoryStore: Block broadcast_75_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.7 MiB)
26/02/13 12:05:26 INFO BlockManagerInfo: Added broadcast_75_piece0 in memory on spark-master:42073 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 12:05:26 INFO SparkContext: Created broadcast 75 from broadcast at DAGScheduler.scala:1585
26/02/13 12:05:26 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 78 (MapPartitionsRDD[269] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
26/02/13 12:05:26 INFO TaskSchedulerImpl: Adding task set 78.0 with 3 tasks resource profile 0
26/02/13 12:05:26 INFO TaskSetManager: Starting task 0.0 in stage 78.0 (TID 153) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:05:26 INFO TaskSetManager: Starting task 1.0 in stage 78.0 (TID 154) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:05:26 INFO TaskSetManager: Starting task 2.0 in stage 78.0 (TID 155) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:05:26 INFO BlockManagerInfo: Added broadcast_75_piece0 in memory on 172.18.0.14:37571 (size: 19.5 KiB, free: 434.0 MiB)
26/02/13 12:05:26 INFO BlockManagerInfo: Added broadcast_75_piece0 in memory on 172.18.0.15:37717 (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 12:05:26 INFO BlockManagerInfo: Added broadcast_74_piece0 in memory on 172.18.0.14:37571 (size: 107.2 KiB, free: 433.9 MiB)
26/02/13 12:05:26 INFO BlockManagerInfo: Added broadcast_74_piece0 in memory on 172.18.0.15:37717 (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 12:05:26 INFO TaskSetManager: Finished task 2.0 in stage 78.0 (TID 155) in 104 ms on 172.18.0.14 (executor 1) (1/3)
26/02/13 12:05:26 INFO TaskSetManager: Finished task 0.0 in stage 78.0 (TID 153) in 105 ms on 172.18.0.14 (executor 1) (2/3)
26/02/13 12:05:26 INFO TaskSetManager: Finished task 1.0 in stage 78.0 (TID 154) in 157 ms on 172.18.0.15 (executor 0) (3/3)
26/02/13 12:05:26 INFO TaskSchedulerImpl: Removed TaskSet 78.0, whose tasks have all completed, from pool 
26/02/13 12:05:26 INFO DAGScheduler: ResultStage 78 (start at NativeMethodAccessorImpl.java:0) finished in 0.163 s
26/02/13 12:05:26 INFO DAGScheduler: Job 50 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 12:05:26 INFO TaskSchedulerImpl: Killing all running tasks in stage 78: Stage finished
26/02/13 12:05:26 INFO DAGScheduler: Job 50 finished: start at NativeMethodAccessorImpl.java:0, took 0.165443 s
26/02/13 12:05:26 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 143, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@6979880e] is committing.
26/02/13 12:05:26 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 143, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@6979880e] committed.
26/02/13 12:05:26 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/143 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.143.64de0f7c-7bb6-42f5-ac85-76ea6669cabe.tmp
26/02/13 12:05:27 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.143.64de0f7c-7bb6-42f5-ac85-76ea6669cabe.tmp to file:/tmp/spark-checkpoint-enrichment/commits/143
26/02/13 12:05:27 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "4a161d66-225b-4b29-812d-ddec3c05364e",
  "name" : null,
  "timestamp" : "2026-02-13T12:05:26.588Z",
  "batchId" : 143,
  "numInputRows" : 131,
  "inputRowsPerSecond" : 135.0515463917526,
  "processedRowsPerSecond" : 282.9373650107991,
  "durationMs" : {
    "addBatch" : 308,
    "commitOffsets" : 69,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 29,
    "triggerExecution" : 463,
    "walCommit" : 56
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 1300,
        "1" : 1570,
        "0" : 1194
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 1342,
        "1" : 1625,
        "0" : 1228
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 1342,
        "1" : 1625,
        "0" : 1228
      }
    },
    "numInputRows" : 131,
    "inputRowsPerSecond" : 135.0515463917526,
    "processedRowsPerSecond" : 282.9373650107991,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@1a6a42c2",
    "numOutputRows" : 26
  }
}
26/02/13 12:05:28 INFO BlockManagerInfo: Removed broadcast_71_piece0 on spark-master:42073 in memory (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 12:05:28 INFO BlockManagerInfo: Removed broadcast_71_piece0 on 172.18.0.14:37571 in memory (size: 107.2 KiB, free: 434.0 MiB)
26/02/13 12:05:28 INFO BlockManagerInfo: Removed broadcast_71_piece0 on 172.18.0.15:37717 in memory (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 12:05:28 INFO BlockManagerInfo: Removed broadcast_75_piece0 on spark-master:42073 in memory (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 12:05:28 INFO BlockManagerInfo: Removed broadcast_75_piece0 on 172.18.0.15:37717 in memory (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 12:05:28 INFO BlockManagerInfo: Removed broadcast_75_piece0 on 172.18.0.14:37571 in memory (size: 19.5 KiB, free: 434.0 MiB)
26/02/13 12:05:28 INFO BlockManagerInfo: Removed broadcast_73_piece0 on spark-master:42073 in memory (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 12:05:28 INFO BlockManagerInfo: Removed broadcast_73_piece0 on 172.18.0.14:37571 in memory (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 12:05:28 INFO BlockManagerInfo: Removed broadcast_72_piece0 on spark-master:42073 in memory (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 12:05:28 INFO BlockManagerInfo: Removed broadcast_72_piece0 on 172.18.0.14:37571 in memory (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 12:05:28 INFO BlockManagerInfo: Removed broadcast_72_piece0 on 172.18.0.15:37717 in memory (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 12:05:37 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:05:42 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/144 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.144.4a6ff533-3ecf-402a-9c64-7221ca8658c2.tmp
26/02/13 12:05:42 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.144.4a6ff533-3ecf-402a-9c64-7221ca8658c2.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/144
26/02/13 12:05:42 INFO MicroBatchExecution: Committed offsets for batch 144. Metadata OffsetSeqMetadata(0,1770984342319,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 12:05:42 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:05:42 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:05:42 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:05:42 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:05:42 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:05:42 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:05:42 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#19885 - origin_code.nullCount#19884) > 0)
26/02/13 12:05:42 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#19890 - destination_code.nullCount#19889) > 0)
26/02/13 12:05:42 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#19920 - callsign.nullCount#19919) > 0)
26/02/13 12:05:42 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 12:05:42 INFO DAGScheduler: Got job 51 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 12:05:42 INFO DAGScheduler: Final stage: ResultStage 80 (start at NativeMethodAccessorImpl.java:0)
26/02/13 12:05:42 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 79)
26/02/13 12:05:42 INFO DAGScheduler: Missing parents: List()
26/02/13 12:05:42 INFO DAGScheduler: Submitting ResultStage 80 (MapPartitionsRDD[274] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:05:42 INFO MemoryStore: Block broadcast_76 stored as values in memory (estimated size 99.3 KiB, free 434.0 MiB)
26/02/13 12:05:42 INFO MemoryStore: Block broadcast_76_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 433.9 MiB)
26/02/13 12:05:42 INFO BlockManagerInfo: Added broadcast_76_piece0 in memory on spark-master:42073 (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 12:05:42 INFO SparkContext: Created broadcast 76 from broadcast at DAGScheduler.scala:1585
26/02/13 12:05:42 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 80 (MapPartitionsRDD[274] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 12:05:42 INFO TaskSchedulerImpl: Adding task set 80.0 with 4 tasks resource profile 0
26/02/13 12:05:42 INFO TaskSetManager: Starting task 0.0 in stage 80.0 (TID 156) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:05:42 INFO TaskSetManager: Starting task 1.0 in stage 80.0 (TID 157) (172.18.0.14, executor 1, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:05:42 INFO BlockManagerInfo: Added broadcast_76_piece0 in memory on 172.18.0.14:37571 (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 12:05:42 INFO TaskSetManager: Starting task 2.0 in stage 80.0 (TID 158) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:05:42 INFO TaskSetManager: Finished task 0.0 in stage 80.0 (TID 156) in 20 ms on 172.18.0.14 (executor 1) (1/4)
26/02/13 12:05:42 INFO TaskSetManager: Starting task 3.0 in stage 80.0 (TID 159) (172.18.0.14, executor 1, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:05:42 INFO TaskSetManager: Finished task 1.0 in stage 80.0 (TID 157) in 24 ms on 172.18.0.14 (executor 1) (2/4)
26/02/13 12:05:42 INFO TaskSetManager: Finished task 2.0 in stage 80.0 (TID 158) in 15 ms on 172.18.0.14 (executor 1) (3/4)
26/02/13 12:05:42 INFO TaskSetManager: Finished task 3.0 in stage 80.0 (TID 159) in 18 ms on 172.18.0.14 (executor 1) (4/4)
26/02/13 12:05:42 INFO TaskSchedulerImpl: Removed TaskSet 80.0, whose tasks have all completed, from pool 
26/02/13 12:05:42 INFO DAGScheduler: ResultStage 80 (start at NativeMethodAccessorImpl.java:0) finished in 0.048 s
26/02/13 12:05:42 INFO DAGScheduler: Job 51 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 12:05:42 INFO TaskSchedulerImpl: Killing all running tasks in stage 80: Stage finished
26/02/13 12:05:42 INFO DAGScheduler: Job 51 finished: start at NativeMethodAccessorImpl.java:0, took 0.050834 s
26/02/13 12:05:42 INFO MemoryStore: Block broadcast_77_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.8 MiB)
26/02/13 12:05:42 INFO BlockManagerInfo: Added broadcast_77_piece0 in memory on spark-master:42073 (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 12:05:42 INFO SparkContext: Created broadcast 77 from start at NativeMethodAccessorImpl.java:0
26/02/13 12:05:42 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 144, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@78c71024]. The input RDD has 3 partitions.
26/02/13 12:05:42 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 12:05:42 INFO DAGScheduler: Got job 52 (start at NativeMethodAccessorImpl.java:0) with 3 output partitions
26/02/13 12:05:42 INFO DAGScheduler: Final stage: ResultStage 81 (start at NativeMethodAccessorImpl.java:0)
26/02/13 12:05:42 INFO DAGScheduler: Parents of final stage: List()
26/02/13 12:05:42 INFO DAGScheduler: Missing parents: List()
26/02/13 12:05:42 INFO DAGScheduler: Submitting ResultStage 81 (MapPartitionsRDD[280] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:05:42 INFO MemoryStore: Block broadcast_78 stored as values in memory (estimated size 49.9 KiB, free 433.8 MiB)
26/02/13 12:05:42 INFO MemoryStore: Block broadcast_78_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.8 MiB)
26/02/13 12:05:42 INFO BlockManagerInfo: Added broadcast_78_piece0 in memory on spark-master:42073 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 12:05:42 INFO SparkContext: Created broadcast 78 from broadcast at DAGScheduler.scala:1585
26/02/13 12:05:42 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 81 (MapPartitionsRDD[280] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
26/02/13 12:05:42 INFO TaskSchedulerImpl: Adding task set 81.0 with 3 tasks resource profile 0
26/02/13 12:05:42 INFO TaskSetManager: Starting task 0.0 in stage 81.0 (TID 160) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:05:42 INFO TaskSetManager: Starting task 1.0 in stage 81.0 (TID 161) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:05:42 INFO TaskSetManager: Starting task 2.0 in stage 81.0 (TID 162) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:05:42 INFO BlockManagerInfo: Added broadcast_78_piece0 in memory on 172.18.0.14:37571 (size: 19.5 KiB, free: 434.0 MiB)
26/02/13 12:05:42 INFO BlockManagerInfo: Added broadcast_78_piece0 in memory on 172.18.0.15:37717 (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 12:05:42 INFO BlockManagerInfo: Added broadcast_77_piece0 in memory on 172.18.0.14:37571 (size: 107.2 KiB, free: 433.9 MiB)
26/02/13 12:05:42 INFO BlockManagerInfo: Added broadcast_77_piece0 in memory on 172.18.0.15:37717 (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 12:05:43 INFO TaskSetManager: Finished task 0.0 in stage 81.0 (TID 160) in 566 ms on 172.18.0.14 (executor 1) (1/3)
26/02/13 12:05:43 INFO TaskSetManager: Finished task 2.0 in stage 81.0 (TID 162) in 566 ms on 172.18.0.14 (executor 1) (2/3)
26/02/13 12:05:43 INFO TaskSetManager: Finished task 1.0 in stage 81.0 (TID 161) in 595 ms on 172.18.0.15 (executor 0) (3/3)
26/02/13 12:05:43 INFO TaskSchedulerImpl: Removed TaskSet 81.0, whose tasks have all completed, from pool 
26/02/13 12:05:43 INFO DAGScheduler: ResultStage 81 (start at NativeMethodAccessorImpl.java:0) finished in 0.601 s
26/02/13 12:05:43 INFO DAGScheduler: Job 52 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 12:05:43 INFO TaskSchedulerImpl: Killing all running tasks in stage 81: Stage finished
26/02/13 12:05:43 INFO DAGScheduler: Job 52 finished: start at NativeMethodAccessorImpl.java:0, took 0.603387 s
26/02/13 12:05:43 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 144, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@78c71024] is committing.
26/02/13 12:05:43 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 144, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@78c71024] committed.
26/02/13 12:05:43 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/144 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.144.a2afafed-ff77-4ce3-a58a-ae4e86bc4326.tmp
26/02/13 12:05:43 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.144.a2afafed-ff77-4ce3-a58a-ae4e86bc4326.tmp to file:/tmp/spark-checkpoint-enrichment/commits/144
26/02/13 12:05:43 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "4a161d66-225b-4b29-812d-ddec3c05364e",
  "name" : null,
  "timestamp" : "2026-02-13T12:05:42.318Z",
  "batchId" : 144,
  "numInputRows" : 89,
  "inputRowsPerSecond" : 7416.666666666666,
  "processedRowsPerSecond" : 91.75257731958763,
  "durationMs" : {
    "addBatch" : 742,
    "commitOffsets" : 58,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 49,
    "triggerExecution" : 970,
    "walCommit" : 118
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 1342,
        "1" : 1625,
        "0" : 1228
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 1373,
        "1" : 1658,
        "0" : 1253
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 1373,
        "1" : 1658,
        "0" : 1253
      }
    },
    "numInputRows" : 89,
    "inputRowsPerSecond" : 7416.666666666666,
    "processedRowsPerSecond" : 91.75257731958763,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@1a6a42c2",
    "numOutputRows" : 24
  }
}
26/02/13 12:05:43 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/145 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.145.b95780c0-d475-410a-80a6-e08631c12b0e.tmp
26/02/13 12:05:43 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.145.b95780c0-d475-410a-80a6-e08631c12b0e.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/145
26/02/13 12:05:43 INFO MicroBatchExecution: Committed offsets for batch 145. Metadata OffsetSeqMetadata(0,1770984343289,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 12:05:43 INFO BlockManagerInfo: Removed broadcast_74_piece0 on spark-master:42073 in memory (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 12:05:43 INFO BlockManagerInfo: Removed broadcast_74_piece0 on 172.18.0.14:37571 in memory (size: 107.2 KiB, free: 434.0 MiB)
26/02/13 12:05:43 INFO BlockManagerInfo: Removed broadcast_74_piece0 on 172.18.0.15:37717 in memory (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 12:05:43 INFO BlockManagerInfo: Removed broadcast_76_piece0 on spark-master:42073 in memory (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 12:05:43 INFO BlockManagerInfo: Removed broadcast_76_piece0 on 172.18.0.14:37571 in memory (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 12:05:43 INFO BlockManagerInfo: Removed broadcast_78_piece0 on spark-master:42073 in memory (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 12:05:43 INFO BlockManagerInfo: Removed broadcast_78_piece0 on 172.18.0.15:37717 in memory (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 12:05:43 INFO BlockManagerInfo: Removed broadcast_78_piece0 on 172.18.0.14:37571 in memory (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 12:05:43 INFO BlockManagerInfo: Removed broadcast_77_piece0 on spark-master:42073 in memory (size: 107.2 KiB, free: 434.4 MiB)
26/02/13 12:05:43 INFO BlockManagerInfo: Removed broadcast_77_piece0 on 172.18.0.15:37717 in memory (size: 107.2 KiB, free: 434.4 MiB)
26/02/13 12:05:43 INFO BlockManagerInfo: Removed broadcast_77_piece0 on 172.18.0.14:37571 in memory (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 12:05:43 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:05:43 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:05:43 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:05:43 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:05:43 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:05:43 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:05:43 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#20739 - origin_code.nullCount#20738) > 0)
26/02/13 12:05:43 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#20744 - destination_code.nullCount#20743) > 0)
26/02/13 12:05:43 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#20774 - callsign.nullCount#20773) > 0)
26/02/13 12:05:43 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 12:05:43 INFO DAGScheduler: Got job 53 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 12:05:43 INFO DAGScheduler: Final stage: ResultStage 83 (start at NativeMethodAccessorImpl.java:0)
26/02/13 12:05:43 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 82)
26/02/13 12:05:43 INFO DAGScheduler: Missing parents: List()
26/02/13 12:05:43 INFO DAGScheduler: Submitting ResultStage 83 (MapPartitionsRDD[285] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:05:43 INFO MemoryStore: Block broadcast_79 stored as values in memory (estimated size 99.3 KiB, free 434.1 MiB)
26/02/13 12:05:43 INFO MemoryStore: Block broadcast_79_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 434.0 MiB)
26/02/13 12:05:43 INFO BlockManagerInfo: Added broadcast_79_piece0 in memory on spark-master:42073 (size: 33.3 KiB, free: 434.3 MiB)
26/02/13 12:05:43 INFO SparkContext: Created broadcast 79 from broadcast at DAGScheduler.scala:1585
26/02/13 12:05:43 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 83 (MapPartitionsRDD[285] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 12:05:43 INFO TaskSchedulerImpl: Adding task set 83.0 with 4 tasks resource profile 0
26/02/13 12:05:43 INFO TaskSetManager: Starting task 0.0 in stage 83.0 (TID 163) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:05:43 INFO TaskSetManager: Starting task 1.0 in stage 83.0 (TID 164) (172.18.0.14, executor 1, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:05:43 INFO BlockManagerInfo: Added broadcast_79_piece0 in memory on 172.18.0.14:37571 (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 12:05:43 INFO TaskSetManager: Starting task 2.0 in stage 83.0 (TID 165) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:05:43 INFO TaskSetManager: Starting task 3.0 in stage 83.0 (TID 166) (172.18.0.14, executor 1, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:05:43 INFO TaskSetManager: Finished task 0.0 in stage 83.0 (TID 163) in 30 ms on 172.18.0.14 (executor 1) (1/4)
26/02/13 12:05:43 INFO TaskSetManager: Finished task 1.0 in stage 83.0 (TID 164) in 31 ms on 172.18.0.14 (executor 1) (2/4)
26/02/13 12:05:43 INFO TaskSetManager: Finished task 3.0 in stage 83.0 (TID 166) in 24 ms on 172.18.0.14 (executor 1) (3/4)
26/02/13 12:05:43 INFO TaskSetManager: Finished task 2.0 in stage 83.0 (TID 165) in 26 ms on 172.18.0.14 (executor 1) (4/4)
26/02/13 12:05:43 INFO TaskSchedulerImpl: Removed TaskSet 83.0, whose tasks have all completed, from pool 
26/02/13 12:05:43 INFO DAGScheduler: ResultStage 83 (start at NativeMethodAccessorImpl.java:0) finished in 0.063 s
26/02/13 12:05:43 INFO DAGScheduler: Job 53 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 12:05:43 INFO TaskSchedulerImpl: Killing all running tasks in stage 83: Stage finished
26/02/13 12:05:43 INFO DAGScheduler: Job 53 finished: start at NativeMethodAccessorImpl.java:0, took 0.065896 s
26/02/13 12:05:43 INFO MemoryStore: Block broadcast_80_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.9 MiB)
26/02/13 12:05:43 INFO BlockManagerInfo: Added broadcast_80_piece0 in memory on spark-master:42073 (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 12:05:43 INFO SparkContext: Created broadcast 80 from start at NativeMethodAccessorImpl.java:0
26/02/13 12:05:43 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 145, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@180d0354]. The input RDD has 3 partitions.
26/02/13 12:05:43 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 12:05:43 INFO DAGScheduler: Got job 54 (start at NativeMethodAccessorImpl.java:0) with 3 output partitions
26/02/13 12:05:43 INFO DAGScheduler: Final stage: ResultStage 84 (start at NativeMethodAccessorImpl.java:0)
26/02/13 12:05:43 INFO DAGScheduler: Parents of final stage: List()
26/02/13 12:05:43 INFO DAGScheduler: Missing parents: List()
26/02/13 12:05:43 INFO DAGScheduler: Submitting ResultStage 84 (MapPartitionsRDD[291] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:05:43 INFO MemoryStore: Block broadcast_81 stored as values in memory (estimated size 49.9 KiB, free 433.9 MiB)
26/02/13 12:05:43 INFO MemoryStore: Block broadcast_81_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.9 MiB)
26/02/13 12:05:43 INFO BlockManagerInfo: Added broadcast_81_piece0 in memory on spark-master:42073 (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 12:05:43 INFO SparkContext: Created broadcast 81 from broadcast at DAGScheduler.scala:1585
26/02/13 12:05:43 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 84 (MapPartitionsRDD[291] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
26/02/13 12:05:43 INFO TaskSchedulerImpl: Adding task set 84.0 with 3 tasks resource profile 0
26/02/13 12:05:43 INFO TaskSetManager: Starting task 1.0 in stage 84.0 (TID 167) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:05:43 INFO TaskSetManager: Starting task 0.0 in stage 84.0 (TID 168) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:05:43 INFO TaskSetManager: Starting task 2.0 in stage 84.0 (TID 169) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:05:43 INFO BlockManagerInfo: Added broadcast_81_piece0 in memory on 172.18.0.14:37571 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 12:05:43 INFO BlockManagerInfo: Added broadcast_81_piece0 in memory on 172.18.0.15:37717 (size: 19.5 KiB, free: 434.4 MiB)
26/02/13 12:05:43 INFO BlockManagerInfo: Added broadcast_80_piece0 in memory on 172.18.0.14:37571 (size: 107.2 KiB, free: 434.0 MiB)
26/02/13 12:05:43 INFO BlockManagerInfo: Added broadcast_80_piece0 in memory on 172.18.0.15:37717 (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 12:05:43 INFO TaskSetManager: Finished task 0.0 in stage 84.0 (TID 168) in 68 ms on 172.18.0.14 (executor 1) (1/3)
26/02/13 12:05:43 INFO TaskSetManager: Finished task 2.0 in stage 84.0 (TID 169) in 76 ms on 172.18.0.14 (executor 1) (2/3)
26/02/13 12:05:43 INFO TaskSetManager: Finished task 1.0 in stage 84.0 (TID 167) in 89 ms on 172.18.0.15 (executor 0) (3/3)
26/02/13 12:05:43 INFO TaskSchedulerImpl: Removed TaskSet 84.0, whose tasks have all completed, from pool 
26/02/13 12:05:43 INFO DAGScheduler: ResultStage 84 (start at NativeMethodAccessorImpl.java:0) finished in 0.095 s
26/02/13 12:05:43 INFO DAGScheduler: Job 54 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 12:05:43 INFO TaskSchedulerImpl: Killing all running tasks in stage 84: Stage finished
26/02/13 12:05:43 INFO DAGScheduler: Job 54 finished: start at NativeMethodAccessorImpl.java:0, took 0.097014 s
26/02/13 12:05:43 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 145, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@180d0354] is committing.
26/02/13 12:05:43 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 145, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@180d0354] committed.
26/02/13 12:05:43 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/145 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.145.73cbadf2-8902-4d70-8fe1-81e82ec3517f.tmp
26/02/13 12:05:43 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.145.73cbadf2-8902-4d70-8fe1-81e82ec3517f.tmp to file:/tmp/spark-checkpoint-enrichment/commits/145
26/02/13 12:05:43 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "4a161d66-225b-4b29-812d-ddec3c05364e",
  "name" : null,
  "timestamp" : "2026-02-13T12:05:43.288Z",
  "batchId" : 145,
  "numInputRows" : 156,
  "inputRowsPerSecond" : 160.82474226804123,
  "processedRowsPerSecond" : 371.42857142857144,
  "durationMs" : {
    "addBatch" : 260,
    "commitOffsets" : 57,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 45,
    "triggerExecution" : 420,
    "walCommit" : 57
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 1373,
        "1" : 1658,
        "0" : 1253
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 1425,
        "1" : 1717,
        "0" : 1298
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 1425,
        "1" : 1717,
        "0" : 1298
      }
    },
    "numInputRows" : 156,
    "inputRowsPerSecond" : 160.82474226804123,
    "processedRowsPerSecond" : 371.42857142857144,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@1a6a42c2",
    "numOutputRows" : 30
  }
}
26/02/13 12:05:52 INFO BlockManagerInfo: Removed broadcast_79_piece0 on spark-master:42073 in memory (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 12:05:52 INFO BlockManagerInfo: Removed broadcast_79_piece0 on 172.18.0.14:37571 in memory (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 12:05:52 INFO BlockManagerInfo: Removed broadcast_81_piece0 on spark-master:42073 in memory (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 12:05:52 INFO BlockManagerInfo: Removed broadcast_81_piece0 on 172.18.0.15:37717 in memory (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 12:05:52 INFO BlockManagerInfo: Removed broadcast_81_piece0 on 172.18.0.14:37571 in memory (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 12:05:53 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:05:59 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/146 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.146.ccbf1333-761d-4d69-81e7-1893a60b4c68.tmp
26/02/13 12:05:59 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.146.ccbf1333-761d-4d69-81e7-1893a60b4c68.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/146
26/02/13 12:05:59 INFO MicroBatchExecution: Committed offsets for batch 146. Metadata OffsetSeqMetadata(0,1770984359110,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 12:05:59 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:05:59 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:05:59 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:05:59 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:05:59 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:05:59 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:05:59 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#21593 - origin_code.nullCount#21592) > 0)
26/02/13 12:05:59 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#21598 - destination_code.nullCount#21597) > 0)
26/02/13 12:05:59 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#21628 - callsign.nullCount#21627) > 0)
26/02/13 12:05:59 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 12:05:59 INFO DAGScheduler: Got job 55 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 12:05:59 INFO DAGScheduler: Final stage: ResultStage 86 (start at NativeMethodAccessorImpl.java:0)
26/02/13 12:05:59 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 85)
26/02/13 12:05:59 INFO DAGScheduler: Missing parents: List()
26/02/13 12:05:59 INFO DAGScheduler: Submitting ResultStage 86 (MapPartitionsRDD[296] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:05:59 INFO MemoryStore: Block broadcast_82 stored as values in memory (estimated size 99.3 KiB, free 434.0 MiB)
26/02/13 12:05:59 INFO MemoryStore: Block broadcast_82_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 433.9 MiB)
26/02/13 12:05:59 INFO BlockManagerInfo: Added broadcast_82_piece0 in memory on spark-master:42073 (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 12:05:59 INFO SparkContext: Created broadcast 82 from broadcast at DAGScheduler.scala:1585
26/02/13 12:05:59 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 86 (MapPartitionsRDD[296] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 12:05:59 INFO TaskSchedulerImpl: Adding task set 86.0 with 4 tasks resource profile 0
26/02/13 12:05:59 INFO TaskSetManager: Starting task 0.0 in stage 86.0 (TID 170) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:05:59 INFO TaskSetManager: Starting task 1.0 in stage 86.0 (TID 171) (172.18.0.14, executor 1, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:05:59 INFO BlockManagerInfo: Added broadcast_82_piece0 in memory on 172.18.0.14:37571 (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 12:05:59 INFO TaskSetManager: Starting task 2.0 in stage 86.0 (TID 172) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:05:59 INFO TaskSetManager: Finished task 0.0 in stage 86.0 (TID 170) in 28 ms on 172.18.0.14 (executor 1) (1/4)
26/02/13 12:05:59 INFO TaskSetManager: Starting task 3.0 in stage 86.0 (TID 173) (172.18.0.14, executor 1, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:05:59 INFO TaskSetManager: Finished task 1.0 in stage 86.0 (TID 171) in 37 ms on 172.18.0.14 (executor 1) (2/4)
26/02/13 12:05:59 INFO TaskSetManager: Finished task 2.0 in stage 86.0 (TID 172) in 17 ms on 172.18.0.14 (executor 1) (3/4)
26/02/13 12:05:59 INFO TaskSetManager: Finished task 3.0 in stage 86.0 (TID 173) in 25 ms on 172.18.0.14 (executor 1) (4/4)
26/02/13 12:05:59 INFO TaskSchedulerImpl: Removed TaskSet 86.0, whose tasks have all completed, from pool 
26/02/13 12:05:59 INFO DAGScheduler: ResultStage 86 (start at NativeMethodAccessorImpl.java:0) finished in 0.073 s
26/02/13 12:05:59 INFO DAGScheduler: Job 55 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 12:05:59 INFO TaskSchedulerImpl: Killing all running tasks in stage 86: Stage finished
26/02/13 12:05:59 INFO DAGScheduler: Job 55 finished: start at NativeMethodAccessorImpl.java:0, took 0.077198 s
26/02/13 12:05:59 INFO MemoryStore: Block broadcast_83_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.8 MiB)
26/02/13 12:05:59 INFO BlockManagerInfo: Added broadcast_83_piece0 in memory on spark-master:42073 (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 12:05:59 INFO SparkContext: Created broadcast 83 from start at NativeMethodAccessorImpl.java:0
26/02/13 12:05:59 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 146, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@40aa734c]. The input RDD has 3 partitions.
26/02/13 12:05:59 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 12:05:59 INFO DAGScheduler: Got job 56 (start at NativeMethodAccessorImpl.java:0) with 3 output partitions
26/02/13 12:05:59 INFO DAGScheduler: Final stage: ResultStage 87 (start at NativeMethodAccessorImpl.java:0)
26/02/13 12:05:59 INFO DAGScheduler: Parents of final stage: List()
26/02/13 12:05:59 INFO DAGScheduler: Missing parents: List()
26/02/13 12:05:59 INFO DAGScheduler: Submitting ResultStage 87 (MapPartitionsRDD[302] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:05:59 INFO MemoryStore: Block broadcast_84 stored as values in memory (estimated size 49.9 KiB, free 433.8 MiB)
26/02/13 12:05:59 INFO MemoryStore: Block broadcast_84_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.8 MiB)
26/02/13 12:05:59 INFO BlockManagerInfo: Added broadcast_84_piece0 in memory on spark-master:42073 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 12:05:59 INFO SparkContext: Created broadcast 84 from broadcast at DAGScheduler.scala:1585
26/02/13 12:05:59 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 87 (MapPartitionsRDD[302] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
26/02/13 12:05:59 INFO TaskSchedulerImpl: Adding task set 87.0 with 3 tasks resource profile 0
26/02/13 12:05:59 INFO TaskSetManager: Starting task 0.0 in stage 87.0 (TID 174) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:05:59 INFO TaskSetManager: Starting task 1.0 in stage 87.0 (TID 175) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:05:59 INFO TaskSetManager: Starting task 2.0 in stage 87.0 (TID 176) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:05:59 INFO BlockManagerInfo: Added broadcast_84_piece0 in memory on 172.18.0.14:37571 (size: 19.5 KiB, free: 434.0 MiB)
26/02/13 12:05:59 INFO BlockManagerInfo: Added broadcast_84_piece0 in memory on 172.18.0.15:37717 (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 12:05:59 INFO BlockManagerInfo: Added broadcast_83_piece0 in memory on 172.18.0.14:37571 (size: 107.2 KiB, free: 433.9 MiB)
26/02/13 12:05:59 INFO BlockManagerInfo: Added broadcast_83_piece0 in memory on 172.18.0.15:37717 (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 12:06:00 INFO TaskSetManager: Finished task 0.0 in stage 87.0 (TID 174) in 600 ms on 172.18.0.14 (executor 1) (1/3)
26/02/13 12:06:00 INFO TaskSetManager: Finished task 2.0 in stage 87.0 (TID 176) in 603 ms on 172.18.0.14 (executor 1) (2/3)
26/02/13 12:06:00 INFO TaskSetManager: Finished task 1.0 in stage 87.0 (TID 175) in 631 ms on 172.18.0.15 (executor 0) (3/3)
26/02/13 12:06:00 INFO TaskSchedulerImpl: Removed TaskSet 87.0, whose tasks have all completed, from pool 
26/02/13 12:06:00 INFO DAGScheduler: ResultStage 87 (start at NativeMethodAccessorImpl.java:0) finished in 0.636 s
26/02/13 12:06:00 INFO DAGScheduler: Job 56 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 12:06:00 INFO TaskSchedulerImpl: Killing all running tasks in stage 87: Stage finished
26/02/13 12:06:00 INFO DAGScheduler: Job 56 finished: start at NativeMethodAccessorImpl.java:0, took 0.638942 s
26/02/13 12:06:00 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 146, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@40aa734c] is committing.
26/02/13 12:06:00 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 146, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@40aa734c] committed.
26/02/13 12:06:00 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/146 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.146.9cbdc11a-943a-4597-9a24-3ccfcf1cf80f.tmp
26/02/13 12:06:00 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.146.9cbdc11a-943a-4597-9a24-3ccfcf1cf80f.tmp to file:/tmp/spark-checkpoint-enrichment/commits/146
26/02/13 12:06:00 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "4a161d66-225b-4b29-812d-ddec3c05364e",
  "name" : null,
  "timestamp" : "2026-02-13T12:05:59.109Z",
  "batchId" : 146,
  "numInputRows" : 95,
  "inputRowsPerSecond" : 7916.666666666666,
  "processedRowsPerSecond" : 88.70214752567693,
  "durationMs" : {
    "addBatch" : 809,
    "commitOffsets" : 88,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 51,
    "triggerExecution" : 1071,
    "walCommit" : 122
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 1425,
        "1" : 1717,
        "0" : 1298
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 1455,
        "1" : 1754,
        "0" : 1326
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 1455,
        "1" : 1754,
        "0" : 1326
      }
    },
    "numInputRows" : 95,
    "inputRowsPerSecond" : 7916.666666666666,
    "processedRowsPerSecond" : 88.70214752567693,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@1a6a42c2",
    "numOutputRows" : 24
  }
}
26/02/13 12:06:00 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/147 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.147.f520c970-345e-4f8d-8960-2ee34e05675d.tmp
26/02/13 12:06:00 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.147.f520c970-345e-4f8d-8960-2ee34e05675d.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/147
26/02/13 12:06:00 INFO MicroBatchExecution: Committed offsets for batch 147. Metadata OffsetSeqMetadata(0,1770984360182,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 12:06:00 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:06:00 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:06:00 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:06:00 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:06:00 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:06:00 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:06:00 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#22447 - origin_code.nullCount#22446) > 0)
26/02/13 12:06:00 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#22452 - destination_code.nullCount#22451) > 0)
26/02/13 12:06:00 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#22482 - callsign.nullCount#22481) > 0)
26/02/13 12:06:00 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 12:06:00 INFO DAGScheduler: Got job 57 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 12:06:00 INFO DAGScheduler: Final stage: ResultStage 89 (start at NativeMethodAccessorImpl.java:0)
26/02/13 12:06:00 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 88)
26/02/13 12:06:00 INFO DAGScheduler: Missing parents: List()
26/02/13 12:06:00 INFO DAGScheduler: Submitting ResultStage 89 (MapPartitionsRDD[307] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:06:00 INFO MemoryStore: Block broadcast_85 stored as values in memory (estimated size 99.3 KiB, free 433.7 MiB)
26/02/13 12:06:00 INFO MemoryStore: Block broadcast_85_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 433.6 MiB)
26/02/13 12:06:00 INFO BlockManagerInfo: Added broadcast_85_piece0 in memory on spark-master:42073 (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 12:06:00 INFO SparkContext: Created broadcast 85 from broadcast at DAGScheduler.scala:1585
26/02/13 12:06:00 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 89 (MapPartitionsRDD[307] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 12:06:00 INFO TaskSchedulerImpl: Adding task set 89.0 with 4 tasks resource profile 0
26/02/13 12:06:00 INFO TaskSetManager: Starting task 0.0 in stage 89.0 (TID 177) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:06:00 INFO TaskSetManager: Starting task 1.0 in stage 89.0 (TID 178) (172.18.0.14, executor 1, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:06:00 INFO BlockManagerInfo: Added broadcast_85_piece0 in memory on 172.18.0.14:37571 (size: 33.3 KiB, free: 433.9 MiB)
26/02/13 12:06:00 INFO TaskSetManager: Starting task 2.0 in stage 89.0 (TID 179) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:06:00 INFO TaskSetManager: Finished task 0.0 in stage 89.0 (TID 177) in 33 ms on 172.18.0.14 (executor 1) (1/4)
26/02/13 12:06:00 INFO TaskSetManager: Starting task 3.0 in stage 89.0 (TID 180) (172.18.0.14, executor 1, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:06:00 INFO TaskSetManager: Finished task 1.0 in stage 89.0 (TID 178) in 34 ms on 172.18.0.14 (executor 1) (2/4)
26/02/13 12:06:00 INFO TaskSetManager: Finished task 3.0 in stage 89.0 (TID 180) in 18 ms on 172.18.0.14 (executor 1) (3/4)
26/02/13 12:06:00 INFO TaskSetManager: Finished task 2.0 in stage 89.0 (TID 179) in 20 ms on 172.18.0.14 (executor 1) (4/4)
26/02/13 12:06:00 INFO TaskSchedulerImpl: Removed TaskSet 89.0, whose tasks have all completed, from pool 
26/02/13 12:06:00 INFO DAGScheduler: ResultStage 89 (start at NativeMethodAccessorImpl.java:0) finished in 0.060 s
26/02/13 12:06:00 INFO DAGScheduler: Job 57 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 12:06:00 INFO TaskSchedulerImpl: Killing all running tasks in stage 89: Stage finished
26/02/13 12:06:00 INFO DAGScheduler: Job 57 finished: start at NativeMethodAccessorImpl.java:0, took 0.062755 s
26/02/13 12:06:00 INFO MemoryStore: Block broadcast_86_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.5 MiB)
26/02/13 12:06:00 INFO BlockManagerInfo: Added broadcast_86_piece0 in memory on spark-master:42073 (size: 107.2 KiB, free: 434.0 MiB)
26/02/13 12:06:00 INFO SparkContext: Created broadcast 86 from start at NativeMethodAccessorImpl.java:0
26/02/13 12:06:00 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 147, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@5c5c8d6c]. The input RDD has 3 partitions.
26/02/13 12:06:00 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 12:06:00 INFO DAGScheduler: Got job 58 (start at NativeMethodAccessorImpl.java:0) with 3 output partitions
26/02/13 12:06:00 INFO DAGScheduler: Final stage: ResultStage 90 (start at NativeMethodAccessorImpl.java:0)
26/02/13 12:06:00 INFO DAGScheduler: Parents of final stage: List()
26/02/13 12:06:00 INFO DAGScheduler: Missing parents: List()
26/02/13 12:06:00 INFO DAGScheduler: Submitting ResultStage 90 (MapPartitionsRDD[313] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:06:00 INFO BlockManagerInfo: Removed broadcast_82_piece0 on spark-master:42073 in memory (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 12:06:00 INFO MemoryStore: Block broadcast_87 stored as values in memory (estimated size 49.9 KiB, free 433.6 MiB)
26/02/13 12:06:00 INFO BlockManagerInfo: Removed broadcast_82_piece0 on 172.18.0.14:37571 in memory (size: 33.3 KiB, free: 433.9 MiB)
26/02/13 12:06:00 INFO MemoryStore: Block broadcast_87_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.6 MiB)
26/02/13 12:06:00 INFO BlockManagerInfo: Added broadcast_87_piece0 in memory on spark-master:42073 (size: 19.5 KiB, free: 434.0 MiB)
26/02/13 12:06:00 INFO SparkContext: Created broadcast 87 from broadcast at DAGScheduler.scala:1585
26/02/13 12:06:00 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 90 (MapPartitionsRDD[313] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
26/02/13 12:06:00 INFO TaskSchedulerImpl: Adding task set 90.0 with 3 tasks resource profile 0
26/02/13 12:06:00 INFO TaskSetManager: Starting task 0.0 in stage 90.0 (TID 181) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:06:00 INFO TaskSetManager: Starting task 1.0 in stage 90.0 (TID 182) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:06:00 INFO TaskSetManager: Starting task 2.0 in stage 90.0 (TID 183) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:06:00 INFO BlockManagerInfo: Removed broadcast_85_piece0 on spark-master:42073 in memory (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 12:06:00 INFO BlockManagerInfo: Removed broadcast_85_piece0 on 172.18.0.14:37571 in memory (size: 33.3 KiB, free: 433.9 MiB)
26/02/13 12:06:00 INFO BlockManagerInfo: Removed broadcast_83_piece0 on spark-master:42073 in memory (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 12:06:00 INFO BlockManagerInfo: Added broadcast_87_piece0 in memory on 172.18.0.14:37571 (size: 19.5 KiB, free: 433.9 MiB)
26/02/13 12:06:00 INFO BlockManagerInfo: Removed broadcast_83_piece0 on 172.18.0.14:37571 in memory (size: 107.2 KiB, free: 434.0 MiB)
26/02/13 12:06:00 INFO BlockManagerInfo: Added broadcast_87_piece0 in memory on 172.18.0.15:37717 (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 12:06:00 INFO BlockManagerInfo: Removed broadcast_83_piece0 on 172.18.0.15:37717 in memory (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 12:06:00 INFO BlockManagerInfo: Removed broadcast_84_piece0 on spark-master:42073 in memory (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 12:06:00 INFO BlockManagerInfo: Removed broadcast_84_piece0 on 172.18.0.15:37717 in memory (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 12:06:00 INFO BlockManagerInfo: Removed broadcast_84_piece0 on 172.18.0.14:37571 in memory (size: 19.5 KiB, free: 434.0 MiB)
26/02/13 12:06:00 INFO BlockManagerInfo: Added broadcast_86_piece0 in memory on 172.18.0.14:37571 (size: 107.2 KiB, free: 433.9 MiB)
26/02/13 12:06:00 INFO BlockManagerInfo: Added broadcast_86_piece0 in memory on 172.18.0.15:37717 (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 12:06:00 INFO TaskSetManager: Finished task 0.0 in stage 90.0 (TID 181) in 88 ms on 172.18.0.14 (executor 1) (1/3)
26/02/13 12:06:00 INFO TaskSetManager: Finished task 2.0 in stage 90.0 (TID 183) in 94 ms on 172.18.0.14 (executor 1) (2/3)
26/02/13 12:06:00 INFO TaskSetManager: Finished task 1.0 in stage 90.0 (TID 182) in 143 ms on 172.18.0.15 (executor 0) (3/3)
26/02/13 12:06:00 INFO TaskSchedulerImpl: Removed TaskSet 90.0, whose tasks have all completed, from pool 
26/02/13 12:06:00 INFO DAGScheduler: ResultStage 90 (start at NativeMethodAccessorImpl.java:0) finished in 0.152 s
26/02/13 12:06:00 INFO DAGScheduler: Job 58 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 12:06:00 INFO TaskSchedulerImpl: Killing all running tasks in stage 90: Stage finished
26/02/13 12:06:00 INFO DAGScheduler: Job 58 finished: start at NativeMethodAccessorImpl.java:0, took 0.158103 s
26/02/13 12:06:00 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 147, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@5c5c8d6c] is committing.
26/02/13 12:06:00 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 147, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@5c5c8d6c] committed.
26/02/13 12:06:00 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/147 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.147.48449c3c-127b-4dd9-982b-d385a7b27157.tmp
26/02/13 12:06:00 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.147.48449c3c-127b-4dd9-982b-d385a7b27157.tmp to file:/tmp/spark-checkpoint-enrichment/commits/147
26/02/13 12:06:00 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "4a161d66-225b-4b29-812d-ddec3c05364e",
  "name" : null,
  "timestamp" : "2026-02-13T12:06:00.181Z",
  "batchId" : 147,
  "numInputRows" : 147,
  "inputRowsPerSecond" : 137.12686567164178,
  "processedRowsPerSecond" : 280.53435114503816,
  "durationMs" : {
    "addBatch" : 316,
    "commitOffsets" : 121,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 25,
    "triggerExecution" : 524,
    "walCommit" : 60
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 1455,
        "1" : 1754,
        "0" : 1326
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 1506,
        "1" : 1810,
        "0" : 1366
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 1506,
        "1" : 1810,
        "0" : 1366
      }
    },
    "numInputRows" : 147,
    "inputRowsPerSecond" : 137.12686567164178,
    "processedRowsPerSecond" : 280.53435114503816,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@1a6a42c2",
    "numOutputRows" : 29
  }
}
26/02/13 12:06:01 INFO BlockManagerInfo: Removed broadcast_87_piece0 on spark-master:42073 in memory (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 12:06:01 INFO BlockManagerInfo: Removed broadcast_87_piece0 on 172.18.0.14:37571 in memory (size: 19.5 KiB, free: 434.0 MiB)
26/02/13 12:06:01 INFO BlockManagerInfo: Removed broadcast_87_piece0 on 172.18.0.15:37717 in memory (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 12:06:10 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:06:12 INFO BlockManagerInfo: Removed broadcast_80_piece0 on spark-master:42073 in memory (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 12:06:12 INFO BlockManagerInfo: Removed broadcast_80_piece0 on 172.18.0.14:37571 in memory (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 12:06:12 INFO BlockManagerInfo: Removed broadcast_80_piece0 on 172.18.0.15:37717 in memory (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 12:06:19 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/148 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.148.939462ac-b373-4c93-adbe-9243564f4895.tmp
26/02/13 12:06:19 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.148.939462ac-b373-4c93-adbe-9243564f4895.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/148
26/02/13 12:06:19 INFO MicroBatchExecution: Committed offsets for batch 148. Metadata OffsetSeqMetadata(0,1770984379183,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 12:06:19 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:06:19 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:06:19 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:06:19 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:06:19 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:06:19 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:06:19 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#23301 - origin_code.nullCount#23300) > 0)
26/02/13 12:06:19 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#23306 - destination_code.nullCount#23305) > 0)
26/02/13 12:06:19 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#23336 - callsign.nullCount#23335) > 0)
26/02/13 12:06:19 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 12:06:19 INFO DAGScheduler: Got job 59 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 12:06:19 INFO DAGScheduler: Final stage: ResultStage 92 (start at NativeMethodAccessorImpl.java:0)
26/02/13 12:06:19 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 91)
26/02/13 12:06:19 INFO DAGScheduler: Missing parents: List()
26/02/13 12:06:19 INFO DAGScheduler: Submitting ResultStage 92 (MapPartitionsRDD[318] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:06:19 INFO MemoryStore: Block broadcast_88 stored as values in memory (estimated size 99.3 KiB, free 434.0 MiB)
26/02/13 12:06:19 INFO MemoryStore: Block broadcast_88_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 433.9 MiB)
26/02/13 12:06:19 INFO BlockManagerInfo: Added broadcast_88_piece0 in memory on spark-master:42073 (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 12:06:19 INFO SparkContext: Created broadcast 88 from broadcast at DAGScheduler.scala:1585
26/02/13 12:06:19 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 92 (MapPartitionsRDD[318] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 12:06:19 INFO TaskSchedulerImpl: Adding task set 92.0 with 4 tasks resource profile 0
26/02/13 12:06:19 INFO TaskSetManager: Starting task 0.0 in stage 92.0 (TID 184) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:06:19 INFO TaskSetManager: Starting task 1.0 in stage 92.0 (TID 185) (172.18.0.14, executor 1, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:06:19 INFO BlockManagerInfo: Added broadcast_88_piece0 in memory on 172.18.0.14:37571 (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 12:06:19 INFO TaskSetManager: Starting task 2.0 in stage 92.0 (TID 186) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:06:19 INFO TaskSetManager: Finished task 0.0 in stage 92.0 (TID 184) in 32 ms on 172.18.0.14 (executor 1) (1/4)
26/02/13 12:06:19 INFO TaskSetManager: Finished task 1.0 in stage 92.0 (TID 185) in 33 ms on 172.18.0.14 (executor 1) (2/4)
26/02/13 12:06:19 INFO TaskSetManager: Starting task 3.0 in stage 92.0 (TID 187) (172.18.0.14, executor 1, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:06:19 INFO TaskSetManager: Finished task 2.0 in stage 92.0 (TID 186) in 20 ms on 172.18.0.14 (executor 1) (3/4)
26/02/13 12:06:19 INFO TaskSetManager: Finished task 3.0 in stage 92.0 (TID 187) in 20 ms on 172.18.0.14 (executor 1) (4/4)
26/02/13 12:06:19 INFO TaskSchedulerImpl: Removed TaskSet 92.0, whose tasks have all completed, from pool 
26/02/13 12:06:19 INFO BlockManagerInfo: Removed broadcast_86_piece0 on spark-master:42073 in memory (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 12:06:19 INFO DAGScheduler: ResultStage 92 (start at NativeMethodAccessorImpl.java:0) finished in 0.065 s
26/02/13 12:06:19 INFO DAGScheduler: Job 59 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 12:06:19 INFO TaskSchedulerImpl: Killing all running tasks in stage 92: Stage finished
26/02/13 12:06:19 INFO DAGScheduler: Job 59 finished: start at NativeMethodAccessorImpl.java:0, took 0.067720 s
26/02/13 12:06:19 INFO BlockManagerInfo: Removed broadcast_86_piece0 on 172.18.0.15:37717 in memory (size: 107.2 KiB, free: 434.4 MiB)
26/02/13 12:06:19 INFO BlockManagerInfo: Removed broadcast_86_piece0 on 172.18.0.14:37571 in memory (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 12:06:19 INFO MemoryStore: Block broadcast_89_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.9 MiB)
26/02/13 12:06:19 INFO BlockManagerInfo: Added broadcast_89_piece0 in memory on spark-master:42073 (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 12:06:19 INFO SparkContext: Created broadcast 89 from start at NativeMethodAccessorImpl.java:0
26/02/13 12:06:19 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 148, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@2c0ac038]. The input RDD has 3 partitions.
26/02/13 12:06:19 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 12:06:19 INFO DAGScheduler: Got job 60 (start at NativeMethodAccessorImpl.java:0) with 3 output partitions
26/02/13 12:06:19 INFO DAGScheduler: Final stage: ResultStage 93 (start at NativeMethodAccessorImpl.java:0)
26/02/13 12:06:19 INFO DAGScheduler: Parents of final stage: List()
26/02/13 12:06:19 INFO DAGScheduler: Missing parents: List()
26/02/13 12:06:19 INFO DAGScheduler: Submitting ResultStage 93 (MapPartitionsRDD[324] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:06:19 INFO MemoryStore: Block broadcast_90 stored as values in memory (estimated size 49.9 KiB, free 433.9 MiB)
26/02/13 12:06:19 INFO MemoryStore: Block broadcast_90_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.9 MiB)
26/02/13 12:06:19 INFO BlockManagerInfo: Added broadcast_90_piece0 in memory on spark-master:42073 (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 12:06:19 INFO SparkContext: Created broadcast 90 from broadcast at DAGScheduler.scala:1585
26/02/13 12:06:19 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 93 (MapPartitionsRDD[324] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
26/02/13 12:06:19 INFO TaskSchedulerImpl: Adding task set 93.0 with 3 tasks resource profile 0
26/02/13 12:06:19 INFO TaskSetManager: Starting task 1.0 in stage 93.0 (TID 188) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:06:19 INFO TaskSetManager: Starting task 0.0 in stage 93.0 (TID 189) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:06:19 INFO TaskSetManager: Starting task 2.0 in stage 93.0 (TID 190) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:06:19 INFO BlockManagerInfo: Added broadcast_90_piece0 in memory on 172.18.0.14:37571 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 12:06:19 INFO BlockManagerInfo: Added broadcast_90_piece0 in memory on 172.18.0.15:37717 (size: 19.5 KiB, free: 434.4 MiB)
26/02/13 12:06:19 INFO BlockManagerInfo: Added broadcast_89_piece0 in memory on 172.18.0.14:37571 (size: 107.2 KiB, free: 434.0 MiB)
26/02/13 12:06:19 INFO BlockManagerInfo: Added broadcast_89_piece0 in memory on 172.18.0.15:37717 (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 12:06:20 INFO TaskSetManager: Finished task 0.0 in stage 93.0 (TID 189) in 569 ms on 172.18.0.14 (executor 1) (1/3)
26/02/13 12:06:20 INFO TaskSetManager: Finished task 2.0 in stage 93.0 (TID 190) in 570 ms on 172.18.0.14 (executor 1) (2/3)
26/02/13 12:06:20 INFO TaskSetManager: Finished task 1.0 in stage 93.0 (TID 188) in 593 ms on 172.18.0.15 (executor 0) (3/3)
26/02/13 12:06:20 INFO TaskSchedulerImpl: Removed TaskSet 93.0, whose tasks have all completed, from pool 
26/02/13 12:06:20 INFO DAGScheduler: ResultStage 93 (start at NativeMethodAccessorImpl.java:0) finished in 0.599 s
26/02/13 12:06:20 INFO DAGScheduler: Job 60 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 12:06:20 INFO TaskSchedulerImpl: Killing all running tasks in stage 93: Stage finished
26/02/13 12:06:20 INFO DAGScheduler: Job 60 finished: start at NativeMethodAccessorImpl.java:0, took 0.600482 s
26/02/13 12:06:20 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 148, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@2c0ac038] is committing.
26/02/13 12:06:20 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 148, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@2c0ac038] committed.
26/02/13 12:06:20 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/148 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.148.50857a76-6973-4bcf-853d-59492c12d16e.tmp
26/02/13 12:06:20 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.148.50857a76-6973-4bcf-853d-59492c12d16e.tmp to file:/tmp/spark-checkpoint-enrichment/commits/148
26/02/13 12:06:20 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "4a161d66-225b-4b29-812d-ddec3c05364e",
  "name" : null,
  "timestamp" : "2026-02-13T12:06:19.181Z",
  "batchId" : 148,
  "numInputRows" : 95,
  "inputRowsPerSecond" : 8636.363636363636,
  "processedRowsPerSecond" : 97.53593429158111,
  "durationMs" : {
    "addBatch" : 761,
    "commitOffsets" : 58,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 43,
    "triggerExecution" : 974,
    "walCommit" : 109
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 1506,
        "1" : 1810,
        "0" : 1366
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 1536,
        "1" : 1847,
        "0" : 1394
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 1536,
        "1" : 1847,
        "0" : 1394
      }
    },
    "numInputRows" : 95,
    "inputRowsPerSecond" : 8636.363636363636,
    "processedRowsPerSecond" : 97.53593429158111,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@1a6a42c2",
    "numOutputRows" : 24
  }
}
26/02/13 12:06:20 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/149 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.149.34d75236-bda9-4992-933c-88608a258d15.tmp
26/02/13 12:06:20 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.149.34d75236-bda9-4992-933c-88608a258d15.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/149
26/02/13 12:06:20 INFO MicroBatchExecution: Committed offsets for batch 149. Metadata OffsetSeqMetadata(0,1770984380157,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 12:06:20 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:06:20 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:06:20 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:06:20 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:06:20 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:06:20 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:06:20 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#24155 - origin_code.nullCount#24154) > 0)
26/02/13 12:06:20 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#24160 - destination_code.nullCount#24159) > 0)
26/02/13 12:06:20 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#24190 - callsign.nullCount#24189) > 0)
26/02/13 12:06:20 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 12:06:20 INFO DAGScheduler: Got job 61 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 12:06:20 INFO DAGScheduler: Final stage: ResultStage 95 (start at NativeMethodAccessorImpl.java:0)
26/02/13 12:06:20 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 94)
26/02/13 12:06:20 INFO DAGScheduler: Missing parents: List()
26/02/13 12:06:20 INFO DAGScheduler: Submitting ResultStage 95 (MapPartitionsRDD[329] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:06:20 INFO MemoryStore: Block broadcast_91 stored as values in memory (estimated size 99.3 KiB, free 433.8 MiB)
26/02/13 12:06:20 INFO MemoryStore: Block broadcast_91_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 433.7 MiB)
26/02/13 12:06:20 INFO BlockManagerInfo: Added broadcast_91_piece0 in memory on spark-master:42073 (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 12:06:20 INFO SparkContext: Created broadcast 91 from broadcast at DAGScheduler.scala:1585
26/02/13 12:06:20 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 95 (MapPartitionsRDD[329] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 12:06:20 INFO TaskSchedulerImpl: Adding task set 95.0 with 4 tasks resource profile 0
26/02/13 12:06:20 INFO TaskSetManager: Starting task 0.0 in stage 95.0 (TID 191) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:06:20 INFO TaskSetManager: Starting task 1.0 in stage 95.0 (TID 192) (172.18.0.14, executor 1, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:06:20 INFO BlockManagerInfo: Added broadcast_91_piece0 in memory on 172.18.0.14:37571 (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 12:06:20 INFO TaskSetManager: Starting task 2.0 in stage 95.0 (TID 193) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:06:20 INFO TaskSetManager: Finished task 1.0 in stage 95.0 (TID 192) in 17 ms on 172.18.0.14 (executor 1) (1/4)
26/02/13 12:06:20 INFO TaskSetManager: Starting task 3.0 in stage 95.0 (TID 194) (172.18.0.14, executor 1, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:06:20 INFO TaskSetManager: Finished task 0.0 in stage 95.0 (TID 191) in 25 ms on 172.18.0.14 (executor 1) (2/4)
26/02/13 12:06:20 INFO TaskSetManager: Finished task 2.0 in stage 95.0 (TID 193) in 21 ms on 172.18.0.14 (executor 1) (3/4)
26/02/13 12:06:20 INFO TaskSetManager: Finished task 3.0 in stage 95.0 (TID 194) in 24 ms on 172.18.0.14 (executor 1) (4/4)
26/02/13 12:06:20 INFO TaskSchedulerImpl: Removed TaskSet 95.0, whose tasks have all completed, from pool 
26/02/13 12:06:20 INFO DAGScheduler: ResultStage 95 (start at NativeMethodAccessorImpl.java:0) finished in 0.055 s
26/02/13 12:06:20 INFO DAGScheduler: Job 61 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 12:06:20 INFO TaskSchedulerImpl: Killing all running tasks in stage 95: Stage finished
26/02/13 12:06:20 INFO DAGScheduler: Job 61 finished: start at NativeMethodAccessorImpl.java:0, took 0.056378 s
26/02/13 12:06:20 INFO MemoryStore: Block broadcast_92_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.6 MiB)
26/02/13 12:06:20 INFO BlockManagerInfo: Added broadcast_92_piece0 in memory on spark-master:42073 (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 12:06:20 INFO SparkContext: Created broadcast 92 from start at NativeMethodAccessorImpl.java:0
26/02/13 12:06:20 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 149, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@c05e1d4]. The input RDD has 3 partitions.
26/02/13 12:06:20 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 12:06:20 INFO DAGScheduler: Got job 62 (start at NativeMethodAccessorImpl.java:0) with 3 output partitions
26/02/13 12:06:20 INFO DAGScheduler: Final stage: ResultStage 96 (start at NativeMethodAccessorImpl.java:0)
26/02/13 12:06:20 INFO DAGScheduler: Parents of final stage: List()
26/02/13 12:06:20 INFO DAGScheduler: Missing parents: List()
26/02/13 12:06:20 INFO DAGScheduler: Submitting ResultStage 96 (MapPartitionsRDD[335] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:06:20 INFO MemoryStore: Block broadcast_93 stored as values in memory (estimated size 49.9 KiB, free 433.6 MiB)
26/02/13 12:06:20 INFO MemoryStore: Block broadcast_93_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.6 MiB)
26/02/13 12:06:20 INFO BlockManagerInfo: Added broadcast_93_piece0 in memory on spark-master:42073 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 12:06:20 INFO SparkContext: Created broadcast 93 from broadcast at DAGScheduler.scala:1585
26/02/13 12:06:20 INFO BlockManagerInfo: Removed broadcast_90_piece0 on spark-master:42073 in memory (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 12:06:20 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 96 (MapPartitionsRDD[335] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
26/02/13 12:06:20 INFO TaskSchedulerImpl: Adding task set 96.0 with 3 tasks resource profile 0
26/02/13 12:06:20 INFO BlockManagerInfo: Removed broadcast_90_piece0 on 172.18.0.14:37571 in memory (size: 19.5 KiB, free: 434.0 MiB)
26/02/13 12:06:20 INFO TaskSetManager: Starting task 1.0 in stage 96.0 (TID 195) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:06:20 INFO BlockManagerInfo: Removed broadcast_90_piece0 on 172.18.0.15:37717 in memory (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 12:06:20 INFO TaskSetManager: Starting task 0.0 in stage 96.0 (TID 196) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:06:20 INFO TaskSetManager: Starting task 2.0 in stage 96.0 (TID 197) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:06:20 INFO BlockManagerInfo: Removed broadcast_89_piece0 on spark-master:42073 in memory (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 12:06:20 INFO BlockManagerInfo: Removed broadcast_89_piece0 on 172.18.0.15:37717 in memory (size: 107.2 KiB, free: 434.4 MiB)
26/02/13 12:06:20 INFO BlockManagerInfo: Removed broadcast_89_piece0 on 172.18.0.14:37571 in memory (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 12:06:20 INFO BlockManagerInfo: Removed broadcast_91_piece0 on spark-master:42073 in memory (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 12:06:20 INFO BlockManagerInfo: Added broadcast_93_piece0 in memory on 172.18.0.14:37571 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 12:06:20 INFO BlockManagerInfo: Removed broadcast_91_piece0 on 172.18.0.14:37571 in memory (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 12:06:20 INFO BlockManagerInfo: Added broadcast_93_piece0 in memory on 172.18.0.15:37717 (size: 19.5 KiB, free: 434.4 MiB)
26/02/13 12:06:20 INFO BlockManagerInfo: Removed broadcast_88_piece0 on spark-master:42073 in memory (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 12:06:20 INFO BlockManagerInfo: Removed broadcast_88_piece0 on 172.18.0.14:37571 in memory (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 12:06:20 INFO BlockManagerInfo: Added broadcast_92_piece0 in memory on 172.18.0.14:37571 (size: 107.2 KiB, free: 434.0 MiB)
26/02/13 12:06:20 INFO BlockManagerInfo: Added broadcast_92_piece0 in memory on 172.18.0.15:37717 (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 12:06:20 INFO TaskSetManager: Finished task 0.0 in stage 96.0 (TID 196) in 88 ms on 172.18.0.14 (executor 1) (1/3)
26/02/13 12:06:20 INFO TaskSetManager: Finished task 2.0 in stage 96.0 (TID 197) in 91 ms on 172.18.0.14 (executor 1) (2/3)
26/02/13 12:06:20 INFO TaskSetManager: Finished task 1.0 in stage 96.0 (TID 195) in 132 ms on 172.18.0.15 (executor 0) (3/3)
26/02/13 12:06:20 INFO TaskSchedulerImpl: Removed TaskSet 96.0, whose tasks have all completed, from pool 
26/02/13 12:06:20 INFO DAGScheduler: ResultStage 96 (start at NativeMethodAccessorImpl.java:0) finished in 0.147 s
26/02/13 12:06:20 INFO DAGScheduler: Job 62 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 12:06:20 INFO TaskSchedulerImpl: Killing all running tasks in stage 96: Stage finished
26/02/13 12:06:20 INFO DAGScheduler: Job 62 finished: start at NativeMethodAccessorImpl.java:0, took 0.149628 s
26/02/13 12:06:20 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 149, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@c05e1d4] is committing.
26/02/13 12:06:20 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 149, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@c05e1d4] committed.
26/02/13 12:06:20 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/149 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.149.c81861f4-c393-4b12-85b5-058d6f8aca19.tmp
26/02/13 12:06:20 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.149.c81861f4-c393-4b12-85b5-058d6f8aca19.tmp to file:/tmp/spark-checkpoint-enrichment/commits/149
26/02/13 12:06:20 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "4a161d66-225b-4b29-812d-ddec3c05364e",
  "name" : null,
  "timestamp" : "2026-02-13T12:06:20.156Z",
  "batchId" : 149,
  "numInputRows" : 147,
  "inputRowsPerSecond" : 150.76923076923077,
  "processedRowsPerSecond" : 327.3942093541203,
  "durationMs" : {
    "addBatch" : 294,
    "commitOffsets" : 69,
    "getBatch" : 1,
    "latestOffset" : 1,
    "queryPlanning" : 33,
    "triggerExecution" : 449,
    "walCommit" : 51
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 1536,
        "1" : 1847,
        "0" : 1394
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 1587,
        "1" : 1902,
        "0" : 1435
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 1587,
        "1" : 1902,
        "0" : 1435
      }
    },
    "numInputRows" : 147,
    "inputRowsPerSecond" : 150.76923076923077,
    "processedRowsPerSecond" : 327.3942093541203,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@1a6a42c2",
    "numOutputRows" : 29
  }
}
26/02/13 12:06:21 INFO BlockManagerInfo: Removed broadcast_93_piece0 on spark-master:42073 in memory (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 12:06:21 INFO BlockManagerInfo: Removed broadcast_93_piece0 on 172.18.0.14:37571 in memory (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 12:06:21 INFO BlockManagerInfo: Removed broadcast_93_piece0 on 172.18.0.15:37717 in memory (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 12:06:30 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:06:37 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/150 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.150.a73c1a71-6864-4e79-b44f-73760a9a8d77.tmp
26/02/13 12:06:37 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.150.a73c1a71-6864-4e79-b44f-73760a9a8d77.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/150
26/02/13 12:06:37 INFO MicroBatchExecution: Committed offsets for batch 150. Metadata OffsetSeqMetadata(0,1770984397498,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 12:06:37 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:06:37 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:06:37 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:06:37 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:06:37 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:06:37 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:06:37 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#25009 - origin_code.nullCount#25008) > 0)
26/02/13 12:06:37 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#25014 - destination_code.nullCount#25013) > 0)
26/02/13 12:06:37 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#25044 - callsign.nullCount#25043) > 0)
26/02/13 12:06:37 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 12:06:37 INFO DAGScheduler: Got job 63 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 12:06:37 INFO DAGScheduler: Final stage: ResultStage 98 (start at NativeMethodAccessorImpl.java:0)
26/02/13 12:06:37 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 97)
26/02/13 12:06:37 INFO DAGScheduler: Missing parents: List()
26/02/13 12:06:37 INFO DAGScheduler: Submitting ResultStage 98 (MapPartitionsRDD[340] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:06:37 INFO MemoryStore: Block broadcast_94 stored as values in memory (estimated size 99.3 KiB, free 434.0 MiB)
26/02/13 12:06:37 INFO MemoryStore: Block broadcast_94_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 433.9 MiB)
26/02/13 12:06:37 INFO BlockManagerInfo: Added broadcast_94_piece0 in memory on spark-master:42073 (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 12:06:37 INFO SparkContext: Created broadcast 94 from broadcast at DAGScheduler.scala:1585
26/02/13 12:06:37 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 98 (MapPartitionsRDD[340] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 12:06:37 INFO TaskSchedulerImpl: Adding task set 98.0 with 4 tasks resource profile 0
26/02/13 12:06:37 INFO TaskSetManager: Starting task 0.0 in stage 98.0 (TID 198) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:06:37 INFO TaskSetManager: Starting task 1.0 in stage 98.0 (TID 199) (172.18.0.14, executor 1, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:06:37 INFO BlockManagerInfo: Added broadcast_94_piece0 in memory on 172.18.0.14:37571 (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 12:06:37 INFO TaskSetManager: Starting task 2.0 in stage 98.0 (TID 200) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:06:37 INFO TaskSetManager: Finished task 1.0 in stage 98.0 (TID 199) in 19 ms on 172.18.0.14 (executor 1) (1/4)
26/02/13 12:06:37 INFO TaskSetManager: Starting task 3.0 in stage 98.0 (TID 201) (172.18.0.14, executor 1, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:06:37 INFO TaskSetManager: Finished task 0.0 in stage 98.0 (TID 198) in 23 ms on 172.18.0.14 (executor 1) (2/4)
26/02/13 12:06:37 INFO TaskSetManager: Finished task 2.0 in stage 98.0 (TID 200) in 15 ms on 172.18.0.14 (executor 1) (3/4)
26/02/13 12:06:37 INFO TaskSetManager: Finished task 3.0 in stage 98.0 (TID 201) in 13 ms on 172.18.0.14 (executor 1) (4/4)
26/02/13 12:06:37 INFO TaskSchedulerImpl: Removed TaskSet 98.0, whose tasks have all completed, from pool 
26/02/13 12:06:37 INFO DAGScheduler: ResultStage 98 (start at NativeMethodAccessorImpl.java:0) finished in 0.041 s
26/02/13 12:06:37 INFO DAGScheduler: Job 63 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 12:06:37 INFO TaskSchedulerImpl: Killing all running tasks in stage 98: Stage finished
26/02/13 12:06:37 INFO DAGScheduler: Job 63 finished: start at NativeMethodAccessorImpl.java:0, took 0.043400 s
26/02/13 12:06:37 INFO MemoryStore: Block broadcast_95_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.8 MiB)
26/02/13 12:06:37 INFO BlockManagerInfo: Added broadcast_95_piece0 in memory on spark-master:42073 (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 12:06:37 INFO SparkContext: Created broadcast 95 from start at NativeMethodAccessorImpl.java:0
26/02/13 12:06:37 INFO BlockManagerInfo: Removed broadcast_94_piece0 on spark-master:42073 in memory (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 12:06:37 INFO BlockManagerInfo: Removed broadcast_94_piece0 on 172.18.0.14:37571 in memory (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 12:06:37 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 150, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@3d8a4ba6]. The input RDD has 3 partitions.
26/02/13 12:06:37 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 12:06:37 INFO DAGScheduler: Got job 64 (start at NativeMethodAccessorImpl.java:0) with 3 output partitions
26/02/13 12:06:37 INFO DAGScheduler: Final stage: ResultStage 99 (start at NativeMethodAccessorImpl.java:0)
26/02/13 12:06:37 INFO DAGScheduler: Parents of final stage: List()
26/02/13 12:06:37 INFO DAGScheduler: Missing parents: List()
26/02/13 12:06:37 INFO DAGScheduler: Submitting ResultStage 99 (MapPartitionsRDD[346] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:06:37 INFO MemoryStore: Block broadcast_96 stored as values in memory (estimated size 49.9 KiB, free 433.9 MiB)
26/02/13 12:06:37 INFO MemoryStore: Block broadcast_96_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.9 MiB)
26/02/13 12:06:37 INFO BlockManagerInfo: Added broadcast_96_piece0 in memory on spark-master:42073 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 12:06:37 INFO SparkContext: Created broadcast 96 from broadcast at DAGScheduler.scala:1585
26/02/13 12:06:37 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 99 (MapPartitionsRDD[346] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
26/02/13 12:06:37 INFO TaskSchedulerImpl: Adding task set 99.0 with 3 tasks resource profile 0
26/02/13 12:06:37 INFO TaskSetManager: Starting task 1.0 in stage 99.0 (TID 202) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:06:37 INFO TaskSetManager: Starting task 0.0 in stage 99.0 (TID 203) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:06:37 INFO TaskSetManager: Starting task 2.0 in stage 99.0 (TID 204) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:06:37 INFO BlockManagerInfo: Added broadcast_96_piece0 in memory on 172.18.0.14:37571 (size: 19.5 KiB, free: 434.0 MiB)
26/02/13 12:06:37 INFO BlockManagerInfo: Added broadcast_96_piece0 in memory on 172.18.0.15:37717 (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 12:06:37 INFO BlockManagerInfo: Added broadcast_95_piece0 in memory on 172.18.0.14:37571 (size: 107.2 KiB, free: 433.9 MiB)
26/02/13 12:06:37 INFO BlockManagerInfo: Added broadcast_95_piece0 in memory on 172.18.0.15:37717 (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 12:06:38 INFO TaskSetManager: Finished task 0.0 in stage 99.0 (TID 203) in 575 ms on 172.18.0.14 (executor 1) (1/3)
26/02/13 12:06:38 INFO TaskSetManager: Finished task 2.0 in stage 99.0 (TID 204) in 577 ms on 172.18.0.14 (executor 1) (2/3)
26/02/13 12:06:38 INFO TaskSetManager: Finished task 1.0 in stage 99.0 (TID 202) in 587 ms on 172.18.0.15 (executor 0) (3/3)
26/02/13 12:06:38 INFO TaskSchedulerImpl: Removed TaskSet 99.0, whose tasks have all completed, from pool 
26/02/13 12:06:38 INFO DAGScheduler: ResultStage 99 (start at NativeMethodAccessorImpl.java:0) finished in 0.591 s
26/02/13 12:06:38 INFO DAGScheduler: Job 64 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 12:06:38 INFO TaskSchedulerImpl: Killing all running tasks in stage 99: Stage finished
26/02/13 12:06:38 INFO DAGScheduler: Job 64 finished: start at NativeMethodAccessorImpl.java:0, took 0.593151 s
26/02/13 12:06:38 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 150, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@3d8a4ba6] is committing.
26/02/13 12:06:38 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 150, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@3d8a4ba6] committed.
26/02/13 12:06:38 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/150 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.150.63ce7734-1cab-472a-b705-7545041f1ceb.tmp
26/02/13 12:06:38 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.150.63ce7734-1cab-472a-b705-7545041f1ceb.tmp to file:/tmp/spark-checkpoint-enrichment/commits/150
26/02/13 12:06:38 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "4a161d66-225b-4b29-812d-ddec3c05364e",
  "name" : null,
  "timestamp" : "2026-02-13T12:06:37.497Z",
  "batchId" : 150,
  "numInputRows" : 111,
  "inputRowsPerSecond" : 9250.0,
  "processedRowsPerSecond" : 117.21224920802536,
  "durationMs" : {
    "addBatch" : 727,
    "commitOffsets" : 61,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 50,
    "triggerExecution" : 947,
    "walCommit" : 106
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 1587,
        "1" : 1902,
        "0" : 1435
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 1624,
        "1" : 1939,
        "0" : 1472
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 1624,
        "1" : 1939,
        "0" : 1472
      }
    },
    "numInputRows" : 111,
    "inputRowsPerSecond" : 9250.0,
    "processedRowsPerSecond" : 117.21224920802536,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@1a6a42c2",
    "numOutputRows" : 27
  }
}
26/02/13 12:06:38 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/151 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.151.9f9feb32-c049-48b5-8f99-4271eb39e5cf.tmp
26/02/13 12:06:38 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.151.9f9feb32-c049-48b5-8f99-4271eb39e5cf.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/151
26/02/13 12:06:38 INFO MicroBatchExecution: Committed offsets for batch 151. Metadata OffsetSeqMetadata(0,1770984398446,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 12:06:38 INFO BlockManagerInfo: Removed broadcast_96_piece0 on spark-master:42073 in memory (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 12:06:38 INFO BlockManagerInfo: Removed broadcast_96_piece0 on 172.18.0.15:37717 in memory (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 12:06:38 INFO BlockManagerInfo: Removed broadcast_96_piece0 on 172.18.0.14:37571 in memory (size: 19.5 KiB, free: 434.0 MiB)
26/02/13 12:06:38 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:06:38 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:06:38 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:06:38 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:06:38 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:06:38 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:06:38 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#25863 - origin_code.nullCount#25862) > 0)
26/02/13 12:06:38 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#25868 - destination_code.nullCount#25867) > 0)
26/02/13 12:06:38 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#25898 - callsign.nullCount#25897) > 0)
26/02/13 12:06:38 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 12:06:38 INFO DAGScheduler: Got job 65 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 12:06:38 INFO DAGScheduler: Final stage: ResultStage 101 (start at NativeMethodAccessorImpl.java:0)
26/02/13 12:06:38 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 100)
26/02/13 12:06:38 INFO DAGScheduler: Missing parents: List()
26/02/13 12:06:38 INFO DAGScheduler: Submitting ResultStage 101 (MapPartitionsRDD[351] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:06:38 INFO MemoryStore: Block broadcast_97 stored as values in memory (estimated size 99.3 KiB, free 433.9 MiB)
26/02/13 12:06:38 INFO MemoryStore: Block broadcast_97_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 433.8 MiB)
26/02/13 12:06:38 INFO BlockManagerInfo: Added broadcast_97_piece0 in memory on spark-master:42073 (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 12:06:38 INFO SparkContext: Created broadcast 97 from broadcast at DAGScheduler.scala:1585
26/02/13 12:06:38 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 101 (MapPartitionsRDD[351] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 12:06:38 INFO TaskSchedulerImpl: Adding task set 101.0 with 4 tasks resource profile 0
26/02/13 12:06:38 INFO TaskSetManager: Starting task 0.0 in stage 101.0 (TID 205) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:06:38 INFO TaskSetManager: Starting task 1.0 in stage 101.0 (TID 206) (172.18.0.14, executor 1, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:06:38 INFO BlockManagerInfo: Added broadcast_97_piece0 in memory on 172.18.0.14:37571 (size: 33.3 KiB, free: 433.9 MiB)
26/02/13 12:06:38 INFO TaskSetManager: Starting task 2.0 in stage 101.0 (TID 207) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:06:38 INFO TaskSetManager: Finished task 0.0 in stage 101.0 (TID 205) in 33 ms on 172.18.0.14 (executor 1) (1/4)
26/02/13 12:06:38 INFO TaskSetManager: Starting task 3.0 in stage 101.0 (TID 208) (172.18.0.14, executor 1, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:06:38 INFO TaskSetManager: Finished task 1.0 in stage 101.0 (TID 206) in 42 ms on 172.18.0.14 (executor 1) (2/4)
26/02/13 12:06:38 INFO TaskSetManager: Finished task 3.0 in stage 101.0 (TID 208) in 26 ms on 172.18.0.14 (executor 1) (3/4)
26/02/13 12:06:38 INFO TaskSetManager: Finished task 2.0 in stage 101.0 (TID 207) in 35 ms on 172.18.0.14 (executor 1) (4/4)
26/02/13 12:06:38 INFO TaskSchedulerImpl: Removed TaskSet 101.0, whose tasks have all completed, from pool 
26/02/13 12:06:38 INFO DAGScheduler: ResultStage 101 (start at NativeMethodAccessorImpl.java:0) finished in 0.081 s
26/02/13 12:06:38 INFO DAGScheduler: Job 65 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 12:06:38 INFO TaskSchedulerImpl: Killing all running tasks in stage 101: Stage finished
26/02/13 12:06:38 INFO DAGScheduler: Job 65 finished: start at NativeMethodAccessorImpl.java:0, took 0.085041 s
26/02/13 12:06:38 INFO BlockManagerInfo: Removed broadcast_92_piece0 on spark-master:42073 in memory (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 12:06:38 INFO BlockManagerInfo: Removed broadcast_92_piece0 on 172.18.0.14:37571 in memory (size: 107.2 KiB, free: 434.0 MiB)
26/02/13 12:06:38 INFO BlockManagerInfo: Removed broadcast_92_piece0 on 172.18.0.15:37717 in memory (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 12:06:38 INFO MemoryStore: Block broadcast_98_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.8 MiB)
26/02/13 12:06:38 INFO BlockManagerInfo: Added broadcast_98_piece0 in memory on spark-master:42073 (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 12:06:38 INFO SparkContext: Created broadcast 98 from start at NativeMethodAccessorImpl.java:0
26/02/13 12:06:38 INFO BlockManagerInfo: Removed broadcast_95_piece0 on spark-master:42073 in memory (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 12:06:38 INFO BlockManagerInfo: Removed broadcast_95_piece0 on 172.18.0.14:37571 in memory (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 12:06:38 INFO BlockManagerInfo: Removed broadcast_95_piece0 on 172.18.0.15:37717 in memory (size: 107.2 KiB, free: 434.4 MiB)
26/02/13 12:06:38 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 151, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@72e342cc]. The input RDD has 3 partitions.
26/02/13 12:06:38 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 12:06:38 INFO DAGScheduler: Got job 66 (start at NativeMethodAccessorImpl.java:0) with 3 output partitions
26/02/13 12:06:38 INFO DAGScheduler: Final stage: ResultStage 102 (start at NativeMethodAccessorImpl.java:0)
26/02/13 12:06:38 INFO DAGScheduler: Parents of final stage: List()
26/02/13 12:06:38 INFO DAGScheduler: Missing parents: List()
26/02/13 12:06:38 INFO DAGScheduler: Submitting ResultStage 102 (MapPartitionsRDD[357] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:06:38 INFO MemoryStore: Block broadcast_99 stored as values in memory (estimated size 49.9 KiB, free 433.9 MiB)
26/02/13 12:06:38 INFO MemoryStore: Block broadcast_99_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.9 MiB)
26/02/13 12:06:38 INFO BlockManagerInfo: Added broadcast_99_piece0 in memory on spark-master:42073 (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 12:06:38 INFO SparkContext: Created broadcast 99 from broadcast at DAGScheduler.scala:1585
26/02/13 12:06:38 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 102 (MapPartitionsRDD[357] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
26/02/13 12:06:38 INFO TaskSchedulerImpl: Adding task set 102.0 with 3 tasks resource profile 0
26/02/13 12:06:38 INFO TaskSetManager: Starting task 1.0 in stage 102.0 (TID 209) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:06:38 INFO TaskSetManager: Starting task 0.0 in stage 102.0 (TID 210) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:06:38 INFO TaskSetManager: Starting task 2.0 in stage 102.0 (TID 211) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:06:38 INFO BlockManagerInfo: Added broadcast_99_piece0 in memory on 172.18.0.14:37571 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 12:06:38 INFO BlockManagerInfo: Added broadcast_99_piece0 in memory on 172.18.0.15:37717 (size: 19.5 KiB, free: 434.4 MiB)
26/02/13 12:06:38 INFO BlockManagerInfo: Added broadcast_98_piece0 in memory on 172.18.0.15:37717 (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 12:06:38 INFO BlockManagerInfo: Added broadcast_98_piece0 in memory on 172.18.0.14:37571 (size: 107.2 KiB, free: 434.0 MiB)
26/02/13 12:06:38 INFO TaskSetManager: Finished task 0.0 in stage 102.0 (TID 210) in 101 ms on 172.18.0.14 (executor 1) (1/3)
26/02/13 12:06:38 INFO TaskSetManager: Finished task 2.0 in stage 102.0 (TID 211) in 102 ms on 172.18.0.14 (executor 1) (2/3)
26/02/13 12:06:38 INFO TaskSetManager: Finished task 1.0 in stage 102.0 (TID 209) in 140 ms on 172.18.0.15 (executor 0) (3/3)
26/02/13 12:06:38 INFO TaskSchedulerImpl: Removed TaskSet 102.0, whose tasks have all completed, from pool 
26/02/13 12:06:38 INFO DAGScheduler: ResultStage 102 (start at NativeMethodAccessorImpl.java:0) finished in 0.147 s
26/02/13 12:06:38 INFO DAGScheduler: Job 66 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 12:06:38 INFO TaskSchedulerImpl: Killing all running tasks in stage 102: Stage finished
26/02/13 12:06:38 INFO DAGScheduler: Job 66 finished: start at NativeMethodAccessorImpl.java:0, took 0.149746 s
26/02/13 12:06:38 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 151, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@72e342cc] is committing.
26/02/13 12:06:38 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 151, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@72e342cc] committed.
26/02/13 12:06:38 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/151 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.151.8f709476-c817-415c-b7be-cbbf4441439c.tmp
26/02/13 12:06:39 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.151.8f709476-c817-415c-b7be-cbbf4441439c.tmp to file:/tmp/spark-checkpoint-enrichment/commits/151
26/02/13 12:06:39 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "4a161d66-225b-4b29-812d-ddec3c05364e",
  "name" : null,
  "timestamp" : "2026-02-13T12:06:38.445Z",
  "batchId" : 151,
  "numInputRows" : 131,
  "inputRowsPerSecond" : 138.18565400843883,
  "processedRowsPerSecond" : 227.43055555555557,
  "durationMs" : {
    "addBatch" : 396,
    "commitOffsets" : 76,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 47,
    "triggerExecution" : 576,
    "walCommit" : 56
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 1624,
        "1" : 1939,
        "0" : 1472
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 1670,
        "1" : 1994,
        "0" : 1502
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 1670,
        "1" : 1994,
        "0" : 1502
      }
    },
    "numInputRows" : 131,
    "inputRowsPerSecond" : 138.18565400843883,
    "processedRowsPerSecond" : 227.43055555555557,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@1a6a42c2",
    "numOutputRows" : 26
  }
}
26/02/13 12:06:48 INFO BlockManagerInfo: Removed broadcast_99_piece0 on spark-master:42073 in memory (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 12:06:48 INFO BlockManagerInfo: Removed broadcast_99_piece0 on 172.18.0.14:37571 in memory (size: 19.5 KiB, free: 434.0 MiB)
26/02/13 12:06:48 INFO BlockManagerInfo: Removed broadcast_99_piece0 on 172.18.0.15:37717 in memory (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 12:06:48 INFO BlockManagerInfo: Removed broadcast_97_piece0 on spark-master:42073 in memory (size: 33.3 KiB, free: 434.3 MiB)
26/02/13 12:06:48 INFO BlockManagerInfo: Removed broadcast_97_piece0 on 172.18.0.14:37571 in memory (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 12:06:49 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:06:53 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/152 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.152.ec496ced-15b7-4da2-8ff6-7185e3b085df.tmp
26/02/13 12:06:53 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.152.ec496ced-15b7-4da2-8ff6-7185e3b085df.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/152
26/02/13 12:06:53 INFO MicroBatchExecution: Committed offsets for batch 152. Metadata OffsetSeqMetadata(0,1770984413697,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 12:06:53 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:06:53 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:06:53 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:06:53 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:06:53 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:06:53 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:06:53 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#26717 - origin_code.nullCount#26716) > 0)
26/02/13 12:06:53 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#26722 - destination_code.nullCount#26721) > 0)
26/02/13 12:06:53 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#26752 - callsign.nullCount#26751) > 0)
26/02/13 12:06:53 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 12:06:53 INFO DAGScheduler: Got job 67 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 12:06:53 INFO DAGScheduler: Final stage: ResultStage 104 (start at NativeMethodAccessorImpl.java:0)
26/02/13 12:06:53 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 103)
26/02/13 12:06:53 INFO DAGScheduler: Missing parents: List()
26/02/13 12:06:53 INFO DAGScheduler: Submitting ResultStage 104 (MapPartitionsRDD[362] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:06:53 INFO MemoryStore: Block broadcast_100 stored as values in memory (estimated size 99.3 KiB, free 434.0 MiB)
26/02/13 12:06:53 INFO MemoryStore: Block broadcast_100_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 433.9 MiB)
26/02/13 12:06:53 INFO BlockManagerInfo: Added broadcast_100_piece0 in memory on spark-master:42073 (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 12:06:53 INFO BlockManagerInfo: Removed broadcast_98_piece0 on spark-master:42073 in memory (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 12:06:53 INFO SparkContext: Created broadcast 100 from broadcast at DAGScheduler.scala:1585
26/02/13 12:06:53 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 104 (MapPartitionsRDD[362] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 12:06:53 INFO TaskSchedulerImpl: Adding task set 104.0 with 4 tasks resource profile 0
26/02/13 12:06:53 INFO TaskSetManager: Starting task 0.0 in stage 104.0 (TID 212) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:06:53 INFO TaskSetManager: Starting task 1.0 in stage 104.0 (TID 213) (172.18.0.14, executor 1, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:06:53 INFO BlockManagerInfo: Removed broadcast_98_piece0 on 172.18.0.14:37571 in memory (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 12:06:53 INFO BlockManagerInfo: Removed broadcast_98_piece0 on 172.18.0.15:37717 in memory (size: 107.2 KiB, free: 434.4 MiB)
26/02/13 12:06:53 INFO BlockManagerInfo: Added broadcast_100_piece0 in memory on 172.18.0.14:37571 (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 12:06:53 INFO TaskSetManager: Starting task 2.0 in stage 104.0 (TID 214) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:06:53 INFO TaskSetManager: Finished task 1.0 in stage 104.0 (TID 213) in 24 ms on 172.18.0.14 (executor 1) (1/4)
26/02/13 12:06:53 INFO TaskSetManager: Starting task 3.0 in stage 104.0 (TID 215) (172.18.0.14, executor 1, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:06:53 INFO TaskSetManager: Finished task 0.0 in stage 104.0 (TID 212) in 28 ms on 172.18.0.14 (executor 1) (2/4)
26/02/13 12:06:53 INFO TaskSetManager: Finished task 2.0 in stage 104.0 (TID 214) in 19 ms on 172.18.0.14 (executor 1) (3/4)
26/02/13 12:06:53 INFO TaskSetManager: Finished task 3.0 in stage 104.0 (TID 215) in 20 ms on 172.18.0.14 (executor 1) (4/4)
26/02/13 12:06:53 INFO TaskSchedulerImpl: Removed TaskSet 104.0, whose tasks have all completed, from pool 
26/02/13 12:06:53 INFO DAGScheduler: ResultStage 104 (start at NativeMethodAccessorImpl.java:0) finished in 0.057 s
26/02/13 12:06:53 INFO DAGScheduler: Job 67 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 12:06:53 INFO TaskSchedulerImpl: Killing all running tasks in stage 104: Stage finished
26/02/13 12:06:53 INFO DAGScheduler: Job 67 finished: start at NativeMethodAccessorImpl.java:0, took 0.060238 s
26/02/13 12:06:53 INFO MemoryStore: Block broadcast_101_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.9 MiB)
26/02/13 12:06:53 INFO BlockManagerInfo: Added broadcast_101_piece0 in memory on spark-master:42073 (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 12:06:53 INFO SparkContext: Created broadcast 101 from start at NativeMethodAccessorImpl.java:0
26/02/13 12:06:53 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 152, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@7fdcc4d4]. The input RDD has 3 partitions.
26/02/13 12:06:53 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 12:06:53 INFO DAGScheduler: Got job 68 (start at NativeMethodAccessorImpl.java:0) with 3 output partitions
26/02/13 12:06:53 INFO DAGScheduler: Final stage: ResultStage 105 (start at NativeMethodAccessorImpl.java:0)
26/02/13 12:06:53 INFO DAGScheduler: Parents of final stage: List()
26/02/13 12:06:53 INFO DAGScheduler: Missing parents: List()
26/02/13 12:06:53 INFO DAGScheduler: Submitting ResultStage 105 (MapPartitionsRDD[368] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:06:53 INFO MemoryStore: Block broadcast_102 stored as values in memory (estimated size 49.9 KiB, free 433.9 MiB)
26/02/13 12:06:53 INFO MemoryStore: Block broadcast_102_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.9 MiB)
26/02/13 12:06:53 INFO BlockManagerInfo: Added broadcast_102_piece0 in memory on spark-master:42073 (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 12:06:53 INFO SparkContext: Created broadcast 102 from broadcast at DAGScheduler.scala:1585
26/02/13 12:06:53 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 105 (MapPartitionsRDD[368] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
26/02/13 12:06:53 INFO TaskSchedulerImpl: Adding task set 105.0 with 3 tasks resource profile 0
26/02/13 12:06:53 INFO TaskSetManager: Starting task 1.0 in stage 105.0 (TID 216) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:06:53 INFO TaskSetManager: Starting task 0.0 in stage 105.0 (TID 217) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:06:53 INFO TaskSetManager: Starting task 2.0 in stage 105.0 (TID 218) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:06:54 INFO BlockManagerInfo: Added broadcast_102_piece0 in memory on 172.18.0.14:37571 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 12:06:54 INFO BlockManagerInfo: Added broadcast_102_piece0 in memory on 172.18.0.15:37717 (size: 19.5 KiB, free: 434.4 MiB)
26/02/13 12:06:54 INFO BlockManagerInfo: Added broadcast_101_piece0 in memory on 172.18.0.14:37571 (size: 107.2 KiB, free: 434.0 MiB)
26/02/13 12:06:54 INFO BlockManagerInfo: Added broadcast_101_piece0 in memory on 172.18.0.15:37717 (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 12:06:54 INFO TaskSetManager: Finished task 0.0 in stage 105.0 (TID 217) in 569 ms on 172.18.0.14 (executor 1) (1/3)
26/02/13 12:06:54 INFO TaskSetManager: Finished task 2.0 in stage 105.0 (TID 218) in 572 ms on 172.18.0.14 (executor 1) (2/3)
26/02/13 12:06:54 INFO TaskSetManager: Finished task 1.0 in stage 105.0 (TID 216) in 616 ms on 172.18.0.15 (executor 0) (3/3)
26/02/13 12:06:54 INFO TaskSchedulerImpl: Removed TaskSet 105.0, whose tasks have all completed, from pool 
26/02/13 12:06:54 INFO DAGScheduler: ResultStage 105 (start at NativeMethodAccessorImpl.java:0) finished in 0.621 s
26/02/13 12:06:54 INFO DAGScheduler: Job 68 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 12:06:54 INFO TaskSchedulerImpl: Killing all running tasks in stage 105: Stage finished
26/02/13 12:06:54 INFO DAGScheduler: Job 68 finished: start at NativeMethodAccessorImpl.java:0, took 0.623046 s
26/02/13 12:06:54 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 152, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@7fdcc4d4] is committing.
26/02/13 12:06:54 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 152, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@7fdcc4d4] committed.
26/02/13 12:06:54 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/152 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.152.ddf3d09b-b518-456c-9e1c-403974627c33.tmp
26/02/13 12:06:54 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.152.ddf3d09b-b518-456c-9e1c-403974627c33.tmp to file:/tmp/spark-checkpoint-enrichment/commits/152
26/02/13 12:06:54 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "4a161d66-225b-4b29-812d-ddec3c05364e",
  "name" : null,
  "timestamp" : "2026-02-13T12:06:53.696Z",
  "batchId" : 152,
  "numInputRows" : 111,
  "inputRowsPerSecond" : 9250.0,
  "processedRowsPerSecond" : 113.14984709480123,
  "durationMs" : {
    "addBatch" : 780,
    "commitOffsets" : 68,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 32,
    "triggerExecution" : 981,
    "walCommit" : 100
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 1670,
        "1" : 1994,
        "0" : 1502
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 1707,
        "1" : 2031,
        "0" : 1539
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 1707,
        "1" : 2031,
        "0" : 1539
      }
    },
    "numInputRows" : 111,
    "inputRowsPerSecond" : 9250.0,
    "processedRowsPerSecond" : 113.14984709480123,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@1a6a42c2",
    "numOutputRows" : 27
  }
}
26/02/13 12:06:54 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/153 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.153.d291f1c4-f3cd-41e1-948f-4ce2ad11ec82.tmp
26/02/13 12:06:54 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.153.d291f1c4-f3cd-41e1-948f-4ce2ad11ec82.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/153
26/02/13 12:06:54 INFO MicroBatchExecution: Committed offsets for batch 153. Metadata OffsetSeqMetadata(0,1770984414679,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 12:06:54 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:06:54 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:06:54 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:06:54 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:06:54 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:06:54 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:06:54 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#27571 - origin_code.nullCount#27570) > 0)
26/02/13 12:06:54 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#27576 - destination_code.nullCount#27575) > 0)
26/02/13 12:06:54 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#27606 - callsign.nullCount#27605) > 0)
26/02/13 12:06:54 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 12:06:54 INFO DAGScheduler: Got job 69 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 12:06:54 INFO DAGScheduler: Final stage: ResultStage 107 (start at NativeMethodAccessorImpl.java:0)
26/02/13 12:06:54 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 106)
26/02/13 12:06:54 INFO DAGScheduler: Missing parents: List()
26/02/13 12:06:54 INFO DAGScheduler: Submitting ResultStage 107 (MapPartitionsRDD[373] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:06:54 INFO MemoryStore: Block broadcast_103 stored as values in memory (estimated size 99.3 KiB, free 433.8 MiB)
26/02/13 12:06:54 INFO MemoryStore: Block broadcast_103_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 433.7 MiB)
26/02/13 12:06:54 INFO BlockManagerInfo: Added broadcast_103_piece0 in memory on spark-master:42073 (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 12:06:54 INFO SparkContext: Created broadcast 103 from broadcast at DAGScheduler.scala:1585
26/02/13 12:06:54 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 107 (MapPartitionsRDD[373] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 12:06:54 INFO TaskSchedulerImpl: Adding task set 107.0 with 4 tasks resource profile 0
26/02/13 12:06:54 INFO TaskSetManager: Starting task 0.0 in stage 107.0 (TID 219) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:06:54 INFO TaskSetManager: Starting task 1.0 in stage 107.0 (TID 220) (172.18.0.14, executor 1, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:06:54 INFO BlockManagerInfo: Added broadcast_103_piece0 in memory on 172.18.0.14:37571 (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 12:06:54 INFO TaskSetManager: Starting task 2.0 in stage 107.0 (TID 221) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:06:54 INFO TaskSetManager: Finished task 1.0 in stage 107.0 (TID 220) in 21 ms on 172.18.0.14 (executor 1) (1/4)
26/02/13 12:06:54 INFO TaskSetManager: Starting task 3.0 in stage 107.0 (TID 222) (172.18.0.14, executor 1, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:06:54 INFO TaskSetManager: Finished task 0.0 in stage 107.0 (TID 219) in 23 ms on 172.18.0.14 (executor 1) (2/4)
26/02/13 12:06:54 INFO TaskSetManager: Finished task 3.0 in stage 107.0 (TID 222) in 15 ms on 172.18.0.14 (executor 1) (3/4)
26/02/13 12:06:54 INFO TaskSetManager: Finished task 2.0 in stage 107.0 (TID 221) in 18 ms on 172.18.0.14 (executor 1) (4/4)
26/02/13 12:06:54 INFO TaskSchedulerImpl: Removed TaskSet 107.0, whose tasks have all completed, from pool 
26/02/13 12:06:54 INFO DAGScheduler: ResultStage 107 (start at NativeMethodAccessorImpl.java:0) finished in 0.045 s
26/02/13 12:06:54 INFO DAGScheduler: Job 69 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 12:06:54 INFO TaskSchedulerImpl: Killing all running tasks in stage 107: Stage finished
26/02/13 12:06:54 INFO DAGScheduler: Job 69 finished: start at NativeMethodAccessorImpl.java:0, took 0.048192 s
26/02/13 12:06:54 INFO MemoryStore: Block broadcast_104_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.6 MiB)
26/02/13 12:06:54 INFO BlockManagerInfo: Added broadcast_104_piece0 in memory on spark-master:42073 (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 12:06:54 INFO SparkContext: Created broadcast 104 from start at NativeMethodAccessorImpl.java:0
26/02/13 12:06:54 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 153, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@785d7119]. The input RDD has 3 partitions.
26/02/13 12:06:54 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 12:06:54 INFO DAGScheduler: Got job 70 (start at NativeMethodAccessorImpl.java:0) with 3 output partitions
26/02/13 12:06:54 INFO DAGScheduler: Final stage: ResultStage 108 (start at NativeMethodAccessorImpl.java:0)
26/02/13 12:06:54 INFO DAGScheduler: Parents of final stage: List()
26/02/13 12:06:54 INFO DAGScheduler: Missing parents: List()
26/02/13 12:06:54 INFO DAGScheduler: Submitting ResultStage 108 (MapPartitionsRDD[379] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:06:54 INFO MemoryStore: Block broadcast_105 stored as values in memory (estimated size 49.9 KiB, free 433.6 MiB)
26/02/13 12:06:54 INFO MemoryStore: Block broadcast_105_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.6 MiB)
26/02/13 12:06:54 INFO BlockManagerInfo: Added broadcast_105_piece0 in memory on spark-master:42073 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 12:06:54 INFO BlockManagerInfo: Removed broadcast_101_piece0 on spark-master:42073 in memory (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 12:06:54 INFO SparkContext: Created broadcast 105 from broadcast at DAGScheduler.scala:1585
26/02/13 12:06:54 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 108 (MapPartitionsRDD[379] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
26/02/13 12:06:54 INFO TaskSchedulerImpl: Adding task set 108.0 with 3 tasks resource profile 0
26/02/13 12:06:54 INFO BlockManagerInfo: Removed broadcast_101_piece0 on 172.18.0.14:37571 in memory (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 12:06:54 INFO BlockManagerInfo: Removed broadcast_101_piece0 on 172.18.0.15:37717 in memory (size: 107.2 KiB, free: 434.4 MiB)
26/02/13 12:06:54 INFO TaskSetManager: Starting task 0.0 in stage 108.0 (TID 223) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:06:54 INFO TaskSetManager: Starting task 1.0 in stage 108.0 (TID 224) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:06:54 INFO TaskSetManager: Starting task 2.0 in stage 108.0 (TID 225) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:06:54 INFO BlockManagerInfo: Removed broadcast_103_piece0 on spark-master:42073 in memory (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 12:06:54 INFO BlockManagerInfo: Added broadcast_105_piece0 in memory on 172.18.0.14:37571 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 12:06:54 INFO BlockManagerInfo: Removed broadcast_103_piece0 on 172.18.0.14:37571 in memory (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 12:06:54 INFO BlockManagerInfo: Added broadcast_105_piece0 in memory on 172.18.0.15:37717 (size: 19.5 KiB, free: 434.4 MiB)
26/02/13 12:06:54 INFO BlockManagerInfo: Removed broadcast_100_piece0 on spark-master:42073 in memory (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 12:06:54 INFO BlockManagerInfo: Removed broadcast_100_piece0 on 172.18.0.14:37571 in memory (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 12:06:54 INFO BlockManagerInfo: Removed broadcast_102_piece0 on spark-master:42073 in memory (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 12:06:54 INFO BlockManagerInfo: Added broadcast_104_piece0 in memory on 172.18.0.14:37571 (size: 107.2 KiB, free: 434.0 MiB)
26/02/13 12:06:54 INFO BlockManagerInfo: Removed broadcast_102_piece0 on 172.18.0.14:37571 in memory (size: 19.5 KiB, free: 434.0 MiB)
26/02/13 12:06:54 INFO BlockManagerInfo: Removed broadcast_102_piece0 on 172.18.0.15:37717 in memory (size: 19.5 KiB, free: 434.4 MiB)
26/02/13 12:06:55 INFO BlockManagerInfo: Added broadcast_104_piece0 in memory on 172.18.0.15:37717 (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 12:06:55 INFO TaskSetManager: Finished task 0.0 in stage 108.0 (TID 223) in 99 ms on 172.18.0.14 (executor 1) (1/3)
26/02/13 12:06:55 INFO TaskSetManager: Finished task 2.0 in stage 108.0 (TID 225) in 98 ms on 172.18.0.14 (executor 1) (2/3)
26/02/13 12:06:55 INFO TaskSetManager: Finished task 1.0 in stage 108.0 (TID 224) in 137 ms on 172.18.0.15 (executor 0) (3/3)
26/02/13 12:06:55 INFO TaskSchedulerImpl: Removed TaskSet 108.0, whose tasks have all completed, from pool 
26/02/13 12:06:55 INFO DAGScheduler: ResultStage 108 (start at NativeMethodAccessorImpl.java:0) finished in 0.150 s
26/02/13 12:06:55 INFO DAGScheduler: Job 70 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 12:06:55 INFO TaskSchedulerImpl: Killing all running tasks in stage 108: Stage finished
26/02/13 12:06:55 INFO DAGScheduler: Job 70 finished: start at NativeMethodAccessorImpl.java:0, took 0.152297 s
26/02/13 12:06:55 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 153, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@785d7119] is committing.
26/02/13 12:06:55 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 153, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@785d7119] committed.
26/02/13 12:06:55 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/153 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.153.c5d0c2ef-a82d-4425-a679-49aa0e696425.tmp
26/02/13 12:06:55 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.153.c5d0c2ef-a82d-4425-a679-49aa0e696425.tmp to file:/tmp/spark-checkpoint-enrichment/commits/153
26/02/13 12:06:55 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "4a161d66-225b-4b29-812d-ddec3c05364e",
  "name" : null,
  "timestamp" : "2026-02-13T12:06:54.678Z",
  "batchId" : 153,
  "numInputRows" : 134,
  "inputRowsPerSecond" : 136.4562118126273,
  "processedRowsPerSecond" : 286.9379014989293,
  "durationMs" : {
    "addBatch" : 305,
    "commitOffsets" : 70,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 27,
    "triggerExecution" : 467,
    "walCommit" : 64
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 1707,
        "1" : 2031,
        "0" : 1539
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 1755,
        "1" : 2085,
        "0" : 1571
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 1755,
        "1" : 2085,
        "0" : 1571
      }
    },
    "numInputRows" : 134,
    "inputRowsPerSecond" : 136.4562118126273,
    "processedRowsPerSecond" : 286.9379014989293,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@1a6a42c2",
    "numOutputRows" : 26
  }
}
26/02/13 12:06:55 INFO BlockManagerInfo: Removed broadcast_105_piece0 on spark-master:42073 in memory (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 12:06:55 INFO BlockManagerInfo: Removed broadcast_105_piece0 on 172.18.0.14:37571 in memory (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 12:06:55 INFO BlockManagerInfo: Removed broadcast_105_piece0 on 172.18.0.15:37717 in memory (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 12:07:05 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:07:10 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/154 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.154.fe85c22a-9b50-492b-b345-763eb51424b6.tmp
26/02/13 12:07:10 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.154.fe85c22a-9b50-492b-b345-763eb51424b6.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/154
26/02/13 12:07:10 INFO MicroBatchExecution: Committed offsets for batch 154. Metadata OffsetSeqMetadata(0,1770984430569,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 12:07:10 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:07:10 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:07:10 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:07:10 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:07:10 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:07:10 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:07:10 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#28425 - origin_code.nullCount#28424) > 0)
26/02/13 12:07:10 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#28430 - destination_code.nullCount#28429) > 0)
26/02/13 12:07:10 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#28460 - callsign.nullCount#28459) > 0)
26/02/13 12:07:10 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 12:07:10 INFO DAGScheduler: Got job 71 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 12:07:10 INFO DAGScheduler: Final stage: ResultStage 110 (start at NativeMethodAccessorImpl.java:0)
26/02/13 12:07:10 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 109)
26/02/13 12:07:10 INFO DAGScheduler: Missing parents: List()
26/02/13 12:07:10 INFO DAGScheduler: Submitting ResultStage 110 (MapPartitionsRDD[384] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:07:10 INFO MemoryStore: Block broadcast_106 stored as values in memory (estimated size 99.3 KiB, free 434.0 MiB)
26/02/13 12:07:10 INFO MemoryStore: Block broadcast_106_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 433.9 MiB)
26/02/13 12:07:10 INFO BlockManagerInfo: Added broadcast_106_piece0 in memory on spark-master:42073 (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 12:07:10 INFO SparkContext: Created broadcast 106 from broadcast at DAGScheduler.scala:1585
26/02/13 12:07:10 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 110 (MapPartitionsRDD[384] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 12:07:10 INFO TaskSchedulerImpl: Adding task set 110.0 with 4 tasks resource profile 0
26/02/13 12:07:10 INFO TaskSetManager: Starting task 0.0 in stage 110.0 (TID 226) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:07:10 INFO TaskSetManager: Starting task 1.0 in stage 110.0 (TID 227) (172.18.0.14, executor 1, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:07:10 INFO BlockManagerInfo: Added broadcast_106_piece0 in memory on 172.18.0.14:37571 (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 12:07:10 INFO TaskSetManager: Starting task 2.0 in stage 110.0 (TID 228) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:07:10 INFO TaskSetManager: Starting task 3.0 in stage 110.0 (TID 229) (172.18.0.14, executor 1, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:07:10 INFO TaskSetManager: Finished task 1.0 in stage 110.0 (TID 227) in 21 ms on 172.18.0.14 (executor 1) (1/4)
26/02/13 12:07:10 INFO TaskSetManager: Finished task 0.0 in stage 110.0 (TID 226) in 22 ms on 172.18.0.14 (executor 1) (2/4)
26/02/13 12:07:10 INFO TaskSetManager: Finished task 2.0 in stage 110.0 (TID 228) in 15 ms on 172.18.0.14 (executor 1) (3/4)
26/02/13 12:07:10 INFO TaskSetManager: Finished task 3.0 in stage 110.0 (TID 229) in 17 ms on 172.18.0.14 (executor 1) (4/4)
26/02/13 12:07:10 INFO TaskSchedulerImpl: Removed TaskSet 110.0, whose tasks have all completed, from pool 
26/02/13 12:07:10 INFO DAGScheduler: ResultStage 110 (start at NativeMethodAccessorImpl.java:0) finished in 0.044 s
26/02/13 12:07:10 INFO DAGScheduler: Job 71 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 12:07:10 INFO TaskSchedulerImpl: Killing all running tasks in stage 110: Stage finished
26/02/13 12:07:10 INFO DAGScheduler: Job 71 finished: start at NativeMethodAccessorImpl.java:0, took 0.046588 s
26/02/13 12:07:10 INFO MemoryStore: Block broadcast_107_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.8 MiB)
26/02/13 12:07:10 INFO BlockManagerInfo: Added broadcast_107_piece0 in memory on spark-master:42073 (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 12:07:10 INFO SparkContext: Created broadcast 107 from start at NativeMethodAccessorImpl.java:0
26/02/13 12:07:10 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 154, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@7c9eb24f]. The input RDD has 3 partitions.
26/02/13 12:07:10 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 12:07:10 INFO DAGScheduler: Got job 72 (start at NativeMethodAccessorImpl.java:0) with 3 output partitions
26/02/13 12:07:10 INFO DAGScheduler: Final stage: ResultStage 111 (start at NativeMethodAccessorImpl.java:0)
26/02/13 12:07:10 INFO DAGScheduler: Parents of final stage: List()
26/02/13 12:07:10 INFO DAGScheduler: Missing parents: List()
26/02/13 12:07:10 INFO DAGScheduler: Submitting ResultStage 111 (MapPartitionsRDD[390] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:07:10 INFO MemoryStore: Block broadcast_108 stored as values in memory (estimated size 49.9 KiB, free 433.8 MiB)
26/02/13 12:07:10 INFO MemoryStore: Block broadcast_108_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.8 MiB)
26/02/13 12:07:10 INFO BlockManagerInfo: Added broadcast_108_piece0 in memory on spark-master:42073 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 12:07:10 INFO SparkContext: Created broadcast 108 from broadcast at DAGScheduler.scala:1585
26/02/13 12:07:10 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 111 (MapPartitionsRDD[390] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
26/02/13 12:07:10 INFO TaskSchedulerImpl: Adding task set 111.0 with 3 tasks resource profile 0
26/02/13 12:07:10 INFO TaskSetManager: Starting task 0.0 in stage 111.0 (TID 230) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:07:10 INFO TaskSetManager: Starting task 1.0 in stage 111.0 (TID 231) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:07:10 INFO TaskSetManager: Starting task 2.0 in stage 111.0 (TID 232) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:07:10 INFO BlockManagerInfo: Added broadcast_108_piece0 in memory on 172.18.0.14:37571 (size: 19.5 KiB, free: 434.0 MiB)
26/02/13 12:07:10 INFO BlockManagerInfo: Added broadcast_108_piece0 in memory on 172.18.0.15:37717 (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 12:07:10 INFO BlockManagerInfo: Added broadcast_107_piece0 in memory on 172.18.0.14:37571 (size: 107.2 KiB, free: 433.9 MiB)
26/02/13 12:07:10 INFO BlockManagerInfo: Added broadcast_107_piece0 in memory on 172.18.0.15:37717 (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 12:07:11 INFO TaskSetManager: Finished task 0.0 in stage 111.0 (TID 230) in 570 ms on 172.18.0.14 (executor 1) (1/3)
26/02/13 12:07:11 INFO TaskSetManager: Finished task 2.0 in stage 111.0 (TID 232) in 570 ms on 172.18.0.14 (executor 1) (2/3)
26/02/13 12:07:11 INFO TaskSetManager: Finished task 1.0 in stage 111.0 (TID 231) in 592 ms on 172.18.0.15 (executor 0) (3/3)
26/02/13 12:07:11 INFO TaskSchedulerImpl: Removed TaskSet 111.0, whose tasks have all completed, from pool 
26/02/13 12:07:11 INFO DAGScheduler: ResultStage 111 (start at NativeMethodAccessorImpl.java:0) finished in 0.598 s
26/02/13 12:07:11 INFO DAGScheduler: Job 72 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 12:07:11 INFO TaskSchedulerImpl: Killing all running tasks in stage 111: Stage finished
26/02/13 12:07:11 INFO DAGScheduler: Job 72 finished: start at NativeMethodAccessorImpl.java:0, took 0.599902 s
26/02/13 12:07:11 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 154, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@7c9eb24f] is committing.
26/02/13 12:07:11 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 154, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@7c9eb24f] committed.
26/02/13 12:07:11 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/154 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.154.594659da-a620-4540-b57d-531e4d8a769d.tmp
26/02/13 12:07:11 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.154.594659da-a620-4540-b57d-531e4d8a769d.tmp to file:/tmp/spark-checkpoint-enrichment/commits/154
26/02/13 12:07:11 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "4a161d66-225b-4b29-812d-ddec3c05364e",
  "name" : null,
  "timestamp" : "2026-02-13T12:07:10.568Z",
  "batchId" : 154,
  "numInputRows" : 111,
  "inputRowsPerSecond" : 10090.909090909092,
  "processedRowsPerSecond" : 114.31513903192585,
  "durationMs" : {
    "addBatch" : 755,
    "commitOffsets" : 57,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 47,
    "triggerExecution" : 971,
    "walCommit" : 111
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 1755,
        "1" : 2085,
        "0" : 1571
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 1792,
        "1" : 2122,
        "0" : 1608
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 1792,
        "1" : 2122,
        "0" : 1608
      }
    },
    "numInputRows" : 111,
    "inputRowsPerSecond" : 10090.909090909092,
    "processedRowsPerSecond" : 114.31513903192585,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@1a6a42c2",
    "numOutputRows" : 28
  }
}
26/02/13 12:07:11 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/155 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.155.5beed3c8-4d15-43d4-815f-60c41e580668.tmp
26/02/13 12:07:11 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.155.5beed3c8-4d15-43d4-815f-60c41e580668.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/155
26/02/13 12:07:11 INFO MicroBatchExecution: Committed offsets for batch 155. Metadata OffsetSeqMetadata(0,1770984431540,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 12:07:11 INFO BlockManagerInfo: Removed broadcast_108_piece0 on spark-master:42073 in memory (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 12:07:11 INFO BlockManagerInfo: Removed broadcast_108_piece0 on 172.18.0.15:37717 in memory (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 12:07:11 INFO BlockManagerInfo: Removed broadcast_108_piece0 on 172.18.0.14:37571 in memory (size: 19.5 KiB, free: 433.9 MiB)
26/02/13 12:07:11 INFO BlockManagerInfo: Removed broadcast_107_piece0 on spark-master:42073 in memory (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 12:07:11 INFO BlockManagerInfo: Removed broadcast_107_piece0 on 172.18.0.14:37571 in memory (size: 107.2 KiB, free: 434.0 MiB)
26/02/13 12:07:11 INFO BlockManagerInfo: Removed broadcast_107_piece0 on 172.18.0.15:37717 in memory (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 12:07:11 INFO BlockManagerInfo: Removed broadcast_106_piece0 on spark-master:42073 in memory (size: 33.3 KiB, free: 434.3 MiB)
26/02/13 12:07:11 INFO BlockManagerInfo: Removed broadcast_106_piece0 on 172.18.0.14:37571 in memory (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 12:07:11 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:07:11 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:07:11 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:07:11 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:07:11 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:07:11 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:07:11 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#29279 - origin_code.nullCount#29278) > 0)
26/02/13 12:07:11 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#29284 - destination_code.nullCount#29283) > 0)
26/02/13 12:07:11 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#29314 - callsign.nullCount#29313) > 0)
26/02/13 12:07:11 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 12:07:11 INFO DAGScheduler: Got job 73 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 12:07:11 INFO DAGScheduler: Final stage: ResultStage 113 (start at NativeMethodAccessorImpl.java:0)
26/02/13 12:07:11 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 112)
26/02/13 12:07:11 INFO DAGScheduler: Missing parents: List()
26/02/13 12:07:11 INFO DAGScheduler: Submitting ResultStage 113 (MapPartitionsRDD[395] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:07:11 INFO MemoryStore: Block broadcast_109 stored as values in memory (estimated size 99.3 KiB, free 434.0 MiB)
26/02/13 12:07:11 INFO MemoryStore: Block broadcast_109_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 433.9 MiB)
26/02/13 12:07:11 INFO BlockManagerInfo: Added broadcast_109_piece0 in memory on spark-master:42073 (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 12:07:11 INFO SparkContext: Created broadcast 109 from broadcast at DAGScheduler.scala:1585
26/02/13 12:07:11 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 113 (MapPartitionsRDD[395] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 12:07:11 INFO TaskSchedulerImpl: Adding task set 113.0 with 4 tasks resource profile 0
26/02/13 12:07:11 INFO TaskSetManager: Starting task 0.0 in stage 113.0 (TID 233) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:07:11 INFO TaskSetManager: Starting task 1.0 in stage 113.0 (TID 234) (172.18.0.14, executor 1, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:07:11 INFO BlockManagerInfo: Added broadcast_109_piece0 in memory on 172.18.0.14:37571 (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 12:07:11 INFO TaskSetManager: Starting task 2.0 in stage 113.0 (TID 235) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:07:11 INFO TaskSetManager: Finished task 0.0 in stage 113.0 (TID 233) in 17 ms on 172.18.0.14 (executor 1) (1/4)
26/02/13 12:07:11 INFO TaskSetManager: Starting task 3.0 in stage 113.0 (TID 236) (172.18.0.14, executor 1, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:07:11 INFO TaskSetManager: Finished task 1.0 in stage 113.0 (TID 234) in 21 ms on 172.18.0.14 (executor 1) (2/4)
26/02/13 12:07:11 INFO TaskSetManager: Finished task 2.0 in stage 113.0 (TID 235) in 12 ms on 172.18.0.14 (executor 1) (3/4)
26/02/13 12:07:11 INFO TaskSetManager: Finished task 3.0 in stage 113.0 (TID 236) in 14 ms on 172.18.0.14 (executor 1) (4/4)
26/02/13 12:07:11 INFO TaskSchedulerImpl: Removed TaskSet 113.0, whose tasks have all completed, from pool 
26/02/13 12:07:11 INFO DAGScheduler: ResultStage 113 (start at NativeMethodAccessorImpl.java:0) finished in 0.044 s
26/02/13 12:07:11 INFO DAGScheduler: Job 73 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 12:07:11 INFO TaskSchedulerImpl: Killing all running tasks in stage 113: Stage finished
26/02/13 12:07:11 INFO DAGScheduler: Job 73 finished: start at NativeMethodAccessorImpl.java:0, took 0.045869 s
26/02/13 12:07:11 INFO MemoryStore: Block broadcast_110_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.8 MiB)
26/02/13 12:07:11 INFO BlockManagerInfo: Added broadcast_110_piece0 in memory on spark-master:42073 (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 12:07:11 INFO SparkContext: Created broadcast 110 from start at NativeMethodAccessorImpl.java:0
26/02/13 12:07:11 INFO BlockManagerInfo: Removed broadcast_104_piece0 on spark-master:42073 in memory (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 12:07:11 INFO BlockManagerInfo: Removed broadcast_104_piece0 on 172.18.0.15:37717 in memory (size: 107.2 KiB, free: 434.4 MiB)
26/02/13 12:07:11 INFO BlockManagerInfo: Removed broadcast_104_piece0 on 172.18.0.14:37571 in memory (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 12:07:11 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 155, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@41c83377]. The input RDD has 3 partitions.
26/02/13 12:07:11 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 12:07:11 INFO DAGScheduler: Got job 74 (start at NativeMethodAccessorImpl.java:0) with 3 output partitions
26/02/13 12:07:11 INFO DAGScheduler: Final stage: ResultStage 114 (start at NativeMethodAccessorImpl.java:0)
26/02/13 12:07:11 INFO DAGScheduler: Parents of final stage: List()
26/02/13 12:07:11 INFO DAGScheduler: Missing parents: List()
26/02/13 12:07:11 INFO DAGScheduler: Submitting ResultStage 114 (MapPartitionsRDD[401] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:07:11 INFO MemoryStore: Block broadcast_111 stored as values in memory (estimated size 49.9 KiB, free 433.9 MiB)
26/02/13 12:07:11 INFO MemoryStore: Block broadcast_111_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.9 MiB)
26/02/13 12:07:11 INFO BlockManagerInfo: Added broadcast_111_piece0 in memory on spark-master:42073 (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 12:07:11 INFO SparkContext: Created broadcast 111 from broadcast at DAGScheduler.scala:1585
26/02/13 12:07:11 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 114 (MapPartitionsRDD[401] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
26/02/13 12:07:11 INFO TaskSchedulerImpl: Adding task set 114.0 with 3 tasks resource profile 0
26/02/13 12:07:11 INFO TaskSetManager: Starting task 1.0 in stage 114.0 (TID 237) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:07:11 INFO TaskSetManager: Starting task 0.0 in stage 114.0 (TID 238) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:07:11 INFO TaskSetManager: Starting task 2.0 in stage 114.0 (TID 239) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:07:11 INFO BlockManagerInfo: Added broadcast_111_piece0 in memory on 172.18.0.14:37571 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 12:07:11 INFO BlockManagerInfo: Added broadcast_111_piece0 in memory on 172.18.0.15:37717 (size: 19.5 KiB, free: 434.4 MiB)
26/02/13 12:07:11 INFO BlockManagerInfo: Added broadcast_110_piece0 in memory on 172.18.0.14:37571 (size: 107.2 KiB, free: 434.0 MiB)
26/02/13 12:07:11 INFO BlockManagerInfo: Added broadcast_110_piece0 in memory on 172.18.0.15:37717 (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 12:07:11 INFO TaskSetManager: Finished task 2.0 in stage 114.0 (TID 239) in 55 ms on 172.18.0.14 (executor 1) (1/3)
26/02/13 12:07:11 INFO TaskSetManager: Finished task 0.0 in stage 114.0 (TID 238) in 57 ms on 172.18.0.14 (executor 1) (2/3)
26/02/13 12:07:11 INFO TaskSetManager: Finished task 1.0 in stage 114.0 (TID 237) in 81 ms on 172.18.0.15 (executor 0) (3/3)
26/02/13 12:07:11 INFO TaskSchedulerImpl: Removed TaskSet 114.0, whose tasks have all completed, from pool 
26/02/13 12:07:11 INFO DAGScheduler: ResultStage 114 (start at NativeMethodAccessorImpl.java:0) finished in 0.086 s
26/02/13 12:07:11 INFO DAGScheduler: Job 74 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 12:07:11 INFO TaskSchedulerImpl: Killing all running tasks in stage 114: Stage finished
26/02/13 12:07:11 INFO DAGScheduler: Job 74 finished: start at NativeMethodAccessorImpl.java:0, took 0.088867 s
26/02/13 12:07:11 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 155, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@41c83377] is committing.
26/02/13 12:07:11 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 155, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@41c83377] committed.
26/02/13 12:07:11 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/155 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.155.17f6d1c2-c486-47cf-b2c2-4edb48e5a885.tmp
26/02/13 12:07:11 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.155.17f6d1c2-c486-47cf-b2c2-4edb48e5a885.tmp to file:/tmp/spark-checkpoint-enrichment/commits/155
26/02/13 12:07:11 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "4a161d66-225b-4b29-812d-ddec3c05364e",
  "name" : null,
  "timestamp" : "2026-02-13T12:07:11.540Z",
  "batchId" : 155,
  "numInputRows" : 135,
  "inputRowsPerSecond" : 138.88888888888889,
  "processedRowsPerSecond" : 323.7410071942446,
  "durationMs" : {
    "addBatch" : 257,
    "commitOffsets" : 59,
    "getBatch" : 0,
    "latestOffset" : 0,
    "queryPlanning" : 47,
    "triggerExecution" : 417,
    "walCommit" : 53
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 1792,
        "1" : 2122,
        "0" : 1608
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 1840,
        "1" : 2175,
        "0" : 1642
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 1840,
        "1" : 2175,
        "0" : 1642
      }
    },
    "numInputRows" : 135,
    "inputRowsPerSecond" : 138.88888888888889,
    "processedRowsPerSecond" : 323.7410071942446,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@1a6a42c2",
    "numOutputRows" : 26
  }
}
26/02/13 12:07:21 INFO BlockManagerInfo: Removed broadcast_111_piece0 on spark-master:42073 in memory (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 12:07:21 INFO BlockManagerInfo: Removed broadcast_111_piece0 on 172.18.0.14:37571 in memory (size: 19.5 KiB, free: 434.0 MiB)
26/02/13 12:07:21 INFO BlockManagerInfo: Removed broadcast_111_piece0 on 172.18.0.15:37717 in memory (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 12:07:21 INFO BlockManagerInfo: Removed broadcast_109_piece0 on spark-master:42073 in memory (size: 33.3 KiB, free: 434.3 MiB)
26/02/13 12:07:21 INFO BlockManagerInfo: Removed broadcast_109_piece0 on 172.18.0.14:37571 in memory (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 12:07:21 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:07:29 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/156 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.156.ff74c354-dfc1-4555-9b24-961b7498921e.tmp
26/02/13 12:07:29 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.156.ff74c354-dfc1-4555-9b24-961b7498921e.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/156
26/02/13 12:07:29 INFO MicroBatchExecution: Committed offsets for batch 156. Metadata OffsetSeqMetadata(0,1770984449508,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 12:07:29 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:07:29 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:07:29 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:07:29 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:07:29 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:07:29 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:07:29 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#30133 - origin_code.nullCount#30132) > 0)
26/02/13 12:07:29 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#30138 - destination_code.nullCount#30137) > 0)
26/02/13 12:07:29 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#30168 - callsign.nullCount#30167) > 0)
26/02/13 12:07:29 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 12:07:29 INFO DAGScheduler: Got job 75 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 12:07:29 INFO DAGScheduler: Final stage: ResultStage 116 (start at NativeMethodAccessorImpl.java:0)
26/02/13 12:07:29 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 115)
26/02/13 12:07:29 INFO DAGScheduler: Missing parents: List()
26/02/13 12:07:29 INFO DAGScheduler: Submitting ResultStage 116 (MapPartitionsRDD[406] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:07:29 INFO MemoryStore: Block broadcast_112 stored as values in memory (estimated size 99.3 KiB, free 434.0 MiB)
26/02/13 12:07:29 INFO MemoryStore: Block broadcast_112_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 433.9 MiB)
26/02/13 12:07:29 INFO BlockManagerInfo: Added broadcast_112_piece0 in memory on spark-master:42073 (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 12:07:29 INFO SparkContext: Created broadcast 112 from broadcast at DAGScheduler.scala:1585
26/02/13 12:07:29 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 116 (MapPartitionsRDD[406] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 12:07:29 INFO TaskSchedulerImpl: Adding task set 116.0 with 4 tasks resource profile 0
26/02/13 12:07:29 INFO TaskSetManager: Starting task 0.0 in stage 116.0 (TID 240) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:07:29 INFO TaskSetManager: Starting task 1.0 in stage 116.0 (TID 241) (172.18.0.14, executor 1, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:07:29 INFO BlockManagerInfo: Added broadcast_112_piece0 in memory on 172.18.0.14:37571 (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 12:07:29 INFO TaskSetManager: Starting task 2.0 in stage 116.0 (TID 242) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:07:29 INFO TaskSetManager: Finished task 1.0 in stage 116.0 (TID 241) in 37 ms on 172.18.0.14 (executor 1) (1/4)
26/02/13 12:07:29 INFO TaskSetManager: Starting task 3.0 in stage 116.0 (TID 243) (172.18.0.14, executor 1, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:07:29 INFO TaskSetManager: Finished task 0.0 in stage 116.0 (TID 240) in 39 ms on 172.18.0.14 (executor 1) (2/4)
26/02/13 12:07:29 INFO TaskSetManager: Finished task 2.0 in stage 116.0 (TID 242) in 20 ms on 172.18.0.14 (executor 1) (3/4)
26/02/13 12:07:29 INFO TaskSetManager: Finished task 3.0 in stage 116.0 (TID 243) in 19 ms on 172.18.0.14 (executor 1) (4/4)
26/02/13 12:07:29 INFO TaskSchedulerImpl: Removed TaskSet 116.0, whose tasks have all completed, from pool 
26/02/13 12:07:29 INFO DAGScheduler: ResultStage 116 (start at NativeMethodAccessorImpl.java:0) finished in 0.066 s
26/02/13 12:07:29 INFO DAGScheduler: Job 75 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 12:07:29 INFO TaskSchedulerImpl: Killing all running tasks in stage 116: Stage finished
26/02/13 12:07:29 INFO DAGScheduler: Job 75 finished: start at NativeMethodAccessorImpl.java:0, took 0.070407 s
26/02/13 12:07:29 INFO MemoryStore: Block broadcast_113_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.8 MiB)
26/02/13 12:07:29 INFO BlockManagerInfo: Added broadcast_113_piece0 in memory on spark-master:42073 (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 12:07:29 INFO SparkContext: Created broadcast 113 from start at NativeMethodAccessorImpl.java:0
26/02/13 12:07:29 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 156, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@fc4be51]. The input RDD has 1 partitions.
26/02/13 12:07:29 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 12:07:29 INFO DAGScheduler: Got job 76 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
26/02/13 12:07:29 INFO DAGScheduler: Final stage: ResultStage 117 (start at NativeMethodAccessorImpl.java:0)
26/02/13 12:07:29 INFO DAGScheduler: Parents of final stage: List()
26/02/13 12:07:29 INFO DAGScheduler: Missing parents: List()
26/02/13 12:07:29 INFO DAGScheduler: Submitting ResultStage 117 (MapPartitionsRDD[412] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:07:29 INFO MemoryStore: Block broadcast_114 stored as values in memory (estimated size 49.9 KiB, free 433.8 MiB)
26/02/13 12:07:29 INFO MemoryStore: Block broadcast_114_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.8 MiB)
26/02/13 12:07:29 INFO BlockManagerInfo: Added broadcast_114_piece0 in memory on spark-master:42073 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 12:07:29 INFO SparkContext: Created broadcast 114 from broadcast at DAGScheduler.scala:1585
26/02/13 12:07:29 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 117 (MapPartitionsRDD[412] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
26/02/13 12:07:29 INFO TaskSchedulerImpl: Adding task set 117.0 with 1 tasks resource profile 0
26/02/13 12:07:29 INFO TaskSetManager: Starting task 0.0 in stage 117.0 (TID 244) (172.18.0.15, executor 0, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:07:29 INFO BlockManagerInfo: Added broadcast_114_piece0 in memory on 172.18.0.15:37717 (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 12:07:29 INFO BlockManagerInfo: Added broadcast_113_piece0 in memory on 172.18.0.15:37717 (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 12:07:30 INFO TaskSetManager: Finished task 0.0 in stage 117.0 (TID 244) in 567 ms on 172.18.0.15 (executor 0) (1/1)
26/02/13 12:07:30 INFO TaskSchedulerImpl: Removed TaskSet 117.0, whose tasks have all completed, from pool 
26/02/13 12:07:30 INFO DAGScheduler: ResultStage 117 (start at NativeMethodAccessorImpl.java:0) finished in 0.574 s
26/02/13 12:07:30 INFO DAGScheduler: Job 76 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 12:07:30 INFO TaskSchedulerImpl: Killing all running tasks in stage 117: Stage finished
26/02/13 12:07:30 INFO DAGScheduler: Job 76 finished: start at NativeMethodAccessorImpl.java:0, took 0.576055 s
26/02/13 12:07:30 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 156, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@fc4be51] is committing.
26/02/13 12:07:30 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 156, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@fc4be51] committed.
26/02/13 12:07:30 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/156 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.156.62114ce1-769b-488a-923c-8904582406a2.tmp
26/02/13 12:07:30 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.156.62114ce1-769b-488a-923c-8904582406a2.tmp to file:/tmp/spark-checkpoint-enrichment/commits/156
26/02/13 12:07:30 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "4a161d66-225b-4b29-812d-ddec3c05364e",
  "name" : null,
  "timestamp" : "2026-02-13T12:07:29.506Z",
  "batchId" : 156,
  "numInputRows" : 37,
  "inputRowsPerSecond" : 3083.3333333333335,
  "processedRowsPerSecond" : 40.43715846994535,
  "durationMs" : {
    "addBatch" : 744,
    "commitOffsets" : 65,
    "getBatch" : 0,
    "latestOffset" : 2,
    "queryPlanning" : 30,
    "triggerExecution" : 915,
    "walCommit" : 73
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 1840,
        "1" : 2175,
        "0" : 1642
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 1840,
        "1" : 2212,
        "0" : 1642
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 1840,
        "1" : 2212,
        "0" : 1642
      }
    },
    "numInputRows" : 37,
    "inputRowsPerSecond" : 3083.3333333333335,
    "processedRowsPerSecond" : 40.43715846994535,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@1a6a42c2",
    "numOutputRows" : 10
  }
}
26/02/13 12:07:30 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/157 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.157.9f4449c1-76c0-4394-b586-25c8515bf99c.tmp
26/02/13 12:07:30 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.157.9f4449c1-76c0-4394-b586-25c8515bf99c.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/157
26/02/13 12:07:30 INFO MicroBatchExecution: Committed offsets for batch 157. Metadata OffsetSeqMetadata(0,1770984450422,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 12:07:30 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:07:30 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:07:30 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:07:30 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:07:30 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:07:30 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:07:30 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#30987 - origin_code.nullCount#30986) > 0)
26/02/13 12:07:30 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#30992 - destination_code.nullCount#30991) > 0)
26/02/13 12:07:30 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#31022 - callsign.nullCount#31021) > 0)
26/02/13 12:07:30 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 12:07:30 INFO DAGScheduler: Got job 77 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 12:07:30 INFO DAGScheduler: Final stage: ResultStage 119 (start at NativeMethodAccessorImpl.java:0)
26/02/13 12:07:30 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 118)
26/02/13 12:07:30 INFO DAGScheduler: Missing parents: List()
26/02/13 12:07:30 INFO DAGScheduler: Submitting ResultStage 119 (MapPartitionsRDD[417] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:07:30 INFO MemoryStore: Block broadcast_115 stored as values in memory (estimated size 99.3 KiB, free 433.7 MiB)
26/02/13 12:07:30 INFO MemoryStore: Block broadcast_115_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 433.6 MiB)
26/02/13 12:07:30 INFO BlockManagerInfo: Added broadcast_115_piece0 in memory on spark-master:42073 (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 12:07:30 INFO SparkContext: Created broadcast 115 from broadcast at DAGScheduler.scala:1585
26/02/13 12:07:30 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 119 (MapPartitionsRDD[417] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 12:07:30 INFO TaskSchedulerImpl: Adding task set 119.0 with 4 tasks resource profile 0
26/02/13 12:07:30 INFO TaskSetManager: Starting task 0.0 in stage 119.0 (TID 245) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:07:30 INFO TaskSetManager: Starting task 1.0 in stage 119.0 (TID 246) (172.18.0.14, executor 1, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:07:30 INFO BlockManagerInfo: Added broadcast_115_piece0 in memory on 172.18.0.14:37571 (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 12:07:30 INFO TaskSetManager: Starting task 2.0 in stage 119.0 (TID 247) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:07:30 INFO TaskSetManager: Starting task 3.0 in stage 119.0 (TID 248) (172.18.0.14, executor 1, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:07:30 INFO TaskSetManager: Finished task 1.0 in stage 119.0 (TID 246) in 36 ms on 172.18.0.14 (executor 1) (1/4)
26/02/13 12:07:30 INFO TaskSetManager: Finished task 0.0 in stage 119.0 (TID 245) in 37 ms on 172.18.0.14 (executor 1) (2/4)
26/02/13 12:07:30 INFO TaskSetManager: Finished task 3.0 in stage 119.0 (TID 248) in 23 ms on 172.18.0.14 (executor 1) (3/4)
26/02/13 12:07:30 INFO TaskSetManager: Finished task 2.0 in stage 119.0 (TID 247) in 26 ms on 172.18.0.14 (executor 1) (4/4)
26/02/13 12:07:30 INFO TaskSchedulerImpl: Removed TaskSet 119.0, whose tasks have all completed, from pool 
26/02/13 12:07:30 INFO DAGScheduler: ResultStage 119 (start at NativeMethodAccessorImpl.java:0) finished in 0.067 s
26/02/13 12:07:30 INFO DAGScheduler: Job 77 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 12:07:30 INFO TaskSchedulerImpl: Killing all running tasks in stage 119: Stage finished
26/02/13 12:07:30 INFO DAGScheduler: Job 77 finished: start at NativeMethodAccessorImpl.java:0, took 0.069712 s
26/02/13 12:07:30 INFO MemoryStore: Block broadcast_116_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.5 MiB)
26/02/13 12:07:30 INFO BlockManagerInfo: Added broadcast_116_piece0 in memory on spark-master:42073 (size: 107.2 KiB, free: 434.0 MiB)
26/02/13 12:07:30 INFO SparkContext: Created broadcast 116 from start at NativeMethodAccessorImpl.java:0
26/02/13 12:07:30 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 157, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@309350ec]. The input RDD has 3 partitions.
26/02/13 12:07:30 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 12:07:30 INFO BlockManagerInfo: Removed broadcast_113_piece0 on spark-master:42073 in memory (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 12:07:30 INFO DAGScheduler: Got job 78 (start at NativeMethodAccessorImpl.java:0) with 3 output partitions
26/02/13 12:07:30 INFO DAGScheduler: Final stage: ResultStage 120 (start at NativeMethodAccessorImpl.java:0)
26/02/13 12:07:30 INFO DAGScheduler: Parents of final stage: List()
26/02/13 12:07:30 INFO DAGScheduler: Missing parents: List()
26/02/13 12:07:30 INFO DAGScheduler: Submitting ResultStage 120 (MapPartitionsRDD[423] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:07:30 INFO MemoryStore: Block broadcast_117 stored as values in memory (estimated size 49.9 KiB, free 433.6 MiB)
26/02/13 12:07:30 INFO MemoryStore: Block broadcast_117_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.6 MiB)
26/02/13 12:07:30 INFO BlockManagerInfo: Added broadcast_117_piece0 in memory on spark-master:42073 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 12:07:30 INFO BlockManagerInfo: Removed broadcast_113_piece0 on 172.18.0.15:37717 in memory (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 12:07:30 INFO SparkContext: Created broadcast 117 from broadcast at DAGScheduler.scala:1585
26/02/13 12:07:30 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 120 (MapPartitionsRDD[423] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
26/02/13 12:07:30 INFO TaskSchedulerImpl: Adding task set 120.0 with 3 tasks resource profile 0
26/02/13 12:07:30 INFO TaskSetManager: Starting task 1.0 in stage 120.0 (TID 249) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:07:30 INFO TaskSetManager: Starting task 0.0 in stage 120.0 (TID 250) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:07:30 INFO TaskSetManager: Starting task 2.0 in stage 120.0 (TID 251) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:07:30 INFO BlockManagerInfo: Removed broadcast_115_piece0 on spark-master:42073 in memory (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 12:07:30 INFO BlockManagerInfo: Removed broadcast_115_piece0 on 172.18.0.14:37571 in memory (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 12:07:30 INFO BlockManagerInfo: Added broadcast_117_piece0 in memory on 172.18.0.14:37571 (size: 19.5 KiB, free: 434.0 MiB)
26/02/13 12:07:30 INFO BlockManagerInfo: Added broadcast_117_piece0 in memory on 172.18.0.15:37717 (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 12:07:30 INFO BlockManagerInfo: Removed broadcast_114_piece0 on spark-master:42073 in memory (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 12:07:30 INFO BlockManagerInfo: Removed broadcast_114_piece0 on 172.18.0.15:37717 in memory (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 12:07:30 INFO BlockManagerInfo: Removed broadcast_112_piece0 on spark-master:42073 in memory (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 12:07:30 INFO BlockManagerInfo: Removed broadcast_112_piece0 on 172.18.0.14:37571 in memory (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 12:07:30 INFO BlockManagerInfo: Added broadcast_116_piece0 in memory on 172.18.0.14:37571 (size: 107.2 KiB, free: 433.9 MiB)
26/02/13 12:07:30 INFO BlockManagerInfo: Added broadcast_116_piece0 in memory on 172.18.0.15:37717 (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 12:07:30 INFO TaskSetManager: Finished task 1.0 in stage 120.0 (TID 249) in 104 ms on 172.18.0.15 (executor 0) (1/3)
26/02/13 12:07:31 INFO TaskSetManager: Finished task 2.0 in stage 120.0 (TID 251) in 593 ms on 172.18.0.14 (executor 1) (2/3)
26/02/13 12:07:31 INFO TaskSetManager: Finished task 0.0 in stage 120.0 (TID 250) in 602 ms on 172.18.0.14 (executor 1) (3/3)
26/02/13 12:07:31 INFO TaskSchedulerImpl: Removed TaskSet 120.0, whose tasks have all completed, from pool 
26/02/13 12:07:31 INFO DAGScheduler: ResultStage 120 (start at NativeMethodAccessorImpl.java:0) finished in 0.615 s
26/02/13 12:07:31 INFO DAGScheduler: Job 78 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 12:07:31 INFO TaskSchedulerImpl: Killing all running tasks in stage 120: Stage finished
26/02/13 12:07:31 INFO DAGScheduler: Job 78 finished: start at NativeMethodAccessorImpl.java:0, took 0.619402 s
26/02/13 12:07:31 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 157, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@309350ec] is committing.
26/02/13 12:07:31 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 157, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@309350ec] committed.
26/02/13 12:07:31 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/157 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.157.a4ec6410-db79-43fe-851a-0c9f9d74ebda.tmp
26/02/13 12:07:31 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.157.a4ec6410-db79-43fe-851a-0c9f9d74ebda.tmp to file:/tmp/spark-checkpoint-enrichment/commits/157
26/02/13 12:07:31 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "4a161d66-225b-4b29-812d-ddec3c05364e",
  "name" : null,
  "timestamp" : "2026-02-13T12:07:30.422Z",
  "batchId" : 157,
  "numInputRows" : 210,
  "inputRowsPerSecond" : 229.25764192139738,
  "processedRowsPerSecond" : 225.08038585209002,
  "durationMs" : {
    "addBatch" : 785,
    "commitOffsets" : 69,
    "getBatch" : 0,
    "latestOffset" : 0,
    "queryPlanning" : 23,
    "triggerExecution" : 933,
    "walCommit" : 55
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 1840,
        "1" : 2212,
        "0" : 1642
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 1924,
        "1" : 2265,
        "0" : 1715
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 1924,
        "1" : 2265,
        "0" : 1715
      }
    },
    "numInputRows" : 210,
    "inputRowsPerSecond" : 229.25764192139738,
    "processedRowsPerSecond" : 225.08038585209002,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@1a6a42c2",
    "numOutputRows" : 45
  }
}
26/02/13 12:07:31 INFO BlockManagerInfo: Removed broadcast_117_piece0 on spark-master:42073 in memory (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 12:07:31 INFO BlockManagerInfo: Removed broadcast_117_piece0 on 172.18.0.14:37571 in memory (size: 19.5 KiB, free: 434.0 MiB)
26/02/13 12:07:31 INFO BlockManagerInfo: Removed broadcast_117_piece0 on 172.18.0.15:37717 in memory (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 12:07:35 INFO NetworkClient: [AdminClient clientId=adminclient-1] Node -1 disconnected.
26/02/13 12:07:41 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:07:43 INFO BlockManagerInfo: Removed broadcast_110_piece0 on spark-master:42073 in memory (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 12:07:43 INFO BlockManagerInfo: Removed broadcast_110_piece0 on 172.18.0.15:37717 in memory (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 12:07:43 INFO BlockManagerInfo: Removed broadcast_110_piece0 on 172.18.0.14:37571 in memory (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 12:07:48 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/158 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.158.6616bfe3-0faa-40e0-8153-49201365cc6a.tmp
26/02/13 12:07:48 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.158.6616bfe3-0faa-40e0-8153-49201365cc6a.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/158
26/02/13 12:07:48 INFO MicroBatchExecution: Committed offsets for batch 158. Metadata OffsetSeqMetadata(0,1770984468340,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 12:07:48 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:07:48 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:07:48 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:07:48 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:07:48 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:07:48 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:07:48 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#31841 - origin_code.nullCount#31840) > 0)
26/02/13 12:07:48 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#31846 - destination_code.nullCount#31845) > 0)
26/02/13 12:07:48 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#31876 - callsign.nullCount#31875) > 0)
26/02/13 12:07:48 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 12:07:48 INFO DAGScheduler: Got job 79 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 12:07:48 INFO DAGScheduler: Final stage: ResultStage 122 (start at NativeMethodAccessorImpl.java:0)
26/02/13 12:07:48 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 121)
26/02/13 12:07:48 INFO DAGScheduler: Missing parents: List()
26/02/13 12:07:48 INFO DAGScheduler: Submitting ResultStage 122 (MapPartitionsRDD[428] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:07:48 INFO MemoryStore: Block broadcast_118 stored as values in memory (estimated size 99.3 KiB, free 434.0 MiB)
26/02/13 12:07:48 INFO MemoryStore: Block broadcast_118_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 433.9 MiB)
26/02/13 12:07:48 INFO BlockManagerInfo: Added broadcast_118_piece0 in memory on spark-master:42073 (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 12:07:48 INFO SparkContext: Created broadcast 118 from broadcast at DAGScheduler.scala:1585
26/02/13 12:07:48 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 122 (MapPartitionsRDD[428] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 12:07:48 INFO TaskSchedulerImpl: Adding task set 122.0 with 4 tasks resource profile 0
26/02/13 12:07:48 INFO TaskSetManager: Starting task 0.0 in stage 122.0 (TID 252) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:07:48 INFO TaskSetManager: Starting task 1.0 in stage 122.0 (TID 253) (172.18.0.14, executor 1, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:07:48 INFO BlockManagerInfo: Added broadcast_118_piece0 in memory on 172.18.0.14:37571 (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 12:07:48 INFO TaskSetManager: Starting task 2.0 in stage 122.0 (TID 254) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:07:48 INFO TaskSetManager: Finished task 1.0 in stage 122.0 (TID 253) in 20 ms on 172.18.0.14 (executor 1) (1/4)
26/02/13 12:07:48 INFO TaskSetManager: Starting task 3.0 in stage 122.0 (TID 255) (172.18.0.14, executor 1, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:07:48 INFO TaskSetManager: Finished task 0.0 in stage 122.0 (TID 252) in 26 ms on 172.18.0.14 (executor 1) (2/4)
26/02/13 12:07:48 INFO TaskSetManager: Finished task 2.0 in stage 122.0 (TID 254) in 21 ms on 172.18.0.14 (executor 1) (3/4)
26/02/13 12:07:48 INFO TaskSetManager: Finished task 3.0 in stage 122.0 (TID 255) in 20 ms on 172.18.0.14 (executor 1) (4/4)
26/02/13 12:07:48 INFO TaskSchedulerImpl: Removed TaskSet 122.0, whose tasks have all completed, from pool 
26/02/13 12:07:48 INFO DAGScheduler: ResultStage 122 (start at NativeMethodAccessorImpl.java:0) finished in 0.053 s
26/02/13 12:07:48 INFO DAGScheduler: Job 79 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 12:07:48 INFO TaskSchedulerImpl: Killing all running tasks in stage 122: Stage finished
26/02/13 12:07:48 INFO DAGScheduler: Job 79 finished: start at NativeMethodAccessorImpl.java:0, took 0.055623 s
26/02/13 12:07:48 INFO MemoryStore: Block broadcast_119_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.8 MiB)
26/02/13 12:07:48 INFO BlockManagerInfo: Added broadcast_119_piece0 in memory on spark-master:42073 (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 12:07:48 INFO SparkContext: Created broadcast 119 from start at NativeMethodAccessorImpl.java:0
26/02/13 12:07:48 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 158, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@262b3e17]. The input RDD has 3 partitions.
26/02/13 12:07:48 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 12:07:48 INFO DAGScheduler: Got job 80 (start at NativeMethodAccessorImpl.java:0) with 3 output partitions
26/02/13 12:07:48 INFO DAGScheduler: Final stage: ResultStage 123 (start at NativeMethodAccessorImpl.java:0)
26/02/13 12:07:48 INFO DAGScheduler: Parents of final stage: List()
26/02/13 12:07:48 INFO DAGScheduler: Missing parents: List()
26/02/13 12:07:48 INFO DAGScheduler: Submitting ResultStage 123 (MapPartitionsRDD[434] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:07:48 INFO MemoryStore: Block broadcast_120 stored as values in memory (estimated size 49.9 KiB, free 433.8 MiB)
26/02/13 12:07:48 INFO MemoryStore: Block broadcast_120_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.8 MiB)
26/02/13 12:07:48 INFO BlockManagerInfo: Added broadcast_120_piece0 in memory on spark-master:42073 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 12:07:48 INFO SparkContext: Created broadcast 120 from broadcast at DAGScheduler.scala:1585
26/02/13 12:07:48 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 123 (MapPartitionsRDD[434] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
26/02/13 12:07:48 INFO TaskSchedulerImpl: Adding task set 123.0 with 3 tasks resource profile 0
26/02/13 12:07:48 INFO TaskSetManager: Starting task 0.0 in stage 123.0 (TID 256) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:07:48 INFO TaskSetManager: Starting task 1.0 in stage 123.0 (TID 257) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:07:48 INFO TaskSetManager: Starting task 2.0 in stage 123.0 (TID 258) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:07:48 INFO BlockManagerInfo: Added broadcast_120_piece0 in memory on 172.18.0.14:37571 (size: 19.5 KiB, free: 434.0 MiB)
26/02/13 12:07:48 INFO BlockManagerInfo: Added broadcast_120_piece0 in memory on 172.18.0.15:37717 (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 12:07:48 INFO BlockManagerInfo: Added broadcast_119_piece0 in memory on 172.18.0.14:37571 (size: 107.2 KiB, free: 433.9 MiB)
26/02/13 12:07:48 INFO BlockManagerInfo: Added broadcast_119_piece0 in memory on 172.18.0.15:37717 (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 12:07:48 INFO BlockManagerInfo: Removed broadcast_118_piece0 on spark-master:42073 in memory (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 12:07:48 INFO BlockManagerInfo: Removed broadcast_118_piece0 on 172.18.0.14:37571 in memory (size: 33.3 KiB, free: 433.9 MiB)
26/02/13 12:07:49 INFO TaskSetManager: Finished task 0.0 in stage 123.0 (TID 256) in 560 ms on 172.18.0.14 (executor 1) (1/3)
26/02/13 12:07:49 INFO TaskSetManager: Finished task 2.0 in stage 123.0 (TID 258) in 563 ms on 172.18.0.14 (executor 1) (2/3)
26/02/13 12:07:49 INFO TaskSetManager: Finished task 1.0 in stage 123.0 (TID 257) in 575 ms on 172.18.0.15 (executor 0) (3/3)
26/02/13 12:07:49 INFO TaskSchedulerImpl: Removed TaskSet 123.0, whose tasks have all completed, from pool 
26/02/13 12:07:49 INFO DAGScheduler: ResultStage 123 (start at NativeMethodAccessorImpl.java:0) finished in 0.581 s
26/02/13 12:07:49 INFO DAGScheduler: Job 80 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 12:07:49 INFO TaskSchedulerImpl: Killing all running tasks in stage 123: Stage finished
26/02/13 12:07:49 INFO DAGScheduler: Job 80 finished: start at NativeMethodAccessorImpl.java:0, took 0.583256 s
26/02/13 12:07:49 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 158, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@262b3e17] is committing.
26/02/13 12:07:49 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 158, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@262b3e17] committed.
26/02/13 12:07:49 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/158 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.158.d26b4bb3-9b0f-4227-8b7d-380d51e8554e.tmp
26/02/13 12:07:49 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.158.d26b4bb3-9b0f-4227-8b7d-380d51e8554e.tmp to file:/tmp/spark-checkpoint-enrichment/commits/158
26/02/13 12:07:49 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "4a161d66-225b-4b29-812d-ddec3c05364e",
  "name" : null,
  "timestamp" : "2026-02-13T12:07:48.339Z",
  "batchId" : 158,
  "numInputRows" : 16,
  "inputRowsPerSecond" : 1333.3333333333333,
  "processedRowsPerSecond" : 17.094017094017094,
  "durationMs" : {
    "addBatch" : 734,
    "commitOffsets" : 73,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 28,
    "triggerExecution" : 936,
    "walCommit" : 99
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 1924,
        "1" : 2265,
        "0" : 1715
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 1927,
        "1" : 2272,
        "0" : 1721
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 1927,
        "1" : 2272,
        "0" : 1721
      }
    },
    "numInputRows" : 16,
    "inputRowsPerSecond" : 1333.3333333333333,
    "processedRowsPerSecond" : 17.094017094017094,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@1a6a42c2",
    "numOutputRows" : 5
  }
}
26/02/13 12:07:49 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/159 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.159.75197ffa-d3d8-4a42-b75c-faa3152af150.tmp
26/02/13 12:07:49 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.159.75197ffa-d3d8-4a42-b75c-faa3152af150.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/159
26/02/13 12:07:49 INFO MicroBatchExecution: Committed offsets for batch 159. Metadata OffsetSeqMetadata(0,1770984469277,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 12:07:49 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:07:49 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:07:49 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:07:49 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:07:49 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:07:49 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:07:49 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#32695 - origin_code.nullCount#32694) > 0)
26/02/13 12:07:49 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#32700 - destination_code.nullCount#32699) > 0)
26/02/13 12:07:49 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#32730 - callsign.nullCount#32729) > 0)
26/02/13 12:07:49 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 12:07:49 INFO DAGScheduler: Got job 81 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 12:07:49 INFO DAGScheduler: Final stage: ResultStage 125 (start at NativeMethodAccessorImpl.java:0)
26/02/13 12:07:49 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 124)
26/02/13 12:07:49 INFO DAGScheduler: Missing parents: List()
26/02/13 12:07:49 INFO DAGScheduler: Submitting ResultStage 125 (MapPartitionsRDD[439] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:07:49 INFO MemoryStore: Block broadcast_121 stored as values in memory (estimated size 99.3 KiB, free 433.8 MiB)
26/02/13 12:07:49 INFO MemoryStore: Block broadcast_121_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 433.8 MiB)
26/02/13 12:07:49 INFO BlockManagerInfo: Added broadcast_121_piece0 in memory on spark-master:42073 (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 12:07:49 INFO BlockManagerInfo: Removed broadcast_120_piece0 on spark-master:42073 in memory (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 12:07:49 INFO BlockManagerInfo: Removed broadcast_120_piece0 on 172.18.0.15:37717 in memory (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 12:07:49 INFO SparkContext: Created broadcast 121 from broadcast at DAGScheduler.scala:1585
26/02/13 12:07:49 INFO BlockManagerInfo: Removed broadcast_120_piece0 on 172.18.0.14:37571 in memory (size: 19.5 KiB, free: 434.0 MiB)
26/02/13 12:07:49 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 125 (MapPartitionsRDD[439] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 12:07:49 INFO TaskSchedulerImpl: Adding task set 125.0 with 4 tasks resource profile 0
26/02/13 12:07:49 INFO TaskSetManager: Starting task 0.0 in stage 125.0 (TID 259) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:07:49 INFO TaskSetManager: Starting task 1.0 in stage 125.0 (TID 260) (172.18.0.14, executor 1, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:07:49 INFO BlockManagerInfo: Removed broadcast_119_piece0 on spark-master:42073 in memory (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 12:07:49 INFO BlockManagerInfo: Removed broadcast_119_piece0 on 172.18.0.15:37717 in memory (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 12:07:49 INFO BlockManagerInfo: Removed broadcast_119_piece0 on 172.18.0.14:37571 in memory (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 12:07:49 INFO BlockManagerInfo: Added broadcast_121_piece0 in memory on 172.18.0.14:37571 (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 12:07:49 INFO TaskSetManager: Starting task 2.0 in stage 125.0 (TID 261) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:07:49 INFO TaskSetManager: Starting task 3.0 in stage 125.0 (TID 262) (172.18.0.14, executor 1, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:07:49 INFO TaskSetManager: Finished task 1.0 in stage 125.0 (TID 260) in 48 ms on 172.18.0.14 (executor 1) (1/4)
26/02/13 12:07:49 INFO TaskSetManager: Finished task 0.0 in stage 125.0 (TID 259) in 53 ms on 172.18.0.14 (executor 1) (2/4)
26/02/13 12:07:49 INFO TaskSetManager: Finished task 3.0 in stage 125.0 (TID 262) in 28 ms on 172.18.0.14 (executor 1) (3/4)
26/02/13 12:07:49 INFO BlockManagerInfo: Removed broadcast_116_piece0 on spark-master:42073 in memory (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 12:07:49 INFO BlockManagerInfo: Removed broadcast_116_piece0 on 172.18.0.14:37571 in memory (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 12:07:49 INFO BlockManagerInfo: Removed broadcast_116_piece0 on 172.18.0.15:37717 in memory (size: 107.2 KiB, free: 434.4 MiB)
26/02/13 12:07:49 INFO TaskSetManager: Finished task 2.0 in stage 125.0 (TID 261) in 58 ms on 172.18.0.14 (executor 1) (4/4)
26/02/13 12:07:49 INFO TaskSchedulerImpl: Removed TaskSet 125.0, whose tasks have all completed, from pool 
26/02/13 12:07:49 INFO DAGScheduler: ResultStage 125 (start at NativeMethodAccessorImpl.java:0) finished in 0.131 s
26/02/13 12:07:49 INFO DAGScheduler: Job 81 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 12:07:49 INFO TaskSchedulerImpl: Killing all running tasks in stage 125: Stage finished
26/02/13 12:07:49 INFO DAGScheduler: Job 81 finished: start at NativeMethodAccessorImpl.java:0, took 0.135015 s
26/02/13 12:07:49 INFO MemoryStore: Block broadcast_122_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.9 MiB)
26/02/13 12:07:49 INFO BlockManagerInfo: Added broadcast_122_piece0 in memory on spark-master:42073 (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 12:07:49 INFO SparkContext: Created broadcast 122 from start at NativeMethodAccessorImpl.java:0
26/02/13 12:07:49 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 159, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@7fa647c7]. The input RDD has 3 partitions.
26/02/13 12:07:49 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 12:07:49 INFO DAGScheduler: Got job 82 (start at NativeMethodAccessorImpl.java:0) with 3 output partitions
26/02/13 12:07:49 INFO DAGScheduler: Final stage: ResultStage 126 (start at NativeMethodAccessorImpl.java:0)
26/02/13 12:07:49 INFO DAGScheduler: Parents of final stage: List()
26/02/13 12:07:49 INFO DAGScheduler: Missing parents: List()
26/02/13 12:07:49 INFO DAGScheduler: Submitting ResultStage 126 (MapPartitionsRDD[445] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:07:49 INFO MemoryStore: Block broadcast_123 stored as values in memory (estimated size 49.9 KiB, free 433.9 MiB)
26/02/13 12:07:49 INFO MemoryStore: Block broadcast_123_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.9 MiB)
26/02/13 12:07:49 INFO BlockManagerInfo: Added broadcast_123_piece0 in memory on spark-master:42073 (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 12:07:49 INFO SparkContext: Created broadcast 123 from broadcast at DAGScheduler.scala:1585
26/02/13 12:07:49 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 126 (MapPartitionsRDD[445] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
26/02/13 12:07:49 INFO TaskSchedulerImpl: Adding task set 126.0 with 3 tasks resource profile 0
26/02/13 12:07:49 INFO TaskSetManager: Starting task 0.0 in stage 126.0 (TID 263) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:07:49 INFO TaskSetManager: Starting task 1.0 in stage 126.0 (TID 264) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:07:49 INFO TaskSetManager: Starting task 2.0 in stage 126.0 (TID 265) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:07:49 INFO BlockManagerInfo: Added broadcast_123_piece0 in memory on 172.18.0.14:37571 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 12:07:49 INFO BlockManagerInfo: Added broadcast_123_piece0 in memory on 172.18.0.15:37717 (size: 19.5 KiB, free: 434.4 MiB)
26/02/13 12:07:49 INFO BlockManagerInfo: Added broadcast_122_piece0 in memory on 172.18.0.14:37571 (size: 107.2 KiB, free: 434.0 MiB)
26/02/13 12:07:49 INFO BlockManagerInfo: Added broadcast_122_piece0 in memory on 172.18.0.15:37717 (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 12:07:49 INFO TaskSetManager: Finished task 2.0 in stage 126.0 (TID 265) in 95 ms on 172.18.0.14 (executor 1) (1/3)
26/02/13 12:07:49 INFO TaskSetManager: Finished task 0.0 in stage 126.0 (TID 263) in 101 ms on 172.18.0.14 (executor 1) (2/3)
26/02/13 12:07:49 INFO TaskSetManager: Finished task 1.0 in stage 126.0 (TID 264) in 147 ms on 172.18.0.15 (executor 0) (3/3)
26/02/13 12:07:49 INFO TaskSchedulerImpl: Removed TaskSet 126.0, whose tasks have all completed, from pool 
26/02/13 12:07:49 INFO DAGScheduler: ResultStage 126 (start at NativeMethodAccessorImpl.java:0) finished in 0.156 s
26/02/13 12:07:49 INFO DAGScheduler: Job 82 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 12:07:49 INFO TaskSchedulerImpl: Killing all running tasks in stage 126: Stage finished
26/02/13 12:07:49 INFO DAGScheduler: Job 82 finished: start at NativeMethodAccessorImpl.java:0, took 0.157424 s
26/02/13 12:07:49 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 159, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@7fa647c7] is committing.
26/02/13 12:07:49 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 159, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@7fa647c7] committed.
26/02/13 12:07:49 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/159 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.159.ef85cd34-3d1e-4ea0-9934-7986de468afc.tmp
26/02/13 12:07:49 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.159.ef85cd34-3d1e-4ea0-9934-7986de468afc.tmp to file:/tmp/spark-checkpoint-enrichment/commits/159
26/02/13 12:07:49 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "4a161d66-225b-4b29-812d-ddec3c05364e",
  "name" : null,
  "timestamp" : "2026-02-13T12:07:49.276Z",
  "batchId" : 159,
  "numInputRows" : 230,
  "inputRowsPerSecond" : 245.4642475987193,
  "processedRowsPerSecond" : 384.61538461538464,
  "durationMs" : {
    "addBatch" : 430,
    "commitOffsets" : 65,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 40,
    "triggerExecution" : 598,
    "walCommit" : 62
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 1927,
        "1" : 2272,
        "0" : 1721
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 2006,
        "1" : 2356,
        "0" : 1788
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 2006,
        "1" : 2356,
        "0" : 1788
      }
    },
    "numInputRows" : 230,
    "inputRowsPerSecond" : 245.4642475987193,
    "processedRowsPerSecond" : 384.61538461538464,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@1a6a42c2",
    "numOutputRows" : 49
  }
}
26/02/13 12:07:59 INFO BlockManagerInfo: Removed broadcast_123_piece0 on spark-master:42073 in memory (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 12:07:59 INFO BlockManagerInfo: Removed broadcast_123_piece0 on 172.18.0.14:37571 in memory (size: 19.5 KiB, free: 434.0 MiB)
26/02/13 12:07:59 INFO BlockManagerInfo: Removed broadcast_123_piece0 on 172.18.0.15:37717 in memory (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 12:07:59 INFO BlockManagerInfo: Removed broadcast_121_piece0 on spark-master:42073 in memory (size: 33.3 KiB, free: 434.3 MiB)
26/02/13 12:07:59 INFO BlockManagerInfo: Removed broadcast_121_piece0 on 172.18.0.14:37571 in memory (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 12:07:59 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:08:09 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/160 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.160.c4f89346-d639-493b-9a5f-26a2a937f047.tmp
26/02/13 12:08:09 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.160.c4f89346-d639-493b-9a5f-26a2a937f047.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/160
26/02/13 12:08:09 INFO MicroBatchExecution: Committed offsets for batch 160. Metadata OffsetSeqMetadata(0,1770984489676,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 12:08:09 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:08:09 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:08:09 INFO BlockManagerInfo: Removed broadcast_122_piece0 on spark-master:42073 in memory (size: 107.2 KiB, free: 434.4 MiB)
26/02/13 12:08:09 INFO BlockManagerInfo: Removed broadcast_122_piece0 on 172.18.0.14:37571 in memory (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 12:08:09 INFO BlockManagerInfo: Removed broadcast_122_piece0 on 172.18.0.15:37717 in memory (size: 107.2 KiB, free: 434.4 MiB)
26/02/13 12:08:09 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:08:09 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:08:09 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:08:09 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:08:09 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#33549 - origin_code.nullCount#33548) > 0)
26/02/13 12:08:09 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#33554 - destination_code.nullCount#33553) > 0)
26/02/13 12:08:09 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#33584 - callsign.nullCount#33583) > 0)
26/02/13 12:08:09 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 12:08:09 INFO DAGScheduler: Got job 83 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 12:08:09 INFO DAGScheduler: Final stage: ResultStage 128 (start at NativeMethodAccessorImpl.java:0)
26/02/13 12:08:09 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 127)
26/02/13 12:08:09 INFO DAGScheduler: Missing parents: List()
26/02/13 12:08:09 INFO DAGScheduler: Submitting ResultStage 128 (MapPartitionsRDD[450] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:08:09 INFO MemoryStore: Block broadcast_124 stored as values in memory (estimated size 99.3 KiB, free 434.1 MiB)
26/02/13 12:08:09 INFO MemoryStore: Block broadcast_124_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 434.0 MiB)
26/02/13 12:08:09 INFO BlockManagerInfo: Added broadcast_124_piece0 in memory on spark-master:42073 (size: 33.3 KiB, free: 434.3 MiB)
26/02/13 12:08:09 INFO SparkContext: Created broadcast 124 from broadcast at DAGScheduler.scala:1585
26/02/13 12:08:09 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 128 (MapPartitionsRDD[450] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 12:08:09 INFO TaskSchedulerImpl: Adding task set 128.0 with 4 tasks resource profile 0
26/02/13 12:08:09 INFO TaskSetManager: Starting task 0.0 in stage 128.0 (TID 266) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:08:09 INFO TaskSetManager: Starting task 1.0 in stage 128.0 (TID 267) (172.18.0.14, executor 1, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:08:09 INFO BlockManagerInfo: Added broadcast_124_piece0 in memory on 172.18.0.14:37571 (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 12:08:09 INFO TaskSetManager: Starting task 2.0 in stage 128.0 (TID 268) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:08:09 INFO TaskSetManager: Finished task 0.0 in stage 128.0 (TID 266) in 38 ms on 172.18.0.14 (executor 1) (1/4)
26/02/13 12:08:09 INFO TaskSetManager: Starting task 3.0 in stage 128.0 (TID 269) (172.18.0.14, executor 1, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:08:10 INFO TaskSetManager: Finished task 1.0 in stage 128.0 (TID 267) in 41 ms on 172.18.0.14 (executor 1) (2/4)
26/02/13 12:08:10 INFO TaskSetManager: Finished task 3.0 in stage 128.0 (TID 269) in 22 ms on 172.18.0.14 (executor 1) (3/4)
26/02/13 12:08:10 INFO TaskSetManager: Finished task 2.0 in stage 128.0 (TID 268) in 25 ms on 172.18.0.14 (executor 1) (4/4)
26/02/13 12:08:10 INFO DAGScheduler: ResultStage 128 (start at NativeMethodAccessorImpl.java:0) finished in 0.071 s
26/02/13 12:08:10 INFO DAGScheduler: Job 83 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 12:08:10 INFO TaskSchedulerImpl: Removed TaskSet 128.0, whose tasks have all completed, from pool 
26/02/13 12:08:10 INFO TaskSchedulerImpl: Killing all running tasks in stage 128: Stage finished
26/02/13 12:08:10 INFO DAGScheduler: Job 83 finished: start at NativeMethodAccessorImpl.java:0, took 0.074979 s
26/02/13 12:08:10 INFO MemoryStore: Block broadcast_125_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.9 MiB)
26/02/13 12:08:10 INFO BlockManagerInfo: Added broadcast_125_piece0 in memory on spark-master:42073 (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 12:08:10 INFO SparkContext: Created broadcast 125 from start at NativeMethodAccessorImpl.java:0
26/02/13 12:08:10 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 160, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@271bcd59]. The input RDD has 3 partitions.
26/02/13 12:08:10 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 12:08:10 INFO DAGScheduler: Got job 84 (start at NativeMethodAccessorImpl.java:0) with 3 output partitions
26/02/13 12:08:10 INFO DAGScheduler: Final stage: ResultStage 129 (start at NativeMethodAccessorImpl.java:0)
26/02/13 12:08:10 INFO DAGScheduler: Parents of final stage: List()
26/02/13 12:08:10 INFO DAGScheduler: Missing parents: List()
26/02/13 12:08:10 INFO DAGScheduler: Submitting ResultStage 129 (MapPartitionsRDD[456] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:08:10 INFO MemoryStore: Block broadcast_126 stored as values in memory (estimated size 49.9 KiB, free 433.9 MiB)
26/02/13 12:08:10 INFO MemoryStore: Block broadcast_126_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.9 MiB)
26/02/13 12:08:10 INFO BlockManagerInfo: Added broadcast_126_piece0 in memory on spark-master:42073 (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 12:08:10 INFO SparkContext: Created broadcast 126 from broadcast at DAGScheduler.scala:1585
26/02/13 12:08:10 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 129 (MapPartitionsRDD[456] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
26/02/13 12:08:10 INFO TaskSchedulerImpl: Adding task set 129.0 with 3 tasks resource profile 0
26/02/13 12:08:10 INFO TaskSetManager: Starting task 1.0 in stage 129.0 (TID 270) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:08:10 INFO TaskSetManager: Starting task 0.0 in stage 129.0 (TID 271) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:08:10 INFO TaskSetManager: Starting task 2.0 in stage 129.0 (TID 272) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:08:10 INFO BlockManagerInfo: Added broadcast_126_piece0 in memory on 172.18.0.14:37571 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 12:08:10 INFO BlockManagerInfo: Added broadcast_126_piece0 in memory on 172.18.0.15:37717 (size: 19.5 KiB, free: 434.4 MiB)
26/02/13 12:08:10 INFO BlockManagerInfo: Added broadcast_125_piece0 in memory on 172.18.0.14:37571 (size: 107.2 KiB, free: 434.0 MiB)
26/02/13 12:08:10 INFO BlockManagerInfo: Added broadcast_125_piece0 in memory on 172.18.0.15:37717 (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 12:08:10 INFO TaskSetManager: Finished task 2.0 in stage 129.0 (TID 272) in 623 ms on 172.18.0.14 (executor 1) (1/3)
26/02/13 12:08:10 INFO TaskSetManager: Finished task 0.0 in stage 129.0 (TID 271) in 639 ms on 172.18.0.14 (executor 1) (2/3)
26/02/13 12:08:10 INFO TaskSetManager: Finished task 1.0 in stage 129.0 (TID 270) in 677 ms on 172.18.0.15 (executor 0) (3/3)
26/02/13 12:08:10 INFO TaskSchedulerImpl: Removed TaskSet 129.0, whose tasks have all completed, from pool 
26/02/13 12:08:10 INFO DAGScheduler: ResultStage 129 (start at NativeMethodAccessorImpl.java:0) finished in 0.686 s
26/02/13 12:08:10 INFO DAGScheduler: Job 84 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 12:08:10 INFO TaskSchedulerImpl: Killing all running tasks in stage 129: Stage finished
26/02/13 12:08:10 INFO DAGScheduler: Job 84 finished: start at NativeMethodAccessorImpl.java:0, took 0.689328 s
26/02/13 12:08:10 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 160, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@271bcd59] is committing.
26/02/13 12:08:10 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 160, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@271bcd59] committed.
26/02/13 12:08:10 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/160 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.160.f0430d7c-c0f9-4041-83e6-c0d357bdb801.tmp
26/02/13 12:08:10 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.160.f0430d7c-c0f9-4041-83e6-c0d357bdb801.tmp to file:/tmp/spark-checkpoint-enrichment/commits/160
26/02/13 12:08:10 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "4a161d66-225b-4b29-812d-ddec3c05364e",
  "name" : null,
  "timestamp" : "2026-02-13T12:08:09.674Z",
  "batchId" : 160,
  "numInputRows" : 85,
  "inputRowsPerSecond" : 7083.333333333333,
  "processedRowsPerSecond" : 75.48845470692719,
  "durationMs" : {
    "addBatch" : 921,
    "commitOffsets" : 61,
    "getBatch" : 0,
    "latestOffset" : 2,
    "queryPlanning" : 60,
    "triggerExecution" : 1126,
    "walCommit" : 81
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 2006,
        "1" : 2356,
        "0" : 1788
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 2035,
        "1" : 2386,
        "0" : 1814
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 2035,
        "1" : 2386,
        "0" : 1814
      }
    },
    "numInputRows" : 85,
    "inputRowsPerSecond" : 7083.333333333333,
    "processedRowsPerSecond" : 75.48845470692719,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@1a6a42c2",
    "numOutputRows" : 26
  }
}
26/02/13 12:08:10 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/161 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.161.112aec68-82cf-4c26-ad66-0ce9314cf9a8.tmp
26/02/13 12:08:10 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.161.112aec68-82cf-4c26-ad66-0ce9314cf9a8.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/161
26/02/13 12:08:10 INFO MicroBatchExecution: Committed offsets for batch 161. Metadata OffsetSeqMetadata(0,1770984490802,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 12:08:10 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:08:10 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:08:10 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:08:10 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:08:10 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:08:10 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:08:10 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#34403 - origin_code.nullCount#34402) > 0)
26/02/13 12:08:10 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#34408 - destination_code.nullCount#34407) > 0)
26/02/13 12:08:10 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#34438 - callsign.nullCount#34437) > 0)
26/02/13 12:08:10 INFO BlockManagerInfo: Removed broadcast_126_piece0 on spark-master:42073 in memory (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 12:08:10 INFO BlockManagerInfo: Removed broadcast_126_piece0 on 172.18.0.14:37571 in memory (size: 19.5 KiB, free: 434.0 MiB)
26/02/13 12:08:10 INFO BlockManagerInfo: Removed broadcast_126_piece0 on 172.18.0.15:37717 in memory (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 12:08:10 INFO BlockManagerInfo: Removed broadcast_124_piece0 on spark-master:42073 in memory (size: 33.3 KiB, free: 434.3 MiB)
26/02/13 12:08:10 INFO BlockManagerInfo: Removed broadcast_124_piece0 on 172.18.0.14:37571 in memory (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 12:08:10 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 12:08:10 INFO DAGScheduler: Got job 85 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 12:08:10 INFO DAGScheduler: Final stage: ResultStage 131 (start at NativeMethodAccessorImpl.java:0)
26/02/13 12:08:10 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 130)
26/02/13 12:08:10 INFO DAGScheduler: Missing parents: List()
26/02/13 12:08:10 INFO BlockManagerInfo: Removed broadcast_125_piece0 on spark-master:42073 in memory (size: 107.2 KiB, free: 434.4 MiB)
26/02/13 12:08:10 INFO DAGScheduler: Submitting ResultStage 131 (MapPartitionsRDD[461] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:08:10 INFO BlockManagerInfo: Removed broadcast_125_piece0 on 172.18.0.14:37571 in memory (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 12:08:10 INFO BlockManagerInfo: Removed broadcast_125_piece0 on 172.18.0.15:37717 in memory (size: 107.2 KiB, free: 434.4 MiB)
26/02/13 12:08:10 INFO MemoryStore: Block broadcast_127 stored as values in memory (estimated size 99.3 KiB, free 434.1 MiB)
26/02/13 12:08:10 INFO MemoryStore: Block broadcast_127_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 434.0 MiB)
26/02/13 12:08:10 INFO BlockManagerInfo: Added broadcast_127_piece0 in memory on spark-master:42073 (size: 33.3 KiB, free: 434.3 MiB)
26/02/13 12:08:10 INFO SparkContext: Created broadcast 127 from broadcast at DAGScheduler.scala:1585
26/02/13 12:08:10 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 131 (MapPartitionsRDD[461] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 12:08:10 INFO TaskSchedulerImpl: Adding task set 131.0 with 4 tasks resource profile 0
26/02/13 12:08:10 INFO TaskSetManager: Starting task 0.0 in stage 131.0 (TID 273) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:08:10 INFO TaskSetManager: Starting task 1.0 in stage 131.0 (TID 274) (172.18.0.14, executor 1, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:08:10 INFO BlockManagerInfo: Added broadcast_127_piece0 in memory on 172.18.0.14:37571 (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 12:08:10 INFO TaskSetManager: Starting task 2.0 in stage 131.0 (TID 275) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:08:10 INFO TaskSetManager: Finished task 0.0 in stage 131.0 (TID 273) in 19 ms on 172.18.0.14 (executor 1) (1/4)
26/02/13 12:08:11 INFO TaskSetManager: Starting task 3.0 in stage 131.0 (TID 276) (172.18.0.14, executor 1, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:08:11 INFO TaskSetManager: Finished task 1.0 in stage 131.0 (TID 274) in 25 ms on 172.18.0.14 (executor 1) (2/4)
26/02/13 12:08:11 INFO TaskSetManager: Finished task 2.0 in stage 131.0 (TID 275) in 14 ms on 172.18.0.14 (executor 1) (3/4)
26/02/13 12:08:11 INFO TaskSetManager: Finished task 3.0 in stage 131.0 (TID 276) in 15 ms on 172.18.0.14 (executor 1) (4/4)
26/02/13 12:08:11 INFO TaskSchedulerImpl: Removed TaskSet 131.0, whose tasks have all completed, from pool 
26/02/13 12:08:11 INFO DAGScheduler: ResultStage 131 (start at NativeMethodAccessorImpl.java:0) finished in 0.044 s
26/02/13 12:08:11 INFO DAGScheduler: Job 85 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 12:08:11 INFO TaskSchedulerImpl: Killing all running tasks in stage 131: Stage finished
26/02/13 12:08:11 INFO DAGScheduler: Job 85 finished: start at NativeMethodAccessorImpl.java:0, took 0.050108 s
26/02/13 12:08:11 INFO BlockManagerInfo: Removed broadcast_127_piece0 on spark-master:42073 in memory (size: 33.3 KiB, free: 434.4 MiB)
26/02/13 12:08:11 INFO BlockManagerInfo: Removed broadcast_127_piece0 on 172.18.0.14:37571 in memory (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 12:08:11 INFO MemoryStore: Block broadcast_128_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 434.1 MiB)
26/02/13 12:08:11 INFO BlockManagerInfo: Added broadcast_128_piece0 in memory on spark-master:42073 (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 12:08:11 INFO SparkContext: Created broadcast 128 from start at NativeMethodAccessorImpl.java:0
26/02/13 12:08:11 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 161, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@f5f31fe]. The input RDD has 3 partitions.
26/02/13 12:08:11 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 12:08:11 INFO DAGScheduler: Got job 86 (start at NativeMethodAccessorImpl.java:0) with 3 output partitions
26/02/13 12:08:11 INFO DAGScheduler: Final stage: ResultStage 132 (start at NativeMethodAccessorImpl.java:0)
26/02/13 12:08:11 INFO DAGScheduler: Parents of final stage: List()
26/02/13 12:08:11 INFO DAGScheduler: Missing parents: List()
26/02/13 12:08:11 INFO DAGScheduler: Submitting ResultStage 132 (MapPartitionsRDD[467] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:08:11 INFO MemoryStore: Block broadcast_129 stored as values in memory (estimated size 49.9 KiB, free 434.0 MiB)
26/02/13 12:08:11 INFO MemoryStore: Block broadcast_129_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 434.0 MiB)
26/02/13 12:08:11 INFO BlockManagerInfo: Added broadcast_129_piece0 in memory on spark-master:42073 (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 12:08:11 INFO SparkContext: Created broadcast 129 from broadcast at DAGScheduler.scala:1585
26/02/13 12:08:11 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 132 (MapPartitionsRDD[467] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
26/02/13 12:08:11 INFO TaskSchedulerImpl: Adding task set 132.0 with 3 tasks resource profile 0
26/02/13 12:08:11 INFO TaskSetManager: Starting task 1.0 in stage 132.0 (TID 277) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:08:11 INFO TaskSetManager: Starting task 0.0 in stage 132.0 (TID 278) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:08:11 INFO TaskSetManager: Starting task 2.0 in stage 132.0 (TID 279) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:08:11 INFO BlockManagerInfo: Added broadcast_129_piece0 in memory on 172.18.0.15:37717 (size: 19.5 KiB, free: 434.4 MiB)
26/02/13 12:08:11 INFO BlockManagerInfo: Added broadcast_129_piece0 in memory on 172.18.0.14:37571 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 12:08:11 INFO BlockManagerInfo: Added broadcast_128_piece0 in memory on 172.18.0.14:37571 (size: 107.2 KiB, free: 434.0 MiB)
26/02/13 12:08:11 INFO BlockManagerInfo: Added broadcast_128_piece0 in memory on 172.18.0.15:37717 (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 12:08:11 INFO TaskSetManager: Finished task 1.0 in stage 132.0 (TID 277) in 122 ms on 172.18.0.15 (executor 0) (1/3)
26/02/13 12:08:11 INFO TaskSetManager: Finished task 0.0 in stage 132.0 (TID 278) in 142 ms on 172.18.0.14 (executor 1) (2/3)
26/02/13 12:08:11 INFO TaskSetManager: Finished task 2.0 in stage 132.0 (TID 279) in 146 ms on 172.18.0.14 (executor 1) (3/3)
26/02/13 12:08:11 INFO TaskSchedulerImpl: Removed TaskSet 132.0, whose tasks have all completed, from pool 
26/02/13 12:08:11 INFO DAGScheduler: ResultStage 132 (start at NativeMethodAccessorImpl.java:0) finished in 0.153 s
26/02/13 12:08:11 INFO DAGScheduler: Job 86 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 12:08:11 INFO TaskSchedulerImpl: Killing all running tasks in stage 132: Stage finished
26/02/13 12:08:11 INFO DAGScheduler: Job 86 finished: start at NativeMethodAccessorImpl.java:0, took 0.155919 s
26/02/13 12:08:11 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 161, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@f5f31fe] is committing.
26/02/13 12:08:11 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 161, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@f5f31fe] committed.
26/02/13 12:08:11 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/161 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.161.34198df0-ad2d-4d27-923c-d27d8e37068e.tmp
26/02/13 12:08:11 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.161.34198df0-ad2d-4d27-923c-d27d8e37068e.tmp to file:/tmp/spark-checkpoint-enrichment/commits/161
26/02/13 12:08:11 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "4a161d66-225b-4b29-812d-ddec3c05364e",
  "name" : null,
  "timestamp" : "2026-02-13T12:08:10.801Z",
  "batchId" : 161,
  "numInputRows" : 160,
  "inputRowsPerSecond" : 141.9698314108252,
  "processedRowsPerSecond" : 333.33333333333337,
  "durationMs" : {
    "addBatch" : 313,
    "commitOffsets" : 80,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 22,
    "triggerExecution" : 480,
    "walCommit" : 63
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 2035,
        "1" : 2386,
        "0" : 1814
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 2088,
        "1" : 2447,
        "0" : 1860
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 2088,
        "1" : 2447,
        "0" : 1860
      }
    },
    "numInputRows" : 160,
    "inputRowsPerSecond" : 141.9698314108252,
    "processedRowsPerSecond" : 333.33333333333337,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@1a6a42c2",
    "numOutputRows" : 29
  }
}
26/02/13 12:08:20 INFO BlockManagerInfo: Removed broadcast_129_piece0 on spark-master:42073 in memory (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 12:08:20 INFO BlockManagerInfo: Removed broadcast_129_piece0 on 172.18.0.14:37571 in memory (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 12:08:20 INFO BlockManagerInfo: Removed broadcast_129_piece0 on 172.18.0.15:37717 in memory (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 12:08:21 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:08:28 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/162 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.162.9a130bd3-e4a4-4944-8f5a-e7f7bc5ab7b4.tmp
26/02/13 12:08:28 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.162.9a130bd3-e4a4-4944-8f5a-e7f7bc5ab7b4.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/162
26/02/13 12:08:28 INFO MicroBatchExecution: Committed offsets for batch 162. Metadata OffsetSeqMetadata(0,1770984508301,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 12:08:28 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:08:28 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:08:28 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:08:28 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:08:28 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:08:28 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:08:28 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#35257 - origin_code.nullCount#35256) > 0)
26/02/13 12:08:28 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#35262 - destination_code.nullCount#35261) > 0)
26/02/13 12:08:28 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#35292 - callsign.nullCount#35291) > 0)
26/02/13 12:08:28 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 12:08:28 INFO DAGScheduler: Got job 87 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 12:08:28 INFO DAGScheduler: Final stage: ResultStage 134 (start at NativeMethodAccessorImpl.java:0)
26/02/13 12:08:28 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 133)
26/02/13 12:08:28 INFO DAGScheduler: Missing parents: List()
26/02/13 12:08:28 INFO DAGScheduler: Submitting ResultStage 134 (MapPartitionsRDD[472] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:08:28 INFO MemoryStore: Block broadcast_130 stored as values in memory (estimated size 99.3 KiB, free 434.0 MiB)
26/02/13 12:08:28 INFO MemoryStore: Block broadcast_130_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 433.9 MiB)
26/02/13 12:08:28 INFO BlockManagerInfo: Added broadcast_130_piece0 in memory on spark-master:42073 (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 12:08:28 INFO SparkContext: Created broadcast 130 from broadcast at DAGScheduler.scala:1585
26/02/13 12:08:28 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 134 (MapPartitionsRDD[472] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 12:08:28 INFO TaskSchedulerImpl: Adding task set 134.0 with 4 tasks resource profile 0
26/02/13 12:08:28 INFO TaskSetManager: Starting task 0.0 in stage 134.0 (TID 280) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:08:28 INFO TaskSetManager: Starting task 1.0 in stage 134.0 (TID 281) (172.18.0.14, executor 1, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:08:28 INFO BlockManagerInfo: Added broadcast_130_piece0 in memory on 172.18.0.14:37571 (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 12:08:28 INFO TaskSetManager: Starting task 2.0 in stage 134.0 (TID 282) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:08:28 INFO TaskSetManager: Finished task 1.0 in stage 134.0 (TID 281) in 19 ms on 172.18.0.14 (executor 1) (1/4)
26/02/13 12:08:28 INFO TaskSetManager: Starting task 3.0 in stage 134.0 (TID 283) (172.18.0.14, executor 1, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:08:28 INFO TaskSetManager: Finished task 0.0 in stage 134.0 (TID 280) in 22 ms on 172.18.0.14 (executor 1) (2/4)
26/02/13 12:08:28 INFO TaskSetManager: Finished task 2.0 in stage 134.0 (TID 282) in 28 ms on 172.18.0.14 (executor 1) (3/4)
26/02/13 12:08:28 INFO TaskSetManager: Finished task 3.0 in stage 134.0 (TID 283) in 25 ms on 172.18.0.14 (executor 1) (4/4)
26/02/13 12:08:28 INFO TaskSchedulerImpl: Removed TaskSet 134.0, whose tasks have all completed, from pool 
26/02/13 12:08:28 INFO DAGScheduler: ResultStage 134 (start at NativeMethodAccessorImpl.java:0) finished in 0.052 s
26/02/13 12:08:28 INFO DAGScheduler: Job 87 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 12:08:28 INFO TaskSchedulerImpl: Killing all running tasks in stage 134: Stage finished
26/02/13 12:08:28 INFO DAGScheduler: Job 87 finished: start at NativeMethodAccessorImpl.java:0, took 0.054973 s
26/02/13 12:08:28 INFO MemoryStore: Block broadcast_131_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.8 MiB)
26/02/13 12:08:28 INFO BlockManagerInfo: Added broadcast_131_piece0 in memory on spark-master:42073 (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 12:08:28 INFO SparkContext: Created broadcast 131 from start at NativeMethodAccessorImpl.java:0
26/02/13 12:08:28 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 162, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@24d28d91]. The input RDD has 3 partitions.
26/02/13 12:08:28 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 12:08:28 INFO DAGScheduler: Got job 88 (start at NativeMethodAccessorImpl.java:0) with 3 output partitions
26/02/13 12:08:28 INFO DAGScheduler: Final stage: ResultStage 135 (start at NativeMethodAccessorImpl.java:0)
26/02/13 12:08:28 INFO DAGScheduler: Parents of final stage: List()
26/02/13 12:08:28 INFO DAGScheduler: Missing parents: List()
26/02/13 12:08:28 INFO DAGScheduler: Submitting ResultStage 135 (MapPartitionsRDD[478] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:08:28 INFO MemoryStore: Block broadcast_132 stored as values in memory (estimated size 49.9 KiB, free 433.8 MiB)
26/02/13 12:08:28 INFO MemoryStore: Block broadcast_132_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.8 MiB)
26/02/13 12:08:28 INFO BlockManagerInfo: Added broadcast_132_piece0 in memory on spark-master:42073 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 12:08:28 INFO SparkContext: Created broadcast 132 from broadcast at DAGScheduler.scala:1585
26/02/13 12:08:28 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 135 (MapPartitionsRDD[478] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
26/02/13 12:08:28 INFO TaskSchedulerImpl: Adding task set 135.0 with 3 tasks resource profile 0
26/02/13 12:08:28 INFO TaskSetManager: Starting task 0.0 in stage 135.0 (TID 284) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:08:28 INFO TaskSetManager: Starting task 1.0 in stage 135.0 (TID 285) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:08:28 INFO TaskSetManager: Starting task 2.0 in stage 135.0 (TID 286) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:08:28 INFO BlockManagerInfo: Added broadcast_132_piece0 in memory on 172.18.0.14:37571 (size: 19.5 KiB, free: 434.0 MiB)
26/02/13 12:08:28 INFO BlockManagerInfo: Added broadcast_132_piece0 in memory on 172.18.0.15:37717 (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 12:08:28 INFO BlockManagerInfo: Added broadcast_131_piece0 in memory on 172.18.0.15:37717 (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 12:08:28 INFO BlockManagerInfo: Added broadcast_131_piece0 in memory on 172.18.0.14:37571 (size: 107.2 KiB, free: 433.9 MiB)
26/02/13 12:08:29 INFO TaskSetManager: Finished task 0.0 in stage 135.0 (TID 284) in 584 ms on 172.18.0.14 (executor 1) (1/3)
26/02/13 12:08:29 INFO TaskSetManager: Finished task 2.0 in stage 135.0 (TID 286) in 588 ms on 172.18.0.14 (executor 1) (2/3)
26/02/13 12:08:29 INFO TaskSetManager: Finished task 1.0 in stage 135.0 (TID 285) in 606 ms on 172.18.0.15 (executor 0) (3/3)
26/02/13 12:08:29 INFO TaskSchedulerImpl: Removed TaskSet 135.0, whose tasks have all completed, from pool 
26/02/13 12:08:29 INFO DAGScheduler: ResultStage 135 (start at NativeMethodAccessorImpl.java:0) finished in 0.614 s
26/02/13 12:08:29 INFO DAGScheduler: Job 88 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 12:08:29 INFO TaskSchedulerImpl: Killing all running tasks in stage 135: Stage finished
26/02/13 12:08:29 INFO DAGScheduler: Job 88 finished: start at NativeMethodAccessorImpl.java:0, took 0.616676 s
26/02/13 12:08:29 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 162, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@24d28d91] is committing.
26/02/13 12:08:29 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 162, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@24d28d91] committed.
26/02/13 12:08:29 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/162 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.162.d0df7bd7-ed13-4fd8-af34-e2ada4fd44de.tmp
26/02/13 12:08:29 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.162.d0df7bd7-ed13-4fd8-af34-e2ada4fd44de.tmp to file:/tmp/spark-checkpoint-enrichment/commits/162
26/02/13 12:08:29 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "4a161d66-225b-4b29-812d-ddec3c05364e",
  "name" : null,
  "timestamp" : "2026-02-13T12:08:28.299Z",
  "batchId" : 162,
  "numInputRows" : 111,
  "inputRowsPerSecond" : 8538.461538461539,
  "processedRowsPerSecond" : 112.91963377416073,
  "durationMs" : {
    "addBatch" : 766,
    "commitOffsets" : 64,
    "getBatch" : 0,
    "latestOffset" : 2,
    "queryPlanning" : 46,
    "triggerExecution" : 983,
    "walCommit" : 104
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 2088,
        "1" : 2447,
        "0" : 1860
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 2125,
        "1" : 2484,
        "0" : 1897
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 2125,
        "1" : 2484,
        "0" : 1897
      }
    },
    "numInputRows" : 111,
    "inputRowsPerSecond" : 8538.461538461539,
    "processedRowsPerSecond" : 112.91963377416073,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@1a6a42c2",
    "numOutputRows" : 30
  }
}
26/02/13 12:08:29 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/163 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.163.c77d8a43-693f-4ddd-a9e8-667288153b2b.tmp
26/02/13 12:08:29 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.163.c77d8a43-693f-4ddd-a9e8-667288153b2b.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/163
26/02/13 12:08:29 INFO MicroBatchExecution: Committed offsets for batch 163. Metadata OffsetSeqMetadata(0,1770984509284,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 12:08:29 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:08:29 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:08:29 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:08:29 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:08:29 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:08:29 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:08:29 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#36111 - origin_code.nullCount#36110) > 0)
26/02/13 12:08:29 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#36116 - destination_code.nullCount#36115) > 0)
26/02/13 12:08:29 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#36146 - callsign.nullCount#36145) > 0)
26/02/13 12:08:29 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 12:08:29 INFO DAGScheduler: Got job 89 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 12:08:29 INFO DAGScheduler: Final stage: ResultStage 137 (start at NativeMethodAccessorImpl.java:0)
26/02/13 12:08:29 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 136)
26/02/13 12:08:29 INFO DAGScheduler: Missing parents: List()
26/02/13 12:08:29 INFO DAGScheduler: Submitting ResultStage 137 (MapPartitionsRDD[483] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:08:29 INFO MemoryStore: Block broadcast_133 stored as values in memory (estimated size 99.3 KiB, free 433.7 MiB)
26/02/13 12:08:29 INFO MemoryStore: Block broadcast_133_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 433.6 MiB)
26/02/13 12:08:29 INFO BlockManagerInfo: Added broadcast_133_piece0 in memory on spark-master:42073 (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 12:08:29 INFO SparkContext: Created broadcast 133 from broadcast at DAGScheduler.scala:1585
26/02/13 12:08:29 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 137 (MapPartitionsRDD[483] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 12:08:29 INFO TaskSchedulerImpl: Adding task set 137.0 with 4 tasks resource profile 0
26/02/13 12:08:29 INFO TaskSetManager: Starting task 0.0 in stage 137.0 (TID 287) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:08:29 INFO TaskSetManager: Starting task 1.0 in stage 137.0 (TID 288) (172.18.0.14, executor 1, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:08:29 INFO BlockManagerInfo: Added broadcast_133_piece0 in memory on 172.18.0.14:37571 (size: 33.3 KiB, free: 433.9 MiB)
26/02/13 12:08:29 INFO TaskSetManager: Starting task 2.0 in stage 137.0 (TID 289) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:08:29 INFO TaskSetManager: Finished task 0.0 in stage 137.0 (TID 287) in 22 ms on 172.18.0.14 (executor 1) (1/4)
26/02/13 12:08:29 INFO TaskSetManager: Starting task 3.0 in stage 137.0 (TID 290) (172.18.0.14, executor 1, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:08:29 INFO TaskSetManager: Finished task 1.0 in stage 137.0 (TID 288) in 23 ms on 172.18.0.14 (executor 1) (2/4)
26/02/13 12:08:29 INFO TaskSetManager: Finished task 3.0 in stage 137.0 (TID 290) in 15 ms on 172.18.0.14 (executor 1) (3/4)
26/02/13 12:08:29 INFO TaskSetManager: Finished task 2.0 in stage 137.0 (TID 289) in 17 ms on 172.18.0.14 (executor 1) (4/4)
26/02/13 12:08:29 INFO TaskSchedulerImpl: Removed TaskSet 137.0, whose tasks have all completed, from pool 
26/02/13 12:08:29 INFO DAGScheduler: ResultStage 137 (start at NativeMethodAccessorImpl.java:0) finished in 0.046 s
26/02/13 12:08:29 INFO DAGScheduler: Job 89 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 12:08:29 INFO TaskSchedulerImpl: Killing all running tasks in stage 137: Stage finished
26/02/13 12:08:29 INFO DAGScheduler: Job 89 finished: start at NativeMethodAccessorImpl.java:0, took 0.048366 s
26/02/13 12:08:29 INFO MemoryStore: Block broadcast_134_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.5 MiB)
26/02/13 12:08:29 INFO BlockManagerInfo: Added broadcast_134_piece0 in memory on spark-master:42073 (size: 107.2 KiB, free: 434.0 MiB)
26/02/13 12:08:29 INFO SparkContext: Created broadcast 134 from start at NativeMethodAccessorImpl.java:0
26/02/13 12:08:29 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 163, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@583d3ec7]. The input RDD has 3 partitions.
26/02/13 12:08:29 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 12:08:29 INFO DAGScheduler: Got job 90 (start at NativeMethodAccessorImpl.java:0) with 3 output partitions
26/02/13 12:08:29 INFO DAGScheduler: Final stage: ResultStage 138 (start at NativeMethodAccessorImpl.java:0)
26/02/13 12:08:29 INFO DAGScheduler: Parents of final stage: List()
26/02/13 12:08:29 INFO DAGScheduler: Missing parents: List()
26/02/13 12:08:29 INFO DAGScheduler: Submitting ResultStage 138 (MapPartitionsRDD[489] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:08:29 INFO MemoryStore: Block broadcast_135 stored as values in memory (estimated size 49.9 KiB, free 433.5 MiB)
26/02/13 12:08:29 INFO MemoryStore: Block broadcast_135_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.5 MiB)
26/02/13 12:08:29 INFO BlockManagerInfo: Added broadcast_135_piece0 in memory on spark-master:42073 (size: 19.5 KiB, free: 433.9 MiB)
26/02/13 12:08:29 INFO SparkContext: Created broadcast 135 from broadcast at DAGScheduler.scala:1585
26/02/13 12:08:29 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 138 (MapPartitionsRDD[489] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
26/02/13 12:08:29 INFO TaskSchedulerImpl: Adding task set 138.0 with 3 tasks resource profile 0
26/02/13 12:08:29 INFO TaskSetManager: Starting task 1.0 in stage 138.0 (TID 291) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:08:29 INFO TaskSetManager: Starting task 0.0 in stage 138.0 (TID 292) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:08:29 INFO TaskSetManager: Starting task 2.0 in stage 138.0 (TID 293) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:08:29 INFO BlockManagerInfo: Added broadcast_135_piece0 in memory on 172.18.0.14:37571 (size: 19.5 KiB, free: 433.9 MiB)
26/02/13 12:08:29 INFO BlockManagerInfo: Added broadcast_135_piece0 in memory on 172.18.0.15:37717 (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 12:08:29 INFO BlockManagerInfo: Added broadcast_134_piece0 in memory on 172.18.0.14:37571 (size: 107.2 KiB, free: 433.7 MiB)
26/02/13 12:08:29 INFO BlockManagerInfo: Added broadcast_134_piece0 in memory on 172.18.0.15:37717 (size: 107.2 KiB, free: 434.0 MiB)
26/02/13 12:08:29 INFO TaskSetManager: Finished task 2.0 in stage 138.0 (TID 293) in 52 ms on 172.18.0.14 (executor 1) (1/3)
26/02/13 12:08:29 INFO TaskSetManager: Finished task 0.0 in stage 138.0 (TID 292) in 58 ms on 172.18.0.14 (executor 1) (2/3)
26/02/13 12:08:29 INFO TaskSetManager: Finished task 1.0 in stage 138.0 (TID 291) in 76 ms on 172.18.0.15 (executor 0) (3/3)
26/02/13 12:08:29 INFO TaskSchedulerImpl: Removed TaskSet 138.0, whose tasks have all completed, from pool 
26/02/13 12:08:29 INFO DAGScheduler: ResultStage 138 (start at NativeMethodAccessorImpl.java:0) finished in 0.083 s
26/02/13 12:08:29 INFO DAGScheduler: Job 90 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 12:08:29 INFO TaskSchedulerImpl: Killing all running tasks in stage 138: Stage finished
26/02/13 12:08:29 INFO DAGScheduler: Job 90 finished: start at NativeMethodAccessorImpl.java:0, took 0.084508 s
26/02/13 12:08:29 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 163, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@583d3ec7] is committing.
26/02/13 12:08:29 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 163, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@583d3ec7] committed.
26/02/13 12:08:29 INFO BlockManagerInfo: Removed broadcast_135_piece0 on spark-master:42073 in memory (size: 19.5 KiB, free: 434.0 MiB)
26/02/13 12:08:29 INFO BlockManagerInfo: Removed broadcast_135_piece0 on 172.18.0.14:37571 in memory (size: 19.5 KiB, free: 433.8 MiB)
26/02/13 12:08:29 INFO BlockManagerInfo: Removed broadcast_135_piece0 on 172.18.0.15:37717 in memory (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 12:08:29 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/163 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.163.0915af6e-d94f-404f-b2c5-ffc6620af710.tmp
26/02/13 12:08:29 INFO BlockManagerInfo: Removed broadcast_133_piece0 on spark-master:42073 in memory (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 12:08:29 INFO BlockManagerInfo: Removed broadcast_133_piece0 on 172.18.0.14:37571 in memory (size: 33.3 KiB, free: 433.8 MiB)
26/02/13 12:08:29 INFO BlockManagerInfo: Removed broadcast_131_piece0 on spark-master:42073 in memory (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 12:08:29 INFO BlockManagerInfo: Removed broadcast_131_piece0 on 172.18.0.15:37717 in memory (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 12:08:29 INFO BlockManagerInfo: Removed broadcast_131_piece0 on 172.18.0.14:37571 in memory (size: 107.2 KiB, free: 433.9 MiB)
26/02/13 12:08:29 INFO BlockManagerInfo: Removed broadcast_128_piece0 on spark-master:42073 in memory (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 12:08:29 INFO BlockManagerInfo: Removed broadcast_128_piece0 on 172.18.0.14:37571 in memory (size: 107.2 KiB, free: 434.0 MiB)
26/02/13 12:08:29 INFO BlockManagerInfo: Removed broadcast_128_piece0 on 172.18.0.15:37717 in memory (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 12:08:29 INFO BlockManagerInfo: Removed broadcast_132_piece0 on spark-master:42073 in memory (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 12:08:29 INFO BlockManagerInfo: Removed broadcast_132_piece0 on 172.18.0.14:37571 in memory (size: 19.5 KiB, free: 434.0 MiB)
26/02/13 12:08:29 INFO BlockManagerInfo: Removed broadcast_132_piece0 on 172.18.0.15:37717 in memory (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 12:08:29 INFO BlockManagerInfo: Removed broadcast_130_piece0 on spark-master:42073 in memory (size: 33.3 KiB, free: 434.3 MiB)
26/02/13 12:08:29 INFO BlockManagerInfo: Removed broadcast_130_piece0 on 172.18.0.14:37571 in memory (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 12:08:29 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.163.0915af6e-d94f-404f-b2c5-ffc6620af710.tmp to file:/tmp/spark-checkpoint-enrichment/commits/163
26/02/13 12:08:29 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "4a161d66-225b-4b29-812d-ddec3c05364e",
  "name" : null,
  "timestamp" : "2026-02-13T12:08:29.283Z",
  "batchId" : 163,
  "numInputRows" : 137,
  "inputRowsPerSecond" : 139.22764227642276,
  "processedRowsPerSecond" : 367.29222520107237,
  "durationMs" : {
    "addBatch" : 211,
    "commitOffsets" : 80,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 26,
    "triggerExecution" : 373,
    "walCommit" : 54
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 2125,
        "1" : 2484,
        "0" : 1897
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 2171,
        "1" : 2538,
        "0" : 1934
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 2171,
        "1" : 2538,
        "0" : 1934
      }
    },
    "numInputRows" : 137,
    "inputRowsPerSecond" : 139.22764227642276,
    "processedRowsPerSecond" : 367.29222520107237,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@1a6a42c2",
    "numOutputRows" : 26
  }
}
26/02/13 12:08:39 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:08:44 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/164 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.164.71876693-6c8f-4f85-bd05-26f648637de1.tmp
26/02/13 12:08:44 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.164.71876693-6c8f-4f85-bd05-26f648637de1.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/164
26/02/13 12:08:44 INFO MicroBatchExecution: Committed offsets for batch 164. Metadata OffsetSeqMetadata(0,1770984524790,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 12:08:44 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:08:44 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:08:44 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:08:44 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:08:44 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:08:44 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:08:44 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#36965 - origin_code.nullCount#36964) > 0)
26/02/13 12:08:44 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#36970 - destination_code.nullCount#36969) > 0)
26/02/13 12:08:44 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#37000 - callsign.nullCount#36999) > 0)
26/02/13 12:08:45 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 12:08:45 INFO DAGScheduler: Got job 91 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 12:08:45 INFO DAGScheduler: Final stage: ResultStage 140 (start at NativeMethodAccessorImpl.java:0)
26/02/13 12:08:45 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 139)
26/02/13 12:08:45 INFO DAGScheduler: Missing parents: List()
26/02/13 12:08:45 INFO DAGScheduler: Submitting ResultStage 140 (MapPartitionsRDD[494] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:08:45 INFO MemoryStore: Block broadcast_136 stored as values in memory (estimated size 99.3 KiB, free 434.0 MiB)
26/02/13 12:08:45 INFO MemoryStore: Block broadcast_136_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 433.9 MiB)
26/02/13 12:08:45 INFO BlockManagerInfo: Added broadcast_136_piece0 in memory on spark-master:42073 (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 12:08:45 INFO SparkContext: Created broadcast 136 from broadcast at DAGScheduler.scala:1585
26/02/13 12:08:45 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 140 (MapPartitionsRDD[494] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 12:08:45 INFO TaskSchedulerImpl: Adding task set 140.0 with 4 tasks resource profile 0
26/02/13 12:08:45 INFO TaskSetManager: Starting task 0.0 in stage 140.0 (TID 294) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:08:45 INFO TaskSetManager: Starting task 1.0 in stage 140.0 (TID 295) (172.18.0.14, executor 1, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:08:45 INFO BlockManagerInfo: Added broadcast_136_piece0 in memory on 172.18.0.14:37571 (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 12:08:45 INFO TaskSetManager: Starting task 2.0 in stage 140.0 (TID 296) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:08:45 INFO TaskSetManager: Finished task 1.0 in stage 140.0 (TID 295) in 20 ms on 172.18.0.14 (executor 1) (1/4)
26/02/13 12:08:45 INFO TaskSetManager: Starting task 3.0 in stage 140.0 (TID 297) (172.18.0.14, executor 1, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:08:45 INFO TaskSetManager: Finished task 0.0 in stage 140.0 (TID 294) in 22 ms on 172.18.0.14 (executor 1) (2/4)
26/02/13 12:08:45 INFO TaskSetManager: Finished task 2.0 in stage 140.0 (TID 296) in 12 ms on 172.18.0.14 (executor 1) (3/4)
26/02/13 12:08:45 INFO TaskSetManager: Finished task 3.0 in stage 140.0 (TID 297) in 11 ms on 172.18.0.14 (executor 1) (4/4)
26/02/13 12:08:45 INFO TaskSchedulerImpl: Removed TaskSet 140.0, whose tasks have all completed, from pool 
26/02/13 12:08:45 INFO DAGScheduler: ResultStage 140 (start at NativeMethodAccessorImpl.java:0) finished in 0.039 s
26/02/13 12:08:45 INFO DAGScheduler: Job 91 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 12:08:45 INFO TaskSchedulerImpl: Killing all running tasks in stage 140: Stage finished
26/02/13 12:08:45 INFO DAGScheduler: Job 91 finished: start at NativeMethodAccessorImpl.java:0, took 0.040161 s
26/02/13 12:08:45 INFO MemoryStore: Block broadcast_137_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.8 MiB)
26/02/13 12:08:45 INFO BlockManagerInfo: Added broadcast_137_piece0 in memory on spark-master:42073 (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 12:08:45 INFO SparkContext: Created broadcast 137 from start at NativeMethodAccessorImpl.java:0
26/02/13 12:08:45 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 164, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@3109d382]. The input RDD has 3 partitions.
26/02/13 12:08:45 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 12:08:45 INFO DAGScheduler: Got job 92 (start at NativeMethodAccessorImpl.java:0) with 3 output partitions
26/02/13 12:08:45 INFO DAGScheduler: Final stage: ResultStage 141 (start at NativeMethodAccessorImpl.java:0)
26/02/13 12:08:45 INFO DAGScheduler: Parents of final stage: List()
26/02/13 12:08:45 INFO DAGScheduler: Missing parents: List()
26/02/13 12:08:45 INFO DAGScheduler: Submitting ResultStage 141 (MapPartitionsRDD[500] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:08:45 INFO MemoryStore: Block broadcast_138 stored as values in memory (estimated size 49.9 KiB, free 433.8 MiB)
26/02/13 12:08:45 INFO MemoryStore: Block broadcast_138_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.8 MiB)
26/02/13 12:08:45 INFO BlockManagerInfo: Added broadcast_138_piece0 in memory on spark-master:42073 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 12:08:45 INFO SparkContext: Created broadcast 138 from broadcast at DAGScheduler.scala:1585
26/02/13 12:08:45 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 141 (MapPartitionsRDD[500] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
26/02/13 12:08:45 INFO TaskSchedulerImpl: Adding task set 141.0 with 3 tasks resource profile 0
26/02/13 12:08:45 INFO TaskSetManager: Starting task 0.0 in stage 141.0 (TID 298) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:08:45 INFO TaskSetManager: Starting task 1.0 in stage 141.0 (TID 299) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:08:45 INFO TaskSetManager: Starting task 2.0 in stage 141.0 (TID 300) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:08:45 INFO BlockManagerInfo: Added broadcast_138_piece0 in memory on 172.18.0.14:37571 (size: 19.5 KiB, free: 434.0 MiB)
26/02/13 12:08:45 INFO BlockManagerInfo: Added broadcast_138_piece0 in memory on 172.18.0.15:37717 (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 12:08:45 INFO BlockManagerInfo: Added broadcast_137_piece0 in memory on 172.18.0.14:37571 (size: 107.2 KiB, free: 433.9 MiB)
26/02/13 12:08:45 INFO BlockManagerInfo: Added broadcast_137_piece0 in memory on 172.18.0.15:37717 (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 12:08:45 INFO TaskSetManager: Finished task 0.0 in stage 141.0 (TID 298) in 553 ms on 172.18.0.14 (executor 1) (1/3)
26/02/13 12:08:45 INFO TaskSetManager: Finished task 2.0 in stage 141.0 (TID 300) in 559 ms on 172.18.0.14 (executor 1) (2/3)
26/02/13 12:08:45 INFO TaskSetManager: Finished task 1.0 in stage 141.0 (TID 299) in 585 ms on 172.18.0.15 (executor 0) (3/3)
26/02/13 12:08:45 INFO TaskSchedulerImpl: Removed TaskSet 141.0, whose tasks have all completed, from pool 
26/02/13 12:08:45 INFO DAGScheduler: ResultStage 141 (start at NativeMethodAccessorImpl.java:0) finished in 0.589 s
26/02/13 12:08:45 INFO DAGScheduler: Job 92 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 12:08:45 INFO TaskSchedulerImpl: Killing all running tasks in stage 141: Stage finished
26/02/13 12:08:45 INFO DAGScheduler: Job 92 finished: start at NativeMethodAccessorImpl.java:0, took 0.590106 s
26/02/13 12:08:45 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 164, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@3109d382] is committing.
26/02/13 12:08:45 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 164, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@3109d382] committed.
26/02/13 12:08:45 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/164 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.164.9f174f49-a7f6-49e9-9bca-46d299bb4284.tmp
26/02/13 12:08:45 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.164.9f174f49-a7f6-49e9-9bca-46d299bb4284.tmp to file:/tmp/spark-checkpoint-enrichment/commits/164
26/02/13 12:08:45 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "4a161d66-225b-4b29-812d-ddec3c05364e",
  "name" : null,
  "timestamp" : "2026-02-13T12:08:44.788Z",
  "batchId" : 164,
  "numInputRows" : 81,
  "inputRowsPerSecond" : 6750.0,
  "processedRowsPerSecond" : 88.91328210757409,
  "durationMs" : {
    "addBatch" : 717,
    "commitOffsets" : 56,
    "getBatch" : 1,
    "latestOffset" : 2,
    "queryPlanning" : 48,
    "triggerExecution" : 911,
    "walCommit" : 87
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 2171,
        "1" : 2538,
        "0" : 1934
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 2199,
        "1" : 2566,
        "0" : 1959
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 2199,
        "1" : 2566,
        "0" : 1959
      }
    },
    "numInputRows" : 81,
    "inputRowsPerSecond" : 6750.0,
    "processedRowsPerSecond" : 88.91328210757409,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@1a6a42c2",
    "numOutputRows" : 26
  }
}
26/02/13 12:08:45 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/165 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.165.29a6bb09-6790-4f75-b6a5-f3ebe08f6288.tmp
26/02/13 12:08:45 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.165.29a6bb09-6790-4f75-b6a5-f3ebe08f6288.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/165
26/02/13 12:08:45 INFO MicroBatchExecution: Committed offsets for batch 165. Metadata OffsetSeqMetadata(0,1770984525700,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 12:08:45 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:08:45 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:08:45 INFO BlockManagerInfo: Removed broadcast_138_piece0 on spark-master:42073 in memory (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 12:08:45 INFO BlockManagerInfo: Removed broadcast_138_piece0 on 172.18.0.14:37571 in memory (size: 19.5 KiB, free: 433.9 MiB)
26/02/13 12:08:45 INFO BlockManagerInfo: Removed broadcast_138_piece0 on 172.18.0.15:37717 in memory (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 12:08:45 INFO BlockManagerInfo: Removed broadcast_136_piece0 on spark-master:42073 in memory (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 12:08:45 INFO BlockManagerInfo: Removed broadcast_136_piece0 on 172.18.0.14:37571 in memory (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 12:08:45 INFO BlockManagerInfo: Removed broadcast_137_piece0 on spark-master:42073 in memory (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 12:08:45 INFO BlockManagerInfo: Removed broadcast_137_piece0 on 172.18.0.14:37571 in memory (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 12:08:45 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:08:45 INFO BlockManagerInfo: Removed broadcast_137_piece0 on 172.18.0.15:37717 in memory (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 12:08:45 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:08:45 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:08:45 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:08:45 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#37819 - origin_code.nullCount#37818) > 0)
26/02/13 12:08:45 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#37824 - destination_code.nullCount#37823) > 0)
26/02/13 12:08:45 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#37854 - callsign.nullCount#37853) > 0)
26/02/13 12:08:45 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 12:08:45 INFO DAGScheduler: Got job 93 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 12:08:45 INFO DAGScheduler: Final stage: ResultStage 143 (start at NativeMethodAccessorImpl.java:0)
26/02/13 12:08:45 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 142)
26/02/13 12:08:45 INFO DAGScheduler: Missing parents: List()
26/02/13 12:08:45 INFO DAGScheduler: Submitting ResultStage 143 (MapPartitionsRDD[505] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:08:45 INFO MemoryStore: Block broadcast_139 stored as values in memory (estimated size 99.3 KiB, free 434.0 MiB)
26/02/13 12:08:45 INFO MemoryStore: Block broadcast_139_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 433.9 MiB)
26/02/13 12:08:45 INFO BlockManagerInfo: Added broadcast_139_piece0 in memory on spark-master:42073 (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 12:08:45 INFO SparkContext: Created broadcast 139 from broadcast at DAGScheduler.scala:1585
26/02/13 12:08:45 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 143 (MapPartitionsRDD[505] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 12:08:45 INFO TaskSchedulerImpl: Adding task set 143.0 with 4 tasks resource profile 0
26/02/13 12:08:45 INFO TaskSetManager: Starting task 0.0 in stage 143.0 (TID 301) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:08:45 INFO TaskSetManager: Starting task 1.0 in stage 143.0 (TID 302) (172.18.0.14, executor 1, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:08:45 INFO BlockManagerInfo: Added broadcast_139_piece0 in memory on 172.18.0.14:37571 (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 12:08:45 INFO TaskSetManager: Starting task 2.0 in stage 143.0 (TID 303) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:08:45 INFO TaskSetManager: Starting task 3.0 in stage 143.0 (TID 304) (172.18.0.14, executor 1, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:08:45 INFO TaskSetManager: Finished task 1.0 in stage 143.0 (TID 302) in 26 ms on 172.18.0.14 (executor 1) (1/4)
26/02/13 12:08:45 INFO TaskSetManager: Finished task 0.0 in stage 143.0 (TID 301) in 28 ms on 172.18.0.14 (executor 1) (2/4)
26/02/13 12:08:45 INFO TaskSetManager: Finished task 3.0 in stage 143.0 (TID 304) in 18 ms on 172.18.0.14 (executor 1) (3/4)
26/02/13 12:08:45 INFO TaskSetManager: Finished task 2.0 in stage 143.0 (TID 303) in 21 ms on 172.18.0.14 (executor 1) (4/4)
26/02/13 12:08:45 INFO TaskSchedulerImpl: Removed TaskSet 143.0, whose tasks have all completed, from pool 
26/02/13 12:08:45 INFO DAGScheduler: ResultStage 143 (start at NativeMethodAccessorImpl.java:0) finished in 0.055 s
26/02/13 12:08:45 INFO DAGScheduler: Job 93 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 12:08:45 INFO TaskSchedulerImpl: Killing all running tasks in stage 143: Stage finished
26/02/13 12:08:45 INFO DAGScheduler: Job 93 finished: start at NativeMethodAccessorImpl.java:0, took 0.058150 s
26/02/13 12:08:45 INFO BlockManagerInfo: Removed broadcast_134_piece0 on spark-master:42073 in memory (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 12:08:45 INFO MemoryStore: Block broadcast_140_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.9 MiB)
26/02/13 12:08:45 INFO BlockManagerInfo: Added broadcast_140_piece0 in memory on spark-master:42073 (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 12:08:45 INFO SparkContext: Created broadcast 140 from start at NativeMethodAccessorImpl.java:0
26/02/13 12:08:45 INFO BlockManagerInfo: Removed broadcast_134_piece0 on 172.18.0.15:37717 in memory (size: 107.2 KiB, free: 434.4 MiB)
26/02/13 12:08:45 INFO BlockManagerInfo: Removed broadcast_134_piece0 on 172.18.0.14:37571 in memory (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 12:08:45 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 165, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@5a0df5dd]. The input RDD has 3 partitions.
26/02/13 12:08:45 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 12:08:45 INFO DAGScheduler: Got job 94 (start at NativeMethodAccessorImpl.java:0) with 3 output partitions
26/02/13 12:08:45 INFO DAGScheduler: Final stage: ResultStage 144 (start at NativeMethodAccessorImpl.java:0)
26/02/13 12:08:45 INFO DAGScheduler: Parents of final stage: List()
26/02/13 12:08:45 INFO DAGScheduler: Missing parents: List()
26/02/13 12:08:45 INFO DAGScheduler: Submitting ResultStage 144 (MapPartitionsRDD[511] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:08:45 INFO MemoryStore: Block broadcast_141 stored as values in memory (estimated size 49.9 KiB, free 433.9 MiB)
26/02/13 12:08:45 INFO MemoryStore: Block broadcast_141_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.9 MiB)
26/02/13 12:08:45 INFO BlockManagerInfo: Added broadcast_141_piece0 in memory on spark-master:42073 (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 12:08:45 INFO SparkContext: Created broadcast 141 from broadcast at DAGScheduler.scala:1585
26/02/13 12:08:45 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 144 (MapPartitionsRDD[511] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
26/02/13 12:08:45 INFO TaskSchedulerImpl: Adding task set 144.0 with 3 tasks resource profile 0
26/02/13 12:08:45 INFO TaskSetManager: Starting task 0.0 in stage 144.0 (TID 305) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:08:45 INFO TaskSetManager: Starting task 1.0 in stage 144.0 (TID 306) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:08:45 INFO TaskSetManager: Starting task 2.0 in stage 144.0 (TID 307) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:08:45 INFO BlockManagerInfo: Added broadcast_141_piece0 in memory on 172.18.0.14:37571 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 12:08:45 INFO BlockManagerInfo: Added broadcast_141_piece0 in memory on 172.18.0.15:37717 (size: 19.5 KiB, free: 434.4 MiB)
26/02/13 12:08:45 INFO BlockManagerInfo: Added broadcast_140_piece0 in memory on 172.18.0.14:37571 (size: 107.2 KiB, free: 434.0 MiB)
26/02/13 12:08:45 INFO BlockManagerInfo: Added broadcast_140_piece0 in memory on 172.18.0.15:37717 (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 12:08:46 INFO TaskSetManager: Finished task 2.0 in stage 144.0 (TID 307) in 86 ms on 172.18.0.14 (executor 1) (1/3)
26/02/13 12:08:46 INFO TaskSetManager: Finished task 0.0 in stage 144.0 (TID 305) in 94 ms on 172.18.0.14 (executor 1) (2/3)
26/02/13 12:08:46 INFO TaskSetManager: Finished task 1.0 in stage 144.0 (TID 306) in 109 ms on 172.18.0.15 (executor 0) (3/3)
26/02/13 12:08:46 INFO TaskSchedulerImpl: Removed TaskSet 144.0, whose tasks have all completed, from pool 
26/02/13 12:08:46 INFO DAGScheduler: ResultStage 144 (start at NativeMethodAccessorImpl.java:0) finished in 0.116 s
26/02/13 12:08:46 INFO DAGScheduler: Job 94 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 12:08:46 INFO TaskSchedulerImpl: Killing all running tasks in stage 144: Stage finished
26/02/13 12:08:46 INFO DAGScheduler: Job 94 finished: start at NativeMethodAccessorImpl.java:0, took 0.118884 s
26/02/13 12:08:46 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 165, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@5a0df5dd] is committing.
26/02/13 12:08:46 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 165, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@5a0df5dd] committed.
26/02/13 12:08:46 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/165 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.165.f4dafeaf-5c4e-4746-887e-d88d211541cd.tmp
26/02/13 12:08:46 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.165.f4dafeaf-5c4e-4746-887e-d88d211541cd.tmp to file:/tmp/spark-checkpoint-enrichment/commits/165
26/02/13 12:08:46 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "4a161d66-225b-4b29-812d-ddec3c05364e",
  "name" : null,
  "timestamp" : "2026-02-13T12:08:45.699Z",
  "batchId" : 165,
  "numInputRows" : 166,
  "inputRowsPerSecond" : 182.21734357848518,
  "processedRowsPerSecond" : 387.85046728971963,
  "durationMs" : {
    "addBatch" : 287,
    "commitOffsets" : 64,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 24,
    "triggerExecution" : 428,
    "walCommit" : 52
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 2199,
        "1" : 2566,
        "0" : 1959
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 2254,
        "1" : 2627,
        "0" : 2009
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 2254,
        "1" : 2627,
        "0" : 2009
      }
    },
    "numInputRows" : 166,
    "inputRowsPerSecond" : 182.21734357848518,
    "processedRowsPerSecond" : 387.85046728971963,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@1a6a42c2",
    "numOutputRows" : 30
  }
}
26/02/13 12:08:56 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:08:56 INFO BlockManagerInfo: Removed broadcast_141_piece0 on spark-master:42073 in memory (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 12:08:56 INFO BlockManagerInfo: Removed broadcast_141_piece0 on 172.18.0.14:37571 in memory (size: 19.5 KiB, free: 434.0 MiB)
26/02/13 12:08:56 INFO BlockManagerInfo: Removed broadcast_141_piece0 on 172.18.0.15:37717 in memory (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 12:08:56 INFO BlockManagerInfo: Removed broadcast_139_piece0 on spark-master:42073 in memory (size: 33.3 KiB, free: 434.3 MiB)
26/02/13 12:08:56 INFO BlockManagerInfo: Removed broadcast_139_piece0 on 172.18.0.14:37571 in memory (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 12:09:01 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/166 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.166.694b75f7-35ad-4952-8743-4b8b60ff464c.tmp
26/02/13 12:09:01 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.166.694b75f7-35ad-4952-8743-4b8b60ff464c.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/166
26/02/13 12:09:01 INFO MicroBatchExecution: Committed offsets for batch 166. Metadata OffsetSeqMetadata(0,1770984541196,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 12:09:01 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:09:01 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:09:01 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:09:01 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:09:01 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:09:01 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:09:01 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#38673 - origin_code.nullCount#38672) > 0)
26/02/13 12:09:01 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#38678 - destination_code.nullCount#38677) > 0)
26/02/13 12:09:01 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#38708 - callsign.nullCount#38707) > 0)
26/02/13 12:09:01 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 12:09:01 INFO DAGScheduler: Got job 95 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 12:09:01 INFO DAGScheduler: Final stage: ResultStage 146 (start at NativeMethodAccessorImpl.java:0)
26/02/13 12:09:01 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 145)
26/02/13 12:09:01 INFO DAGScheduler: Missing parents: List()
26/02/13 12:09:01 INFO DAGScheduler: Submitting ResultStage 146 (MapPartitionsRDD[516] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:09:01 INFO MemoryStore: Block broadcast_142 stored as values in memory (estimated size 99.3 KiB, free 434.0 MiB)
26/02/13 12:09:01 INFO MemoryStore: Block broadcast_142_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 433.9 MiB)
26/02/13 12:09:01 INFO BlockManagerInfo: Added broadcast_142_piece0 in memory on spark-master:42073 (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 12:09:01 INFO SparkContext: Created broadcast 142 from broadcast at DAGScheduler.scala:1585
26/02/13 12:09:01 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 146 (MapPartitionsRDD[516] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 12:09:01 INFO TaskSchedulerImpl: Adding task set 146.0 with 4 tasks resource profile 0
26/02/13 12:09:01 INFO TaskSetManager: Starting task 0.0 in stage 146.0 (TID 308) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:09:01 INFO TaskSetManager: Starting task 1.0 in stage 146.0 (TID 309) (172.18.0.14, executor 1, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:09:01 INFO BlockManagerInfo: Added broadcast_142_piece0 in memory on 172.18.0.14:37571 (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 12:09:01 INFO TaskSetManager: Starting task 2.0 in stage 146.0 (TID 310) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:09:01 INFO TaskSetManager: Finished task 1.0 in stage 146.0 (TID 309) in 23 ms on 172.18.0.14 (executor 1) (1/4)
26/02/13 12:09:01 INFO TaskSetManager: Starting task 3.0 in stage 146.0 (TID 311) (172.18.0.14, executor 1, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:09:01 INFO TaskSetManager: Finished task 0.0 in stage 146.0 (TID 308) in 32 ms on 172.18.0.14 (executor 1) (2/4)
26/02/13 12:09:01 INFO TaskSetManager: Finished task 2.0 in stage 146.0 (TID 310) in 21 ms on 172.18.0.14 (executor 1) (3/4)
26/02/13 12:09:01 INFO TaskSetManager: Finished task 3.0 in stage 146.0 (TID 311) in 39 ms on 172.18.0.14 (executor 1) (4/4)
26/02/13 12:09:01 INFO TaskSchedulerImpl: Removed TaskSet 146.0, whose tasks have all completed, from pool 
26/02/13 12:09:01 INFO DAGScheduler: ResultStage 146 (start at NativeMethodAccessorImpl.java:0) finished in 0.079 s
26/02/13 12:09:01 INFO DAGScheduler: Job 95 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 12:09:01 INFO TaskSchedulerImpl: Killing all running tasks in stage 146: Stage finished
26/02/13 12:09:01 INFO DAGScheduler: Job 95 finished: start at NativeMethodAccessorImpl.java:0, took 0.082590 s
26/02/13 12:09:01 INFO MemoryStore: Block broadcast_143_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.8 MiB)
26/02/13 12:09:01 INFO BlockManagerInfo: Added broadcast_143_piece0 in memory on spark-master:42073 (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 12:09:01 INFO SparkContext: Created broadcast 143 from start at NativeMethodAccessorImpl.java:0
26/02/13 12:09:01 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 166, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@267f2840]. The input RDD has 3 partitions.
26/02/13 12:09:01 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 12:09:01 INFO DAGScheduler: Got job 96 (start at NativeMethodAccessorImpl.java:0) with 3 output partitions
26/02/13 12:09:01 INFO DAGScheduler: Final stage: ResultStage 147 (start at NativeMethodAccessorImpl.java:0)
26/02/13 12:09:01 INFO DAGScheduler: Parents of final stage: List()
26/02/13 12:09:01 INFO DAGScheduler: Missing parents: List()
26/02/13 12:09:01 INFO DAGScheduler: Submitting ResultStage 147 (MapPartitionsRDD[522] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:09:01 INFO MemoryStore: Block broadcast_144 stored as values in memory (estimated size 49.9 KiB, free 433.8 MiB)
26/02/13 12:09:01 INFO MemoryStore: Block broadcast_144_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.8 MiB)
26/02/13 12:09:01 INFO BlockManagerInfo: Added broadcast_144_piece0 in memory on spark-master:42073 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 12:09:01 INFO SparkContext: Created broadcast 144 from broadcast at DAGScheduler.scala:1585
26/02/13 12:09:01 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 147 (MapPartitionsRDD[522] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
26/02/13 12:09:01 INFO TaskSchedulerImpl: Adding task set 147.0 with 3 tasks resource profile 0
26/02/13 12:09:01 INFO TaskSetManager: Starting task 0.0 in stage 147.0 (TID 312) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:09:01 INFO TaskSetManager: Starting task 1.0 in stage 147.0 (TID 313) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:09:01 INFO TaskSetManager: Starting task 2.0 in stage 147.0 (TID 314) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:09:01 INFO BlockManagerInfo: Added broadcast_144_piece0 in memory on 172.18.0.15:37717 (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 12:09:01 INFO BlockManagerInfo: Added broadcast_144_piece0 in memory on 172.18.0.14:37571 (size: 19.5 KiB, free: 434.0 MiB)
26/02/13 12:09:01 INFO BlockManagerInfo: Added broadcast_143_piece0 in memory on 172.18.0.15:37717 (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 12:09:01 INFO BlockManagerInfo: Added broadcast_143_piece0 in memory on 172.18.0.14:37571 (size: 107.2 KiB, free: 433.9 MiB)
26/02/13 12:09:02 INFO TaskSetManager: Finished task 0.0 in stage 147.0 (TID 312) in 584 ms on 172.18.0.14 (executor 1) (1/3)
26/02/13 12:09:02 INFO TaskSetManager: Finished task 2.0 in stage 147.0 (TID 314) in 584 ms on 172.18.0.14 (executor 1) (2/3)
26/02/13 12:09:02 INFO TaskSetManager: Finished task 1.0 in stage 147.0 (TID 313) in 604 ms on 172.18.0.15 (executor 0) (3/3)
26/02/13 12:09:02 INFO TaskSchedulerImpl: Removed TaskSet 147.0, whose tasks have all completed, from pool 
26/02/13 12:09:02 INFO DAGScheduler: ResultStage 147 (start at NativeMethodAccessorImpl.java:0) finished in 0.611 s
26/02/13 12:09:02 INFO DAGScheduler: Job 96 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 12:09:02 INFO TaskSchedulerImpl: Killing all running tasks in stage 147: Stage finished
26/02/13 12:09:02 INFO DAGScheduler: Job 96 finished: start at NativeMethodAccessorImpl.java:0, took 0.613094 s
26/02/13 12:09:02 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 166, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@267f2840] is committing.
26/02/13 12:09:02 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 166, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@267f2840] committed.
26/02/13 12:09:02 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/166 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.166.d5dbcd0f-98b1-4360-81b5-757c705828a4.tmp
26/02/13 12:09:02 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.166.d5dbcd0f-98b1-4360-81b5-757c705828a4.tmp to file:/tmp/spark-checkpoint-enrichment/commits/166
26/02/13 12:09:02 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "4a161d66-225b-4b29-812d-ddec3c05364e",
  "name" : null,
  "timestamp" : "2026-02-13T12:09:01.195Z",
  "batchId" : 166,
  "numInputRows" : 111,
  "inputRowsPerSecond" : 8538.461538461539,
  "processedRowsPerSecond" : 121.44420131291028,
  "durationMs" : {
    "addBatch" : 772,
    "commitOffsets" : 60,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 20,
    "triggerExecution" : 914,
    "walCommit" : 59
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 2254,
        "1" : 2627,
        "0" : 2009
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 2291,
        "1" : 2664,
        "0" : 2046
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 2291,
        "1" : 2664,
        "0" : 2046
      }
    },
    "numInputRows" : 111,
    "inputRowsPerSecond" : 8538.461538461539,
    "processedRowsPerSecond" : 121.44420131291028,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@1a6a42c2",
    "numOutputRows" : 29
  }
}
26/02/13 12:09:02 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/167 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.167.483afdcf-c2b6-45a5-abe5-20d2830a7204.tmp
26/02/13 12:09:02 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.167.483afdcf-c2b6-45a5-abe5-20d2830a7204.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/167
26/02/13 12:09:02 INFO MicroBatchExecution: Committed offsets for batch 167. Metadata OffsetSeqMetadata(0,1770984542110,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 12:09:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:09:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:09:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:09:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:09:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:09:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:09:02 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#39527 - origin_code.nullCount#39526) > 0)
26/02/13 12:09:02 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#39532 - destination_code.nullCount#39531) > 0)
26/02/13 12:09:02 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#39562 - callsign.nullCount#39561) > 0)
26/02/13 12:09:02 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 12:09:02 INFO DAGScheduler: Got job 97 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 12:09:02 INFO DAGScheduler: Final stage: ResultStage 149 (start at NativeMethodAccessorImpl.java:0)
26/02/13 12:09:02 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 148)
26/02/13 12:09:02 INFO DAGScheduler: Missing parents: List()
26/02/13 12:09:02 INFO DAGScheduler: Submitting ResultStage 149 (MapPartitionsRDD[527] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:09:02 INFO MemoryStore: Block broadcast_145 stored as values in memory (estimated size 99.3 KiB, free 433.7 MiB)
26/02/13 12:09:02 INFO MemoryStore: Block broadcast_145_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 433.6 MiB)
26/02/13 12:09:02 INFO BlockManagerInfo: Added broadcast_145_piece0 in memory on spark-master:42073 (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 12:09:02 INFO SparkContext: Created broadcast 145 from broadcast at DAGScheduler.scala:1585
26/02/13 12:09:02 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 149 (MapPartitionsRDD[527] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 12:09:02 INFO TaskSchedulerImpl: Adding task set 149.0 with 4 tasks resource profile 0
26/02/13 12:09:02 INFO TaskSetManager: Starting task 0.0 in stage 149.0 (TID 315) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:09:02 INFO TaskSetManager: Starting task 1.0 in stage 149.0 (TID 316) (172.18.0.14, executor 1, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:09:02 INFO BlockManagerInfo: Added broadcast_145_piece0 in memory on 172.18.0.14:37571 (size: 33.3 KiB, free: 433.9 MiB)
26/02/13 12:09:02 INFO TaskSetManager: Starting task 2.0 in stage 149.0 (TID 317) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:09:02 INFO TaskSetManager: Finished task 0.0 in stage 149.0 (TID 315) in 29 ms on 172.18.0.14 (executor 1) (1/4)
26/02/13 12:09:02 INFO TaskSetManager: Starting task 3.0 in stage 149.0 (TID 318) (172.18.0.14, executor 1, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:09:02 INFO TaskSetManager: Finished task 1.0 in stage 149.0 (TID 316) in 31 ms on 172.18.0.14 (executor 1) (2/4)
26/02/13 12:09:02 INFO TaskSetManager: Finished task 2.0 in stage 149.0 (TID 317) in 23 ms on 172.18.0.14 (executor 1) (3/4)
26/02/13 12:09:02 INFO TaskSetManager: Finished task 3.0 in stage 149.0 (TID 318) in 24 ms on 172.18.0.14 (executor 1) (4/4)
26/02/13 12:09:02 INFO TaskSchedulerImpl: Removed TaskSet 149.0, whose tasks have all completed, from pool 
26/02/13 12:09:02 INFO DAGScheduler: ResultStage 149 (start at NativeMethodAccessorImpl.java:0) finished in 0.065 s
26/02/13 12:09:02 INFO DAGScheduler: Job 97 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 12:09:02 INFO TaskSchedulerImpl: Killing all running tasks in stage 149: Stage finished
26/02/13 12:09:02 INFO DAGScheduler: Job 97 finished: start at NativeMethodAccessorImpl.java:0, took 0.067050 s
26/02/13 12:09:02 INFO MemoryStore: Block broadcast_146_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.5 MiB)
26/02/13 12:09:02 INFO BlockManagerInfo: Added broadcast_146_piece0 in memory on spark-master:42073 (size: 107.2 KiB, free: 434.0 MiB)
26/02/13 12:09:02 INFO SparkContext: Created broadcast 146 from start at NativeMethodAccessorImpl.java:0
26/02/13 12:09:02 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 167, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@7338e8c3]. The input RDD has 3 partitions.
26/02/13 12:09:02 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 12:09:02 INFO DAGScheduler: Got job 98 (start at NativeMethodAccessorImpl.java:0) with 3 output partitions
26/02/13 12:09:02 INFO DAGScheduler: Final stage: ResultStage 150 (start at NativeMethodAccessorImpl.java:0)
26/02/13 12:09:02 INFO DAGScheduler: Parents of final stage: List()
26/02/13 12:09:02 INFO DAGScheduler: Missing parents: List()
26/02/13 12:09:02 INFO DAGScheduler: Submitting ResultStage 150 (MapPartitionsRDD[533] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:09:02 INFO MemoryStore: Block broadcast_147 stored as values in memory (estimated size 49.9 KiB, free 433.5 MiB)
26/02/13 12:09:02 INFO MemoryStore: Block broadcast_147_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.5 MiB)
26/02/13 12:09:02 INFO BlockManagerInfo: Added broadcast_147_piece0 in memory on spark-master:42073 (size: 19.5 KiB, free: 433.9 MiB)
26/02/13 12:09:02 INFO SparkContext: Created broadcast 147 from broadcast at DAGScheduler.scala:1585
26/02/13 12:09:02 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 150 (MapPartitionsRDD[533] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
26/02/13 12:09:02 INFO TaskSchedulerImpl: Adding task set 150.0 with 3 tasks resource profile 0
26/02/13 12:09:02 INFO TaskSetManager: Starting task 1.0 in stage 150.0 (TID 319) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:09:02 INFO TaskSetManager: Starting task 0.0 in stage 150.0 (TID 320) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:09:02 INFO TaskSetManager: Starting task 2.0 in stage 150.0 (TID 321) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:09:02 INFO BlockManagerInfo: Added broadcast_147_piece0 in memory on 172.18.0.15:37717 (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 12:09:02 INFO BlockManagerInfo: Added broadcast_147_piece0 in memory on 172.18.0.14:37571 (size: 19.5 KiB, free: 433.9 MiB)
26/02/13 12:09:02 INFO BlockManagerInfo: Added broadcast_146_piece0 in memory on 172.18.0.14:37571 (size: 107.2 KiB, free: 433.7 MiB)
26/02/13 12:09:02 INFO BlockManagerInfo: Added broadcast_146_piece0 in memory on 172.18.0.15:37717 (size: 107.2 KiB, free: 434.0 MiB)
26/02/13 12:09:02 INFO TaskSetManager: Finished task 0.0 in stage 150.0 (TID 320) in 92 ms on 172.18.0.14 (executor 1) (1/3)
26/02/13 12:09:02 INFO TaskSetManager: Finished task 2.0 in stage 150.0 (TID 321) in 106 ms on 172.18.0.14 (executor 1) (2/3)
26/02/13 12:09:02 INFO TaskSetManager: Finished task 1.0 in stage 150.0 (TID 319) in 138 ms on 172.18.0.15 (executor 0) (3/3)
26/02/13 12:09:02 INFO TaskSchedulerImpl: Removed TaskSet 150.0, whose tasks have all completed, from pool 
26/02/13 12:09:02 INFO DAGScheduler: ResultStage 150 (start at NativeMethodAccessorImpl.java:0) finished in 0.145 s
26/02/13 12:09:02 INFO DAGScheduler: Job 98 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 12:09:02 INFO TaskSchedulerImpl: Killing all running tasks in stage 150: Stage finished
26/02/13 12:09:02 INFO DAGScheduler: Job 98 finished: start at NativeMethodAccessorImpl.java:0, took 0.151001 s
26/02/13 12:09:02 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 167, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@7338e8c3] is committing.
26/02/13 12:09:02 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 167, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@7338e8c3] committed.
26/02/13 12:09:02 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/167 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.167.c00ab240-ca8d-4ab7-b763-c1056cfcf506.tmp
26/02/13 12:09:02 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.167.c00ab240-ca8d-4ab7-b763-c1056cfcf506.tmp to file:/tmp/spark-checkpoint-enrichment/commits/167
26/02/13 12:09:02 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "4a161d66-225b-4b29-812d-ddec3c05364e",
  "name" : null,
  "timestamp" : "2026-02-13T12:09:02.110Z",
  "batchId" : 167,
  "numInputRows" : 139,
  "inputRowsPerSecond" : 151.9125683060109,
  "processedRowsPerSecond" : 300.21598272138226,
  "durationMs" : {
    "addBatch" : 308,
    "commitOffsets" : 72,
    "getBatch" : 0,
    "latestOffset" : 0,
    "queryPlanning" : 21,
    "triggerExecution" : 463,
    "walCommit" : 60
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 2291,
        "1" : 2664,
        "0" : 2046
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 2339,
        "1" : 2716,
        "0" : 2085
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 2339,
        "1" : 2716,
        "0" : 2085
      }
    },
    "numInputRows" : 139,
    "inputRowsPerSecond" : 151.9125683060109,
    "processedRowsPerSecond" : 300.21598272138226,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@1a6a42c2",
    "numOutputRows" : 26
  }
}
26/02/13 12:09:02 INFO BlockManagerInfo: Removed broadcast_145_piece0 on spark-master:42073 in memory (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 12:09:02 INFO BlockManagerInfo: Removed broadcast_145_piece0 on 172.18.0.14:37571 in memory (size: 33.3 KiB, free: 433.8 MiB)
26/02/13 12:09:02 INFO BlockManagerInfo: Removed broadcast_143_piece0 on spark-master:42073 in memory (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 12:09:02 INFO BlockManagerInfo: Removed broadcast_143_piece0 on 172.18.0.14:37571 in memory (size: 107.2 KiB, free: 433.9 MiB)
26/02/13 12:09:02 INFO BlockManagerInfo: Removed broadcast_143_piece0 on 172.18.0.15:37717 in memory (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 12:09:02 INFO BlockManagerInfo: Removed broadcast_142_piece0 on spark-master:42073 in memory (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 12:09:02 INFO BlockManagerInfo: Removed broadcast_142_piece0 on 172.18.0.14:37571 in memory (size: 33.3 KiB, free: 433.9 MiB)
26/02/13 12:09:02 INFO BlockManagerInfo: Removed broadcast_147_piece0 on spark-master:42073 in memory (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 12:09:02 INFO BlockManagerInfo: Removed broadcast_147_piece0 on 172.18.0.14:37571 in memory (size: 19.5 KiB, free: 433.9 MiB)
26/02/13 12:09:02 INFO BlockManagerInfo: Removed broadcast_147_piece0 on 172.18.0.15:37717 in memory (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 12:09:02 INFO BlockManagerInfo: Removed broadcast_144_piece0 on spark-master:42073 in memory (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 12:09:02 INFO BlockManagerInfo: Removed broadcast_144_piece0 on 172.18.0.15:37717 in memory (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 12:09:02 INFO BlockManagerInfo: Removed broadcast_144_piece0 on 172.18.0.14:37571 in memory (size: 19.5 KiB, free: 434.0 MiB)
26/02/13 12:09:12 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:09:15 INFO BlockManagerInfo: Removed broadcast_140_piece0 on spark-master:42073 in memory (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 12:09:15 INFO BlockManagerInfo: Removed broadcast_140_piece0 on 172.18.0.14:37571 in memory (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 12:09:15 INFO BlockManagerInfo: Removed broadcast_140_piece0 on 172.18.0.15:37717 in memory (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 12:09:17 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/168 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.168.1a2ea72a-b04e-4985-ae98-e4303e1cb31c.tmp
26/02/13 12:09:17 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.168.1a2ea72a-b04e-4985-ae98-e4303e1cb31c.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/168
26/02/13 12:09:17 INFO MicroBatchExecution: Committed offsets for batch 168. Metadata OffsetSeqMetadata(0,1770984557664,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 12:09:17 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:09:17 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:09:17 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:09:17 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:09:17 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:09:17 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:09:17 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#40381 - origin_code.nullCount#40380) > 0)
26/02/13 12:09:17 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#40386 - destination_code.nullCount#40385) > 0)
26/02/13 12:09:17 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#40416 - callsign.nullCount#40415) > 0)
26/02/13 12:09:17 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 12:09:17 INFO DAGScheduler: Got job 99 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 12:09:17 INFO DAGScheduler: Final stage: ResultStage 152 (start at NativeMethodAccessorImpl.java:0)
26/02/13 12:09:17 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 151)
26/02/13 12:09:17 INFO DAGScheduler: Missing parents: List()
26/02/13 12:09:17 INFO DAGScheduler: Submitting ResultStage 152 (MapPartitionsRDD[538] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:09:17 INFO MemoryStore: Block broadcast_148 stored as values in memory (estimated size 99.3 KiB, free 434.0 MiB)
26/02/13 12:09:17 INFO MemoryStore: Block broadcast_148_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 433.9 MiB)
26/02/13 12:09:17 INFO BlockManagerInfo: Added broadcast_148_piece0 in memory on spark-master:42073 (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 12:09:17 INFO SparkContext: Created broadcast 148 from broadcast at DAGScheduler.scala:1585
26/02/13 12:09:17 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 152 (MapPartitionsRDD[538] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 12:09:17 INFO TaskSchedulerImpl: Adding task set 152.0 with 4 tasks resource profile 0
26/02/13 12:09:17 INFO TaskSetManager: Starting task 0.0 in stage 152.0 (TID 322) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:09:17 INFO TaskSetManager: Starting task 1.0 in stage 152.0 (TID 323) (172.18.0.14, executor 1, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:09:17 INFO BlockManagerInfo: Added broadcast_148_piece0 in memory on 172.18.0.14:37571 (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 12:09:17 INFO TaskSetManager: Starting task 2.0 in stage 152.0 (TID 324) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:09:17 INFO TaskSetManager: Finished task 1.0 in stage 152.0 (TID 323) in 17 ms on 172.18.0.14 (executor 1) (1/4)
26/02/13 12:09:17 INFO TaskSetManager: Starting task 3.0 in stage 152.0 (TID 325) (172.18.0.14, executor 1, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:09:17 INFO TaskSetManager: Finished task 0.0 in stage 152.0 (TID 322) in 18 ms on 172.18.0.14 (executor 1) (2/4)
26/02/13 12:09:17 INFO TaskSetManager: Finished task 2.0 in stage 152.0 (TID 324) in 12 ms on 172.18.0.14 (executor 1) (3/4)
26/02/13 12:09:17 INFO TaskSetManager: Finished task 3.0 in stage 152.0 (TID 325) in 11 ms on 172.18.0.14 (executor 1) (4/4)
26/02/13 12:09:17 INFO TaskSchedulerImpl: Removed TaskSet 152.0, whose tasks have all completed, from pool 
26/02/13 12:09:17 INFO DAGScheduler: ResultStage 152 (start at NativeMethodAccessorImpl.java:0) finished in 0.035 s
26/02/13 12:09:17 INFO DAGScheduler: Job 99 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 12:09:17 INFO TaskSchedulerImpl: Killing all running tasks in stage 152: Stage finished
26/02/13 12:09:17 INFO DAGScheduler: Job 99 finished: start at NativeMethodAccessorImpl.java:0, took 0.036849 s
26/02/13 12:09:17 INFO MemoryStore: Block broadcast_149_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.8 MiB)
26/02/13 12:09:17 INFO BlockManagerInfo: Added broadcast_149_piece0 in memory on spark-master:42073 (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 12:09:17 INFO SparkContext: Created broadcast 149 from start at NativeMethodAccessorImpl.java:0
26/02/13 12:09:17 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 168, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@3a537aa8]. The input RDD has 3 partitions.
26/02/13 12:09:17 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 12:09:17 INFO DAGScheduler: Got job 100 (start at NativeMethodAccessorImpl.java:0) with 3 output partitions
26/02/13 12:09:17 INFO DAGScheduler: Final stage: ResultStage 153 (start at NativeMethodAccessorImpl.java:0)
26/02/13 12:09:17 INFO DAGScheduler: Parents of final stage: List()
26/02/13 12:09:17 INFO DAGScheduler: Missing parents: List()
26/02/13 12:09:17 INFO DAGScheduler: Submitting ResultStage 153 (MapPartitionsRDD[544] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:09:17 INFO MemoryStore: Block broadcast_150 stored as values in memory (estimated size 49.9 KiB, free 433.8 MiB)
26/02/13 12:09:17 INFO MemoryStore: Block broadcast_150_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.8 MiB)
26/02/13 12:09:17 INFO BlockManagerInfo: Added broadcast_150_piece0 in memory on spark-master:42073 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 12:09:17 INFO SparkContext: Created broadcast 150 from broadcast at DAGScheduler.scala:1585
26/02/13 12:09:17 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 153 (MapPartitionsRDD[544] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
26/02/13 12:09:17 INFO TaskSchedulerImpl: Adding task set 153.0 with 3 tasks resource profile 0
26/02/13 12:09:17 INFO TaskSetManager: Starting task 0.0 in stage 153.0 (TID 326) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:09:17 INFO TaskSetManager: Starting task 1.0 in stage 153.0 (TID 327) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:09:17 INFO TaskSetManager: Starting task 2.0 in stage 153.0 (TID 328) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:09:17 INFO BlockManagerInfo: Added broadcast_150_piece0 in memory on 172.18.0.14:37571 (size: 19.5 KiB, free: 434.0 MiB)
26/02/13 12:09:17 INFO BlockManagerInfo: Added broadcast_150_piece0 in memory on 172.18.0.15:37717 (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 12:09:17 INFO BlockManagerInfo: Added broadcast_149_piece0 in memory on 172.18.0.14:37571 (size: 107.2 KiB, free: 433.9 MiB)
26/02/13 12:09:17 INFO BlockManagerInfo: Added broadcast_149_piece0 in memory on 172.18.0.15:37717 (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 12:09:18 INFO TaskSetManager: Finished task 0.0 in stage 153.0 (TID 326) in 560 ms on 172.18.0.14 (executor 1) (1/3)
26/02/13 12:09:18 INFO TaskSetManager: Finished task 2.0 in stage 153.0 (TID 328) in 560 ms on 172.18.0.14 (executor 1) (2/3)
26/02/13 12:09:18 INFO TaskSetManager: Finished task 1.0 in stage 153.0 (TID 327) in 579 ms on 172.18.0.15 (executor 0) (3/3)
26/02/13 12:09:18 INFO TaskSchedulerImpl: Removed TaskSet 153.0, whose tasks have all completed, from pool 
26/02/13 12:09:18 INFO DAGScheduler: ResultStage 153 (start at NativeMethodAccessorImpl.java:0) finished in 0.583 s
26/02/13 12:09:18 INFO DAGScheduler: Job 100 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 12:09:18 INFO TaskSchedulerImpl: Killing all running tasks in stage 153: Stage finished
26/02/13 12:09:18 INFO DAGScheduler: Job 100 finished: start at NativeMethodAccessorImpl.java:0, took 0.584744 s
26/02/13 12:09:18 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 168, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@3a537aa8] is committing.
26/02/13 12:09:18 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 168, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@3a537aa8] committed.
26/02/13 12:09:18 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/168 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.168.f4ab1e81-2163-4176-84b6-0ec6068fa645.tmp
26/02/13 12:09:18 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.168.f4ab1e81-2163-4176-84b6-0ec6068fa645.tmp to file:/tmp/spark-checkpoint-enrichment/commits/168
26/02/13 12:09:18 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "4a161d66-225b-4b29-812d-ddec3c05364e",
  "name" : null,
  "timestamp" : "2026-02-13T12:09:17.663Z",
  "batchId" : 168,
  "numInputRows" : 90,
  "inputRowsPerSecond" : 7500.0,
  "processedRowsPerSecond" : 102.85714285714286,
  "durationMs" : {
    "addBatch" : 701,
    "commitOffsets" : 58,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 23,
    "triggerExecution" : 875,
    "walCommit" : 91
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 2339,
        "1" : 2716,
        "0" : 2085
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 2369,
        "1" : 2747,
        "0" : 2114
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 2369,
        "1" : 2747,
        "0" : 2114
      }
    },
    "numInputRows" : 90,
    "inputRowsPerSecond" : 7500.0,
    "processedRowsPerSecond" : 102.85714285714286,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@1a6a42c2",
    "numOutputRows" : 26
  }
}
26/02/13 12:09:18 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/169 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.169.1f21c2da-f600-4724-81fc-21cebd686a07.tmp
26/02/13 12:09:18 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.169.1f21c2da-f600-4724-81fc-21cebd686a07.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/169
26/02/13 12:09:18 INFO MicroBatchExecution: Committed offsets for batch 169. Metadata OffsetSeqMetadata(0,1770984558540,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 12:09:18 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:09:18 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:09:18 INFO BlockManagerInfo: Removed broadcast_150_piece0 on spark-master:42073 in memory (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 12:09:18 INFO BlockManagerInfo: Removed broadcast_150_piece0 on 172.18.0.14:37571 in memory (size: 19.5 KiB, free: 433.9 MiB)
26/02/13 12:09:18 INFO BlockManagerInfo: Removed broadcast_150_piece0 on 172.18.0.15:37717 in memory (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 12:09:18 INFO BlockManagerInfo: Removed broadcast_149_piece0 on spark-master:42073 in memory (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 12:09:18 INFO BlockManagerInfo: Removed broadcast_149_piece0 on 172.18.0.15:37717 in memory (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 12:09:18 INFO BlockManagerInfo: Removed broadcast_149_piece0 on 172.18.0.14:37571 in memory (size: 107.2 KiB, free: 434.0 MiB)
26/02/13 12:09:18 INFO BlockManagerInfo: Removed broadcast_148_piece0 on spark-master:42073 in memory (size: 33.3 KiB, free: 434.3 MiB)
26/02/13 12:09:18 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:09:18 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:09:18 INFO BlockManagerInfo: Removed broadcast_148_piece0 on 172.18.0.14:37571 in memory (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 12:09:18 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:09:18 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:09:18 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#41235 - origin_code.nullCount#41234) > 0)
26/02/13 12:09:18 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#41240 - destination_code.nullCount#41239) > 0)
26/02/13 12:09:18 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#41270 - callsign.nullCount#41269) > 0)
26/02/13 12:09:18 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 12:09:18 INFO DAGScheduler: Got job 101 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 12:09:18 INFO DAGScheduler: Final stage: ResultStage 155 (start at NativeMethodAccessorImpl.java:0)
26/02/13 12:09:18 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 154)
26/02/13 12:09:18 INFO DAGScheduler: Missing parents: List()
26/02/13 12:09:18 INFO DAGScheduler: Submitting ResultStage 155 (MapPartitionsRDD[549] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:09:18 INFO MemoryStore: Block broadcast_151 stored as values in memory (estimated size 99.3 KiB, free 434.0 MiB)
26/02/13 12:09:18 INFO MemoryStore: Block broadcast_151_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 433.9 MiB)
26/02/13 12:09:18 INFO BlockManagerInfo: Added broadcast_151_piece0 in memory on spark-master:42073 (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 12:09:18 INFO SparkContext: Created broadcast 151 from broadcast at DAGScheduler.scala:1585
26/02/13 12:09:18 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 155 (MapPartitionsRDD[549] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 12:09:18 INFO TaskSchedulerImpl: Adding task set 155.0 with 4 tasks resource profile 0
26/02/13 12:09:18 INFO TaskSetManager: Starting task 0.0 in stage 155.0 (TID 329) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:09:18 INFO TaskSetManager: Starting task 1.0 in stage 155.0 (TID 330) (172.18.0.14, executor 1, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:09:18 INFO BlockManagerInfo: Added broadcast_151_piece0 in memory on 172.18.0.14:37571 (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 12:09:18 INFO TaskSetManager: Starting task 2.0 in stage 155.0 (TID 331) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:09:18 INFO TaskSetManager: Starting task 3.0 in stage 155.0 (TID 332) (172.18.0.14, executor 1, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:09:18 INFO TaskSetManager: Finished task 1.0 in stage 155.0 (TID 330) in 18 ms on 172.18.0.14 (executor 1) (1/4)
26/02/13 12:09:18 INFO TaskSetManager: Finished task 0.0 in stage 155.0 (TID 329) in 18 ms on 172.18.0.14 (executor 1) (2/4)
26/02/13 12:09:18 INFO TaskSetManager: Finished task 3.0 in stage 155.0 (TID 332) in 13 ms on 172.18.0.14 (executor 1) (3/4)
26/02/13 12:09:18 INFO TaskSetManager: Finished task 2.0 in stage 155.0 (TID 331) in 14 ms on 172.18.0.14 (executor 1) (4/4)
26/02/13 12:09:18 INFO TaskSchedulerImpl: Removed TaskSet 155.0, whose tasks have all completed, from pool 
26/02/13 12:09:18 INFO DAGScheduler: ResultStage 155 (start at NativeMethodAccessorImpl.java:0) finished in 0.041 s
26/02/13 12:09:18 INFO DAGScheduler: Job 101 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 12:09:18 INFO TaskSchedulerImpl: Killing all running tasks in stage 155: Stage finished
26/02/13 12:09:18 INFO DAGScheduler: Job 101 finished: start at NativeMethodAccessorImpl.java:0, took 0.042764 s
26/02/13 12:09:18 INFO MemoryStore: Block broadcast_152_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.8 MiB)
26/02/13 12:09:18 INFO BlockManagerInfo: Added broadcast_152_piece0 in memory on spark-master:42073 (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 12:09:18 INFO SparkContext: Created broadcast 152 from start at NativeMethodAccessorImpl.java:0
26/02/13 12:09:18 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 169, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@8c2ce73]. The input RDD has 3 partitions.
26/02/13 12:09:18 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 12:09:18 INFO DAGScheduler: Got job 102 (start at NativeMethodAccessorImpl.java:0) with 3 output partitions
26/02/13 12:09:18 INFO DAGScheduler: Final stage: ResultStage 156 (start at NativeMethodAccessorImpl.java:0)
26/02/13 12:09:18 INFO DAGScheduler: Parents of final stage: List()
26/02/13 12:09:18 INFO DAGScheduler: Missing parents: List()
26/02/13 12:09:18 INFO DAGScheduler: Submitting ResultStage 156 (MapPartitionsRDD[555] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:09:18 INFO MemoryStore: Block broadcast_153 stored as values in memory (estimated size 49.9 KiB, free 433.8 MiB)
26/02/13 12:09:18 INFO MemoryStore: Block broadcast_153_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.8 MiB)
26/02/13 12:09:18 INFO BlockManagerInfo: Added broadcast_153_piece0 in memory on spark-master:42073 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 12:09:18 INFO SparkContext: Created broadcast 153 from broadcast at DAGScheduler.scala:1585
26/02/13 12:09:18 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 156 (MapPartitionsRDD[555] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
26/02/13 12:09:18 INFO TaskSchedulerImpl: Adding task set 156.0 with 3 tasks resource profile 0
26/02/13 12:09:18 INFO TaskSetManager: Starting task 0.0 in stage 156.0 (TID 333) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:09:18 INFO TaskSetManager: Starting task 1.0 in stage 156.0 (TID 334) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:09:18 INFO TaskSetManager: Starting task 2.0 in stage 156.0 (TID 335) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:09:18 INFO BlockManagerInfo: Removed broadcast_146_piece0 on spark-master:42073 in memory (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 12:09:18 INFO BlockManagerInfo: Removed broadcast_146_piece0 on 172.18.0.15:37717 in memory (size: 107.2 KiB, free: 434.4 MiB)
26/02/13 12:09:18 INFO BlockManagerInfo: Removed broadcast_146_piece0 on 172.18.0.14:37571 in memory (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 12:09:18 INFO BlockManagerInfo: Added broadcast_153_piece0 in memory on 172.18.0.14:37571 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 12:09:18 INFO BlockManagerInfo: Added broadcast_153_piece0 in memory on 172.18.0.15:37717 (size: 19.5 KiB, free: 434.4 MiB)
26/02/13 12:09:18 INFO BlockManagerInfo: Added broadcast_152_piece0 in memory on 172.18.0.14:37571 (size: 107.2 KiB, free: 434.0 MiB)
26/02/13 12:09:18 INFO BlockManagerInfo: Added broadcast_152_piece0 in memory on 172.18.0.15:37717 (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 12:09:18 INFO TaskSetManager: Finished task 2.0 in stage 156.0 (TID 335) in 64 ms on 172.18.0.14 (executor 1) (1/3)
26/02/13 12:09:18 INFO TaskSetManager: Finished task 0.0 in stage 156.0 (TID 333) in 74 ms on 172.18.0.14 (executor 1) (2/3)
26/02/13 12:09:18 INFO TaskSetManager: Finished task 1.0 in stage 156.0 (TID 334) in 91 ms on 172.18.0.15 (executor 0) (3/3)
26/02/13 12:09:18 INFO TaskSchedulerImpl: Removed TaskSet 156.0, whose tasks have all completed, from pool 
26/02/13 12:09:18 INFO DAGScheduler: ResultStage 156 (start at NativeMethodAccessorImpl.java:0) finished in 0.104 s
26/02/13 12:09:18 INFO DAGScheduler: Job 102 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 12:09:18 INFO TaskSchedulerImpl: Killing all running tasks in stage 156: Stage finished
26/02/13 12:09:18 INFO DAGScheduler: Job 102 finished: start at NativeMethodAccessorImpl.java:0, took 0.105604 s
26/02/13 12:09:18 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 169, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@8c2ce73] is committing.
26/02/13 12:09:18 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 169, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@8c2ce73] committed.
26/02/13 12:09:18 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/169 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.169.68bb9884-7900-42fe-9fef-7cdfab70a731.tmp
26/02/13 12:09:18 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.169.68bb9884-7900-42fe-9fef-7cdfab70a731.tmp to file:/tmp/spark-checkpoint-enrichment/commits/169
26/02/13 12:09:18 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "4a161d66-225b-4b29-812d-ddec3c05364e",
  "name" : null,
  "timestamp" : "2026-02-13T12:09:18.539Z",
  "batchId" : 169,
  "numInputRows" : 160,
  "inputRowsPerSecond" : 182.64840182648402,
  "processedRowsPerSecond" : 404.04040404040404,
  "durationMs" : {
    "addBatch" : 248,
    "commitOffsets" : 67,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 22,
    "triggerExecution" : 396,
    "walCommit" : 57
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 2369,
        "1" : 2747,
        "0" : 2114
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 2424,
        "1" : 2805,
        "0" : 2161
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 2424,
        "1" : 2805,
        "0" : 2161
      }
    },
    "numInputRows" : 160,
    "inputRowsPerSecond" : 182.64840182648402,
    "processedRowsPerSecond" : 404.04040404040404,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@1a6a42c2",
    "numOutputRows" : 29
  }
}
26/02/13 12:09:28 INFO BlockManagerInfo: Removed broadcast_153_piece0 on spark-master:42073 in memory (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 12:09:28 INFO BlockManagerInfo: Removed broadcast_153_piece0 on 172.18.0.14:37571 in memory (size: 19.5 KiB, free: 434.0 MiB)
26/02/13 12:09:28 INFO BlockManagerInfo: Removed broadcast_153_piece0 on 172.18.0.15:37717 in memory (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 12:09:28 INFO BlockManagerInfo: Removed broadcast_151_piece0 on spark-master:42073 in memory (size: 33.3 KiB, free: 434.3 MiB)
26/02/13 12:09:28 INFO BlockManagerInfo: Removed broadcast_151_piece0 on 172.18.0.14:37571 in memory (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 12:09:28 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:09:34 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/170 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.170.1f4669ba-fa05-4cb6-96fa-aa2d0d69626a.tmp
26/02/13 12:09:34 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.170.1f4669ba-fa05-4cb6-96fa-aa2d0d69626a.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/170
26/02/13 12:09:34 INFO MicroBatchExecution: Committed offsets for batch 170. Metadata OffsetSeqMetadata(0,1770984573997,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 12:09:34 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:09:34 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:09:34 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:09:34 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:09:34 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:09:34 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:09:34 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#42089 - origin_code.nullCount#42088) > 0)
26/02/13 12:09:34 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#42094 - destination_code.nullCount#42093) > 0)
26/02/13 12:09:34 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#42124 - callsign.nullCount#42123) > 0)
26/02/13 12:09:34 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 12:09:34 INFO DAGScheduler: Got job 103 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 12:09:34 INFO DAGScheduler: Final stage: ResultStage 158 (start at NativeMethodAccessorImpl.java:0)
26/02/13 12:09:34 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 157)
26/02/13 12:09:34 INFO DAGScheduler: Missing parents: List()
26/02/13 12:09:34 INFO DAGScheduler: Submitting ResultStage 158 (MapPartitionsRDD[560] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:09:34 INFO MemoryStore: Block broadcast_154 stored as values in memory (estimated size 99.3 KiB, free 434.0 MiB)
26/02/13 12:09:34 INFO MemoryStore: Block broadcast_154_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 433.9 MiB)
26/02/13 12:09:34 INFO BlockManagerInfo: Added broadcast_154_piece0 in memory on spark-master:42073 (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 12:09:34 INFO SparkContext: Created broadcast 154 from broadcast at DAGScheduler.scala:1585
26/02/13 12:09:34 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 158 (MapPartitionsRDD[560] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 12:09:34 INFO TaskSchedulerImpl: Adding task set 158.0 with 4 tasks resource profile 0
26/02/13 12:09:34 INFO TaskSetManager: Starting task 0.0 in stage 158.0 (TID 336) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:09:34 INFO TaskSetManager: Starting task 1.0 in stage 158.0 (TID 337) (172.18.0.14, executor 1, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:09:34 INFO BlockManagerInfo: Added broadcast_154_piece0 in memory on 172.18.0.14:37571 (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 12:09:34 INFO TaskSetManager: Starting task 2.0 in stage 158.0 (TID 338) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:09:34 INFO TaskSetManager: Finished task 0.0 in stage 158.0 (TID 336) in 25 ms on 172.18.0.14 (executor 1) (1/4)
26/02/13 12:09:34 INFO TaskSetManager: Starting task 3.0 in stage 158.0 (TID 339) (172.18.0.14, executor 1, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:09:34 INFO TaskSetManager: Finished task 1.0 in stage 158.0 (TID 337) in 26 ms on 172.18.0.14 (executor 1) (2/4)
26/02/13 12:09:34 INFO TaskSetManager: Finished task 2.0 in stage 158.0 (TID 338) in 19 ms on 172.18.0.14 (executor 1) (3/4)
26/02/13 12:09:34 INFO TaskSetManager: Finished task 3.0 in stage 158.0 (TID 339) in 18 ms on 172.18.0.14 (executor 1) (4/4)
26/02/13 12:09:34 INFO TaskSchedulerImpl: Removed TaskSet 158.0, whose tasks have all completed, from pool 
26/02/13 12:09:34 INFO DAGScheduler: ResultStage 158 (start at NativeMethodAccessorImpl.java:0) finished in 0.057 s
26/02/13 12:09:34 INFO DAGScheduler: Job 103 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 12:09:34 INFO TaskSchedulerImpl: Killing all running tasks in stage 158: Stage finished
26/02/13 12:09:34 INFO DAGScheduler: Job 103 finished: start at NativeMethodAccessorImpl.java:0, took 0.059193 s
26/02/13 12:09:34 INFO MemoryStore: Block broadcast_155_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.8 MiB)
26/02/13 12:09:34 INFO BlockManagerInfo: Added broadcast_155_piece0 in memory on spark-master:42073 (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 12:09:34 INFO SparkContext: Created broadcast 155 from start at NativeMethodAccessorImpl.java:0
26/02/13 12:09:34 INFO BlockManagerInfo: Removed broadcast_154_piece0 on spark-master:42073 in memory (size: 33.3 KiB, free: 434.2 MiB)
26/02/13 12:09:34 INFO BlockManagerInfo: Removed broadcast_154_piece0 on 172.18.0.14:37571 in memory (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 12:09:34 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 170, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@114bb4df]. The input RDD has 3 partitions.
26/02/13 12:09:34 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 12:09:34 INFO DAGScheduler: Got job 104 (start at NativeMethodAccessorImpl.java:0) with 3 output partitions
26/02/13 12:09:34 INFO DAGScheduler: Final stage: ResultStage 159 (start at NativeMethodAccessorImpl.java:0)
26/02/13 12:09:34 INFO DAGScheduler: Parents of final stage: List()
26/02/13 12:09:34 INFO DAGScheduler: Missing parents: List()
26/02/13 12:09:34 INFO DAGScheduler: Submitting ResultStage 159 (MapPartitionsRDD[566] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:09:34 INFO MemoryStore: Block broadcast_156 stored as values in memory (estimated size 49.9 KiB, free 433.9 MiB)
26/02/13 12:09:34 INFO MemoryStore: Block broadcast_156_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.9 MiB)
26/02/13 12:09:34 INFO BlockManagerInfo: Added broadcast_156_piece0 in memory on spark-master:42073 (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 12:09:34 INFO SparkContext: Created broadcast 156 from broadcast at DAGScheduler.scala:1585
26/02/13 12:09:34 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 159 (MapPartitionsRDD[566] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
26/02/13 12:09:34 INFO TaskSchedulerImpl: Adding task set 159.0 with 3 tasks resource profile 0
26/02/13 12:09:34 INFO TaskSetManager: Starting task 0.0 in stage 159.0 (TID 340) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:09:34 INFO TaskSetManager: Starting task 1.0 in stage 159.0 (TID 341) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:09:34 INFO TaskSetManager: Starting task 2.0 in stage 159.0 (TID 342) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:09:34 INFO BlockManagerInfo: Added broadcast_156_piece0 in memory on 172.18.0.14:37571 (size: 19.5 KiB, free: 434.0 MiB)
26/02/13 12:09:34 INFO BlockManagerInfo: Added broadcast_156_piece0 in memory on 172.18.0.15:37717 (size: 19.5 KiB, free: 434.3 MiB)
26/02/13 12:09:34 INFO BlockManagerInfo: Added broadcast_155_piece0 in memory on 172.18.0.14:37571 (size: 107.2 KiB, free: 433.9 MiB)
26/02/13 12:09:34 INFO BlockManagerInfo: Added broadcast_155_piece0 in memory on 172.18.0.15:37717 (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 12:09:34 INFO TaskSetManager: Finished task 1.0 in stage 159.0 (TID 341) in 597 ms on 172.18.0.15 (executor 0) (1/3)
26/02/13 12:09:34 INFO TaskSetManager: Finished task 0.0 in stage 159.0 (TID 340) in 605 ms on 172.18.0.14 (executor 1) (2/3)
26/02/13 12:09:34 INFO TaskSetManager: Finished task 2.0 in stage 159.0 (TID 342) in 606 ms on 172.18.0.14 (executor 1) (3/3)
26/02/13 12:09:34 INFO TaskSchedulerImpl: Removed TaskSet 159.0, whose tasks have all completed, from pool 
26/02/13 12:09:34 INFO DAGScheduler: ResultStage 159 (start at NativeMethodAccessorImpl.java:0) finished in 0.614 s
26/02/13 12:09:34 INFO DAGScheduler: Job 104 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 12:09:34 INFO TaskSchedulerImpl: Killing all running tasks in stage 159: Stage finished
26/02/13 12:09:34 INFO DAGScheduler: Job 104 finished: start at NativeMethodAccessorImpl.java:0, took 0.616824 s
26/02/13 12:09:34 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 170, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@114bb4df] is committing.
26/02/13 12:09:34 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 170, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@114bb4df] committed.
26/02/13 12:09:34 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/170 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.170.5c3c939e-ca8d-4660-b786-1acb3abe77d2.tmp
26/02/13 12:09:34 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.170.5c3c939e-ca8d-4660-b786-1acb3abe77d2.tmp to file:/tmp/spark-checkpoint-enrichment/commits/170
26/02/13 12:09:34 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "4a161d66-225b-4b29-812d-ddec3c05364e",
  "name" : null,
  "timestamp" : "2026-02-13T12:09:33.996Z",
  "batchId" : 170,
  "numInputRows" : 8,
  "inputRowsPerSecond" : 615.3846153846154,
  "processedRowsPerSecond" : 8.333333333333334,
  "durationMs" : {
    "addBatch" : 791,
    "commitOffsets" : 75,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 25,
    "triggerExecution" : 960,
    "walCommit" : 66
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 2424,
        "1" : 2805,
        "0" : 2161
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 2425,
        "1" : 2808,
        "0" : 2165
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 2425,
        "1" : 2808,
        "0" : 2165
      }
    },
    "numInputRows" : 8,
    "inputRowsPerSecond" : 615.3846153846154,
    "processedRowsPerSecond" : 8.333333333333334,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@1a6a42c2",
    "numOutputRows" : 3
  }
}
26/02/13 12:09:34 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/offsets/171 using temp file file:/tmp/spark-checkpoint-enrichment/offsets/.171.32f92010-0b9a-472c-bbf7-d1ec08410038.tmp
26/02/13 12:09:35 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/offsets/.171.32f92010-0b9a-472c-bbf7-d1ec08410038.tmp to file:/tmp/spark-checkpoint-enrichment/offsets/171
26/02/13 12:09:35 INFO MicroBatchExecution: Committed offsets for batch 171. Metadata OffsetSeqMetadata(0,1770984574958,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 4))
26/02/13 12:09:35 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:09:35 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:09:35 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:09:35 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:09:35 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:09:35 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
26/02/13 12:09:35 INFO DefaultCachedBatchSerializer: Predicate isnotnull(origin_code#26) generates partition filter: ((origin_code.count#42943 - origin_code.nullCount#42942) > 0)
26/02/13 12:09:35 INFO DefaultCachedBatchSerializer: Predicate isnotnull(destination_code#27) generates partition filter: ((destination_code.count#42948 - destination_code.nullCount#42947) > 0)
26/02/13 12:09:35 INFO DefaultCachedBatchSerializer: Predicate isnotnull(callsign#67) generates partition filter: ((callsign.count#42978 - callsign.nullCount#42977) > 0)
26/02/13 12:09:35 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 12:09:35 INFO DAGScheduler: Got job 105 (start at NativeMethodAccessorImpl.java:0) with 4 output partitions
26/02/13 12:09:35 INFO DAGScheduler: Final stage: ResultStage 161 (start at NativeMethodAccessorImpl.java:0)
26/02/13 12:09:35 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 160)
26/02/13 12:09:35 INFO DAGScheduler: Missing parents: List()
26/02/13 12:09:35 INFO DAGScheduler: Submitting ResultStage 161 (MapPartitionsRDD[571] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:09:35 INFO MemoryStore: Block broadcast_157 stored as values in memory (estimated size 99.3 KiB, free 433.8 MiB)
26/02/13 12:09:35 INFO MemoryStore: Block broadcast_157_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 433.8 MiB)
26/02/13 12:09:35 INFO BlockManagerInfo: Added broadcast_157_piece0 in memory on spark-master:42073 (size: 33.3 KiB, free: 434.1 MiB)
26/02/13 12:09:35 INFO SparkContext: Created broadcast 157 from broadcast at DAGScheduler.scala:1585
26/02/13 12:09:35 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 161 (MapPartitionsRDD[571] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
26/02/13 12:09:35 INFO TaskSchedulerImpl: Adding task set 161.0 with 4 tasks resource profile 0
26/02/13 12:09:35 INFO TaskSetManager: Starting task 0.0 in stage 161.0 (TID 343) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:09:35 INFO TaskSetManager: Starting task 1.0 in stage 161.0 (TID 344) (172.18.0.14, executor 1, partition 1, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:09:35 INFO BlockManagerInfo: Added broadcast_157_piece0 in memory on 172.18.0.14:37571 (size: 33.3 KiB, free: 433.9 MiB)
26/02/13 12:09:35 INFO TaskSetManager: Starting task 2.0 in stage 161.0 (TID 345) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:09:35 INFO TaskSetManager: Finished task 0.0 in stage 161.0 (TID 343) in 15 ms on 172.18.0.14 (executor 1) (1/4)
26/02/13 12:09:35 INFO TaskSetManager: Starting task 3.0 in stage 161.0 (TID 346) (172.18.0.14, executor 1, partition 3, PROCESS_LOCAL, 7619 bytes) 
26/02/13 12:09:35 INFO TaskSetManager: Finished task 1.0 in stage 161.0 (TID 344) in 15 ms on 172.18.0.14 (executor 1) (2/4)
26/02/13 12:09:35 INFO TaskSetManager: Finished task 2.0 in stage 161.0 (TID 345) in 16 ms on 172.18.0.14 (executor 1) (3/4)
26/02/13 12:09:35 INFO TaskSetManager: Finished task 3.0 in stage 161.0 (TID 346) in 17 ms on 172.18.0.14 (executor 1) (4/4)
26/02/13 12:09:35 INFO TaskSchedulerImpl: Removed TaskSet 161.0, whose tasks have all completed, from pool 
26/02/13 12:09:35 INFO DAGScheduler: ResultStage 161 (start at NativeMethodAccessorImpl.java:0) finished in 0.038 s
26/02/13 12:09:35 INFO DAGScheduler: Job 105 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 12:09:35 INFO TaskSchedulerImpl: Killing all running tasks in stage 161: Stage finished
26/02/13 12:09:35 INFO DAGScheduler: Job 105 finished: start at NativeMethodAccessorImpl.java:0, took 0.040848 s
26/02/13 12:09:35 INFO MemoryStore: Block broadcast_158_piece0 stored as bytes in memory (estimated size 107.2 KiB, free 433.7 MiB)
26/02/13 12:09:35 INFO BlockManagerInfo: Added broadcast_158_piece0 in memory on spark-master:42073 (size: 107.2 KiB, free: 434.0 MiB)
26/02/13 12:09:35 INFO SparkContext: Created broadcast 158 from start at NativeMethodAccessorImpl.java:0
26/02/13 12:09:35 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 171, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@5445c209]. The input RDD has 3 partitions.
26/02/13 12:09:35 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
26/02/13 12:09:35 INFO DAGScheduler: Got job 106 (start at NativeMethodAccessorImpl.java:0) with 3 output partitions
26/02/13 12:09:35 INFO DAGScheduler: Final stage: ResultStage 162 (start at NativeMethodAccessorImpl.java:0)
26/02/13 12:09:35 INFO DAGScheduler: Parents of final stage: List()
26/02/13 12:09:35 INFO DAGScheduler: Missing parents: List()
26/02/13 12:09:35 INFO DAGScheduler: Submitting ResultStage 162 (MapPartitionsRDD[577] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
26/02/13 12:09:35 INFO MemoryStore: Block broadcast_159 stored as values in memory (estimated size 49.9 KiB, free 433.6 MiB)
26/02/13 12:09:35 INFO MemoryStore: Block broadcast_159_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 433.6 MiB)
26/02/13 12:09:35 INFO BlockManagerInfo: Added broadcast_159_piece0 in memory on spark-master:42073 (size: 19.5 KiB, free: 434.0 MiB)
26/02/13 12:09:35 INFO SparkContext: Created broadcast 159 from broadcast at DAGScheduler.scala:1585
26/02/13 12:09:35 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 162 (MapPartitionsRDD[577] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
26/02/13 12:09:35 INFO TaskSchedulerImpl: Adding task set 162.0 with 3 tasks resource profile 0
26/02/13 12:09:35 INFO TaskSetManager: Starting task 0.0 in stage 162.0 (TID 347) (172.18.0.14, executor 1, partition 0, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:09:35 INFO TaskSetManager: Starting task 1.0 in stage 162.0 (TID 348) (172.18.0.15, executor 0, partition 1, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:09:35 INFO TaskSetManager: Starting task 2.0 in stage 162.0 (TID 349) (172.18.0.14, executor 1, partition 2, PROCESS_LOCAL, 8594 bytes) 
26/02/13 12:09:35 INFO BlockManagerInfo: Added broadcast_159_piece0 in memory on 172.18.0.15:37717 (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 12:09:35 INFO BlockManagerInfo: Added broadcast_159_piece0 in memory on 172.18.0.14:37571 (size: 19.5 KiB, free: 433.9 MiB)
26/02/13 12:09:35 INFO BlockManagerInfo: Added broadcast_158_piece0 in memory on 172.18.0.14:37571 (size: 107.2 KiB, free: 433.8 MiB)
26/02/13 12:09:35 INFO BlockManagerInfo: Added broadcast_158_piece0 in memory on 172.18.0.15:37717 (size: 107.2 KiB, free: 434.0 MiB)
26/02/13 12:09:35 INFO TaskSetManager: Finished task 0.0 in stage 162.0 (TID 347) in 84 ms on 172.18.0.14 (executor 1) (1/3)
26/02/13 12:09:35 INFO TaskSetManager: Finished task 2.0 in stage 162.0 (TID 349) in 83 ms on 172.18.0.14 (executor 1) (2/3)
26/02/13 12:09:35 INFO TaskSetManager: Finished task 1.0 in stage 162.0 (TID 348) in 102 ms on 172.18.0.15 (executor 0) (3/3)
26/02/13 12:09:35 INFO TaskSchedulerImpl: Removed TaskSet 162.0, whose tasks have all completed, from pool 
26/02/13 12:09:35 INFO DAGScheduler: ResultStage 162 (start at NativeMethodAccessorImpl.java:0) finished in 0.109 s
26/02/13 12:09:35 INFO DAGScheduler: Job 106 is finished. Cancelling potential speculative or zombie tasks for this job
26/02/13 12:09:35 INFO TaskSchedulerImpl: Killing all running tasks in stage 162: Stage finished
26/02/13 12:09:35 INFO DAGScheduler: Job 106 finished: start at NativeMethodAccessorImpl.java:0, took 0.111959 s
26/02/13 12:09:35 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 171, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@5445c209] is committing.
26/02/13 12:09:35 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 171, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@5445c209] committed.
26/02/13 12:09:35 INFO CheckpointFileManager: Writing atomically to file:/tmp/spark-checkpoint-enrichment/commits/171 using temp file file:/tmp/spark-checkpoint-enrichment/commits/.171.553a1444-5819-4e44-afcb-337f4e9b4071.tmp
26/02/13 12:09:35 INFO CheckpointFileManager: Renamed temp file file:/tmp/spark-checkpoint-enrichment/commits/.171.553a1444-5819-4e44-afcb-337f4e9b4071.tmp to file:/tmp/spark-checkpoint-enrichment/commits/171
26/02/13 12:09:35 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ebfab33f-0465-4e53-8f7a-961711cffb90",
  "runId" : "4a161d66-225b-4b29-812d-ddec3c05364e",
  "name" : null,
  "timestamp" : "2026-02-13T12:09:34.957Z",
  "batchId" : 171,
  "numInputRows" : 243,
  "inputRowsPerSecond" : 252.86160249739856,
  "processedRowsPerSecond" : 636.1256544502618,
  "durationMs" : {
    "addBatch" : 234,
    "commitOffsets" : 69,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 22,
    "triggerExecution" : 382,
    "walCommit" : 55
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[aviation-india-states]]",
    "startOffset" : {
      "aviation-india-states" : {
        "2" : 2425,
        "1" : 2808,
        "0" : 2165
      }
    },
    "endOffset" : {
      "aviation-india-states" : {
        "2" : 2510,
        "1" : 2894,
        "0" : 2237
      }
    },
    "latestOffset" : {
      "aviation-india-states" : {
        "2" : 2510,
        "1" : 2894,
        "0" : 2237
      }
    },
    "numInputRows" : 243,
    "inputRowsPerSecond" : 252.86160249739856,
    "processedRowsPerSecond" : 636.1256544502618,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@1a6a42c2",
    "numOutputRows" : 50
  }
}
26/02/13 12:09:38 INFO BlockManagerInfo: Removed broadcast_157_piece0 on spark-master:42073 in memory (size: 33.3 KiB, free: 434.0 MiB)
26/02/13 12:09:38 INFO BlockManagerInfo: Removed broadcast_157_piece0 on 172.18.0.14:37571 in memory (size: 33.3 KiB, free: 433.8 MiB)
26/02/13 12:09:38 INFO BlockManagerInfo: Removed broadcast_159_piece0 on spark-master:42073 in memory (size: 19.5 KiB, free: 434.0 MiB)
26/02/13 12:09:38 INFO BlockManagerInfo: Removed broadcast_159_piece0 on 172.18.0.14:37571 in memory (size: 19.5 KiB, free: 433.8 MiB)
26/02/13 12:09:38 INFO BlockManagerInfo: Removed broadcast_159_piece0 on 172.18.0.15:37717 in memory (size: 19.5 KiB, free: 434.1 MiB)
26/02/13 12:09:38 INFO BlockManagerInfo: Removed broadcast_155_piece0 on spark-master:42073 in memory (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 12:09:38 INFO BlockManagerInfo: Removed broadcast_155_piece0 on 172.18.0.15:37717 in memory (size: 107.2 KiB, free: 434.2 MiB)
26/02/13 12:09:38 INFO BlockManagerInfo: Removed broadcast_155_piece0 on 172.18.0.14:37571 in memory (size: 107.2 KiB, free: 433.9 MiB)
26/02/13 12:09:38 INFO BlockManagerInfo: Removed broadcast_156_piece0 on spark-master:42073 in memory (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 12:09:38 INFO BlockManagerInfo: Removed broadcast_156_piece0 on 172.18.0.15:37717 in memory (size: 19.5 KiB, free: 434.2 MiB)
26/02/13 12:09:38 INFO BlockManagerInfo: Removed broadcast_156_piece0 on 172.18.0.14:37571 in memory (size: 19.5 KiB, free: 434.0 MiB)
26/02/13 12:09:45 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:09:55 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:10:05 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:10:15 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:10:18 INFO BlockManagerInfo: Removed broadcast_152_piece0 on spark-master:42073 in memory (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 12:10:18 INFO BlockManagerInfo: Removed broadcast_152_piece0 on 172.18.0.15:37717 in memory (size: 107.2 KiB, free: 434.3 MiB)
26/02/13 12:10:18 INFO BlockManagerInfo: Removed broadcast_152_piece0 on 172.18.0.14:37571 in memory (size: 107.2 KiB, free: 434.1 MiB)
26/02/13 12:10:25 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:10:35 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:10:45 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:10:55 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:11:05 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:11:15 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:11:25 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:11:35 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:11:45 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:11:55 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:12:05 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:12:15 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:12:25 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:12:35 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:12:45 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:12:55 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:13:05 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:13:15 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:13:25 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:13:35 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:13:45 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:13:55 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:14:05 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:14:15 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:14:25 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:14:35 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:14:45 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:14:55 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:15:05 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:15:15 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:15:25 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:15:35 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:15:45 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:15:55 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:16:05 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:16:15 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:16:25 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:16:35 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:16:45 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:16:55 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:17:05 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:17:15 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:17:25 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:17:35 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:17:45 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:17:55 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:18:05 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:18:15 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:18:25 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:18:35 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:18:45 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:18:55 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:19:05 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:19:15 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:19:25 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:19:35 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:19:45 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:19:55 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:20:05 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:20:15 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:20:25 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:20:35 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:20:45 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:20:55 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:21:05 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:21:15 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:21:25 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:21:35 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:21:45 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:21:55 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:22:05 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:22:15 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:22:25 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:22:35 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:22:45 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:22:55 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:23:05 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:23:15 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:23:25 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:23:35 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:23:45 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:23:55 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:24:05 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:24:15 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:24:25 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:24:35 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:24:45 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:24:55 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:25:05 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:25:15 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:25:25 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:25:35 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:25:45 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:25:55 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:26:05 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:26:15 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:26:25 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:26:35 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:26:45 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:26:55 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:27:05 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:27:15 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:27:25 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:27:35 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:27:45 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:27:55 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:28:05 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:28:15 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:28:25 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:28:35 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:28:45 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:28:55 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:29:05 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:29:15 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:29:26 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:29:36 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:29:46 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:29:56 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:30:06 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:30:16 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:30:26 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:30:36 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:30:46 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:30:56 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:31:06 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:31:16 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:31:26 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:31:36 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:31:46 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:31:56 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:32:06 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:32:16 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:32:26 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:32:36 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:32:46 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:32:56 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:33:06 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:33:16 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:33:26 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:33:36 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:33:46 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:33:56 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:34:06 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:34:16 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:34:26 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:34:36 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:34:46 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:34:56 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:35:06 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:35:16 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:35:26 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:35:36 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:35:46 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:35:56 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:36:06 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:36:16 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:36:26 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:36:36 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:36:46 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:36:56 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:37:06 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:37:16 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:37:26 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:37:36 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:37:46 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:37:56 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:38:06 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:38:16 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:38:26 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:38:36 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:38:46 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:38:56 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:39:06 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:39:16 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:39:26 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:39:36 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:39:46 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:39:56 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:40:06 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:40:16 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:40:26 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:40:36 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:40:46 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:40:56 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:41:06 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:41:16 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:41:26 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:41:36 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:41:46 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:41:56 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:42:06 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:42:16 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:42:26 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:42:36 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:42:46 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:42:56 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:43:06 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:43:16 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
26/02/13 12:43:26 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20260213120207-0001/0 is now LOST (worker lost: 172.18.0.15:35515 got disassociated)
26/02/13 12:43:26 INFO StandaloneSchedulerBackend: Executor app-20260213120207-0001/0 removed: worker lost: 172.18.0.15:35515 got disassociated
26/02/13 12:43:26 ERROR TaskSchedulerImpl: Lost executor 0 on 172.18.0.15: worker lost: 172.18.0.15:35515 got disassociated
26/02/13 12:43:26 INFO DAGScheduler: Executor lost: 0 (epoch 4)
26/02/13 12:43:26 INFO BlockManagerMasterEndpoint: Trying to remove executor 0 from BlockManagerMaster.
26/02/13 12:43:26 INFO BlockManagerMasterEndpoint: Removing block manager BlockManagerId(0, 172.18.0.15, 37717, None)
26/02/13 12:43:26 INFO BlockManagerMaster: Removed 0 successfully in removeExecutor
26/02/13 12:43:26 INFO DAGScheduler: Shuffle files lost for host: 172.18.0.15 (epoch 4)
26/02/13 12:43:26 INFO StandaloneAppClient$ClientEndpoint: Master removed worker worker-20260213114143-172.18.0.15-35515: 172.18.0.15:35515 got disassociated
26/02/13 12:43:26 INFO StandaloneSchedulerBackend: Worker worker-20260213114143-172.18.0.15-35515 removed: 172.18.0.15:35515 got disassociated
26/02/13 12:43:26 INFO TaskSchedulerImpl: Handle removed worker worker-20260213114143-172.18.0.15-35515: 172.18.0.15:35515 got disassociated
26/02/13 12:43:26 INFO DAGScheduler: Shuffle files lost for worker worker-20260213114143-172.18.0.15-35515 on host 172.18.0.15
26/02/13 12:43:26 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20260213120207-0001/1 is now LOST (worker lost: 172.18.0.14:33335 got disassociated)
26/02/13 12:43:26 INFO StandaloneSchedulerBackend: Executor app-20260213120207-0001/1 removed: worker lost: 172.18.0.14:33335 got disassociated
26/02/13 12:43:26 INFO StandaloneAppClient$ClientEndpoint: Master removed worker worker-20260213114143-172.18.0.14-33335: 172.18.0.14:33335 got disassociated
26/02/13 12:43:26 INFO StandaloneSchedulerBackend: Worker worker-20260213114143-172.18.0.14-33335 removed: 172.18.0.14:33335 got disassociated
26/02/13 12:43:26 ERROR TaskSchedulerImpl: Lost executor 1 on 172.18.0.14: worker lost: 172.18.0.14:33335 got disassociated
26/02/13 12:43:26 INFO DAGScheduler: Executor lost: 1 (epoch 6)
26/02/13 12:43:26 INFO TaskSchedulerImpl: Handle removed worker worker-20260213114143-172.18.0.14-33335: 172.18.0.14:33335 got disassociated
26/02/13 12:43:26 INFO BlockManagerMasterEndpoint: Trying to remove executor 1 from BlockManagerMaster.
26/02/13 12:43:26 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_29_3 !
26/02/13 12:43:26 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_29_2 !
26/02/13 12:43:26 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_29_1 !
26/02/13 12:43:26 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_29_0 !
26/02/13 12:43:26 INFO BlockManagerMasterEndpoint: Removing block manager BlockManagerId(1, 172.18.0.14, 37571, None)
26/02/13 12:43:26 INFO BlockManagerMaster: Removed 1 successfully in removeExecutor
26/02/13 12:43:26 INFO DAGScheduler: Shuffle files lost for host: 172.18.0.14 (epoch 6)
26/02/13 12:43:26 INFO DAGScheduler: Shuffle files lost for worker worker-20260213114143-172.18.0.14-33335 on host 172.18.0.14
26/02/13 12:43:26 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
