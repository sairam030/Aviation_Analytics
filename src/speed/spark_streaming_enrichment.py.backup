"""
Spark Structured Streaming - Flight Data Enrichment
Reads raw data from Kafka, enriches with route mapping, writes enriched data back to Kafka
"""
from pyspark.sql import SparkSession
from pyspark.sql.functions import (
    from_json, col, expr, when, lit, 
    udf, struct, to_json, current_timestamp,
    regexp_extract, coalesce
)
from pyspark.sql.types import (
    StructType, StructField, StringType, DoubleType, 
    BooleanType, LongType, IntegerType, ArrayType
)

from src.speed.config import (
    KAFKA_BOOTSTRAP_SERVERS, 
    KAFKA_TOPIC_RAW,
    KAFKA_TOPIC_ENRICHED
)
from src.speed.route_mapping import (
    INDIAN_AIRLINES, 
    INDIAN_AIRPORTS,
)


# =============================================================================
# CONFIGURATION
# =============================================================================

MINIO_ENDPOINT = "http://minio:9000"
MINIO_ACCESS_KEY = "minioadmin"
MINIO_SECRET_KEY = "minioadmin"
MINIO_BUCKET = "aviation-mapping"

# Checkpoint location for streaming
CHECKPOINT_LOCATION = f"s3a://{MINIO_BUCKET}/checkpoints/spark-streaming"


# =============================================================================
# SCHEMAS
# =============================================================================

# Input schema from Kafka (OpenSky flight data)
FLIGHT_SCHEMA = StructType([
    StructField("icao24", StringType(), True),
    StructField("callsign", StringType(), True),
    StructField("origin_country", StringType(), True),
    StructField("time_position", LongType(), True),
    StructField("last_contact", LongType(), True),
    StructField("longitude", DoubleType(), True),
    StructField("latitude", DoubleType(), True),
    StructField("baro_altitude", DoubleType(), True),
    StructField("on_ground", BooleanType(), True),
    StructField("velocity", DoubleType(), True),
    StructField("true_track", DoubleType(), True),
    StructField("vertical_rate", DoubleType(), True),
    StructField("sensors", ArrayType(IntegerType()), True),
    StructField("geo_altitude", DoubleType(), True),
    StructField("squawk", StringType(), True),
    StructField("spi", BooleanType(), True),
    StructField("position_source", IntegerType(), True),
    StructField("ingestion_time", StringType(), True),
])

# Route mapping schema
ROUTE_MAPPING_SCHEMA = StructType([
    StructField("airline_prefix", StringType(), True),
    StructField("airline_name", StringType(), True),
    StructField("airline_iata", StringType(), True),
    StructField("airline_icao", StringType(), True),
    StructField("min_flight_num", IntegerType(), True),
    StructField("max_flight_num", IntegerType(), True),
    StructField("origin_code", StringType(), True),
    StructField("origin_city", StringType(), True),
    StructField("origin_airport", StringType(), True),
    StructField("origin_lat", DoubleType(), True),
    StructField("origin_lon", DoubleType(), True),
    StructField("destination_code", StringType(), True),
    StructField("destination_city", StringType(), True),
    StructField("destination_airport", StringType(), True),
    StructField("destination_lat", DoubleType(), True),
    StructField("destination_lon", DoubleType(), True),
])


# =============================================================================
# SPARK SESSION
# =============================================================================

def create_spark_session() -> SparkSession:
    """Create Spark session with Kafka and S3A support."""
    return (SparkSession.builder
        .appName("FlightDataEnrichment")
        .master("spark://spark-master:7077")
        .config("spark.jars.packages", 
                "org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1,"
                "org.apache.hadoop:hadoop-aws:3.3.4,"
                "com.amazonaws:aws-java-sdk-bundle:1.12.262")
        .config("spark.hadoop.fs.s3a.endpoint", MINIO_ENDPOINT)
        .config("spark.hadoop.fs.s3a.access.key", MINIO_ACCESS_KEY)
        .config("spark.hadoop.fs.s3a.secret.key", MINIO_SECRET_KEY)
        .config("spark.hadoop.fs.s3a.path.style.access", "true")
        .config("spark.hadoop.fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem")
        .config("spark.hadoop.fs.s3a.connection.ssl.enabled", "false")
        .config("spark.sql.streaming.checkpointLocation", CHECKPOINT_LOCATION)
        .config("spark.streaming.stopGracefullyOnShutdown", "true")
        .config("spark.sql.shuffle.partitions", "4")
        .getOrCreate())


# =============================================================================
# ROUTE MAPPING TABLE
# =============================================================================

def create_route_mapping_df(spark: SparkSession):
    """Create route mapping DataFrame from static data."""
    rows = generate_route_mapping_rows()
    return spark.createDataFrame(rows, schema=ROUTE_MAPPING_SCHEMA)


def save_route_mapping_to_minio(spark: SparkSession):
    """Save route mapping table to MinIO for persistence."""
    df = create_route_mapping_df(spark)
    
    output_path = f"s3a://{MINIO_BUCKET}/reference/route_mapping"
    
    df.write \
        .mode("overwrite") \
        .parquet(output_path)
    
    print(f"✓ Route mapping saved to {output_path}")
    return df


def load_route_mapping_from_minio(spark: SparkSession):
    """Load route mapping table from MinIO."""
    path = f"s3a://{MINIO_BUCKET}/reference/route_mapping"
    
    try:
        df = spark.read.parquet(path)
        print(f"✓ Loaded route mapping from {path}")
        return df
    except Exception as e:
        print(f"⚠️ Route mapping not found, creating new: {e}")
        return save_route_mapping_to_minio(spark)


# =============================================================================
# STREAMING ENRICHMENT
# =============================================================================

def run_enrichment_stream(spark: SparkSession, route_mapping_df):
    """Run Spark Structured Streaming to enrich flight data."""
    
    print("="*60)
    print("SPARK STREAMING - FLIGHT ENRICHMENT")
    print(f"Input Topic: {KAFKA_INPUT_TOPIC}")
    print(f"Output Topic: {KAFKA_OUTPUT_TOPIC}")
    print(f"Kafka: {KAFKA_BOOTSTRAP_SERVERS}")
    print("="*60)
    
    # Read from Kafka
    kafka_df = (spark
        .readStream
        .format("kafka")
        .option("kafka.bootstrap.servers", KAFKA_BOOTSTRAP_SERVERS)
        .option("subscribe", KAFKA_INPUT_TOPIC)
        .option("startingOffsets", "latest")
        .option("failOnDataLoss", "false")
        .load())
    
    # Parse JSON from Kafka value
    parsed_df = kafka_df.select(
        from_json(col("value").cast("string"), FLIGHT_SCHEMA).alias("flight"),
        col("timestamp").alias("kafka_timestamp")
    ).select("flight.*", "kafka_timestamp")
    
    # Extract airline prefix and flight number from callsign
    # Callsign format: "IGO6135 " -> prefix="IGO", flight_num=6135
    enriched_df = parsed_df \
        .withColumn("callsign_clean", expr("trim(callsign)")) \
        .withColumn("airline_prefix_extracted", 
                    regexp_extract(col("callsign_clean"), r"^([A-Z]{2,3})", 1)) \
        .withColumn("flight_num_extracted",
                    regexp_extract(col("callsign_clean"), r"(\d+)", 1).cast(IntegerType()))
    
    # Broadcast the small route mapping table for efficient join
    route_mapping_bc = broadcast(route_mapping_df)
    
    # Join with route mapping
    # Match on: prefix AND flight_num between min and max
    joined_df = enriched_df.join(
        route_mapping_bc,
        (col("airline_prefix_extracted") == col("airline_prefix")) &
        (col("flight_num_extracted") >= col("min_flight_num")) &
        (col("flight_num_extracted") <= col("max_flight_num")),
        "left"
    )
    
    # Select final columns
    final_df = joined_df.select(
        # Original flight data
        col("icao24"),
        col("callsign"),
        col("callsign_clean"),
        col("origin_country"),
        col("time_position"),
        col("last_contact"),
        col("longitude"),
        col("latitude"),
        col("baro_altitude"),
        col("on_ground"),
        col("velocity"),
        col("true_track"),
        col("vertical_rate"),
        col("geo_altitude"),
        col("squawk"),
        col("spi"),
        col("position_source"),
        col("ingestion_time"),
        col("kafka_timestamp"),
        
        # Extracted fields
        col("flight_num_extracted").alias("flight_number"),
        
        # Enriched airline info
        coalesce(col("airline_name"), lit("Unknown")).alias("airline_name"),
        coalesce(col("airline_iata"), lit("--")).alias("airline_iata"),
        coalesce(col("airline_icao"), col("airline_prefix_extracted")).alias("airline_icao"),
        
        # Enriched route info
        coalesce(col("origin_code"), lit("UNK")).alias("origin_code"),
        coalesce(col("origin_city"), lit("Unknown")).alias("origin_city"),
        col("origin_airport"),
        col("origin_lat"),
        col("origin_lon"),
        coalesce(col("destination_code"), lit("UNK")).alias("destination_code"),
        coalesce(col("destination_city"), lit("Unknown")).alias("destination_city"),
        col("destination_airport"),
        col("destination_lat"),
        col("destination_lon"),
        
        # Processing metadata
        current_timestamp().alias("enriched_at")
    )
    
    # Convert to JSON for Kafka output
    output_df = final_df.select(
        col("icao24").alias("key"),
        to_json(struct("*")).alias("value")
    )
    
    # Write to Kafka
    query = (output_df
        .writeStream
        .format("kafka")
        .option("kafka.bootstrap.servers", KAFKA_BOOTSTRAP_SERVERS)
        .option("topic", KAFKA_OUTPUT_TOPIC)
        .option("checkpointLocation", f"{CHECKPOINT_LOCATION}/enrichment")
        .outputMode("append")
        .trigger(processingTime="5 seconds")
        .start())
    
    print(f"✓ Streaming query started: {query.id}")
    print(f"✓ Writing enriched data to: {KAFKA_OUTPUT_TOPIC}")
    
    return query


def run_console_debug(spark: SparkSession, route_mapping_df):
    """Run with console output for debugging."""
    
    kafka_df = (spark
        .readStream
        .format("kafka")
        .option("kafka.bootstrap.servers", KAFKA_BOOTSTRAP_SERVERS)
        .option("subscribe", KAFKA_INPUT_TOPIC)
        .option("startingOffsets", "latest")
        .load())
    
    parsed_df = kafka_df.select(
        from_json(col("value").cast("string"), FLIGHT_SCHEMA).alias("flight")
    ).select("flight.*")
    
    enriched_df = parsed_df \
        .withColumn("callsign_clean", expr("trim(callsign)")) \
        .withColumn("airline_prefix_extracted", 
                    regexp_extract(col("callsign_clean"), r"^([A-Z]{2,3})", 1)) \
        .withColumn("flight_num_extracted",
                    regexp_extract(col("callsign_clean"), r"(\d+)", 1).cast(IntegerType()))
    
    route_mapping_bc = broadcast(route_mapping_df)
    
    joined_df = enriched_df.join(
        route_mapping_bc,
        (col("airline_prefix_extracted") == col("airline_prefix")) &
        (col("flight_num_extracted") >= col("min_flight_num")) &
        (col("flight_num_extracted") <= col("max_flight_num")),
        "left"
    )
    
    output_df = joined_df.select(
        "callsign_clean",
        "airline_name",
        "origin_city",
        "destination_city",
        "latitude",
        "longitude",
        "baro_altitude",
        "velocity"
    )
    
    query = (output_df
        .writeStream
        .format("console")
        .option("truncate", False)
        .outputMode("append")
        .trigger(processingTime="10 seconds")
        .start())
    
    return query


# =============================================================================
# MAIN
# =============================================================================

def main(debug=False):
    """Main entry point for streaming enrichment."""
    print("\n" + "="*60)
    print("INITIALIZING SPARK STREAMING ENRICHMENT")
    print("="*60 + "\n")
    
    spark = create_spark_session()
    spark.sparkContext.setLogLevel("WARN")
    
    # Load or create route mapping
    route_mapping_df = load_route_mapping_from_minio(spark)
    route_mapping_df.cache()
    
    print(f"\n✓ Route mapping loaded: {route_mapping_df.count()} routes\n")
    
    if debug:
        query = run_console_debug(spark, route_mapping_df)
    else:
        query = run_enrichment_stream(spark, route_mapping_df)
    
    try:
        query.awaitTermination()
    except KeyboardInterrupt:
        print("\n⚠️ Stopping streaming...")
        query.stop()
        spark.stop()
        print("✓ Stopped gracefully")


if __name__ == "__main__":
    import sys
    debug = "--debug" in sys.argv
    main(debug=debug)
